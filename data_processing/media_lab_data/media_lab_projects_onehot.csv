active,created,description,end_on,groups,hero_image_url,id,location,modified,people,published,slug,start_on,tags,title,visibility,website,Technology Focus: AI/ML,Technology Focus: Data Visualization & Interface Design,Technology Focus: Digital Fabrication,Technology Focus: Education & Learning,Technology Focus: Entertainment & Creative Arts,Technology Focus: IoT & Connected Devices,Technology Focus: Media & Communication,Technology Focus: Robotics & Automation,Technology Focus: Synthetic Biology,Technology Focus: Virtual/Augmented Reality,Application Domain: Data Visualization & Interface Design,Application Domain: Education & Learning,Application Domain: Entertainment & Creative Arts,Application Domain: Healthcare & Wellbeing,Application Domain: Media & Communication,Application Domain: Social Justice & Equity,Application Domain: Sustainability & Environment,Application Domain: Urban Planning & Mobility,Stage of Development: Advanced Prototyping,Stage of Development: Community or Social Collaboration,Stage of Development: Exploratory Prototyping,Stage of Development: Fundamental Research,Stage of Development: Ongoing,Stage of Development: Ongoing Project,Stage of Development: Ready for Commercialization,Collaboration Type: Academic-led,Collaboration Type: Community or Social Collaboration,Collaboration Type: Industry Partnership,"Collaboration Type: Interdisciplinary (cross-group, cross-departmental)",Geographic Relevance: Africa & Middle East,Geographic Relevance: Asia-Pacific,Geographic Relevance: Europe,Geographic Relevance: Global / No specific geographic focus,Geographic Relevance: Latin America,Geographic Relevance: North America,Methodology Type: Comparative/Simulative,Methodology Type: Computational/Simulative,Methodology Type: Experimental,Methodology Type: Mixed Methods,Methodology Type: Qualitative Research,Methodology Type: Quantitative Research,Impact Level: Community Level,"Impact Level: Community Level (local impacts, small-scale adoption)",Impact Level: Individual Level,"Impact Level: Individual Level (personal tech, user experience)",Impact Level: Societal Level,Impact Level: Societal Level (large-scale systemic impacts),Emotional or Experiential Dimension: Empowering / Inclusive,Emotional or Experiential Dimension: Exploratory,Emotional or Experiential Dimension: Functional / Utilitarian,Emotional or Experiential Dimension: Playful / Exploratory,Emotional or Experiential Dimension: Provocative / Critical
False,2016-07-15T00:07:41.470Z,"Time perception is a fundamental component in our ability to build mental models of our world. Without accurate and precise time perception, we might have trouble understanding speech, fumble social interactions, have poor motor control, hallucinate, or remember events incorrectly. Slight distortions in time perception are commonplace and may lead to slight dyslexia, memory shifts, poor eye-hand coordination, and other relatively benign symptoms, but could a diminishing sense of time signal the onset of a serious brain disorder? Could time perception training help prevent or reverse brain disorders? This project is a series of experimental tools built to assist and increase human time perception. By approaching time-perception training from various perspectives, we hope to find a tool or collection of tools to increase time perception, and in turn discover what an increase in time perception might afford us.",2015-07-01,[],https://dam-prod2.media.mit.edu/x/files/Display/IMG_6665.jpg,1384,--Choose Location,2022-06-09T18:57:48.681Z,"['slavin@media.mit.edu', 'cwwang@media.mit.edu']",True,tools-for-super-human-time-perception,2014-01-01,[],Tools for Super-Human Time Perception,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:41.565Z,"We have designed a novel system to promote kindness and gratitude. We leverage pervasive technologies to naturally embed gratitude inspiration in everyday life. Mobile sensor data is utilized to infer optimal moments for stimulating contextually relevant thankfulness and appreciation. We analyze the interplay between mood, contextual cues, and gratitude expressions.",2018-12-31,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/2016/09/06/Screen Shot 2016-09-06 at 11.30.48 AM.png,1392,--Choose Location,2022-05-12T17:22:27.287Z,"['picard@media.mit.edu', 'asma_gh@media.mit.edu', 'azaria@media.mit.edu', 'sataylor@media.mit.edu']",True,kind-and-grateful-promoting-kindness-and-gratitude-with-pervasive-technology,2015-01-01,[],"""Kind and Grateful"": Promoting kindness and gratitude with pervasive technology",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:41.666Z,"In September 2014, 150 parents, engineers, designers, and healthcare practitioners gathered at the MIT Media Lab for the ""Make the Breast Pump Not Suck!"" Hackathon. As one of the midwives at our first hackathon said, ""Maternal health lags behind other sectors for innovation."" This project brought together people from diverse fields, sectors, and backgrounds to take a crack at making life better for moms, babies, and new families.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/12/14/15281053000_d0025ec797_o+copy.jpg,1400,--Choose Location,2022-08-24T21:02:08.091Z,"['ethanz@media.mit.edu', 'dignazio@media.mit.edu', 'ahope@media.mit.edu', 'tjlevy@media.mit.edu', 'cwwang@media.mit.edu', 'achituv@media.mit.edu']",True,make-the-breast-pump-not-suck-hackathon,2013-01-01,"['civic-technology', 'consumer-electronics', 'politics', 'interfaces', 'makers']",Make the Breast Pump Not Suck Hackathon,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:41.858Z,"(Im)possible Baby is a speculative design project that aims to stimulate discussions about the social, cultural, and ethical implications of emerging biotechnologies that could enable same-sex couples to have their own, genetically related children. Delivering a baby from same-sex parents is not a sci-fi dream anymore, due to recent developments in genetics and stem cell research. In this project, the DNA data of a lesbian couple was analyzed using 23andme to simulate and visualize their potential children, and then we created a set of fictional, ""what if"" future family photos using this information to produce a hardcover album which was presented to the couple as a gift. To achieve more public outreach, we worked with the Japanese national television service, NHK, to create a 30-minute documentary film following the whole process, which aired in October 2015.",2017-08-31,"['ethics', 'design-fiction']",https://dam-prod2.media.mit.edu/x/files/Display/(im)possible_baby_breakfast.jpg,1416,--Choose Location,2017-10-11T20:32:40.918Z,"['sputniko@media.mit.edu', 'aih@media.mit.edu']",True,impossible-baby,2014-01-01,[],(Im)possible Baby,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:07:41.958Z,"(void*) is a virtual lounge that unites the physical and digital, allowing people and a cast of virtual characters to interact with each other. In this novel gathering place, as in a nightclub, the interaction focuses mostly on group dynamics and body language, and little on verbal communication. Building on SWAMPED!, this installation continues our work in �Intentional Characters,� �Sympathetic Interfaces� and �Autonomous Cinematography.� It also adds �Dynamic Music Composition� as an integral part of the system. (void *) will premiere this summer at Siggraph �99. Today we will be showing a video and demonstrating the technology.",1999-09-01,['synthetic-characters'],,1424,Pond,2016-12-05T00:16:06.566Z,[],True,void-a-cast-of-characters,1999-01-01,[],(void *): A Cast of Characters,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:07:42.040Z,"Typically, 1-teacher schools are viewed as relatively backwards due to lack of resources, but research shows that teachers at such schools can have stronger, fuller, and deeper relationships with their students, and that the students can be more autonomous and in control of their own learning while still more supportive of and cooperative with their peers. Technology can provide connectivity, eliminating some resource constraints. We go further, using new learning methodologies and technologies to build upon the strengths of 1-teacher schools, creating models of learning environments to serve as exemplars not only for rural education, but also for large urban environments.",2000-01-01,[],,1432,E15-320,2016-12-05T00:16:06.400Z,"['papert@media.mit.edu', 'cavallo@media.mit.edu', 'calla@media.mit.edu']",True,1-teacher-schools,2000-01-01,[],1-Teacher Schools,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:42.119Z,"Varied important problems can be solved using surprisingly approximate arithmetic.  We've designed a co-processor for such arithmetic that provides 100,000 cores on a single standard chip, or 1,000 cores in a sub-watt mobile device.  We are exploring applications of such machines in image and video processing.  Cost can be under a penny per core, and compared to CPUs, improvements in speed and energy use can exceed 10,000x.",2011-01-01,['social-machines'],,1440,,2016-12-05T00:16:06.336Z,['dkroy@media.mit.edu'],True,10000x-more-efficient-computing,2010-01-01,[],"10,000x More Efficient Computing",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:42.200Z,"The 1998 Everest Expedition was the first major scientific expedition to  Mount Everest.  We joined forces with the Boston Museum of Science, NASA,  Yale, the Explorer's Club, and a world-class climbing team, to investigate  questions of geology, climatology, and human physiology.
.",1998-09-01,[],,1448,--Choose Location,2016-12-05T00:16:06.645Z,['mike@media.mit.edu'],True,1998-everest-expedition,1998-01-01,[],1998 Everest Expedition,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0
False,2016-07-15T00:07:42.279Z,"Suppose that one could put a connected, portable computer into the hands of all the children of the world who lack access to the kind of education that would prepare them for participation in a modern, knowledge-based society. Would you use the computers to put the children through the standard curriculum of ""developed"" countries? This project—a complex undertaking—sets out to design radically different approaches. Some of its components, such as theoretical studies of the epistemology of learning, extend work we have pursued in the past. A component not undertaken before is developing a scientific basis for the design and implementation of a whole essentially new curriculum—or more likely, the process through which such a curriculum will evolve. Experiences such as the pilot program, which implemented a 1:1 computer infrastructure in a small rural community in Costa Rica, are been developed and studied.",2005-01-01,[],,1456,--Choose Location,2017-09-21T20:54:54.899Z,"['papert@media.mit.edu', 'cavallo@media.mit.edu', 'calla@media.mit.edu', 'mbletsas@media.mit.edu']",True,11-laptops-learning-with-the-hundred-dollar-laptop,2005-01-01,"['robotics', 'developing-countries', 'kids', 'learning-teaching']",1:1 Laptops: Learning with the Hundred-Dollar Laptop,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:42.356Z,"Touch screen systems allow users to interact directly with content displayed on a 2-D surface. However, current systems have severe limitations in usability because they do not preserve the familiar mechanical response of traditional writing and drawing tools. Furthermore, current touch-screen technologies can often require extensive modifications to the drawing/writing substrate, which make them expensive and impractical for large drawing surfaces or entire desktops. We are developing a touch-screen system based on an effect called Frustrated Total Internal Reflection (FTIR). This technique involves coupling light into a pane of glass and then detecting contact to the glass by measuring the absorption of the light by the object in contact, using a linear array of photodetectors adhered to the glass. An LCD screen or other video display can then be fitted behind the glass to produce an appropriate response. All hardware is inside the glass�there is no need to mount anything on the side where the interaction takes place. We believe this system will be a practical and economical way to detect contact with brushes, felt pens, and fingers across large interactive surfaces.",2003-09-01,['responsive-environments'],,1464,E15-441,2016-12-05T00:16:06.424Z,['joep@media.mit.edu'],True,2-d-input-device-based-on-frustrated-total-internal-reflection,2002-01-01,[],2-D Input Device Based on Frustrated Total Internal Reflection,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:42.437Z,"20 Day Stranger is a mobile app that creates an intimate and anonymous connection between you and another person. For 20 days, you get continuous updates about where they are, what they are doing, and eventually even how they are feeling, and them likewise about you. But you will never know who this person is. Does this change the way you think about other people you see throughout your day, any one of which could be your stranger?",2017-06-01,['playful-systems'],https://dam-prod2.media.mit.edu/x/files/Display/20DS-logo-11.png,1472,--Choose Location,2022-06-10T14:14:48.759Z,"['slavin@media.mit.edu', 'tjlevy@media.mit.edu', 'cwwang@media.mit.edu']",True,20-day-stranger,2013-09-01,[],20 Day Stranger,PUBLIC,http://20daystranger.net/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:07:42.523Z,"DuoSkin is a fabrication process that enables anyone to create customized functional devices that can be attached directly to the skin. Using gold metal leaf, a material that is cheap, skin-friendly, and robust for everyday wear, we demonstrate three types of on-skin interfaces: sensing touch input, displaying output, and wireless communication. DuoSkin draws from the aesthetics found in metallic jewelry-like temporary tattoos to create on-skin devices which resemble jewelry. DuoSkin devices enable users to control their mobile devices, display information, and store information on their skin while serving as a statement of personal style. We believe that in the future, on-skin electronics will no longer be black-boxed and mystified; instead, they will converge towards the user friendliness, extensibility, and aesthetics of body decorations, forming a DuoSkin integrated to the extent that it has seemingly disappeared.Credits:Cindy Hsin-Liu Kao, Asta Roseway*, Christian Holz*, Paul Johns*, Andres Calvo, Chris SchmandtMIT Media Lab in collaboration with Microsoft Research*",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/10/20/1_Capacitive touch slider_img1.jpg,1480,--Choose Location,2022-06-10T14:01:01.868Z,"['cindykao@media.mit.edu', 'andresc@media.mit.edu', 'geek@media.mit.edu']",True,2ndskin,2015-09-01,"['art', 'fashion', 'wearable-computing', 'interfaces']",DuoSkin,PUBLIC,http://duoskin.media.mit.edu,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:42.695Z,"Multi-layer 3-D microfluidic circuits are the driving platform for micro-total analysis systems in the current era. The traditional fabrication for these
fluidic circuits is planar. For building multi-layer structures a precise
alignment for multiple layers is required, which makes the fabrication very difficult. We have invented a flow-based extrusion process for multi-layer microfluidic circuits in polymer substrates. The channels can be as small as 100nm with multi-materials embedded in the substrate. Such a direct-write 3-D process will enhance testing and prototyping of microfluidic circuits and enable fabrication of true 3-D structures.",2003-01-01,['physics-and-media'],,1496,E15-023,2016-12-05T00:16:06.627Z,['neilg@media.mit.edu'],True,3-d-rapid-microfluidic-circuit-fabrication,2004-01-01,[],3-D Rapid Microfluidic Circuit Fabrication,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:42.772Z,"The ""300M IT Edition"" project is a collaboration between the Media Lab, MIT's Aero/Astro Department, DaimlerChrysler, and Motorola. Using the project platform�a fully instrumented version of the Chrysler 300M�we collected data on the driver's mental workload and emotional state. Several demonstrations showed examples of how an intelligent cockpit could improve driving. For example, it could remind drivers to drive their best, and make bicycling near the vehicle safer.",2003-01-01,['context-aware-computing'],,1504,E15-320,2016-12-05T00:17:02.853Z,[],True,300m-it-edition,2002-01-01,[],300M IT Edition,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:42.860Z,"The crystal oscillator inside a quartz wristwatch vibrates at 32,768 times per second. This is too fast for a human to perceive, and it's even more difficult to imagine its interaction with the mechanical circulation of a clock. 32,768 Times Per Second is a diagrammatic, procedural, and fully functional sculpture of the electro-mechanical landscape inside a common wristwatch. Through a series of electronic transformations, the signal from a crystal is broken down over and over, and then built back up to the human sense of time.",2015-09-01,['playful-systems'],,1512,--Choose Location,2016-12-05T00:17:01.854Z,"['slavin@media.mit.edu', 'tjlevy@media.mit.edu']",True,32768-times-per-second,2013-09-01,[],"32,768 Times Per Second",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:42.937Z,"This project presents a new approach for the design and fabrication of the acoustic instrument, using digital fabrication technologies, and specifically 3D printing. A concert flute was 3D printed, including (almost) all of its mechanisms, without the need to assemble the moving parts. In order to fulfill this challenge a new design of the instrument was made. Instead of proposing the machine can easily replace the human craft - we use this research to examine the possible contributions of rapid prototyping to the design and fabrication of traditional instruments.",2011-01-01,['responsive-environments'],,1520,,2016-12-05T00:16:06.907Z,['joep@media.mit.edu'],True,3d-printed-flute,2010-01-01,[],3D Printed Flute,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:43.019Z,"Functionally graded materials—materials with spatially varying composition or microstructure—are omnipresent in nature. From palm trees with radial density gradients, to the spongy trabeculae structure of bone, to the hardness gradient found in many types of beaks, graded materials offer material and structural efficiency. But in man-made structures such as concrete pillars, materials are typically volumetrically homogenous. While using homogenous materials allows for ease of production, improvements in strength, weight, and material usage can be obtained by designing with functionally graded materials. To achieve graded material objects, we are working to construct a 3D printer capable of dynamic mixing of composition material. Starting with concrete and UV-curable polymers, we aim to create structures, such as a bone-inspired beam, which have functionally graded materials. This research was sponsored by the NSF EAGER award: Bio-Beams: FGM Digital Design & Fabrication.",2020-05-31,['mediated-matter'],,1528,--Choose Location,2021-05-21T13:16:54.759Z,"['stevenk@media.mit.edu', 'neri@media.mit.edu']",True,3d-printing-of-functionally-graded-materials,2011-01-01,[],3D printing of functionally graded materials,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:43.096Z,"An autostereoscopic (no glasses) 3D display engine is combined with a ""Pepper's Ghost"" setup to create an office chair that appears to contain a remote meeting participant. The system geometry is also suitable for other applications, such as tabletop or automotive heads-up displays.",2020-03-19,['object-based-media'],,1536,--Choose Location,2020-03-19T18:50:05.395Z,['novysan@media.mit.edu'],True,3d-telepresence-chair,2012-09-01,[],3D Telepresence Chair,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:43.171Z,"The project explores how to best engage design collaborators in an ongoing dialogue following a design charette process. A video record of the charette is recorded, parsed, posted, and transformed into an active web of meaning so that salient ideas and themes can be followed through the many phases of discussion. Initial processing is human intensive; however the intention is to apply common-sense technology in the future.",2008-09-01,[],,1544,E15-368,2016-12-05T00:16:06.459Z,['gid@media.mit.edu'],True,3d-digital-dialogs-for-design,2007-09-01,[],3D: Digital Dialogs for Design,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:43.248Z,"The 3DprintedClock project is the result of ready-assembled 3D printed computational mechanisms, and is related to research in the fields of rapid prototyping and digital fabrication. The clock was modeled in CAD software after an existing clock, and uses a weight and a pendulum to keep track of time. The CAD model was created according to the specifications of the 3D printer, assuring sufficient gaps and clearances for the different parts. In addition, support material, drainage, and perforations were added to allow for excess support material being removed after printing. The 3DprintedClock is intended to demonstrate the superior capabilities of 3D printing as a fabrication process. It should contribute toward the future use of 3D printers to replace injection molding and expensive tooling processes, and allow for on demand, customized, and �greener� consumer products.",2011-01-01,['personal-robots'],,1552,E15-001,2016-12-05T00:16:06.934Z,"['cynthiab@media.mit.edu', 'rswartz@media.mit.edu']",True,3dprintedclock,2008-09-01,[],3DprintedClock,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:43.329Z,"Many of our online interactions take place in community spaces. We keep track of friends and share pictures on Facebook, chatter with friends on Twitter, and participate in discussions on online forums. But how do the design choices we make impact the kinds of social spaces that develop? To better understand this relationship, we conducted a study of a discussion forum with a particularly unusual design: 4chan.org. Perhaps best known for its role in driving Internet culture and its involvement with the ""Anonymous"" group, we believe 4chan's design plays a large role in its success, despite its counter-intuitiveness. In our first paper exploring this area, we quantify 4chan's ephemerality (there are no archives; most posts are deleted in a matter of minutes) and anonymity (there are no traditional user accounts, and most posts are fully anonymous) and discuss how the community adapts to these unusual design strategies.",2011-09-01,"['living-mobile', 'lifelong-kindergarten']",https://dam-prod2.media.mit.edu/x/files/Display/pldb_logo.png,1560,,2016-12-05T00:17:05.443Z,['geek@media.mit.edu'],True,4chan-and-b-anonymity-and-ephemerality,2010-01-01,[],4chan and /b/: Anonymity and Ephemerality,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:07:43.419Z,"4K/8K Comics applies the affordances of ultra-high-resolution screens to traditional print media such as comic books, graphic novels, and other sequential art forms. The comic panel becomes the entry point to the corresponding moment in the film adaptation, while scenes from the film indicate the source frames of the graphic novel. The relationships among comics, films, social media, parodies, and other support materials can be navigated using native touch screens, gestures, or novel wireless control devices. Big data techniques are used to sift, store, and explore vast catalogs of long-running titles, enabling sharing and remixing among friends, fans, and collectors.",2020-03-19,"['ultimate-media', 'object-based-media']",https://dam-prod2.media.mit.edu/x/2018/06/20/4KComics.jpg,1568,--Choose Location,2020-03-19T18:07:18.756Z,"['vmb@media.mit.edu', 'novysan@media.mit.edu']",True,4k8k-comics,2014-01-01,[],4K/8K Comics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:43.500Z,Is it possible to create passive displays that respond to changes in viewpoint and incident light conditions? Holograms and 4D displays respond to changes in viewpoint. 6D displays respond to changes in viewpoint as well as surrounding light. We encode the 6D reflectance field into an ordinary 2D film. These displays are completely passive and do not require any power. Applications include novel instruction manuals and mood lights.,,['camera-culture'],,1576,--Choose Location,2016-12-05T00:17:03.790Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,6d-display,2007-09-01,[],6D Display,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:43.579Z,"The 8D Display combines a glasses-free 3D display (4D light field output) with a relightable display (4D light field input). The ultimate effect of this extension to our earlier BiDi Screen project will be a display capable of showing physically realistic objects that respond to scene lighting as we would expect. Imagine a shiny virtual teapot in which you see your own reflection, a 3D model that can be lighted with a real flashlight to expose small surface features, or a virtual flashlight that illuminates real objects in front of the display. As the 8D Display captures light field input, gestural interaction as seen in the BiDi Screen project is also possible.",2014-09-01,['information-ecology'],,1584,--Choose Location,2016-12-05T00:16:07.170Z,['holtzman@media.mit.edu'],True,8d-display,2012-01-01,[],8D Display,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:43.659Z,"8K Time into Space is a user interface for a video exploration system with an 8K display. 8K is an ultra high-definition video system and it can present a huge amount of visual content on one display. In our system, video thumbnails with shifted playback time in chronological order are spaced out like tiles. The time range of a scene that a viewer wants to check can be adjusted with a touch interface, and resolution of the thumbnails is changed depending of the range. 8K Time into Space aims to provide responsive and intuitive experiences for video consumption.",2018-12-31,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/8KTiS.png,1592,--Choose Location,2019-04-17T22:34:57.493Z,"['ohmata@media.mit.edu', 'lip@media.mit.edu']",True,8k-time-into-space,2015-09-01,[],8K Time into Space,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:43.739Z,"Archived TV programs evoke earlier times. This application combines a video and music archive with an immersive screen and a simple user interface suitable for everyone, from children to the elderly, to create a ""Time Machine"" effect. The only key for exploring is the user's age. People can enjoy over 1,300 TV programs from the last seven decades without having to do tedious text searches. This  catalogue intuitively guides the user with an image array (64 different videos on one screen at the same time) that simplifies navigation and makes it immediate, rather than referencing it to previous screens.",2019-10-01,"['ultimate-media', 'ce-20', 'future-storytelling', 'object-based-media']",,1600,--Choose Location,2020-06-10T15:36:48.122Z,['vmb@media.mit.edu'],True,8k-time-machine,2016-01-01,[],8K Time Machine,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:43.832Z,"People cannot type as fast as they think, especially when faced with the constraints of mobile devices. The focus of this project is developing an alternative approach to predictive text entry using Open Mind Common Sense.",2006-09-01,['software-agents'],,1608,E15-383,2016-12-05T00:16:15.736Z,['lieber@media.mit.edu'],True,a-common-sense-approach-to-predictive-text-entry,2003-09-01,[],A Common-Sense Approach to Predictive Text Entry,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:44.007Z,"We propose cutting as a novel paradigm for ad hoc customization of printed electronic components. As a first example we created a printed, capacitive, multi-touch sensor, which can be cut by the end-user to modify its size and shape. This very direct manipulation allows the end-user to easily make real-world objects and surfaces touch-interactive, to augment physical prototypes and to enhance paper craft. We contribute a set of technical principles for the design of printable circuitry that makes the sensor more robust against cuts, damages, and removed areas. This includes novel physical topologies and printed forward error correction. A technical evaluation compares different topologies and shows that the sensor remains functional when cut to a different shape.",2014-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/coverHP%20(1).jpg,1624,--Choose Location,2016-12-05T00:16:15.574Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,a-cuttable-multi-touch-sensor,2013-01-01,[],A Cuttable Multi-Touch Sensor,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:44.293Z,"Users have high-level goals when they browse the Web or perform searches.  However, the two primary user interfaces positioned between users and the Web�Web browsers and search engines�have very little interest in users� goals.  Present-day Web browsers provide only a thin interface between users and the Web, and present-day search engines rely solely on keyword matching. This project leverages large knowledge bases of semantic information to provide users with a goal-oriented Web browsing experience. By understanding the meaning of Web pages and search queries, this project demonstrates how Web browsers and search engines can proactively suggest content and services to users that are both contextually relevant and personalized. This project presents (1) Creo, a Programming by Example system that allows users to teach their computers how to automate interactions with their favorite Web sites by providing a single demonstration; (2) Miro, a data detector that matches the content of a Web page to high-level user goals, and allows users to perform semantic searches; and (3) Adeo, an application that streamlines browsing the Web on mobile devices, allowing users to complete actions with a minimal amount of input and output.",2006-09-01,['software-agents'],,1640,E15-383,2016-12-05T00:17:05.587Z,['lieber@media.mit.edu'],True,a-goal-oriented-user-interface-for-personalized-semantic-search,2004-09-01,[],A Goal-Oriented User Interface for Personalized Semantic Search,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:44.446Z,The Kit-of-No-Parts is an approach to crafting electronics rather than designing discrete components. The collection of recipes and ingredients on the Kit-of-No-Parts website describes how to build electronics from a wide variety of conductive and non-conductive materials using a range of traditional and contemporary craft techniques.,2011-01-01,['high-low-tech'],,1648,,2016-12-05T00:17:05.472Z,['leah@media.mit.edu'],True,a-kit-of-no-parts,2011-01-01,[],A Kit-of-No-Parts,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:07:44.768Z,"The SEC is an extension to the free open-source program EyesWeb�that contains a large number of machine learning and signal processing algorithms that have been specifically designed for real-time pattern and gesture recognition.  All the algorithms within the SEC are encapsulated as individual blocks, allowing the user to connect the output of one block to the input of another to create a signal flow chain.  This allows a user to quickly build and train their own custom gesture recognition system, without having to write a single line of code or explicitly understand how any of the machine learning algorithms within their recognition system work.",2013-09-01,['responsive-environments'],,1664,,2016-12-05T00:16:07.479Z,['joep@media.mit.edu'],True,a-machine-learning-toolbox-for-musician-computer-interaction,2009-09-01,[],A Machine Learning Toolbox for Musician Computer Interaction,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:45.052Z,"Mood classification is very subjective. There have been many psychological studies performed throughout the past century relating to music and emotion, and as a result there exist many different representations of human emotion. This work proposes to utilize the best possible psychological model of emotion by incorporating the findings of these studies into an innovative front-end for a digital music library where music can be queried, browsed, and explored by mood, rather than artist, album, or genre. With the use of state-of-the-art audio and textual analysis tools this work proposes to automatically and accurately classify music by mood.",2007-01-01,['music-mind-and-machine'],,1680,E15-484,2016-12-05T00:16:15.283Z,['bv@media.mit.edu'],True,a-mood-based-music-classification-and-exploration-system,2007-01-01,[],A Mood-Based Music Classification and Exploration System,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:45.172Z,"One of the fundamental goals of Montessori education is to create productive, stress-free educational environments for children. In addition to traditional methods of observation, we argue that teachers would benefit from tools that could provide supplemental data identifying stress responses in students using psychophysiological data. The child-suited wearable device we have designed incorporates sensors that track signs linked to emotional and sympathetic responses, such as heart rate variability and electro-dermal activity. Through these data points, teachers and parents can better understand the child's emotional responses to activities and social interactions at school, and tailor programs to support wellbeing and stress reduction.",2017-05-01,['social-computing'],,1688,--Choose Location,2022-06-10T14:40:01.877Z,"['sdkamvar@media.mit.edu', 'koren@media.mit.edu']",True,a-multi-sensor-wearable-device-for-analyzing-stress-response-in-preschool-classrooms,2016-09-01,[],A Multi-Sensor Wearable Device for Analyzing Stress Response in Preschool Classrooms,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:45.532Z,"We have developed tightly coupled sensor network that tries to mimic some of the sensory characteristics of a natural skin. Developed in a flexible substrate (Kapton Polyimide), this array forms a flexible surface of small, networked sensor elements. Each node is capable of sensing four channels of pressure, two axes of strain, sound, temperature, ambient light, and two channels of whisker mechanoreception. By seamlessly blending sensing, computation, and local and global networking, our ""skin"" is ultimately self-configuring and scalable�it can be built as large as desired and signals can be extracted from the entire array at any node.",2006-09-01,['responsive-environments'],,1712,E15-353,2021-08-09T15:59:56.091Z,['joep@media.mit.edu'],True,a-scalable-networked-smart-sensate-skin,2005-09-01,"['architecture', 'environment', 'manufacturing', 'sensors', 'materials', 'fabrication']",A Scalable Networked Smart Sensate Skin,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:45.646Z,"We propose a flexible light-field camera architecture that represents a convergence of optics, sensor electronics, and applied mathematics. Through the co-design of a sensor that comprises tailored, angle-sensitive pixels and advanced reconstruction algorithms, we show that, contrary to light-field cameras today, our system can use the same measurements captured in a single sensor image to recover either a high-resolution 2D image, a low-resolution 4D light field using fast, linear processing, or a high-resolution light field using sparsity-constrained optimization.",2015-09-15,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/pldb_logo.png,1720,--Choose Location,2016-12-05T00:16:46.070Z,"['raskar@media.mit.edu', 'gordonw@media.mit.edu', 'naik@media.mit.edu']",True,a-switchable-light-field-camera,2013-01-01,[],A Switchable Light-Field Camera,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:45.872Z,"A design project done in collaboration with the MIT Media Lab and the Laboratorio para la Ciudad (Laboratory for the City), Mexico City's experimental office for civic innovation and urban creativity, A-pops is a networked learning experience across Mexico City that supports young learners in engaging in emergent and playful opportunities in and beyond their local communities. In line with the ""Playful City"" goal, this project aims to embed playful learning experiences across Mexico City that are creative, collaborative, and public, by leveraging existing public spaces throughout neighborhoods and micro-communities across the city. By embedding a variety of playful learning experiences across a variety of locations, a wide range of learners have the ability to easily and socially engage in transformative experiences that support key skills in design, collaboration, creativity, programming, and learner agency.",2017-06-01,"['ml-learning', 'lifelong-kindergarten']",https://dam-prod2.media.mit.edu/x/2016/10/18/5.jpg,1736,--Choose Location,2018-10-20T18:17:15.133Z,"['jgroff@media.mit.edu', 'gbernal@media.mit.edu', 'thomassl@media.mit.edu', 'minasg@media.mit.edu']",True,a-pops,2015-09-01,"['kids', 'learning-teaching', 'interfaces', 'social-science']",A-pops,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:07:45.976Z,"AAGO is a mobile app for Apple devices focusing on media creation, organization and sharing. Targeted to teens, the app aims to help them document group or individual creative projects (filmmaking, tech or art projects, citizen journalism) by creating ""stories"" made up of mobile photos, videos, and audio clips, which can then be arranged and exported to the web.",2012-09-01,['civic-media'],,1744,,2016-12-05T00:17:05.703Z,"['ethanz@media.mit.edu', 'nitin@media.mit.edu']",True,aago-mobile-media-diaries-for-youth-citizens-journalists,2011-01-01,[],AAGO: Mobile Media Diaries for Youth Citizens Journalists,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:46.154Z,"About Us examines technology workplaces from a feminist perspective. The objective is to mobilize workers to collect simple gender-specific data, such as the male-female ratio of their workplace. This data will be made available to others to inform performances, wearables, and games. The idea is to create collaboratively a data infrastructure to support projects and perspectives that shed light on the complicated issue of gender imbalance in spaces of high-tech labor.",2008-01-01,['computing-culture'],,1752,E15-020A,2016-12-05T00:16:05.922Z,['csik@media.mit.edu'],True,about-us,2008-01-01,[],About Us,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:46.321Z,"AboutFace is a user-dependent system that is able to learn patterns and  discriminate the different facial movements characterizing confusion and  interest. The system uses a piezoelectric sensor to detect eyebrow  movements and begins with a training session to calibrate the unique values  for each user. After the training session, the system uses these levels to  develop an expression profile for the individual user.  The system has many  potential uses, ranging from computer and video-mediated conversations to  interactions with computer agents.  This system is an alternative to using  camera-based computer vision analysis to detect faces and expressions.  Additionally, when communicating with other people, users of this system  also have the option of conveying their expressions anonymously by wearing  a pair of glasses that conceals their expressions and the sensing device.
",1999-12-30,['affective-computing'],,1760,,2016-12-05T00:16:06.081Z,['picard@media.mit.edu'],True,aboutface,1996-12-31,[],AboutFace,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:46.633Z,We are studying how social accountability information about companies affects consumer behavior. We are also developing Web-based tools to help consumers learn about and act on such information.,2010-09-01,[],,1776,--Choose Location,2016-12-05T00:16:06.230Z,['aithpao@media.mit.edu'],True,account-ability,2008-09-01,[],Account-Ability,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:46.896Z,"Technical devices are generally considered to be compliant instruments that are fully controllable tools, and from this perspective, humans have total power to operate machines at will. However, technologies often act back on humans, reversing the intended command direction. The Acoustic Chase project examines processes in which machines are intentionally provided with �degrees of freedom� not fully controllable by people. This will partially invert the relationship of human and tool, and create a system of balanced power. Expanding on the Haptic Opposition project, which combined touch and vision to create a human-machine interaction wherein the apparatuses could push back against human motion, Acoustic Chase will extend the concept of machines with a �free� acting will, but use sound as the primary output. The sensation of sound is direct; it offers quick information flow and a great variety of expressions. It is also difficult to escape: people can�t ""hear away"" or ""close their ears."" Tracking human body movements will provide an input channel. In combination with spatial sound, Acoustic Chase will enable intuitive human reactions, like discovering, playing, or fleeing.",2004-01-01,['computing-culture'],,1792,E15-001,2016-12-05T00:16:06.272Z,['csik@media.mit.edu'],True,acoustic-chase,2004-01-01,[],Acoustic Chase,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:47.290Z,"This work seeks to apply acoustical analysis to a hierarchical model of discourse structure so as to allow for the division of an audio recording into layers of detail. This ""outline"" structure is then used in interactive presentations, which allow a listener to browse at various levels of detail.",1999-12-30,['living-mobile'],,1816,,2016-12-05T00:17:05.735Z,['geek@media.mit.edu'],True,acoustical-cues-to-discourse-structure,1992-12-31,[],Acoustical Cues to Discourse Structure,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:47.403Z,"We are interested in building systems which can learn to connect natural language to objects and actions in video. In our current experiment, we have created videos of a person moving objects around on a table top. Naive subjects have been asked to describe verbally the video sequences. Our goal is to create a learning system which is able to ""ground"" the meaning of words and phrases in terms of observations found in the video. Applications of this work include verbal control of robots and natural-language-based access to video archives.",2001-12-30,['social-machines'],,1824,E15-483,2016-12-05T00:16:06.376Z,['dkroy@media.mit.edu'],True,acquiring-verbs-from-speech-and-video,2000-12-31,[],Acquiring Verbs from Speech and Video,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:47.510Z,"Agents (human or computer) provide their users with recommendations. This project investigates whether the type of agent making the recommendations (human vs. computer, collaborative vs. individual) influences the decision strategy that people utilize. Specifically, we examine whether the type of agent used influences decision makers' price/quality tradeoffs, risk attitude, willingness to compromise or to follow their gut feeling, or their likelihood of becoming more sensitive to social influences.",2007-01-01,['erationality'],,1832,E15-441,2016-12-05T00:16:06.880Z,[],True,acting-on-advice-from-computer-agents,2001-01-01,[],Acting on Advice from Computer Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:07:47.615Z,"Action Path is a mobile app to help people learn about and engage with issues in their community. The app uses push notifications tied to geography that invite people to provide meaningful feedback on nearby issues as they traverse the city. Most platforms for civic engagement, whether online or offline, are inconvenient and disconnected from the source of the issues they are meant to address. Action Path addresses barriers to effective civic engagement by inviting people's input, converting individual actions into collective action, and providing context and a sense of efficacy.",2017-09-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/actionpath-logo6.png,1840,--Choose Location,2018-04-27T14:33:05.572Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'erhardt@media.mit.edu', 'emreiser@media.mit.edu']",True,action-path,2013-09-01,"['civic-media', 'civic-technology']",Action Path,PUBLIC,http://actionpath.org,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:47.721Z,"Activ8 is a system of three short games: See-Saw, a balancing game for Google Glass; Jump Beat, a music beat matching game for Google Glass; and Learning to Fly, a Kinect game where two users keep a virtual bird in the air by flapping their arms in sync. Recent epidemiological evidence points at sitting as being the most common contributor to an inactive lifestyle.  We aim to offer a starting point towards designing and building an understanding about how ""physical casual games"" can contribute to helping address the perils of sitting.",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/11/22/jumpbeatpulse.png,1848,--Choose Location,2022-06-10T14:01:02.030Z,"['sra@media.mit.edu', 'geek@media.mit.edu']",True,activ8,,[],Activ8,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:47.824Z,"Sleepiness is known to decrease driving performance, resulting in one of the major causes of fatal car accidents. An effective countermeasure to drowsiness is to increase activation, as activity and moderate exercise increase alertness. We are currently using redesigned accelerator pedals to integrate some form of physical activity while driving to maintain alertness levels in drivers.",2007-09-01,['context-aware-computing'],,1856,E15-320,2016-12-05T00:16:07.885Z,[],True,active-driver,2006-09-01,[],Active Driver,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:47.931Z,"Active Essays were forms of narrative expression in which computational objects were integrated with text, graphics, and video. For example, a simulation of a scientific concept could be integrated with text descriptions of the concept, so that ""readers"" actively explore the concept.",2005-01-01,['lifelong-kindergarten'],,1864,E15-489,2016-12-05T00:16:07.932Z,"['mres@media.mit.edu', 'bss@media.mit.edu']",True,active-essays,1996-01-01,[],Active Essays,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:48.036Z,"Active Messenger filters email messages according to dynamic rules, and automatically routes messages to a variety of wired and wireless delivery channels. It observes traffic from a user by knowing which devices have been used to originate or to respond to messages, recent log-ins, and caller ID when checking voice or email over the phone. Its goal is to make sure that desired messages always reach the subscriber, while paring messages down when the user is less reachable. Active Messenger also acts as a proxy, hiding the identity of the multiple device addresses at which the subscriber may be found. More than just a router, Active Messenger is a dynamic process which monitors a message's progress through various channels over time.",2006-09-01,['living-mobile'],,1872,E15-344,2016-12-05T00:16:15.612Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,active-messenger,1999-01-01,[],Active Messenger,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:48.186Z,"We have developed an extremely low-power and low-cost wireless sensor network aimed at applications in asset tracking and ubiquitous activity monitoring. In addition to identifying an object, these nodes (termed active radio-frequency identification [RFID] tags) employ sensors to monitor its state, enabling new applications in fields like security, home automation, and supply-chain management. Although they contain a battery, these tags are not limited by it: by minimizing power consumption and quasi-passively waking on diverse stimuli (changes in light, RF carrier presence, shock or acceleration, and sound), they can last for years. Furthermore, their low cost and small size make them a good candidate for large-scale experiments at the intersection of RFID and wireless sensor networks.",2008-01-01,['responsive-environments'],,1880,E15-344,2016-12-05T00:16:16.630Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,active-rfid-tags-for-security-and-supply-chain-management,2006-01-01,[],Active RFID Tags for Security and Supply-Chain Management,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:48.310Z,"Smart agents represent users by creating models of particular users, and the agents' recommendations are based on these models. But models are rarely complete; this problem is particularly acute when there are changes either in the tastes and preferences of the individual, or in the marketplace (e.g., new products). Our agent attempts to solve this by actively trying to learn a user's preferences. The active agent balances two goals: to be immediately useful and to make high-quality recommendations (""selling""); and to learn more about the user. These recommendations have a lower quality, but also a higher ""up-side"" potential (""learning"").",2004-09-01,['erationality'],,1888,E15-448,2016-12-05T00:17:05.775Z,[],True,active-learning-smart-agents,2001-01-01,[],Active-Learning Smart Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:48.413Z,"The activeShelves are a networked system of surfaces, servers, and displays that give real-time data about a collection of physical objects. Shelf surfaces are embedded with radiofrequency (RF) readers to track tagged containers. The system is connected to a server, and can be used as a relational database for physical objects. The user gets information directly about usage, location, and status of objects on activeShelves, or can query remotely via the web. The activeShelves can display simple text and graphics on LED displays. Based on our work with TouchCounters, activeShelves continues the exploration of enhancing typical organizational systems. The new system senses using RFID tags instead of mechanical magnetic contacts. The system's data-entry components and o utput display LEDs are located on the shelves instead of just the physical containers. The activeShelves system acts as a relational database instead of just tracking usages of individual containers.",2001-09-01,['tangible-media'],,1896,E15-441,2016-12-05T00:17:05.806Z,['ishii@media.mit.edu'],True,activeshelves-interactive-storage-shelf-system,2000-01-01,[],activeShelves: Interactive Storage Shelf System,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:48.672Z,"The Actuated Workbench is a device that uses magnetic forces to move objects on a table in two dimensions. It is intended for use with existing tabletop tangible interfaces, such as Sensetable, providing an additional feedback loop for computer output and helping to resolve inconsistencies that otherwise arise from the computer�s inability to move objects on the table. Current developments include the physical synchronizing of two distributed tables for remote collaboration, and the use of actuation to program physical constraints into a simulation, allowing users to quickly set up a physical system that determines possible parameters for the underlying computational simulation.",2006-09-01,['tangible-media'],,1920,E15-344,2016-12-05T00:17:05.950Z,"['ishii@media.mit.edu', 'jpatten@media.mit.edu']",True,actuated-workbench,2001-01-01,[],Actuated Workbench,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:48.866Z,"String instruments are typically close-miked, to capture qualitative presence in recordings. This has been impossible for woodwind instruments, since each note comes from a different place on the instrument. We are developing microphones that can overcome this difficulty for woodwind instruments. By embedding the microphone with proximity detection, pitch detection, and tonal analysis, a microphone can continually adapt the audio signal. The instrument�s natural voicing will be preserved when close-miked, making it possible to record woodwind instruments with more detail.",2005-01-01,['music-mind-and-machine'],,1936,E15-484,2016-12-05T00:16:08.431Z,['bv@media.mit.edu'],True,adaptive-microphones,2004-01-01,[],Adaptive Microphones,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:49.144Z,"In this project, we propose a new Bayesian receiver for signal detection in flat-fading channels. First, the detection problem is formulated as an inference problem in a hybrid dynamic system that has both continuous and discrete variables. Then, an expectation propagation algorithm is proposed to address the inference problem. As an extension of belief propagation, expectation propagation efficiently approximates a Bayesian estimation by iteratively propagating information between different nodes in the dynamic system and projecting exact messages into the exponential family. Compared to sequential Monte Carlo filters and smoothers, the new method has much lower computational complexity since it makes analytically deterministic approximations instead of Monte Carlo approximations. Our simulations demonstrate that the new receiver achieves accurate detection without the aid of any training symbols or decision feedbacks. Future work involves joint decoding and channel estimation, where convolutional codes are used to protect signals from noise corruption. Initial results are promising.",2004-01-01,['affective-computing'],,1960,E15-001,2016-12-05T00:17:05.831Z,['picard@media.mit.edu'],True,adaptive-wireless-signal-detection-and-decoding,2003-01-01,[],"Adaptive, Wireless, Signal Detection and Decoding",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:49.226Z,"The Additive Lathe is a 3D printer that prints onto rotating objects. It has two degrees of freedom: the first degree is the x-axis that moves with a rack and pinion mechanism. The second degree is the rotation of the object. Unlike regular lathes, in the Additive Lathe the object can spin in both directions and can spin in steps due to the stepper motor that spins the spindle. The additive lathe can produce objects that are difficult to make with regular 3D printer like threads and springs.",2011-09-01,['mediated-matter'],,1968,,2016-12-05T00:16:08.168Z,['neri@media.mit.edu'],True,additive-lathe,2011-09-01,[],Additive Lathe,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:49.301Z,"Our initial experiments in spark electrosintering fabrication have demonstrated a capacity to solidify granular materials (35-88 micron soda ash glass powder) rapidly using high voltages and power in excess of 1 kW. The testbed high-voltage setup comprises a 220V 60A variable autotransformer and a 14,400V line transformer. There are two methods to form members using electrosintering: the one-electrode drag (1ED) and two-electrode drag (2ED) techniques. The 1ED leaves the first electrode static while dragging the second through the granular mixture. This maintains a live current through the drag path and increases the thickness of the member due to the dissipation of heat. Large member elements have been produced with a tube diameter of around 0.75"". The 2ED method pulls both electrodes through the granular mixture together, sintering the material between the electrodes in a more controlled manner.",2018-05-31,['mediated-matter'],,1976,--Choose Location,2021-05-21T13:14:40.974Z,"['stevenk@media.mit.edu', 'j_klein@media.mit.edu']",True,additive-manufacturing-in-glass-electrosintering-and-spark-gap-glass,2013-09-01,[],Additive Manufacturing in Glass: Electrosintering and spark gap glass,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:49.380Z,"Users don�t want to browse the Web on mobile devices, they want to complete actions. Adeo is an application that streamlines accessing the Web on mobile devices. Users can teach a computer how to interact with their favorite Web sites by demonstrating the actions in a Web browser, and then play these actions back with a single click on their mobile device. Adeo reduces complex procedures to the minimal amount of required input and output, improving both the efficiency and usability of mobile browsing.",2007-01-01,['software-agents'],,1984,E15-383,2016-12-05T00:17:05.891Z,['lieber@media.mit.edu'],True,adeo-a-streamlined-user-interface-for-accessing-the-web-on-mobile-devices,2004-09-01,[],Adeo: A Streamlined User Interface for Accessing the Web on Mobile Devices,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:49.525Z,"This project explores the contribution of advanced audio systems to live performance, their design and construction, and their integration into the theatrical design process. We look specifically at innovative input and control systems for shaping the analysis and processing of live performance; and at large-scale output systems which provide a meaningful virtual abstraction to DSP in order to create flexible audio systems that can both adapt to many environments and achieve a consistent and precise sound field for large audiences. ",2012-01-01,['opera-of-the-future'],,2000,--Choose Location,2016-12-05T00:16:08.466Z,"['tod@media.mit.edu', 'benb@media.mit.edu']",True,advanced-audio-systems-for-live-performance,2007-09-01,[],Advanced Audio Systems for Live Performance,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:07:49.676Z,"Computer modeling and simulation are changing the nature of scientific investigation, by enabling researchers to pose new kinds of questions and explore phenomena in ways that were not possible just a short time ago. Just as the technological revolution continues to influence the practice of scientific research, it also presents opportunities to change the way that science is taught in the classroom. The Adventures in Modeling Project introduced students and teachers to the process of designing, creating, and analyzing their own models of complex, dynamic systems using StarLogo. Our aims were to educate and motivate teachers to transform the way that they teach science and to engage students in authentic science practice by giving them the tools and the ability to pose, investigate, and answer their own questions. Through model creation, and participation in a variety of off-computer activities, teachers and students learned the science of modeling to develop a deeper understanding of patterns and processes in the world.",2000-09-01,['lifelong-kindergarten'],,2016,E15-001,2016-12-05T00:17:05.997Z,['mres@media.mit.edu'],True,adventures-in-modeling,1998-01-01,[],Adventures in Modeling,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:49.756Z,"Suitable for anywhere a ""Pepper's Ghost"" display could be deployed, this display adds 3D with motion parallax, as well as optically relaying the image into free space such that gestural and haptic interfaces can be used to interact with it. The current version is able to display a person at approximately full-size.",2020-03-19,"['ultimate-media', 'ce-20', 'future-storytelling', 'object-based-media']",https://dam-prod2.media.mit.edu/x/files/Display/3D_aerial_display.jpg,2024,--Choose Location,2020-03-19T18:51:31.744Z,"['vmb@media.mit.edu', 'novysan@media.mit.edu']",True,aerial-light-field-display,2015-01-01,[],Aerial Light-Field Display,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:50.226Z,"Affect as Index is a tool that takes group physiological data as input, aggregates it across different demographic dimensions, and attaches them to media content. Users can review videotaped or prerecorded events by clicking on points of interest in a physiological graph. This software addresses two challenges: the difficulty of expressing and sharing emotions with others, and the laborious task of monitoring interpersonal interactions within natural settings. For the former, groups interested in discussing shared and dissimilar emotions evoked during experiences can use this tool to place context around their dialogue. For the latter, ""meaningful moments"" observed within natural interactions can be marked and superimposed on the physiological data collected. In this way, affect and observations of affect can be used to index group-level significant moments that occur within volumes of video data.",2010-01-01,['affective-computing'],,2072,E15-448,2016-12-05T00:16:08.720Z,['picard@media.mit.edu'],True,affect-as-index,2007-09-01,[],Affect as Index,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:50.303Z,"The aim of this project is to build a database of natural speech showing a range of affective variability. It is an extension of our ongoing research focused on building models for automatic detection of affect in speech. At a very basic level, training such systems requires a large corpus of speech containing a range of emotional vocal variation. A traditional approach to this research has been to assemble databases where actors have provided the affective variation on demand. However, this method often results in unnatural sounding speech and/or exaggerated expressions. 

We have developed a prototype of an interactive system that guides a user through a question and answer session. Without any rehearsals or scripts, the user navigates through touch and spoken language an interface guided by embodied conversational agents which prompt the user to speak about an emotional experience. Some of the issues we are addressing include the design of the text and character behavior (including speech and gesture) so as to obtain a convincing and disclosing interaction with the user.",2003-01-01,"['cc', 'affective-computing']",,2080,E15-001,2016-12-05T00:17:06.077Z,['picard@media.mit.edu'],True,affect-in-speech-assembling-a-database,2001-01-01,[],Affect in Speech:  Assembling a Database,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:50.383Z,"The ""Affective Carpet"" is a soft, deformable surface made of cloth and  foam, which detects continuous pressure with excellent sensitivity and  resolution.  It is being used as an interface for projects in affective expression, including as a controller to measure a musical performer's direction and intensity in leaning and weight-shifting patterns. 
",1999-12-30,['affective-computing'],,2088,,2016-12-05T00:16:08.751Z,['picard@media.mit.edu'],True,affective-carpet,1997-12-31,[],Affective Carpet,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:50.465Z,"Affective Circles combines the Chat Circles 2 interface with affective information, including skin conductivity data, to enhance online communication.",2002-09-01,['sociable-media'],,2096,E15-468,2016-12-05T00:16:09.030Z,['judith@media.mit.edu'],True,affective-circles,2002-01-01,[],Affective Circles,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:50.636Z,"The Affective Mirror is an attempt to build a fully automated system that intelligently responds to a person's affective state in real time. Current work is focused on building an agent that realistically mirrors a person's facial expression and posture. The agent detects affective cues through a facial-feature tracker and a posture-recognition system developed in the Affective Computing group; based on what affect a person is displaying, such as interest, boredom, frustration, or confusion, the system responds with matching facial affect and/or posture. This project is designed to be integrated into the Learning Companion Project, as part of an early phase of showing rapport-building behaviors between the computer agent and the human learner.",2004-01-01,['affective-computing'],,2112,E15-001,2016-12-05T00:17:06.018Z,['picard@media.mit.edu'],True,affective-mirror,2002-01-01,[],Affective Mirror,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:50.715Z,"This study attempts to examine humans' affective responses to superimposed sinusoidal signals. These signals can be perceived either through sound, in the case of electronically synthesized musical notes, or through vibro-tactile stimulation, in the case of vibrations produced by vibrotactile actuators. This study is concerned with the perception of superimposed vibrations, whereby two or more sinusoisal signals are perceived simultaneously, producing a perceptual impression that is substantially different than of each signal alone, owing to the interactions between perceived sinusoidal vibrations that give rise to a unified percept of a sinusoidal chord. The theory of interval affect was derived from systematic analyses of Indian, Chinese, Greek, and Arabic music theory and tradition, and proposes a universal organization of affective response to intervals organized using a multidimensional system. We hypothesize that this interval affect system is multi-modal and will transfer to the vibrotactile domain.",2018-12-31,['affective-computing'],,2120,--Choose Location,2022-05-12T17:27:02.148Z,"['picard@media.mit.edu', 'gleslie@media.mit.edu']",True,affective-response-to-haptic-signals,2015-09-01,[],Affective Response to Haptic Signals,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:50.799Z,"ASQ investigates ways to teach social-emotion skills to children interactively with toys. One of the  first goals is to help autistic children recognize expressions of emotion in social situations. The system  uses four ""dwarfs"" expressing sad, happy, surprise, and angry, and each  communicates wirelessly to  the system and detects which plush doll was selected by the child.  The computer plays short  entertaining video clips displaying examples of the four emotions and cues the child to pick a dwarf  that closely matches that emotion.  Future work includes improving the ability of the system to  recognize direct displays of emotion by the child. 
",1999-12-30,"['toys-of-tomorrow', 'affective-computing']",,2129,,2016-12-05T00:17:06.104Z,['picard@media.mit.edu'],True,affective-social-quest,1996-12-31,[],Affective Social Quest,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:50.891Z,"Predicting media effectiveness from automatically measured affective responses collected ""in-the-wild"" over the Internet. This project shows that it is possible to predict powerful measures of media effectiveness automatically from natural and spontaneous facial responses captured via the web.",2014-01-01,[],https://dam-prod2.media.mit.edu/x/files/Display/Smiley.png,2137,--Choose Location,2016-12-05T00:16:08.772Z,['picard@media.mit.edu'],True,affective-storytelling,2013-01-01,[],Affective Storytelling,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:50.969Z,"People naturally express frustration through the use of their motor skills. The purpose of the Affective Tangibles project is to develop physical objects that can be grasped, squeezed, thrown, or otherwise manipulated via a natural display of affect. Constructed tangibles include a PressureMouse, affective pinwheels that are mapped to skin conductance, and a voodoo doll that can be shaken to express frustration. We have found that people often increase the intensity of muscle movements when experiencing frustrating interactions.",2005-09-01,['affective-computing'],,2145,E15-001,2016-12-05T00:16:43.232Z,['picard@media.mit.edu'],True,affective-tangibles,1999-01-01,[],Affective Tangibles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:51.065Z,"The Affective Tigger is a plush toy designed to recognize and react to certain emotinal behaviors of its playmate.  For example the toy enters a state of ""happy,"" moving its ears upward and emitting a happy vocalization when it recognizes that the child has postured the toy upright and is bouncing it along the floor.  Tigger has five such states, involving recognizing and responding with an emotional behavior.  The resulting behavior Tigger demonstrates allows it to serve as an affective mirror for the child's expression.  This work involved designing the toy, and evaluating sessions of play with it with dozens of kids.  The toy was shown to successfully communicate some aspects of emotion, and to prompt behaviors that are interesting to researchers trying to learn about the development of human emotional skills such as empathy.  ",1999-12-30,"['toys-of-tomorrow', 'affective-computing']",,2154,E15-384,2016-12-05T00:16:09.272Z,['picard@media.mit.edu'],True,affective-tigger,1996-12-31,[],Affective Tigger,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:51.149Z,"Recent findings in affective neuroscience and psychology indicate that human affect and emotional experience play a significant and useful role in human learning and decision-making. Most machine-learning and decision-making models, however, are based on old, purely cognitive models, and are slow, brittle, and awkward to adapt. We aim to redress many of these classic problems by developing new models that integrate affect with cognition. Ultimately, such improvements will allow machines to make smarter and more human-like decisions for better human-machine interaction.",2010-01-01,['affective-computing'],,2162,E15-443,2016-12-05T00:17:02.896Z,['picard@media.mit.edu'],True,affective-cognitive-framework-for-machine-learning-and-decision-making,2005-01-01,[],Affective-Cognitive Framework for Machine Learning and Decision Making,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:51.286Z,"Companies would like more new products to be successful in the marketplace, but current evaluation methods such as focus groups do not accurately predict customer decisions. We are developing new technology-assisted methods to try to improve the customer-evaluation process and better predict customer decisions. The new methods involve multi-modal affective measures (such as facial expression and skin conductance) together with behavioral measures, anticipatory-motivational measures, and self-report cognitive measures. These measures are combined into a novel computational model, the form of which is motivated by findings in affective neuroscience and human behavior. The model is being trained and tested with customer product evaluations and marketplace outcomes from real product launches.",2010-01-01,['affective-computing'],,2171,E15-443,2016-12-05T00:17:06.058Z,['picard@media.mit.edu'],True,affective-cognitive-product-evaluation-and-prediction-of-customer-decisions,2007-01-01,[],Affective-Cognitive Product Evaluation and Prediction of Customer Decisions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:51.451Z,"AffQuake is an attempt to incorporate signals that relate to a player's affect into ID Software's Quake II in a way that alters game play. Several modifications have been made that cause the player's avatar within Quake to alter its behaviors depending upon one of these signals. In StartleQuake, when a player becomes startled, his or her avatar also becomes startled and jumps back. Quake changes the size of the player's avatar in relation to the user's response as well, representing player excitement by average skin conductance level, and growing the avatar's size when this level is high.  A taller avatar means the player can see further; however, it also makes him or her an easier target. 
",1999-09-01,['affective-computing'],,2188,E15-383,2016-12-05T00:16:09.071Z,['picard@media.mit.edu'],True,affquake,1999-01-01,[],AffQuake,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:51.722Z,"The AFK cookset is designed for the hungry role-playing gamer who can connect her food items, e.g., Spicy Wolf Dumplings, to her online cooking habits. By scanning in the food items, the video game physically adjusts a hot plate to cook the item for the correct amount of time. The virtual character then jubilantly announces the status of the meal to both the gamer and the other individuals playing online: �O la la my roasted raptor is about to be done!� When the food is ready, the system automatically puts the character in AFK (�Away From Keyboard�) mode to provide the gamer with a moment to eat. When the player resumes playing, he or she might just discover his or her character�s behavior is affected by the food consumed in real life�sluggish from overeating or alternately exuberant and energetic.",2009-09-01,['tangible-media'],,2205,E15-344,2016-12-05T00:16:09.133Z,['ishii@media.mit.edu'],True,afk-cookset,2008-01-01,[],AFK Cookset,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:51.828Z,"The AmEC Infrastructure is an attempt to mitigate the well-known problem of software reuse in the domain of agent-mediated electronic commerce. Most of the systems developed by the AmEC initiative share a set of requirements that, once met, could serve as the foundation of future electronic commerce systems. The AmEC Infrastructure will define both a framework for the development of new systems and a set of pre-built subsystems that could be incorporated into new electronic commerce agent systems.",2001-12-30,['software-agents'],,2213,,2016-12-05T00:16:06.712Z,[],True,agent-mediated-electronic-commerce-amec-infrastructure,1999-12-31,[],Agent Mediated Electronic Commerce (AmEC) Infrastructure,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:52.074Z,"Current experiments in agent software rely mostly on domain-specific applications that either have been programmed from scratch, or explicitly modified with an agent in mind. Is it possible to make a tool kit or protocol that would allow an agent to communicate and to control applications that have been constructed more conventionally? Can the agent ""take the place"" of the user in the interface? Can the agent have access to the application's data and behavior? Will commercial ""inter-application communication"" mechanisms suffice? What is the division of labor between the agent and the application? This work will explore these questions.",2010-09-01,['software-agents'],,2230,E15-383,2016-12-05T00:16:16.050Z,['lieber@media.mit.edu'],True,agent-application-communication,2000-01-01,[],Agent-Application Communication,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:52.171Z,"What would it mean to browse collaboratively? In business meetings or family situations, several people may be attending a browsing session, even if only one ""controls"" the browser. The success of browsing depends on satisfying the interests of the passive participants as well. We are working on an agent that will provide assistance to group browsing, representing the interests of the entire group, and performing ""reconnaissance"" in anticipation of what pages are likely to be of common interest. It features automatic detection of the presence of users, acquisition of information about the interests of users, automated ""channel surfing,"" and dynamic display of the user profiles and explanation of recommendations.",2002-01-01,['software-agents'],,2239,E15-305A,2016-12-05T00:16:09.324Z,['lieber@media.mit.edu'],True,agents-for-collaborative-browsing,2000-01-01,[],Agents for Collaborative Browsing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:52.269Z,"Effective image annotation and retrieval is bound up with image use. In this project, annotation, retrieval, and use are integrated, facilitating the finding and using images. A proactive user-interface agent seeks chances for image annotation and retrieval in the context of the user's everyday work, using an agent that sit in the user's text editor or other application and continuously monitors typing. Searches are automatically performed from an image library, and images relevant to the current text can be inserted in a single operation. Descriptions of images for storytelling can be seamlessly employed as raw material for annotation. Common-sense knowledge about situations in which pictures are taken, described, or used can help provide semi-automatic annotation and indirect inference for retrieval. Our approach does not completely automate the annotation/retrieval process, but it does reduce user-interface overhead, leading to better-annotated image libraries and fewer missed opportunities for image use.",2010-09-01,['software-agents'],,2247,E15-383,2016-12-05T00:17:06.177Z,['lieber@media.mit.edu'],True,agents-for-integrated-annotation-and-retrieval-of-images,2000-01-01,[],Agents for Integrated Annotation and Retrieval of Images,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:52.366Z,"AgI connects isolated, rural farmers to markets, to knowledge, and to each other. Farmers will have access to market price information, weather data, agricultural extension services, buyers and intermediaries, and relevant experts. Importantly, a synchronous communication module enables farmers to establish voice and data communication with peer farmers from other rural communities. The system supports a geography-based user interface that respects a range of written literacy levels. For instance, all market price information includes text representations and machine-generated, spoken audio.",2002-12-30,"['e-markets', 'edevelopment']",,2256,E15-391,2016-12-05T00:16:09.537Z,[],True,agi-agricultural-information-system,1999-12-31,[],AgI: Agricultural Information System,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:52.464Z,"The first of this project's two goals is to enable those with internalized, psychologically distressing states to speak their own stories out loud to others without fear. The second goal involves understanding the �architecture� of current public spaces, both physical and online, and designing the structure and interaction of a physically located audiospace project that enables and encourages citizens to respond easily, and to be responsible towards each other in urban public places. This project revolves around the design and building of a public installation to let people call from any touch-tone phone into a public space. At the site of this public space will be a sculpture of sorts�a physical tele-presence for the person calling. The person calling will actually speak out of the physical sculpture. A full duplex audio link will be opened up between the caller and the tele-presence sculpture via existing telephone infrastructures. People in the public space can talk with the remote caller by talking to the sculpture. This project brings the privacy and anonymity of online communication into a physical public space to facilitate emotional processing by individuals. Public disclosure of private information may open dialogues around topics otherwise private or dangerous. Individuals may take steps towards dealing with personal situations while simultaneously offering public citizens voices articulating broadly pertinent issues. People present at the site of the tele-presence sculpture will be called upon to be responsive towards others while simultaneously feeling less alone in facing issues they may also be dealing with personally. Public spaces will potentially become more vital, and private issues less alienating.",2002-09-01,['sociable-media'],,2264,E15-468,2016-12-05T00:17:06.229Z,"['judith@media.mit.edu', 'monster@media.mit.edu']",True,agoraphone,2002-01-01,[],AgoraPhone,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:52.542Z,"Drivers spend a significant amount of time multi-tasking while they are behind the wheel. These dangerous behaviors, particularly texting while driving, can lead to distractions and ultimately to accidents. Many in-car interfaces designed to address this issue still neither take a proactive role to assist the driver nor leverage aspects of the driver's daily life to make the driving experience more seamless. In collaboration with Volkswagen/Audi and the SENSEable City Lab, we are developing AIDA (Affective Intelligent Driving Agent), a robotic driver-vehicle interface that acts as a sociable partner. AIDA elicits facial expressions and strong non-verbal cues for engaging social interaction with the driver. AIDA also leverages the driver's mobile device as its face, which promotes safety, offers proactive driver support, and fosters deeper personalization to the driver. ",2013-09-01,['personal-robots'],,2272,E15-468,2017-05-31T18:24:21.710Z,['cynthiab@media.mit.edu'],True,aida-affective-intelligent-driving-agent,2008-09-01,[],AIDA: Affective Intelligent Driving Agent,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:52.632Z,"A major problem for natural language interfaces is their inability to handle text whose meaning depends in part on context.  If a user asks his car radio to play ""a fast song,"" or his calendar to schedule ""a short meeting,"" the interpreter would have to accommodate vagueness and ambiguity to figure out what he meant based on what he said.  For it to understand what songs or events the speaker intended, it must make decisions that depend on assumed common knowledge about the world and language.  Our research presents two approaches for reducing uncertainty in natural language interfaces, by modeling interpretation as a plan recognition problem.",2014-09-01,['software-agents'],https://dam-prod2.media.mit.edu/x/files/Display/interpretation-the-lightest-one.png,2281,--Choose Location,2022-06-10T14:18:57.353Z,['lieber@media.mit.edu'],True,aigre-natural-language-interface-that-accommodates-vague-and-ambiguous-input,2011-01-01,[],AIGRE: Natural Language Interface that Accommodates Vague and Ambiguous Input,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:52.934Z,"This project is aimed at building a machine learning pipeline that will discover and predict links between the visible structure of villages and cities (using satellite and aerial imaging) and their inhabiting social networks. The goal is to estimate digitally invisible villages in India and Sub-Saharan Africa. By estimating the social structure of these communities, our goal is to enable targeted intervention and optimized distribution of information, education technologies, goods, and medical aid. Currently, this pipeline is implemented using a GPU-powered Deep Learning system. It is able to detect buildings and roads and provide detailed information about the organization of the villages. The output will be used to construct probabilistic models of the underlying social network of the village. Moreover, it will provide information on the population, distribution of wealth, rate and direction of development (when longitudinal imaging data is available), and disaster profile of the village.",2016-12-31,['social-machines'],,2315,--Choose Location,2017-02-16T16:35:50.415Z,"['dkroy@media.mit.edu', 'mmv@media.mit.edu']",True,aina-aerial-imaging-and-network-analysis,2015-01-01,[],AINA: Aerial Imaging and Network Analysis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:53.016Z,"Air Mobs creates a local mobile community to allow users to freely share Internet access among diverse carrier 3G and 4G data accounts. We have built an app where anyone can advertise that they have bits and battery to spare and are willing to let other Air Mob members tether to them. They might do this if they are near their data cap and either need a little more data, or have some they are willing to let others use before it expires. A website tracks the evolution of the community and posts the biggest donators and users of the system. To date, this app works on Android devices. It is designed to be open and community-based.  We may experiment with market credits for sharing airtime and adding other devices and features.",2013-09-01,"['information-ecology', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/AirMobs-logos_small.png,2323,--Choose Location,2016-12-05T00:16:10.372Z,"['holtzman@media.mit.edu', 'lip@media.mit.edu']",True,air-mobs,2011-01-01,[],Air Mobs,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:53.101Z,"aireForm is a dress of many forms that fluidly morph from one to another, animated by air, reflecting the shifting of our own personas. Pneumatic pillows transform the shape of sections of the dress, revealing new forms and evoking classic feminine silhouettes, from sleek to supple to striking.",2013-01-01,"['lifelong-kindergarten', 'tangible-media', 'information-ecology']",,2331,--Choose Location,2016-12-05T00:16:09.894Z,"['holtzman@media.mit.edu', 'ishii@media.mit.edu', 'mres@media.mit.edu', 'jacobsj@media.mit.edu', 'pip@media.mit.edu', 'x_x@media.mit.edu']",True,aireform,2013-01-01,[],aireForm,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:53.502Z,"A smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is low-cost and the power consumption is minimal making it portable to perform a range of studies in the field. Essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing that fits into a pocket and the entire device weighs 48 g. The device has a dedicated app on the smartphone to communicate, receive, plot and analyze spectral data. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruits. Ultra-Violet fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/03/31/Apples.jpg,2373,--Choose Location,2017-03-31T23:01:55.075Z,"['raskar@media.mit.edu', 'ajdas@media.mit.edu']",True,ajdass-untitled-project,2015-01-01,[],Smartphone spectrometer for food sensing,PUBLIC,http://www.mit.edu/~ajdas,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:54.030Z,"""All Cows Eat Grass"" is a mnemonic used in music instruction for A C E G�the notes on the white spaces of the bass clef. The All Cows Eat Grass project is an online platform that provides cost-effective, real-time private music lessons. The system connects music instructors and students using a low-latency audio and video link, provides motivational support to practice between lessons, and lowers the barrier to learn music with self-instruction material and self-evaluation musical games. ",2011-01-01,['music-mind-and-machine'],https://dam-prod2.media.mit.edu/x/files/Display/ACEG_logo.png,2424,,2016-12-05T00:17:05.919Z,['bv@media.mit.edu'],True,all-cows-eat-grass,2011-09-01,[],All Cows Eat Grass,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:54.202Z,"AlphaWolf presents a synthetic wolf pack comprised of autonomous and semi-autonomous wolves who interact with each other much as real wolves do: forming dynamic social relationships based on their past experience. Several people can interact with this installation at the same time. Each participant affects the emotional state of her given wolf by howling, growling, or whimpering into a microphone. In addition, participants may encourage their wolves to interact with specific other wolves. The dynamics of how the wolves interact is determined by the wolves' internal state, their social position in the pack, and their previous experiences with their pack-mates. By letting participants ""get inside the mind and body"" of a wolf, we hope to give them a compelling opportunity to explore the meaning of social behavior. The installation features a suite of supporting technology, including evocative real-time computer graphics, autonomous cinematography, and dynamic scoring and sound design.",2002-12-30,['synthetic-characters'],,2441,E15-441,2016-12-05T00:16:11.369Z,[],True,alphawolf,1999-12-31,"['archives', 'history']",AlphaWolf,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:55.074Z,"Urban noise pollution has been a problem since the days of Buddha. Walkmans help, but issues of both social and accoustic isolation have become more urgent with the popularity of the iPod. Addressing these issues may require a look at how recorded music devices work at a fundamental level. Ambient Addition is a Walkman-like device, built on a DSP core, that synthesizes music by sampling the sound around the listener, creating harmony and rhythm from the chaos and noise of the environment. By simultaneously opening music to incorporate the environment, but also turning the environment into music, the sound stays fresh and the listener is encouraged to explore new territory.",2010-01-01,['computing-culture'],,2532,E15-001,2022-06-10T13:40:30.226Z,['csik@media.mit.edu'],True,ambient-addition,2006-01-01,[],Ambient Addition,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:55.151Z,"This project explores the use of physical fixtures and objects to display temporal information within an architectural space in a subtle and aesthetically pleasing way. Ambient displays present and communicate information through their changes in form, movement, sound, color, and light. Ambient displays are persistently connected to digital information sources, continually displaying changing information within the physical environment.",2000-09-01,['tangible-media'],,2540,E15-344,2016-12-05T00:16:12.032Z,['ishii@media.mit.edu'],True,ambient-displays,1998-01-01,[],Ambient Displays,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:55.230Z,We design and evaluate an ambient blood glucose level visualization and feedback system that uses an Ambient Orb for diabetes self-care and social support.  The social support is provided by a friend or family member of an individual with diabetes. This research study was carried out with adult patients at Joslin Diabetes Center.,2004-01-01,"['gray-matters', 'affective-computing']",,2548,--Choose Location,2016-12-05T00:17:06.396Z,"['picard@media.mit.edu', 'amohan@media.mit.edu']",True,ambient-displays-for-social-support-and-diabetes-management,2003-01-01,[],Ambient Displays for Social Support and Diabetes Management,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:55.314Z,"Furniture is the infrastructure for human activity. Every day we open cabinets and drawers, pull up to desks, recline in recliners, and fall into bed. How can technology augment these everyday rituals in elegant and useful ways? The Ambient Furniture project mixes apps with the IKEA catalog to make couches more relaxing, tables more conversational, desks more productive, lamps more enlightening, and beds more restful. With input from Vitra and Steelcase, we are prototyping a line of furniture to explore ideas about peripheral awareness (Google Latitude doorbell), incidental gestures (Amazon restocking trash can and the Pandora lounge chair), pre-attentive processing (energy clock), and eavesdropping interfaces (Facebook photo coffee table).",2013-01-01,['tangible-media'],,2557,--Choose Location,2016-12-05T00:17:06.426Z,"['ishii@media.mit.edu', 'drose@media.mit.edu']",True,ambient-furniture,2011-09-01,[],Ambient Furniture,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:55.476Z,"Traditional music production and studio engineering depends on dynamic range compression audio signal processors that precisely and dynamically control the gain of an audio signal in the time domain. This project expands on the traditional dynamic range compression model by adding a spatial dimension. Ambisonic Compression allows audio engineers to dynamically control the spatial properties of a three-dimensional sound field, opening new possibilities for surround-sound design and spatial music performance.",,['opera-of-the-future'],,2574,--Choose Location,2019-04-17T19:58:29.909Z,"['tod@media.mit.edu', 'holbrow@media.mit.edu']",True,ambisonic-surround-sound-audio-compression,2015-01-01,[],Ambisonic surround-sound audio compression,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:55.560Z,"Amino is a design-driven mini-lab that allows users to carry out a bacterial transformation and enables the subsequent care and feeding of the cells that are grown. Inspired by Tamagotchis, the genetic transformation of an organism's DNA is performed by the user through guided interactions, resulting in a synthetic organism that can be cared for like a pet. Amino is developed using low-cost ways of carrying out lab-like procedures in the home, and is packaged in a suitcase-sized continuous bioreactor for cells.",2015-07-01,['playful-systems'],,2582,--Choose Location,2016-12-05T00:17:06.454Z,['slavin@media.mit.edu'],True,amino-a-tamagotchi-for-synthetic-biology,2014-09-01,[],Amino: A Tamagotchi for Synthetic Biology,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:55.941Z,"Amphibian allows users to easily imprint digital functions onto common everyday physical objects. Amphibian is a low cost, low infrastructure system that enables users to choose their own physical objects and imprint onto them almost any standard interface functions that take place on a GUI desktop. The goal of Amphibian is to create a system that the common user can implement and operate so that we may learn more about the digital-physical object relationships people will form. ",2007-01-01,['tangible-media'],,2623,E15-331,2016-12-05T00:16:12.747Z,['ishii@media.mit.edu'],True,amphibian,2006-09-01,[],Amphibian,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:56.024Z,"SCUBA diving as a sport has enabled people to explore the magnificent ocean diversity of beautiful corals, striking fish, and mysterious wrecks. However, only a small number of people are able to experience these wonders, as diving is expensive, mentally and physically challenging, needs a large time investment, and requires access to large bodies of water. Most existing SCUBA diving simulations in VR are limited to visual and aural displays. We propose a virtual reality system, Amphibian, that provides an immersive SCUBA diving experience through a convenient terrestrial simulator. Users lie on their torso on a motion platform with their outstretched arms and legs placed in a suspended harness. Users receive visual and aural feedback through the Oculus Rift head-mounted display and a pair of headphones. Additionally, we simulate buoyancy, drag, and temperature changes through various sensors.",2018-09-01,['living-mobile'],,2631,--Choose Location,2022-06-10T14:01:01.896Z,"['geek@media.mit.edu', 'djain@media.mit.edu']",True,amphibian-terrestrial-scuba-diving-simulator-using-virtual-reality,2015-01-01,[],Amphibian: Terrestrial SCUBA Diving Simulator Using Virtual Reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:56.108Z,"Shape-changing materials have been part of sci-fi literature for decades. But if tomorrow we invent them how are we going to use them? Amphorm is a kinetic sculpture that resembles a vase. Since Amphorm is a dual citizen between the digital and the physical worlds, its shape can be altered both in the  physical world through hand gestures and in the digital realms through an iPad app. Through this project, we explore how the physical world could be synchronized to the digital world and how tools from both worlds can jointly alter dual-citizens.",2012-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/IMG_0115.JPG,2639,,2016-12-05T00:16:12.387Z,"['ishii@media.mit.edu', 'dlakatos@media.mit.edu']",True,amphorm,2012-01-01,[],Amphorm,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:56.186Z,"We are building a new kind of agent that acts as a user's assistant in browsing the World Wide Web. Many current Web tools perform searches for the user, but our approach is to consider the search for information as a cooperative venture between the human user and an intelligent software agent. Rather than search a pre-indexed portion of the Web according to user-stated keywords, the agent, Letizia, infers interest implicitly from observing user actions and tries to stay just a few steps ahead of the user, searching the user's immediately accessible links dynamically. ",2008-09-01,['software-agents'],,2647,E15-383,2016-12-05T00:16:09.300Z,['lieber@media.mit.edu'],True,an-advisory-agent-for-web-browsing,2000-01-01,[],An Advisory Agent for Web Browsing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:56.339Z,"This project examines how the expression granted by new musical interfaces can be harnessed to create positive changes in health and wellbeing. We are conducting experiments to measure EEG dynamics and physical movements performed by participants who are using software designed to invite physical and musical expression of the basic emotions. The present demonstration of this system incorporates an expressive gesture sonification system using a Leap Motion device, paired with an ambient music engine controlled by EEG-based affective indices. Our intention is to better understand affective engagement, by creating both a new musical interface to invite it, and a method to measure and monitor it. We are exploring the use of this device and protocol in therapeutic settings in which mood recognition and regulation are a primary goal.",2018-12-31,['affective-computing'],,2663,--Choose Location,2022-05-12T17:18:38.705Z,"['picard@media.mit.edu', 'gleslie@media.mit.edu']",True,an-eeg-and-motion-capture-based-expressive-music-interface-for-affective-neurofeedback,2014-09-01,"['human-machine-interaction', 'music', 'affective-computing']",An EEG and motion-capture based expressive music interface for affective neurofeedback,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:56.419Z,"We are experimenting with how the Web can augment the research process, focusing on Web software as vehicles for experimentation, design, and learning. We believe that software tools can be constructed to achieve intrinsic empirical research goals and that the open sharing and dissemination of these results is a key component of the design process. Recycled research software is intended to be sharable, embeddable, and complimentary to explicit research goals. Some key areas of interest for this investigation include: viral experimentation, explicit research licensing, open data sharing and analysis, and pseudo genetic adjustment.",2007-01-01,['context-aware-computing'],,2671,E15-320,2016-12-05T00:16:12.829Z,[],True,an-experiment-in-recycled-research,2007-01-01,[],An Experiment in Recycled Research,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:07:56.503Z,"This project explores performing statistical signal processing with analog circuits. The first instantiation is a low-complexity circuit for phase acquisition and tracking of a pseudo-random sequence generated by an LFSR. Eventually we hope to scale the complexity of this kind of cicuitry to be able to perform any high-speed signal processing desired for a digital wireless front-end. In many application domains, this approach could be 10 to 100 times more efficient in terms of power consumption and square millimeters of silicon than the equivalent digital signal processor (DSP).",2003-09-01,['physics-and-media'],,2679,E15-023,2016-12-05T00:17:06.637Z,['neilg@media.mit.edu'],True,analog-vlsi-graphical-models,2002-01-01,[],Analog VLSI Graphical Models,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:56.583Z,"Despite many studies of marine mammal vocalizations, there is no generally accepted theory of sound production by whales or dolphins, and only qualitative descriptions of their spectra. Of particular interest are the unusual sounds made by killer whales, which appear to arise from two independent sources and are called ""biphonic."" We have analyzed a number of these sounds from Northern Resident Whales and find the resulting spectrum is due to convolution of the spectra of two sources. Since convolution in the frequency domain translates to multiplication in the time domain, it means that the sound of one source is amplitude modulating the sound from the second source.  This is a non-linear interaction rather than a simple linear superposition of two sources.  We are attempting to find a physical model which would explain this behavior and have a Matlab program that simulates the original sounds.",2008-01-01,['music-mind-and-machine'],,2687,E15-484,2016-12-05T00:17:06.524Z,"['bv@media.mit.edu', 'brown@media.mit.edu']",True,analysis-and-synthesis-of-pulsed-vocalizations-by-killer-whales,2007-09-01,[],Analysis and Synthesis of Pulsed Vocalizations by Killer Whales,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0
False,2016-07-15T00:07:56.749Z,"We are examining autonomic sleep patterns using a wrist-worn biosensor that enables comfortable measurement of skin conductance, skin temperature, and motion. The skin conductance reflects sympathetic arousal. We are looking at sleep patterns in healthy groups, in groups with autism, and in groups with sleep disorders. We are looking especially at sleep quality and at performance on learning and memory tasks.",2013-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/W_HOS_timestamp_forEMBCppt.jpg,2703,,2016-12-05T00:16:12.769Z,"['picard@media.mit.edu', 'akanes@media.mit.edu']",True,analysis-of-autonomic-sleep-patterns,2010-09-01,[],Analysis of Autonomic Sleep Patterns,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:56.839Z,"Andante is a representation of music as animated characters walking along the piano keyboard that appear to play the physical keys with each step. Based on a view of music pedagogy that emphasizes expressive, full-body communication early in the learning process, Andante promotes an understanding of the music rooted in the body, taking advantage of walking as one of the most fundamental human rhythms. ",2015-09-01,['tangible-media'],,2711,--Choose Location,2016-12-05T00:16:12.904Z,"['ishii@media.mit.edu', 'x_x@media.mit.edu']",True,andante,2013-09-01,[],Andante,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:56.917Z,"The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy & Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation. The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.",,['changing-places'],"https://dam-prod2.media.mit.edu/x/2017/04/12/2015,02.01 Andorra-13497.jpg",2719,--Choose Location,2017-05-30T17:05:50.125Z,"['jiw@media.mit.edu', 'noriega@media.mit.edu', 'agnis@media.mit.edu', 'csmuts@media.mit.edu', 'yleng@media.mit.edu', 'noyman@media.mit.edu', 'naichun@media.mit.edu', 'lrocher@media.mit.edu', 'mcllin@media.mit.edu', 'devisj@media.mit.edu', 'kll@media.mit.edu', 'alonsolp@media.mit.edu', 'agrignar@media.mit.edu']",True,andorra-living-lab,2015-01-01,"['data', 'transportation', 'urban-planning', 'social-science', 'behavioral-science']",Andorra Living Lab,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:57.063Z,"Cell phones are great for communication in a virtual manner, but lack expressiveness in personal surroundings. Many people try to give their phones a personal touch by customizing them. Android Meets Arduino is a toolkit to connect Android-driven mobile devices with Arduino microcontrollers via Bluetooth. The toolkit provides easy access to internal phone events which can be further processed on the Arduino open-source prototyping platform. This toolkit seeks to empower people to externalize their phone events to creatively demonstrate them on wearables, living spaces, or other tangibles.",2010-01-01,['high-low-tech'],,2727,E15-368,2016-12-05T00:17:06.696Z,['leah@media.mit.edu'],True,android-meets-arduino,2009-09-01,[],Android Meets Arduino,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:57.139Z,"Eye exams via a slit lamp are critical in early diagnosis of diseases such as cataracts, corneal injury, and pterygia, in order to avert vision loss. The slit lamp is one of the most versatile tools in an ophthalmologist's clinic, but is big, expensive, and is designed with specialized ophthalmic clinics in mind. AnEye is a suite of portable, computationally driven solutions that leverage modern optics and commercially available consumer electronics to extend the reach of examinations of the anterior segment of the eye well beyond large hospitals and clinics, into resource-constrained settings such as rural mass-screening camps, mobile ophthalmology clinics, and even primary care.",,"['emerging-worlds', 'camera-culture']",,2735,--Choose Location,2019-04-19T17:46:56.865Z,"['raskar@media.mit.edu', 'sssinha@media.mit.edu']",True,aneye-extending-the-reach-of-anterior-segment-ophthalmic-imaging,2015-09-01,[],AnEye: Extending the reach of anterior segment ophthalmic imaging,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:07:57.222Z,"Transients occur in human walking during a transition to, from, and between steady state walking and acts as an impulse destabilizing a gait cycle. Turns, rapid stops, and accelerated starts are all common transients encountered and managed intelligently by humans everyday. Humanoid bipeds are rapidly becoming a more common part of our everyday life. Therefore, they must also be able to navigate our environments adroitly if they are to assist us in our daily living. This project takes biomechanical principals of angular momentum and applies them to design of controllers for bipeds using angular momentum primitives. These primitives are basic units that simplify the control problem and reduce the dimensionality of the state-space and the objective task. The task in this project is to accomplish transient behaviors and steady state walking together. Through this we are able to realize a more efficient and effective control for humanoid robots.",2009-09-01,['biomechatronics'],,2743,E15-054,2016-12-05T00:17:06.585Z,['hherr@media.mit.edu'],True,angular-momentum-primitives-modeling-of-transients-in-human-walking,2007-09-01,[],Angular Momentum Primitives: Modeling of Transients in Human Walking,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:07:57.389Z,"The goal of Animal Blocks is to provide a sketch book for children's collaborative storytelling by collecting and connecting their narratives. Each animal in Animal Blocks can tell the stories that were told by children who played with it in the past. When a child picks up an animal and puts it on a play scene, a colorful shadow of the toy appears on the scene. Then the shadow walks up to the story book placed next to the play scene, and illustrates its story in the book with text and pictures. The child can write her own stories in the book by using the physical animal blocks in addition to the keyboard. The goal is to encourage children to transfer their oral storytelling, using toys such as Animal Blocks, into written representations, making a connection between children's oral storytelling play and their literacy activities.",2003-01-01,[],,2759,E15-320,2016-12-05T00:16:12.878Z,[],True,animal-blocks,2000-01-01,[],Animal Blocks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:57.461Z,"Like people, dogs and cats live among technologies that affect their lives. Yet little of this technology has been designed with pets in mind. We are developing systems that interact intelligently with animals to entertain, exercise, and empower them. Currently, we are developing a laser-chasing game, in which dogs or cats are tracked by a ceiling-mounted webcam, and a computer-controlled laser moves with knowledge of the pet's position and movement. Machine learning will be applied to optimize the specific laser strategy. We envision enabling owners to initiate and view the interaction remotely through a web interface, providing stimulation and exercise to pets when the owners are at work or otherwise cannot be present.",2014-05-31,['personal-robots'],,2767,--Choose Location,2017-05-31T18:44:59.591Z,['cynthiab@media.mit.edu'],True,animal-robot-interaction,2013-01-01,[],Animal-Robot Interaction,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:57.541Z,Can animated playground props support and possibly enhance open-ended and physically active play in playgrounds? This project expands the repertoire of objects conceived specifically for children�s outdoor play environments. A category of playground prop called space explorer suggests new opportunities for children to experience their outdoor play environment.,2009-01-01,['smart-cities'],,2775,E15-001,2016-12-05T00:16:13.017Z,['susanne@media.mit.edu'],True,animated-playground-props,2005-09-01,[],Animated Playground Props,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:07:57.627Z,"Animated Vines is an interactive paper installation that comes to life in the presence of a viewer. Normally a static wall hanging, as the viewer approaches the vines begin to curl and slither up and down.  Each vine is made up of eight units, and each unit actuated to curl using muscle wire sewn directly to the paper. While a single unit can only curl slightly, cascading the units sums the individual movements to create a lifelike dance. The muscle wires� movements are silent so that the interaction is accompanied only by the sound of gently crackling and creaking paper.",2012-01-01,['high-low-tech'],,2783,,2016-12-05T00:16:13.037Z,"['leah@media.mit.edu', 'jieqi@media.mit.edu']",True,animated-vines,2011-01-01,[],Animated Vines,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:57.704Z,Most story representation and annotation focuses on low-level annotations such as parts of speech. We are trying to annotate English problem-solving narratives with mental processes and ways to think used in the story. This would help for an automated common-sense reasoning system to activate similar mental processes and ways to think in analogy problems.,2007-01-01,"['software-agents', 'society-of-mind']",,2791,E15-385,2016-12-05T00:16:13.055Z,"['lieber@media.mit.edu', 'minsky@media.mit.edu']",True,annotating-stories-with-mental-processes,2007-01-01,[],Annotating Stories with Mental Processes,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:57.785Z,"Ant-Based Modeling explores the possibility of implementing agent-based modeling with living ants and external stimuli such as electromagnetic radiations, magnetic fields, and electric fields. In an experiment with fire ants, we discovered that ultraviolet and infrared lights can affect their behavior in the form of attraction and dispersion towards the light, respectively. The video highlights some of the LEGOmade landscapes we use in our explorations and how the behavior of ants can be influenced by ultraviolet light to achieve certain purposes such as exploring a new area or dragging a ping pong ball to a specific location. These experiments have allowed us to learn more about ants and have inspired us to explore novel forms of human-ant interaction.",2017-04-22,[],https://dam-prod2.media.mit.edu/x/2016/10/12/3.jpg,2799,--Choose Location,2018-04-26T15:20:39.904Z,"['csmuts@media.mit.edu', 'oi7@media.mit.edu', 'm_kayser@media.mit.edu', 'javierhr@media.mit.edu']",True,ant-based-modeling,2016-04-22,"['data', 'systems', 'urban-planning', 'collective-intelligence']",Ant-Based Modeling,PUBLIC,http://oi7.me,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:07:57.866Z,"A 3D-printed dress was debuted during Paris Fashion Week Spring 2013 as part of collaboration with fashion designer Iris Van Herpen for her show ""Voltage."" The 3D-printed skirt and cape were produced using Stratasys' unique Objet Connex multi-material 3D printing technology, which allows a variety of material properties to be printed in a single build. This allowed both hard and soft materials to be incorporated within the design, crucial to the movement and texture of the piece. Core contributers include: Iris Van Herpen, fashion designer (Amsterdam); Keren Oxman, artist and designer (NY); and W. Craig Carter (Department of Materials Science and Engineering, MIT). Fabricated by Stratasys.",2015-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/Anthozoa%20Image%201.jpg,2807,--Choose Location,2021-05-21T13:07:39.132Z,"['stevenk@media.mit.edu', 'neri@media.mit.edu']",True,anthozoa,2013-01-01,[],Anthozoa,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:57.948Z,Anthropomorphic visualization is a new approach to presenting historical information about participants in online spaces using the human form as the basis for the visualization. We use several pieces of data from the messages an author writes in a Usenet newsgroup to create a depiction that looks somewhat like a person. We are currently investigating users' understanding and reactions to the visualization to assess the advantages and disadvantages of this visual approach.,2004-09-01,['sociable-media'],,2815,E15-391,2016-12-05T00:16:12.995Z,['judith@media.mit.edu'],True,anthroviz,2003-01-01,[],AnthroViz,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:58.025Z,"App Inventor is an intuitive, visual programming environment that allows everyone, even those with no prior coding experience, to build fully functional applications for smartphones and tablets. Those new to App Inventor can have a simple first app up and running in under 30 minutes. The tool allows anyone to program more complex, impactful apps in significantly less time than with more traditional programming environments. The MIT App Inventor project seeks to democratize software development by empowering all people, especially young people, to transition from being consumers of technology to becoming creators of it. MIT students and staff, led by Professor Hal Abelson, form the nucleus of an international movement of inventors. In addition to leading educational outreach around MIT App Inventor and conducting research on its impacts, this core team maintains the free online app development environment that serves more than four million registered users.",,[],https://dam-prod2.media.mit.edu/x/files/Display/cml_logo_url_cropped.png,2823,--Choose Location,2016-12-05T00:16:57.950Z,"['hal@media.mit.edu', 'mckinney@media.mit.edu']",True,app-inventor,2011-09-01,[],App Inventor,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:07:58.262Z,"AR-Jig is a new, handheld, tangible user interface for 3-D digital modeling. AR-Jig has a pin array that displays a 2-D physical curve coincident with a contour of a digitally displayed 3-D form. It allows physical interaction with a portion of a 3-D digital representation, allowing 3-D forms to be touched and modified directly. In contrast to traditional tangible interfaces, which physically embody the entirety of data, this project leaves the majority of the data in the digital domain but gives physicality to any portion of the larger digital dataset via a handheld tool. This intersection of tangible and digital tools results in the ability to flexibly manipulate digital artifacts both tangibly and virtually.",2007-01-01,['tangible-media'],,2847,E15-344,2016-12-05T00:16:13.141Z,"['ishii@media.mit.edu', 'dick@media.mit.edu']",True,ar-jig,2006-01-01,[],AR-Jig,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:07:58.366Z,"Arabiia is a caricature of media stereotypes typically associated with arab women. Aside from exotic Daisy Duck with her dance of the seven veils, and the mute abiding second class citizen in a black burka, not many images spring to mind when thinking of an Arab woman. The convertible outfit is equipped with two servo motors and a switch. It enables its wearer to voluntarily choose which of two extreme representations fits her mood and audience.",2005-01-01,['computing-culture'],,2855,Cube,2016-12-05T00:16:13.076Z,['csik@media.mit.edu'],True,arabiia,2005-09-01,[],Arabiia,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:07:58.442Z,"Economic theories of valuation generally assume that prices of commodities and assets are derived from underlying ""fundamental"" values. For example, consumer microeconomics assumes that demand curves for consumer products can ultimately be traced to the valuation of pleasures that consumers anticipate receiving from these products. Current work suggests that preferences are initially malleable, but become ""imprinted"" (precisely defined and largely invariant) after the individual makes an initial decision. Prior to imprinting, preferences are ""arbitrary,"" (highly responsive both to normative and non-normative influences). Following imprinting, preferences are ""coherent"" (more precisely defined and largely fixed in subsequent decisions). The model predicts that consumers will respond to changes in conditions in a coherent fashion, as if supported by demand curves derived from fundamental preference, even when their initial valuations are arbitrary.",2008-01-01,['erationality'],,2863,--Choose Location,2016-12-05T00:17:06.719Z,[],True,arbitrary-coherence-in-behavioral-economics,2001-01-01,[],Arbitrary Coherence in Behavioral Economics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:07:58.600Z,"We are augmenting Computer Numeric Controlled (CNC) machines used in fabrication, prototyping, and construction. This project aims to develop processes that enable additive prototyping and construction at a large, architectural scale. One implementation combines robot arms with 3D weaving technology to create a new, high-accuracy prototyping machine for on-site fabrication in industries such as architecture, aerospace, and automotive. It would also be suitable for environments that are difficult for humans to inhabit�remote mountain or desert regions, deep sea or even outer space! Currently, industrial robot arms are not only used for repetitive assembly line tasks, but also for composite lay-up, bricklaying, milling and routing, welding, applying adhesives, but these automated CNC systems are mostly stationary and depend on molds to form the final shapes. We are investigating the potential for on-site construction machines that would cut down on overhead in management, coordination, fabrication, and transportation.",2010-01-01,['smart-cities'],,2879,E15-001,2016-12-05T00:17:06.793Z,[],True,architectural-machines,2009-01-01,[],Architectural Machines,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:07:58.677Z,"We often have 'idle' time during which we consume various types of audio media (radio, audio books, MP3s, CDs, podcasts)�while driving to work, or waiting at a cafe for a friend. Are We There Yet? helps an audio-playing device put together a listening program that will neatly fit within, for example, your bus ride home. It has audio time-compression techniques to match the playback duration to the user's available listening time.",2007-01-01,['living-mobile'],,2887,E15-368,2016-12-05T00:16:13.230Z,['geek@media.mit.edu'],True,are-we-there-yet,2006-01-01,[],Are We There Yet?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:59.396Z,"Like the visible layers of a canyon, the layers in Artifacts of the Presence Era tell a story of past events. The images and sounds produced in a gallery at Boston's Institute for Contemporary Arts were captured and then visualized as a growing, organic landscape that served as an historical record. Like its natural counterpart, this process revealed long-term patterns (the rhythm of night and day, periods of great activity or empty silence), while retaining occasionally serendipitous, but often mundane, samples of the passage of life. The project visualized the accumulating layers of data and allowed visitors to navigate the captured images and ambient sounds, peeking back into the history of the gallery.",2003-01-01,['sociable-media'],,2943,E15-468,2016-12-05T00:17:06.756Z,['judith@media.mit.edu'],True,artifacts-of-the-presence-era,2003-09-01,"['archives', 'history']",Artifacts of the Presence Era,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:07:59.536Z,"Human walking neuromechanical models show how each muscle works during normal, level-ground walking. They are mainly modeled with clutches and linear springs, and are able to capture dominant normal walking behavior. This suggests to us to use a series-elastic clutch at the knee joint for below-knee amputees. We have developed the powered ankle prosthesis, which generates enough force to enable a user to walk ""normally."" However, amputees still have problems at the knee joint due to the lack of gastrocnemius, which works as an ankle-knee flexor and a plantar flexor. We hypothesize that metabolic cost and EMG patterns of an amputee with our powered ankle and virtual gastrocnemius will dramatically improve.",,['biomechatronics'],,2951,E15-054,2021-09-17T19:14:45.948Z,['hherr@media.mit.edu'],True,artificial-gastrocnemius,2007-01-01,[],Artificial Gastrocnemius,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:59.665Z,"Video or broadcast news is viewed in a far wider set of circumstances than it ever has been before. It is composed with the assumption of a complete, situated viewing, but in fact it is often grabbed on-the-fly as a momentary experience. As You Need It is a semantic summarizer that deconstructs a multi-part segment for presentation as ""chunks of importance."" We are learning if a story can be cut down to a useful update that takes less time than a traffic light, or as much time as a given user has. This project uses and contributes to another group project, SuperGlue.",,"['ultimate-media', 'viral-communications']",,2959,--Choose Location,2016-12-05T00:16:49.848Z,"['jasrub@media.mit.edu', 'lip@media.mit.edu']",True,as-you-need-it,2016-01-01,[],As You Need It,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:07:59.798Z,"Askii is an SMS-based system that allows adult learners to study for a certification exam while on their commute. When learners have a spare five minutes, they can simply text Askii to begin their customized lessons. Askii will respond with a curated set of questions and links to content that learners can study on the go. We have begun building this prototype for learners to study for the US Naturalization Exam and plan to expand to other certification courses. Askii is a prototype within the larger Making Learning Work project.",2015-09-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/askii_square.jpeg,2967,--Choose Location,2016-12-05T00:16:13.398Z,"['mres@media.mit.edu', 'ps1@media.mit.edu', 'jnazare@media.mit.edu']",True,askii,2015-01-01,[],Askii,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:07:59.930Z,"AskMobi is a tool for adolescents to engage in social science inquiry about issues important to them. The system has two components: a cell-phone application for collecting data and a web-based system for creating studies, analyzing data, and presenting results in a public forum where they can be critiqued. The system scaffolds adolescents� learning experiences by taking them through basic social science methodology step-by-step, while allowing the users to investigate issues they choose.
",2007-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/AskMobiPhonePicSq.jpg,2975,Cube,2016-12-05T00:16:13.434Z,"['mres@media.mit.edu', 'sylvan@media.mit.edu']",True,askmobi,2006-01-01,[],AskMobi,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:00.874Z,"We are developing a wireless system of devices to monitor the condition of patients with chronic diseases. The patient will use these devices at home to measure relevant medical parameters. Measurements will be relayed by wireless links to a base station in the patient's home. The base station can be used to store the data, perform initial analysis, and relay the data to a central caregiver for further analysis, charting, and alert scanning, as well as reporting to both the physician and the patient.",2001-12-30,['responsive-environments'],,3023,E15-441,2016-12-05T00:16:13.356Z,['joep@media.mit.edu'],True,at-home-chronic-health-care-monitoring,2000-12-31,[],At-Home Chronic Health Care Monitoring,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:01.023Z,"This project redefines the future performance vehicle by utilizing an articulating chassis that is activated by using fluid air muscles and a hybrid powerplant. The body morphs, conforms to the road, and is complimented by a soft body skin. Traditional driver controls are replaced by a sensate and wearable seat that allows the driver to steer, accelerate, and brake by body movement. The simplest version of the athlete car consists of only the ""Interactive Seat"" (or ""Wearable Seat"") and the ""Wheel Robot,"" creating a vehicle that provides an exhilarating, sensual driving experience with high driver involvement, similar to non-motorized sports, such as skiing, surfing, and ice skating.",2007-09-01,['smart-cities'],,3031,E15-001,2016-12-05T00:16:13.467Z,[],True,athlete-car,2005-01-01,[],Athlete Car,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:01.166Z,"An upgrade to the ubiquitous iRx board, atmex is an Atmel ""digital glue"" microcontroller experimentation system (hardware, firmware, and software). It is low cost ($10), does not require a programmer, and eliminates many of the annoyances that are associated with iRx.",2004-09-01,['computing-culture'],,3039,E15-020D,2016-12-05T00:16:13.451Z,['csik@media.mit.edu'],True,atmex,2004-09-01,[],Atmex,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:01.737Z,"With the Attentional Mixer we are investigating how attentional cues can be used to selectively amplify events in a person's environment. Audio streams coming from various sources are mixed and delivered to the user according  to what the person is actively paying attention to. In a noisy cocktail party the Attentional Mixer can improve a person's ability to hear their conversational partner, or in a rock band it can allow the musicians to change their personal ""monitor mix"" without skipping a beat, just by looking at the band member to be amplified. Opportunities for collaborative filtering arise when the number of devices grows.",2003-01-01,['context-aware-computing'],,3071,E15-320,2016-12-05T00:16:06.821Z,[],True,attentional-mixer,2003-01-01,[],Attentional Mixer,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:01.983Z,"By designing relationships between a large body of pictorial information and a viewer's attention at single, or multiple viewpoints, it is possible to prepare rich, large-grain, temporal texturing of information landscapes in a manner that is uniquely relevant to the viewer. A variety of scenarios are being investigated in the construction of a soft immersive experience into very large pools of visual information.",2001-12-30,['aesthetics-computation'],,3087,,2016-12-05T00:17:06.848Z,[],True,attentive-reaction,1999-12-31,[],Attentive Reaction,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:02.100Z,"How do you get from a TV channel to a rich video archive, how do you get there automatically, and how can you use that content to create augmented media experiences? ATTN-SPAN converts C-SPAN into a series of overlapping video segments that are identified in terms of state, politician, topic, party, action, and legislative item.  Those clips are then used to augment and personalize media experiences, providing information layers that have been crafted to fit the context and consumer.",2011-09-01,['information-ecology'],,3095,,2016-12-05T00:16:13.528Z,['holtzman@media.mit.edu'],True,attn-span,2011-01-01,[],ATTN-SPAN,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:02.206Z,"As online auctions become more popular, it is important to understand better the process by which people set their bids and the factors that influence them. In this project, we use a survey methodology to survey real bidders in real, online auctions and to try to understand the factors that influence both their willingness to bid, and the magnitude of their bids.",2007-01-01,['erationality'],,3103,E15-313,2016-12-05T00:17:06.865Z,[],True,auction-behavior,2001-01-01,[],Auction Behavior,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:02.303Z,"Standard loudspeakers transmit sound which necessarily spreads very quickly, and the control of sound projection and position is only about as flexible as where you can hang a loudspeaker. The Audio Spotlight is a device that will project sound much like a spotlight projects light: shining it at a listener allows only that person to hear it, while shining it at a surface causes the sound to appear to originate from there, creating something of a ""virtual loudspeaker."" Beamsteering will allow the sound to move, enabling the user dynamically to place sound�exactly, and only, where it is desired.",2010-01-01,['music-mind-and-machine'],,3111,E15-484,2016-12-05T00:17:06.901Z,['bv@media.mit.edu'],True,audio-spotlight,1998-01-01,[],Audio Spotlight,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:02.500Z,"We have developed a technique by which devices like Pocket PCs, phones, or laptop computers can use audio processing to estimate their locations relative to one another.  This will enable the creation of ad hoc speaker or microphone arrays from heterogeneous collections of mobile devices.",2005-09-01,['object-based-media'],,3127,E15-368,2016-12-05T00:16:13.589Z,['vmb@media.mit.edu'],True,audio-based-self-localization,2004-09-01,[],Audio-Based Self-Localization,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:02.582Z,"AudioFile overlays imperceptible tones on standard audio tracks to embed digital information that can be decoded by standard mobile devices. AudioFile lets users explore their media more deeply by granting them access to a new channel of communication. The project creates sound that is simultaneously meaningful to humans and machines. Movie tracks can be annotated with actor details, songs can be annotated with artist information, or public announcements can be infused with targeted, meaningful data. ",2013-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/audiofile_pldb.png,3135,--Choose Location,2016-12-05T00:16:13.620Z,"['trich@media.mit.edu', 'lip@media.mit.edu']",True,audiofile,2012-01-01,[],AudioFile,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:02.685Z,"We have a limited range of hearing, defined primarily by volume and distance. As one moves further away from a constant sound, it becomes quieter. What if it didn't have to? Audiograph looks at how positional and orientation information can help us bridge distance barriers for audio, and create seamless audio interactions between individuals, places, and information. Applications include social interactions, serendipitous encounters, navigation, and a re-evaluation of phones and remote audio-based communication.",2010-01-01,['information-ecology'],,3143,E15-301,2016-12-05T00:16:13.555Z,['holtzman@media.mit.edu'],True,audiograph-superhero-hearing,2009-09-01,[],Audiograph: Superhero Hearing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:02.767Z,"We have developed Audiopad, an interface for musical performance that aims to combine the modularity of knob-based controllers with the expressive character of multidimensional tracking interfaces. Audiopad uses a series of electromagnetically tracked objects called �pucks� as input devices. The performer assigns each puck to a set of samples that he wishes to control. Audiopad determines the position and orientation of these objects on a tabletop surface and maps this data into musical cues such as volume and effects parameters. Graphical information is projected onto the tabletop surface from above, so that information corresponding to a particular physical object on the table appears directly on and around the object. Our exploration suggests that this seamless coupling of physical input and graphical output can yield a musical interface that has great flexibility and expressive control.",2006-01-01,['tangible-media'],,3151,E15-344,2016-12-05T00:16:13.654Z,"['ishii@media.mit.edu', 'jpatten@media.mit.edu']",True,audiopad,2002-01-01,[],Audiopad,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:02.850Z,"AudioVortex is an interactive audio spatialization installation that explores the difference between sound propagation in an open space versus sound propagation in a closed space. A circle of eight loudspeakers is divided in two by a physical barrier, while a microphone on either side of the barrier samples sounds from its respective environment. The two modes of operation include a spatial inversion, where sounds from outside are heard inside while sounds from inside are heard outside. The second mode blurs the boundary between inside and out both by moving the sound of the two spaces in a circular motion and by distorting the natural sounds with effects such as flange, delay, and reverberation. AudioVortex attempts to overcome physical boundaries through the use of sound, thus giving people the ability to aurally interact and communicate with each other in what would otherwise be non-communicable spaces.",2006-01-01,['music-mind-and-machine'],,3159,E15-485A,2016-12-05T00:17:06.934Z,['bv@media.mit.edu'],True,audiovortex,2006-01-01,[],AudioVortex,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:02.932Z,"Persons on the autism spectrum often report hypersensitivity to sound. Efforts have been made to manage this condition, but there is wide room for improvement. One approach�exposure therapy�has promise, and a recent study showed that it helped several individuals diagnosed with autism overcome their sound sensitivities. In this project, we borrow principles from exposure therapy, and use fun, engaging games to help individuals gradually get used to sounds that they might ordinarily find frightening or painful.",2013-09-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/ssmash.jpg,3167,E15-450,2016-12-05T00:17:07.009Z,"['picard@media.mit.edu', 'rmorris@media.mit.edu', 'mgoodwin@media.mit.edu']",True,auditory-desensitization-games,2008-01-01,[],Auditory Desensitization Games,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:03.011Z,"We present an augmented handheld airbrush that allows unskilled painters to experience the art of spray painting. Inspired by similar smart tools for fabrication, our handheld device uses 6DOF tracking, mechanical augmentation of the airbrush trigger, and a specialized algorithm to let the painter apply color only where indicated by a reference image. It acts both as a physical spraying device and as an intelligent digital guiding tool that provides manual and computerized control. Using an inverse rendering approach allows for a new augmented painting experience with unique results. We present our novel hardware design, control software, and a discussion of the implications of human-computer collaborative painting.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/AB_1.jpg,3175,--Choose Location,2016-12-05T00:17:07.027Z,"['pattie@media.mit.edu', 'joep@media.mit.edu']",True,augmented-airbrush,2014-09-01,[],Augmented Airbrush,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:03.313Z,"Augmented Physicality aims to explore the causal relationship between virtual information and its physical counterpart. We look at the explicit coupling between tangible XYZ coordinates and digital XYZ coordinates,  focusing on adaptive rendering and perspective based on the factors of proximity and orientation. Ultimately, Augmented Physicality hopes to provide a multi-user, multi-perspective environment where both physical and digital objects can be collaboratively explored and manipulated.",2012-09-01,['tangible-media'],,3207,--Choose Location,2016-12-05T00:16:13.741Z,"['ishii@media.mit.edu', 'ndepalma@media.mit.edu']",True,augmented-physicality,2011-01-01,[],Augmented Physicality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:03.852Z,"This work consists of two projects. The first, a hypersonic sound system, collects outdoor impressions and transforms them into lush and soothing indoor atmospheres. The second, Aural Augmentation, pipes in natural environment sounds into an interior space.",2009-01-01,[],,3263,E15-301,2016-12-05T00:16:13.766Z,[],True,aural-ambience,2009-01-01,[],Aural Ambience,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:04.009Z,"This project examines the psycho-acoustic bases of the perception of musical structure by human listeners. Computational models will be built to mimic basic musical perception, such as parsing music into phrases or sections (i.e., recurrent structural analysis), identifying the main themes or hooks of a musical piece (i.e., music summarization), and detecting the most �informative� parts of music for making certain judgments (i.e., detection of musical salience), upon taking complex acoustic signals as input. It will inquire scientifically into the nature of the music-listening process, and offer a practical solution to difficult problems in computer-based multimedia.",2005-01-01,['music-mind-and-machine'],,3279,E15-484,2016-12-05T00:17:07.337Z,['bv@media.mit.edu'],True,automated-analysis-of-musical-structure-for-music-segmentation,2004-09-01,[],Automated Analysis of Musical Structure for Music Segmentation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:04.082Z,"A common practice in Traditional Chinese Medicine (TCM) is visual examination of the patient's tongue. This study will examine ways to make this process more objective and to test its efficacy for understanding stress- and health-related changes in people over time. We start by developing an app that makes it comfortable and easy for people to collect tongue data in daily life together with other stress- and health-related information. We will obtain assessment from expert practitioners of TCM, and also use pattern analysis and machine learning to attempt to create state-of-the-art algorithms able to help provide better insights for health and prevention of sickness.",2017-10-01,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/2017/09/21/cover_tongue_WuP48BJ.png,3287,--Choose Location,2021-10-07T16:47:28.395Z,"['picard@media.mit.edu', 'javierhr@media.mit.edu', 'akanes@media.mit.edu', 'fergusoc@media.mit.edu', 'cvx@media.mit.edu']",True,automated-tongue-analysis,2015-09-01,[],Automated Tongue Analysis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:04.162Z,"Both professional and hobbyist musicians rely heavily on their computers to make music, and usually find themselves with hard drives full of samples. The majority are individual drum samples, often called ""hits"" or ""one shots."" Arranging these samples in folders is usually done manually, by listening to each sample and moving it into a desired folder. While making music, retrieval of these samples is done, once more, by tedious auditions of each and every one. This project is a first step towards making the life of the computer-based musician a little bit easier by automatically classifying these samples and allowing better methods of retrieval.",2011-09-01,['opera-of-the-future'],,3295,,2016-12-05T00:16:27.276Z,['tod@media.mit.edu'],True,automatic-drum-sample-classification,2010-09-01,[],Automatic Drum Sample Classification,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:04.239Z,"Recognizing non-verbal cues, which constitute a large percentage of our communication, is a prime facet of building emotionally intelligent systems. Facial expressions and movements such as a smile or a nod are used either to fulfill a semantic function, to communicate emotions, or as conversational cues. We are developing an automatic tool using computer vision and various machine-learning techniques, which can detect the different facial movements and head gestures of people while they are interacting naturally with the computer. Past work on this project determined techniques to track upper facial features (eyes and eyebrows) and detect facial actions corresponding to those features (eyes squintint or widening, eyebrows raised). The ongoing project is expanding its scope to track and detect facial actions corresponding to the lower features. Further, we hope to integrate the facial expression analysis module with other sensors developed by the Affective Computing group to reliably detect and recognize different emotions.",2003-09-01,['affective-computing'],,3303,E15-001,2016-12-05T00:17:07.149Z,['picard@media.mit.edu'],True,automatic-facial-expression-analysis,2001-01-01,[],Automatic Facial Expression Analysis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:04.316Z,"Broadcasting and recording bird vocalizations through cell-phone networks is a simple yet powerful idea with strong repercussions in conservation projects, born out of the need to automate and facilitate bird population surveys. Of concern is the broadcast quality bird-song analysis and synthesis that can leverage the cell channels for transmission. This work aims to extract environmental information from vocalization recordings through cell-phone networks.",2009-09-01,[],,3311,E15-320,2016-12-05T00:16:13.885Z,[],True,automatic-polling-of-wildlife-creatures,2006-01-01,[],Automatic Polling of Wildlife Creatures,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:04.381Z,"Occupational stress can be described as a harmful emotional and physical response that occurs when high demanding job conditions cannot be met by the resources of the worker. This type of stress is usually associated with feelings of frustration, anger, and fear and can lead to dissatisfaction and lack of motivation in the long-term. Furthermore, high levels of stress can impair decision making, decrease productivity, and lead to high amounts of accidents and job absenteeism. Despite the well-studied negative outcomes, workplace stress is still considered a necessary evil by many people as it helps us keep up with the pace of modern society. Leveraging state-of-the-art sensing technologies and AI, this project seeks to advance the measurement, understanding, and management of stress in real-life settings.",2018-12-31,"['advancing-wellbeing', 'affective-computing']",,3319,--Choose Location,2022-05-12T17:12:36.305Z,"['picard@media.mit.edu', 'rmorris@media.mit.edu', 'javierhr@media.mit.edu', 'sfedor@media.mit.edu', 'exposito@media.mit.edu', 'djmcduff@media.mit.edu', 'amores@media.mit.edu']",True,automatic-stress-recognition-in-real-life-settings,2011-01-01,[],Onsite Stress Measurement,PUBLIC,http://javierhr.com,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:04.469Z,"A tabletop set of cellular automata ready to exhibit complex systems through simple behaviors, AutomaTiles explores emergent behavior through tangible objects. Individually they live as simple organisms, imbued with a simple personality; together they exhibit something ""other"" than the sum of their parts. Through communication with their neighbors, complex interactions arise. What will you discover with AutomaTiles?",2017-06-01,"['ultimate-media', 'playful-systems']",https://dam-prod2.media.mit.edu/x/2016/12/14/AutomaTiles_Uncased_gyZPwib.jpg,3327,--Choose Location,2022-06-10T14:17:21.076Z,"['slavin@media.mit.edu', 'jbobrow@media.mit.edu']",True,automatiles,2015-01-01,[],AutomaTiles,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:04.643Z,"We are performing long-term measurements of autonomic nervous system (ANS) activity on patients with epilepsy. In certain cases, autonomic symptoms are known to precede seizures. Usually in our data, the autonomic changes start when the seizure shows in the EEG, and can be measured with a wristband (much easier to wear every day than wearing an EEG). We found that the larger the signal we measure on the wrist, the longer the duration of cortical brain-wave suppression following the seizure.  The duration of the latter is a strong candidate for a biomarker for SUDEP (Sudden Unexpected Death in Epilepsy), and we are working with scientists and doctors to better understand this. In addition, bilateral changes in ANS activity may provide valuable information regarding seizure focus localization and semiology.",2014-04-14,['affective-computing'],,3335,--Choose Location,2019-04-18T03:27:11.639Z,"['picard@media.mit.edu', 'zher@media.mit.edu']",True,autonomic-nervous-system-activity-in-epilepsy,2008-09-01,"['health', 'wearable-computing']",Autonomic Nervous System Activity in Epilepsy,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:04.807Z,"We are characterizing changes in autonomic nervous system (ANS) during sleep. This can potentially provide insight into circadian rhythms as well as identification of various sleep stages. Furthermore, we are investigating differences between ANS activity in neurotypicals and people with sleep disorders or electrical status epilepticus of sleep (ESES).",2011-01-01,['affective-computing'],,3343,--Choose Location,2016-12-05T00:17:07.216Z,"['picard@media.mit.edu', 'zher@media.mit.edu', 'akanes@media.mit.edu']",True,autonomic-nervous-system-activity-in-sleep,2008-09-01,[],Autonomic Nervous System Activity in Sleep,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:05.655Z,"This project investigates soft mechanisms, origami, and fashion. We created a modified Miura-fold skirt that changes shape through pneumatic actuation. In the future, our skirt could dynamically adapt to the climatic, functional, and emotional needs of the user; for example, it might become shorter in warm weather.",2014-01-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2016/10/20/_MG_4982.JPG,3391,--Choose Location,2016-12-05T00:16:59.501Z,"['vmb@media.mit.edu', 'pip@media.mit.edu', 'perovich@media.mit.edu', 'kll@media.mit.edu']",True,awakened-apparel,2013-01-01,['fashion'],Awakened Apparel,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:05.779Z,An Awareness Portal turns unused wall space into a community news resource. It responds to viewers' presence and maintains a trace of community browsing activity.,2000-12-30,"['e-markets', 'living-mobile']",,3399,E15-344,2016-12-05T00:16:13.954Z,"['nitin@media.mit.edu', 'geek@media.mit.edu']",True,awareness-portal,1998-12-31,[],Awareness Portal,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:06.777Z,"A loop engine that records a voice-generated drum pattern. It can instantly and automatically loop it, quantize it, slow it down or speed it up without distording the sound. Or it can turn your voice into a real-sounding drum kit.",2003-01-01,['opera-of-the-future'],,3495,E15-484,2016-12-05T00:16:14.052Z,"['tod@media.mit.edu', 'tristan@media.mit.edu']",True,b-box,2001-01-01,[],B-Box,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:06.943Z,"Babble Baubles, wearable bracelet-like devices for kids that send secret messages composed of changing patterns of color, let children program different display patterns and respond to other people's Baubles. In using the Baubles, kids learned how to construct language conventions and how to encrypt and decrypt codes.",2005-09-01,['lifelong-kindergarten'],,3511,E15-020A,2016-12-05T00:16:14.086Z,"['mres@media.mit.edu', 'sylvan@media.mit.edu']",True,babble-bauble,2004-01-01,[],Babble Bauble,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:07.280Z,"The living room is the heart of social and communal interactions in a home. Often present in this space is a screen: the television. When in use, this communal gathering space brings together people and their interests, and their varying needs for company, devices, and content. This project focuses on using  personal devices such as mobile phones with the television; the phone serves as a controller and social interface by offering a channel to convey engagement, laughter, and viewer comments, and to create remote co-presence.",2011-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/BigScreen_Pictures.jpg,3527,E15-368,2016-12-05T00:16:14.008Z,['geek@media.mit.edu'],True,back-talk,2009-09-01,[],Back Talk,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:07.446Z,"In this project, we construct a virtual desktop centered around the smartphone display with the surface around the display opportunistically used for input. We use a 3-pixel optical time-of-flight sensor, Mime, to capture hand motion. The sensor on the phone allows the table surface next to the phone to be mapped to conventional desktop windows, and the phone's display is a small viewport onto this desktop. Moving the hand is like moving the mouse, and as the user shifts into another part of the desktop, the phone viewport display moves with it. We demonstrate that instead of writing new applications to use smart surfaces, existing applications can be readily controlled with the hands.",2014-01-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/map1.png,3535,--Choose Location,2016-12-05T00:17:07.460Z,['geek@media.mit.edu'],True,back-to-the-desktop,2014-01-01,[],Back to the Desktop,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:07.588Z,"While people attend presentations, panels, and lectures to learn something from the people at the front of the room, there's a lot of potential for creating spaces where audience members can interact with each other and the people presenting. This project focuses on augmenting the physical space of the auditorium to provide a venue for the audience to ask (and filter) questions for presenters. backchan.nl is a simple, Web-based tool that allows audience members to identify themselves, post questions, and vote on other people's questions. The current top questions are projected at the front of the room so that the audience can see them, as well as shown on a monitor visible to presenters. Upcoming posts are shown to users participating on the Web, but not on the main screen.",2017-05-01,['sociable-media'],,3543,E15-383,2022-06-10T14:41:29.707Z,['judith@media.mit.edu'],True,backchannl,2007-09-01,[],backchan.nl,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:07.737Z,"The LEVI (Low-Error Voter Interface) ballot design seeks to create a new ballot for voting systems that is flexible, efficient, and easy to use. We have incorporated new navigation features that serve as a visual representation of a voter's status within the voting session. In addition to this, we are exploring the use of audio verification as a tool to further improve voter confidence and accuracy.",2007-09-01,['context-aware-computing'],,3551,E15-320,2016-12-05T00:17:01.359Z,[],True,ballot-design-with-audio-verification,2004-09-01,[],Ballot Design with Audio Verification,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:07.879Z,"bandicoot provides a complete, easy-to-use environment for researchers using mobile phone metadata. It allows them to easily load their data, perform analysis, and export their results with a few lines of code. It computes 100+ standardized metrics in three categories: individual (number of calls, text response rate), spatial (radius of gyration, entropy of places), and social network (clustering coefficient, assortativity). The toolbox is easy to extend and contains extensive documentation with guides and examples.",,['human-dynamics'],,3559,--Choose Location,2019-04-19T14:40:08.264Z,"['sandy@media.mit.edu', 'yva@media.mit.edu']",True,bandicoot-a-python-toolbox-for-mobile-phone-metadata,2015-09-01,[],bandicoot: A Python toolbox for mobile phone metadata,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:08.033Z,"We exploit the sub-picosecond time resolution along with spectral resolution provided by terahertz time-domain spectroscopy to extract occluding content from layers whose thicknesses are wavelength comparable. The method uses the statistics of the THz E-field at subwavelength gaps to lock into each layer position and then uses a time-gated spectral kurtosis to tune to highest spectral contrast of the content on that specific layer. To demonstrate, occluding textual content was successfully extracted from a sample similar to a closed book down to nine pages without human supervision. The method provides over an order of magnitude enhancement in the signal contrast and can impact inspection of structural defects in wooden objects, plastic components, composites, drugs, and especially cultural artifacts with subwavelength or wavelength comparable layers.",,"['terrestrial-sensing', 'camera-culture']",,3567,--Choose Location,2016-10-24T19:44:56.028Z,"['raskar@media.mit.edu', 'barmak@media.mit.edu', 'redosan@media.mit.edu', 'aghasi@media.mit.edu']",False,barmaks-untitled-project-2,2015-09-01,[],Reading through Closed Books: THz Time-Gated Spectral Imaging for Content Extraction through Layered Structures,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:08.577Z,"Creative and productive information interchange in organizations is often stymied by a perverse incentive setting among the members. We transform that competition into a positive exchange by using market principles. Specifically, we apply innovative market mechanisms to construct incentives while still encouraging pro-social behaviors. Barter includes means to enhance knowledge sharing, innovation creation, and productivity. Barter provides managerial capability by using economic tools to stimulate activities and modify behaviors. We will measure the results and test the effectiveness of an information market in addressing organizational challenges. We are learning that transactions in rich markets can become an organizing principle among people, potentially as strong as social networks.",2012-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/001_07.png,3599,E15-483,2016-12-05T00:16:14.223Z,['lip@media.mit.edu'],True,barter-a-market-incented-wisdom-exchange,2009-01-01,[],Barter: A Market-Incented Wisdom Exchange,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:08:08.837Z,"This project developed efficient versions of Bayesian techniques for a variety of inference problems, including curve fitting, mixture-density estimation, principal-components analysis (PCA), automatic relevance determination, and spectral analysis. One of the surprising methods that resulted is a new Bayesian spectral analysis tool for nonstationary and unevenly sampled signals, such as electrocardiogram (EKG) signals, where there is a sample with each new (irregularly spaced) R wave. The new method outperforms other methods such as Burg, Music, and Welch, and compares favorably to the multitaper method without requiring any windowing. The ability to use unevenly spaced data helps avoid problems with aliasing.  The method runs in real time on either evenly or unevenly sampled data.",2005-01-01,['affective-computing'],,3615,E15-001,2016-12-05T00:16:14.256Z,['picard@media.mit.edu'],True,bayesian-spectral-estimation,1996-01-01,[],Bayesian Spectral Estimation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:09.136Z,"The Smart Cities and Cognitive Machines groups have teamed up with the Center for Future Banking to design a concept banking store in the Boston/Cambridge area. This will be a fully functional banking center that simultaneously serves as a living laboratory–a place where new technologies and interior configurations can quickly be installed, electronically monitored (unobtrusively, and with due concern for privacy) to evaluate their effectiveness in use under demanding real-world conditions, and iteratively redesigned in response to this feedback. Utilizing the Media Lab's extensive expertise in sensing, data collection, management and analysis of large-scale datasets, and data visualization, we will be able to create an adaptive environment that embodies a robotic cognitive architecture capable of intelligently responding to the occupants and visitors to the building. Architecturally, the flagship should vividly represent commitments to effective engagement with the community that it serves, sustainability, and forward-looking innovation.",2009-01-01,"['social-machines', 'smart-cities']",,3631,E15-001,2018-03-29T17:09:43.769Z,"['dkroy@media.mit.edu', 'rchin@media.mit.edu']",True,beacon-concept-store-with-the-center-for-future-banking,2008-09-01,[],Beacon Concept Store with the Center for Future Banking,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:09.327Z,"Beast is an organic-like entity created synthetically by the incorporation of physical parameters into digital form-generation protocols. A single continuous surface, acting both as structure and as skin, is locally modulated for both structural support and corporeal aid. Beast combines structural, environmental, and corporeal performance by adapting its thickness, pattern density, stiffness, flexibility, and translucency to load, curvature, and skin-pressured areas respectively.",2014-05-31,['mediated-matter'],,3639,,2021-05-21T13:24:53.645Z,['neri@media.mit.edu'],True,beast,2010-01-01,"['design', 'art', 'biology', 'materials', 'covid19']",Beast,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:08:09.493Z,"The Beat Browser is a music and sound browser designed to give users fast feedback while browsing arbitrarily large collections of audio. The goal of Beat Browser is to give the user a sense of exploring �live� and continuous audio, while rapidly moving between sources by mouse. The primary design goal is to facilitate rapid browsing of music by genre, artist, and album in order to interactively build playlists. Beat Browser is a product of larger design strategies identified to build efficient browsing engines, applicable beyond sound.",2007-01-01,['living-mobile'],,3647,E15-368,2016-12-05T00:16:14.375Z,['geek@media.mit.edu'],True,beat-browser-browsing-audio-libraries-and-playlist-construction,2007-01-01,[],Beat Browser: Browsing Audio Libraries and Playlist Construction,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:09.670Z,"The Behavior Expression Animation Toolkit (BEAT) allows animators to input typed text that they wish to be spoken by an animated human figure, and to obtain as output appropriate and synchronized nonverbal behaviors and synthesized speech in a form that can be sent to a number of different animation systems. The nonverbal behaviors are assigned on the basis of actual linguistic and contextual analysis of the typed text, relying on rules derived from extensive research into human conversational behavior.",2003-01-01,[],,3655,E15-320,2016-12-05T00:17:07.494Z,['bickmore@media.mit.edu'],True,beat-the-behavior-expression-animation-toolkit,2000-01-01,[],BEAT: the Behavior Expression Animation Toolkit,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:09.833Z,"The Beatbugs are handheld rhythm instruments that allow creation, manipulation, and sharing of rhythmic motives through a simple interface. When multiple Beatbugs are connected in a network, players can share and develop each others' motives to form larger-scale compositions. In ""Nerve"" (a composition written for the Beatbugs in the framework of the Toy Symphony), players can choose between manipulating their peers' motives and entering their own, creating an evolving musical system driven by the interaction between the players' choices and the system's stochastic algorithm.",2005-09-01,['opera-of-the-future'],,3663,--Choose Location,2016-12-13T19:41:59.045Z,['tod@media.mit.edu'],True,beatbugs,2001-01-01,[],Beatbugs,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:10.001Z,"The standard model of of human behavior is a model of rational actors. This model is widely accepted by individuals, policy makers, and businesses. In this work we are attempting to challenge some of the basic assumptions of economics and rationality. The primary focus of the project is consumer preference, trying to understand better the forces that define the demand in the marketplace.",2007-01-01,['erationality'],,3671,E15-314,2016-12-05T00:16:14.349Z,[],True,behavioral-economics,2004-01-01,[],Behavioral Economics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:08:10.153Z,"Integrating information processing and networking technologies into kinetic sculpture creates new opportunities for exploring the aesthetics of movement and interaction. These technologies enable the creation of sculpture that can receive, store, modify, and transmit information and make possible a new type of work: computational abstractions of living biological systems.",2001-12-30,['aesthetics-computation'],,3679,E15-443,2016-12-05T00:17:07.520Z,[],True,behavioral-kinetic-sculpture,2000-12-31,[],Behavioral Kinetic Sculpture,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:08:10.296Z,"Sculptural artifacts that model and reveal the embedded history of human thought and scientific principles hidden inside banal digital technologies. These artifacts provide alternative ways to engage and understand the deepest interior of our everyday devices, below the circuit, below the chip. They build a sense of the machines within the machine, the material, the grit of computation.",2015-09-01,[],https://dam-prod2.media.mit.edu/x/files/Display/movement.jpg,3687,--Choose Location,2016-12-05T00:17:01.746Z,"['slavin@media.mit.edu', 'tjlevy@media.mit.edu']",True,beneath-the-chip,2014-01-01,[],beneath the chip,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:08:10.545Z,"Instead of ordering produce or groceries through the Web, is there a radically different future built on distributed food production? Working with advanced hydroponic technology in the kitchen, we are inventing and exploring methods for bringing efficient microfarming directly into the home. The thesis here is that networked home gardening can enable greater engagement, empowerment, and knowledge about the foods we eat. An essential enabler for this is a new home web of intelligent appliances�for instance, radically improved tools for heating and cooling food. The refrigerator generally uses more electricity than any other appliance in the home. We are devising ways to make these and other appliances much more efficient by tracking patterns of use in homes and dorms and using fuzzy logic algorithms to modify appliance behavior. Likewise, the stove can be one of the most dangerous appliances in the home, and most stoves allow only the crudest control over temperature. By looking at the fundamental relationships between the food and the pot, the pot and the stove, and the users, we will invent safer, easier, more accurate and enjoyable ways to cook.",1999-12-30,"['counter-intelligence', 'personal-information-architecture']",,3703,,2016-12-05T00:16:14.326Z,['mike@media.mit.edu'],True,better-home-freezing-cooking-and-farming,1998-12-31,[],"Better Home Freezing, Cooking, and Farming",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:10.786Z,"We propose a system that reveals hidden physical properties of objects and the environment, and that reveals objects and aspects of the environment via their hidden physical properties.  The display of these revealed properties should be coincident with their sources, both in space and in time.  This coincidence serves to simplify the mapping of property to source, and to facilitate collaborative evaluation of these properties.  The display should also be a simultaneous field rather than a sequentially revealed series of points, so that adjacent entities may be instantly compared.  To demonstrate the strengths of such a system, we will build and evaluate a thermographic projector.",2007-09-01,['tangible-media'],,3719,E15-331,2016-12-05T00:16:14.582Z,['ishii@media.mit.edu'],True,better-thermal-understanding,2007-01-01,[],Better Thermal Understanding,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:10.902Z,"A wind-up energy prototype that utilizes an ergonomic, resonant mechanism to efficiently harvest human power. In the developed world, the Bettery can serve to power consumer electronics. In the developing world where power is scarce, the Bettery can power low-cost computing such as the Pengachu Linux Server.",2003-01-01,"['personal-fabrication', 'physics-and-media', 'molecular-machines']",,3727,E15-015,2020-10-22T18:13:26.942Z,"['jacobson@media.mit.edu', 'neilg@media.mit.edu']",True,bettery,2002-01-01,[],Bettery,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:11.021Z,"Between the Bars is a blogging platform for one out of every 142 Americans�prisoners�that makes it easy to blog using standard postal mail. It consists of software tools that make it easy to upload PDF scans of letters, crowd-sourced transcriptions of the scanned images. Between the Bars includes the usual full-featured blogging tools including comments, tagging, RSS feeds, and notifications for friends and family when new posts are available.",2013-09-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/btb.png,3735,--Choose Location,2016-12-05T00:16:14.539Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,between-the-bars,2011-01-01,[],Between the Bars,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:11.201Z,"We developed new computational tools that allowed children to create their own scientific instruments�enabling today's children, like scientists of earlier eras, to become engaged in scientific inquiry not only through observing and measuring, but also through designing and building. We found that when students build their own scientific instruments, they not only become more motivated in science activities, but also develop critical capacities in evaluating scientific measurements and knowledge, make stronger connections to the scientific concepts underlying their investigations, and develop deeper understandings of the relationship between science and technology.",2005-09-01,['lifelong-kindergarten'],,3751,E15-001,2016-12-05T00:16:14.637Z,"['mres@media.mit.edu', 'calla@media.mit.edu', 'bss@media.mit.edu']",True,beyond-black-boxes,1996-01-01,[],Beyond Black Boxes,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:08:11.400Z,This concept gallery shows the chain of startups and ideas that will follow after the emergence of self-driving cars.,,['camera-culture'],,3767,--Choose Location,2016-12-05T00:16:14.777Z,"['raskar@media.mit.edu', 'barmak@media.mit.edu']",True,beyond-the-self-driving-car,2016-01-01,[],Beyond the Self-Driving Car ,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:08:11.531Z,"Beyond is a collapsible input device for direct 3D manipulation. When pressed against a screen, Beyond collapses in the physical world and extends into the digital space of the screen, so that users have an illusion that they are inserting the tool into the virtual space. Beyond allows users to interact directly with 3D media without having to wear special glasses, avoiding inconsistencies of input and output. Users can select, draw, and sculpt in 3D virtual space, and seamlessly transition between 2D and 3D manipulation.",2011-01-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/beyond.png,3775,--Choose Location,2016-12-05T00:16:07.044Z,['ishii@media.mit.edu'],True,beyond-a-collapsible-input-device-for-3d-direct-manipulation,2009-09-01,[],Beyond: A Collapsible Input Device for 3D Direct Manipulation,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:11.686Z,"Bibliodoptera is an installation commissioned for the MIT 150th Anniversary Celebration FAST Festival of Art, Science, and Technology. A cloud of vellum butterflies, newly emerged from the chrysalis of MIT�s diverse library pages, floats above in the corridor between the Lewis and Hayden Libraries on MIT�s campus. Trajectories through the cloud illuminate to guide passersby along the length of the corridor. This installation is an unobtrusive but strikingly beautiful symbol of the guiding knowledge of the arts and humanities that have been developed and pursued at MIT over the last 150 years. The butterflies, printed with text from books, sheet music and pages of MIT theses, are interactively illuminated by small lights from within.",2011-01-01,['opera-of-the-future'],,3783,,2016-12-05T00:16:14.804Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'ejessop@media.mit.edu']",True,bibliodoptera,2011-01-01,[],Bibliodoptera,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:11.978Z,"The BiDi Screen is an example of a new type of thin I/O device that possesses the ability both to capture images and display them. Scene depth can be derived from BiDi Screen imagery, allowing for 3D gestural and 2D multi-touch interfaces. This bidirectional screen extends the latest trend in LCD devices, which has seen the incorporation of photo-transistors into every display pixel. Using a novel optical masking technique developed at the Media Lab, the BiDi Screen can capture light field-like quantities, unlocking a wide array of applications from 3D gesture and touch interaction with CE devices, to seamless video communication.",2012-01-01,"['information-ecology', 'camera-culture']",https://dam-prod2.media.mit.edu/x/files/Display/logo.png,3799,E15-301,2016-12-05T00:16:14.823Z,"['holtzman@media.mit.edu', 'raskar@media.mit.edu']",True,bidi-screen,2008-09-01,[],BiDi Screen,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:12.163Z,"Big Data for Small Places is a quantitative study of the qualities that define our neighborhoods and our collective role in the production of local places over time. We are translating the potentials of big data from the scale of the city to the scale of the urban block, the scale at which we physically experience urban space, to gain a better understanding of the local patterns and social spaces that aggregate to form metropolitan identity. We hope that this study will improve our collective understanding of the urban environments we shape and the stories they generate, that it will allow us to more sensitively test and implement real change in our shared public realm and support the invisible narratives it generates.",2017-05-01,['social-computing'],,3807,--Choose Location,2022-06-10T14:40:01.981Z,"['sdkamvar@media.mit.edu', 'srife@media.mit.edu', 'zhangjia@media.mit.edu', 'saquib@media.mit.edu', 'cjaffe@media.mit.edu', 'echristo@media.mit.edu']",True,big-data-for-small-places,2015-01-01,[],Big Data for Small Places,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:12.339Z,"The Big Thing is a massive, child-oriented music construction kit that allows multiple children to experiment with creating musical structures, and then to manipulate and to perform with them. Big Things are made up of a two-tiered, functional base about three feet in diameter, which can be turned and twisted using sensing elastic chords. On top of such a base, children use a collection of hand-sized, soft, sensing objects, with a variety of functions: chunks each contain a musical fragment or parameter description; connectors establish functional and time-based relationships between chunks; poking areas allow musical data to be created within each chunk; sensors can be attached to the Big Thing at will, allowing body gesture or touch to control the entire Big Thing. Up to twelve Big Thing ""islands"" can be interconnected at a time, forming a stage-sized musical sculpture that invites group creative collaboration, and demonstrates through its physical form the basic principles of musical expression.",2000-12-30,"['toys-of-tomorrow', 'opera-of-the-future']",,3815,E15-483,2016-12-05T00:16:14.700Z,['tod@media.mit.edu'],True,big-thing,1999-12-31,[],Big Thing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:12.529Z,"BigBarChart is an immersive, 3D bar chart that provides a new physical way for people to interact with data. It takes data beyond visualizations to map out a new area--data experiences--that are multisensory, embodied, and aesthetic interactions. BigBarChart is made up of a number of bars that extend up to 10 feet to create an immersive experience. Bars change height and color in response to interactions that are direct (a person entering the room), tangible (pushing down on a bar to get meta information), or digital (controlling bars and performing statistical analyses through a tablet). BigBarChart helps both scientists and the general public understand information from a new perspective. Early prototypes are available.",2019-08-05,['object-based-media'],https://dam-prod2.media.mit.edu/x/files/Display/BigBarChart.jpeg,3823,--Choose Location,2020-06-10T15:46:44.767Z,"['vmb@media.mit.edu', 'perovich@media.mit.edu']",True,bigbarchart,2013-01-01,[],BigBarChart,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:08:13.018Z,"Bin-ary is a self-contained gas detector that analyzes organic trash odor compounds and releases a subtle burst of scent when bad odor is detected. The prototype is meant to be used as a plugin to make trash bins and dumpsters smarter and prevent insalubrity in cities and villages. In order to detect the state of organic trash we primarily focused on the chemical compounds that are produced when food starts to rot or ripen: Hydrogen Sulfide (H2S), Ethylene (C2H4) and Carbon Dioxide (CO2).",2015-09-01,['fluid-interfaces'],,3847,--Choose Location,2016-12-05T00:17:07.567Z,"['pattie@media.mit.edu', 'amores@media.mit.edu']",True,bin-ary-detecting-the-state-of-organic-trash-to-prevent-insalubrity,2015-09-01,[],Bin-ary: Detecting the State of Organic Trash to Prevent Insalubrity,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:13.555Z,"Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to human sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity within a few seconds in response to environmental humidity gradients. Experimental characterization and mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.",2016-12-31,['tangible-media'],,3879,--Choose Location,2020-06-09T17:38:30.261Z,"['ishii@media.mit.edu', 'liningy@media.mit.edu', 'jifei@media.mit.edu', 'wwen@media.mit.edu', 'guanyun@media.mit.edu', 'oksana@media.mit.edu', 'steinerh@media.mit.edu', 'chinyich@media.mit.edu']",True,biologic,2014-01-01,"['human-computer-interaction', 'fashion', 'sensors', 'biology', 'biomechanics', 'physiology']",bioLogic—Science Advances,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:13.679Z,"Nothing triggers memories like smell.  Momentary, fleeting, and at times unexpected, one scent can conjure up the warmth of a grandparent, or the heat of a first kiss.   Certain botanicals are known for their olfactive properties.  Evolved to seduce pollinators and to proliferate the plant's own genes, the fragrance of flowers have also become entangled in our human dance of seduction.  In the art of perfumery, we have long extracted the scents of flowers to apply to ourselves- what if we did the contrary and engineered a plant to emit the odor profile of a person instead?   Could we design new rituals for mourning, new biologies for remembering? This project is the speculative design of a plant that smells like a person who is emotionally significant to me, but has passed away.  Commercial agendas often drive the progress of certain trajectories of engineering.  This project explores the an alternative design of plants that is not driven nor thoroughly integrated in capitalist production.  Exploring emotions such as loneliness, isolation, and feelings of guilt and anxiety towards human impacts on the environment, the function of these inquiries is to reflect on past, current, and future trajectories of human influences on plant life.",2017-08-31,['design-fiction'],https://dam-prod2.media.mit.edu/x/2016/12/14/Ani Liu Experimental Art and Olfaction_Page_7_ihXmTh9.png,3887,--Choose Location,2017-10-11T20:31:21.574Z,"['sputniko@media.mit.edu', 'wonder@media.mit.edu']",True,biome-botany-for-sensorial-memory-2,2016-01-01,"['design', 'art', 'bioengineering']",Forget Me Not: The Botany of Desire & Loss,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:08:13.989Z,"Using biologically inspired design principles, a biomimetic robotic knee prosthesis is proposed that uses a clutchable series-elastic actuator. In this design, a clutch is placed in parallel to a combined motor and spring. This architecture permits the mechanism to provide biomimetic walking dynamics while requiring minimal electromechanical energy from the prosthesis. The overarching goal for this project is to design a new generation of robotic knee prostheses capable of generating significant energy during level-ground walking, that can be stored in a battery and used to power a robotic ankle prosthesis and other net-positive locomotion modes (e.g., stair ascent).",,['biomechatronics'],,3911,E15-054,2021-09-17T19:24:29.691Z,"['hherr@media.mit.edu', 'lmooney@media.mit.edu', 'mcarney@media.mit.edu']",True,biomimetic-active-prosthesis-for-above-knee-amputees,2007-01-01,[],Biomimetic active prosthesis for above-knee amputees,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:14.085Z,"BioMod is an integrated interface for users of mobile and wearable devices, monitoring various physiological signals such as the electrocardiogram, with the intention of providing useful and comfortable feedback about medically important information. The first version of this system includes new software for monitoring stress and its impact on heart functioning, and the ability to wirelessly communicate this information over a Motorola cell phone. One application under development is the monitoring of stress in patients who desire to stop smoking: the system will alert an ""on-call"" trained behavior-change assistant when the smoker is exhibiting physiological patterns indicative of stress or likely relapse, offering an opportunity for encouraging intervention at a point of weakness. Challenges in this project include the development of an interface that is easy and efficient to use on the go, is sensitive to user feelings about the nature of the information being communicated, and accurately recognizes the patterns of physiological signals related to the conditions of interest.",2004-09-01,['affective-computing'],,3919,E15-001,2016-12-05T00:16:15.219Z,"['picard@media.mit.edu', 'amohan@media.mit.edu']",True,biomod,2002-01-01,[],BioMod,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:14.276Z,"A microworld for learning about ecology: every action has a consequence! Whether planning a mission to Mars or imagining improvements to their own environments, kids can change conditions inside the Biosphera and observe perturbations in the system. The microworld enables changes to environmental parameters such as light, humidity, temperature, chemical conditions, and populations of micro-organisms. A computer interface works with sensors and actuators to control the physical world of the Biosphera. This interplay of perceptual accessibility and virtual representations supports experimentation and learning about biology, chemistry, and dynamic systems.",2004-01-01,[],,3935,MLE,2016-12-05T00:16:15.442Z,[],True,biosphera,2001-01-01,[],Biosphera,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:08:14.461Z,"We are developing a control architecture for bipedal locomotion devices such as robots and powered orthotics. Advanced nonlinear control techniques, including feedback linearization, sliding control, and multivariable optimization, are utilized in this control architecture, yielding a highly stable and tunable controller for a highly unstable and nonlinear plant. Tests�using a 3-D, 12-degree-of-freedom humanoid model�include a variety of disturbed initial states and of control goals for the center of mass, swing foot, and other points being controlled. An interesting property of this controller is the emergence of appropriate non-contact limb behavior in response to disturbances. Also, due to its large range of operation, this control architecture can reject significant disturbances more easily than simpler controllers, and requires a less-detailed reference trajectory than simpler controllers. This has the additional benefit of reducing the computational workload of a motion planner in an integrated motion planning and control system. Such control architectures will find use in assistive devices for the elderly and handicapped.",2009-09-01,['biomechatronics'],,3951,E15-054,2016-12-05T00:17:07.741Z,['hherr@media.mit.edu'],True,bipedal-balancing-using-integrated-movement-of-non-contact-limbs,2004-01-01,[],Bipedal Balancing Using Integrated Movement of Non-Contact Limbs,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:14.543Z,"A collage of face pictures provides a bird's-eye-view over the Media Lab community, displayed on a public touch-screen. Contrary to hierarchical directories, this collage provides one-click access to personal information for anyone in the community, lending itself more towards browsing than specific  searching. We connect to individual Twitter accounts and mobile phone numbers, allowing the visitor to view a person's status and place a call to them. We also highlight the people that happened to walk by the screen up to five minutes ago by detecting their RFID tags.",2011-01-01,['information-ecology'],,3959,,2016-12-05T00:16:15.323Z,"['holtzman@media.mit.edu', 'ypod@media.mit.edu']",True,birds-eye-view,2010-01-01,[],Bird's-Eye-View,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:14.621Z,"Millions of humans share their homes with pet parrots. Parrots are popular pets not only because of their intelligence, but also because of their ability to communicate vocally with their owners. But this vocal ability evolved in the wild for interparrot communication (i.e., for maintaining connections with other members of its flock). This connection with the flock is crucial to survival: a lone parrot cannot eat and search for predators simultaneously. What happens when a parrot, brought into a human home, adopts its human family as its flock and its flock members go to school, to work, or to do chores? Many times birds left alone engage in loud raucous calling that the owners--or neighbors--find unacceptable. Often parrots are given up for adoption because the owners cannot find ways of bringing this calling down to an acceptable decibel level. Our device is an attempt to engage the parrot and shape its behavior through positive rewards and mildly negative experiences that are completely under its own control.",2001-12-30,"['toys-of-tomorrow', 'pet-projects']",,3967,E15-358,2016-12-05T00:16:15.251Z,['impepper@media.mit.edu'],True,birdsitter,1999-12-31,[],BirdSitter,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:14.705Z,"The Bishop project explores the subtleties of human language when talking about spatial scenes. In particular, we investigate the various descriptive strategies human speakers employ in talking about objects in relation to other objects. These strategies include ordering objects, visually grouping them, describing their spatial relation or even referring back to objects that used to be in the scene. Furthermore, human subjects frequently perform combinations of these strategies, for example ""the green one to the left of the three purple ones."" We are building a computational system that replicates both the individual phenomena and their  compositional behavior. As a result, this system understands relatively complex expressions referring to a scene of objects and can indicate the object being described. This work has direct applications for understanding for natural-language user interfaces, especially in augmenting direct-manipulation interfaces with intelligent speech control. A good example is speech interfaces for GPS map devices in cars where users speak about objects on the map.",2003-09-01,['social-machines'],,3975,E15-483,2016-12-05T00:17:07.666Z,['dkroy@media.mit.edu'],True,bishop-understanding-complex-spatial-language,2003-01-01,[],Bishop: Understanding Complex Spatial Language,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:15.055Z,"Blendie is an interactive, sensitive, intelligent, voice-controlled blender with a mind of its own. Materials are a 1950s Osterizer blender altered with custom-made hardware and software for sound analysis and motor control. Machines influence self-conception, expression, social perception, and perception of responsibility or action. By accessing and vitalizing the interplay of people and machines through custom interaction design and psychotherapeutic techniques, a social awareness is brought out and individuals are invited to reinvent their own existence.",2007-01-01,['computing-culture'],,4015,E15-020D,2016-12-05T00:16:15.777Z,"['csik@media.mit.edu', 'monster@media.mit.edu']",True,blendie,2003-09-01,[],Blendie,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:08:15.201Z,"Fluorescence lifetime imaging is a significant bio-imaging tool that finds important applications in life-sciences. Widely known applications include cancer detection and DNA sequencing. To that end, fluorescence microscopy which is at the heart of bio-imaging is an electronically and optically sophisticated device which is prohibitively expensive. Our work is demonstrates the fluorescence microscopy like functionality can be achieved by a simple, consumer sensor such as the Microsoft Kinect which costs about $100. This is done by trading-off the precision in optics and electronics for sophistication in computational methods. Not only this allows for massive cost reduction but leads to several advances in the area. For example, our method is calibration-free in that we do not assume sample's relative placement with respect to the sensor. Furthermore, our work opens new pathways of interaction between bio-imaging, optics and computer vision communities.",,['camera-culture'],,4031,--Choose Location,2019-04-19T17:48:42.345Z,"['raskar@media.mit.edu', 'ayush@media.mit.edu']",True,blind-and-reference-free-fluorescence-lifetime-estimation-via-consumer-time-of-flight-sensors,2015-01-01,[],Blind and reference-free fluorescence lifetime estimation via consumer time-of-flight sensors,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:15.281Z,"BlitzScribe is a new approach to speech transcription driven by the demands of today's massive multimedia corpora. High-quality annotations are essential for indexing and analyzing many multimedia datasets; in particular, our study of language development for the Human Speechome Project depends on speech transcripts. Unfortunately, automatic speech transcription is inadequate for many natural speech recordings, and traditional approaches to manual transcription are extremely labor intensive and expensive. BlitzScribe uses a semi-automatic approach, combining human and machine efforts to dramatically improve transcription speed. Automatic methods identify and segment speech in dense, multitrack audio recordings, allowing us to build streamlined user interfaces maximizing human productivity. The first version of BlitzScribe is already about 4-6 times faster than existing systems. We are exploring user-interface design, machine-learning and pattern-recognition techniques to build a human-machine collaborative system that will make massive transcription tasks feasible and affordable.",2014-01-01,['social-machines'],,4039,E15-441,2016-12-05T00:17:07.803Z,['dkroy@media.mit.edu'],True,blitzscribe-speech-transcription-for-the-human-speechome-project,2007-01-01,[],BlitzScribe: Speech Transcription for the Human Speechome Project,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:15.355Z,"Block Exchange is a website where Scratch users can share data sets and data sources in the form of Scratch programming blocks. For example, a soccer enthusiast can share a block that retrieves data about the number of goals scored by different players in the English Premier League. Other blocks in the Exchange can retrieve information from web-based dynamic data sources. For example, a Scratch user can share a block that dynamically retrieves meta-data on the books that are currently on the New York Times bestsellers list. With Block Exchange, you can create a large variety of projects, ranging from interactive data visualizations to stories and animations which incorporate information from the real world dynamically and in real-time.",2011-09-01,['lifelong-kindergarten'],,4047,,2016-12-05T00:16:16.084Z,"['mres@media.mit.edu', 'jmaloney@media.mit.edu', 'sdg1@media.mit.edu']",True,block-exchange,2010-09-01,[],Block Exchange,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:15.433Z,"""Weblogs"" are a method of distributing personal news, essentially an individual's log of activities, news, and thoughts presented in a public manner on the Web. As a publishing medium, Weblogs are ultimately democratic, often as timely as traditional news sources, and have a potential distribution much greater than print media. One problem with these personal information sources is the inability to find an audience. Blogdex is a system built to harness the power of personal news, amalgamating and organizing personal news content into one navigable source, moving this democratic media to a wider audience.",2005-09-01,['electronic-publishing'],,4055,E15-320,2016-12-05T00:16:15.822Z,['walter@media.mit.edu'],True,blogdex,2001-01-01,[],Blogdex,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:08:15.742Z,"BodyChat is a prototype of a graphical chat system that allows users to communicate via text while their avatars automatically animate attention, salutations, turn-taking, back-channel feedback, and facial expression, according to the rules of conversational behavior.",1999-12-30,['gesture-and-narrative-language'],,4087,E15-320,2016-12-05T00:16:16.113Z,[],True,bodychat,1995-12-31,[],BodyChat,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:15.825Z,"With over a billion people carrying camera-phones worldwide, we have a new opportunity to upgrade the classic bar code to encourage a flexible interface between the machine world and the human world. Current bar codes must be read within a short range, and the codes occupy valuable space on products. We present a new, low-cost, passive optical design so that bar codes can be shrunk to fewer than 3mm and can be read by unmodified ordinary cameras several meters away.",2012-01-01,['camera-culture'],,4095,E15-320,2017-04-03T17:47:50.500Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,bokode-imperceptible-visual-tags-for-camera-based-interaction-from-a-distance,2008-01-01,"['computer-vision', 'sensors', 'imaging', 'visualization']",Bokode: Imperceptible Visual Tags for Camera-Based Interaction from a Distance,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:16.004Z,"Bosu is a design tool offering kinetic memory�the ability to record and play back motion in 3-D space�for soft materials. It is used for motion prototyping and digitally augmented form finding, combining dynamic modeling with coincident sensing and actuation to create transformable structures. The system consists of varying modular units of bend sensors paired with shape memory alloy (nitinol) actuators woven into a bendable plastic frame and embedded in fabric. Each module can actuate between two positions and together form three-dimensional motion pixels.",2009-01-01,['tangible-media'],,4111,E15-344,2016-12-05T00:16:16.138Z,['ishii@media.mit.edu'],True,bosu,2008-09-01,[],Bosu,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:16.145Z,"The Biblical story of the Tower of Babel involved a deliberate plan hatched by mankind to construct a platform from which man could fight God. The tower represented the first documented attempt at constructing a vertical city. The divine response to the master plan was to sever communication by instilling a different language in each builder. Tragically, the building's ultimate destruction came about through the breakdown of communications between its fabricators. In this installation we redeem the Tower of Babel by creating its antithesis. We will construct a virtuous, decentralized, yet highly communicative building environment of cable-suspended fabrication bots that together build structures bigger than themselves. We explore themes of asynchronous motion, multi-nodal fabrication, lightweight additive manufacturing, and the emergence of form through fabrication. (With contributions from Carlos Gonzalez Uribe and Dr. James Weaver (WYSS Institute and Harvard University))",2016-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/IMG_2247.JPG,4119,--Choose Location,2021-05-21T13:19:54.454Z,"['jlaucks@media.mit.edu', 'm_kayser@media.mit.edu', 'dumo@media.mit.edu', 'j_duro@media.mit.edu', 'neri@media.mit.edu']",True,bots-of-babel,2013-01-01,[],Bots of Babel,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:08:16.292Z,"We have added inexpensive, low-power, wireless sensors to product packages to detect user interactions with products. Thus, a bottle can register when and how often its contents are dispensed (and generate side effects, like causing a music player to play music when the bottle is picked up, or generating an automatic refill order when near-emptiness is detected). A box can understand usage patterns of its contents. Consumers can vote for their favorites among several alternatives simply by handling them more often. ",2016-01-13,['object-based-media'],,4127,--Choose Location,2020-01-13T19:09:37.717Z,"['vmb@media.mit.edu', 'novysan@media.mit.edu']",True,bottlesboxes-packaging-with-sensors,2013-09-01,[],Bottles and Boxes: Packaging with sensors,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:16.437Z,"Glass bottles have been a part of human culture for thousands of years, serving both practical and aesthetic functions. The ""bottles"" project explores the transparency of an interface that weaves itself into the fabric of everyday life. Seamless extension of physical affordances and metaphors into the digital domain is a key principle for the design. The ""bottlogues"" piece explores narrative contents for our bottles. A set of three bottles is filled with a story told by three characters. Upon opening each bottle, the man, eagle, and stag start telling their part of the narrative.  Physical manipulation of the bottles - opening and closing - is the primary mode of interaction with digital contents. This project grew out of the ""musicBottles"" project to explore a wider variety of contents as well as both artistic and practical applications of the idea: bottles as containers for bits.",2001-12-30,['tangible-media'],,4135,E15-441,2016-12-05T00:16:16.348Z,['ishii@media.mit.edu'],True,bottlogues,1999-12-31,[],bottlogues,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:16.559Z,"How do we leverage people to make systems more intelligent, efficient, and successful? Is it worthwhile to involve people heavily in the goals of a system? How does a system most effectively coax stories out of people? To investigate these questions, a robot was built that facilitated interaction and documentary gathering within an ubiquitous media framework. We then let the robot roam freely, with the goal of capturing stories about its environment. This was done by leveraging human mobility and intelligence, as the robot relied upon people to move long distances and achieve its goals. The end products were a study of how people related to a robot asking for assistance and interaction in various ways, and a set of movies showing the robot navigating the resulting ""thread"" of a narrative.",2012-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/DSC_0195.JPG,4143,,2016-12-05T00:17:07.879Z,['joep@media.mit.edu'],True,boxie-the-robot-interactive-physical-agents-for-story-gathering,2010-01-01,"['robotics', 'human-computer-interaction', 'social-robotics', 'engineering']",Boxie the Robot: Interactive Physical Agents for Story Gathering,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:16.676Z,"Designed as a platform to enable rich contextual data collection in real homes, BoxLab uses a broad array of wireless sensing devices to study responsive applications situated in natural home settings. BoxLab has been deployed in homes around the Boston area, and has generated a dataset containing over 10,000 hours of sensor data to be used as training libraries for computational activity recognition and other applications of artificial intelligence. BoxLab also enables rapid deployment of context-triggered applications that allow systems to respond to occupant activities in real time.",2016-12-01,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/13/Kiosk23.jpg,4151,1CC-4th Floor,2018-05-04T18:19:26.740Z,['kll@media.mit.edu'],True,boxlab,2008-01-01,[],BoxLab,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:16.817Z,"The Boycott Toolkit provides tools for consumers to organize collective economic action. Users can learn about the politics behind everyday companies and products, pledge to join a campaign, and share information with their friends through social networks.",2010-01-01,['computing-culture'],,4159,,2022-06-10T13:40:30.254Z,['csik@media.mit.edu'],True,boycott-toolkit,2009-09-01,[],Boycott Toolkit,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:16.937Z,"We are developing a multimodal interface for hand rehabilitation following stroke. EMG forearm sensors read attempted finger presses in disordered limbs, and serve as an input to an expressive feedback interface.  Auditory, visual, and tactile cues are presented to support rehabilitation of the representation of finger movements across sensory domains. The multisensory feedback is embedded in a rich task, situated between piano learning and expressive music performance.  A user of this system will rehabilitate finger movement while developing an expressive music performance. Imagine a complete shift in the form and function of rehabilitation, towards something empowering, where individuals strive in tandem with tailored interfaces, mapped to push them forward at each step, and as part of fundamentally enriching expressive tasks. Our rehabilitative health care environments can sculpt our minds, while changing our lives, if we invent the right tools.",2010-09-01,['opera-of-the-future'],,4167,--Choose Location,2016-12-05T00:17:07.897Z,['tod@media.mit.edu'],True,brain-instrument-interfaces,2009-09-01,[],Brain Instrument Interfaces,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:17.041Z,"The Brain Opera is an attempt to bring expression and creativity to everyone, in public or at home, by combining an exceptionally large number of interactive modes into a single, coherent experience. The project connects a series of hyperinstruments designed for the general public with a performance and a series of real-time music activities on the Internet. Audiences explore the hands-on instruments (i.e., Harmonic Driving, Melody Easel, Gesture Wall, Rhythm Tree, Speaking and Signing Trees, etc.) as preparation for the performance, creating personal music that makes each performance unique. The Brain Opera attempts to redefine the nature of collective interaction in public places, as well as to explore the possibilities of expressive objects and environments for the workplace and home. Since its 1996 premiere and world tour through 1998 (United States, Europe, Asia, and South America), much has been learned about interactive software and interfaces, Internet music systems, and intelligent, evolving environments for creativity and expression. We are currently incorporating these new ideas, and designing significant upgrades and new features for the final version of the Brain Opera, which was permanently installed at the new House of Music in Vienna, Austria in July 2000.",2000-12-30,['opera-of-the-future'],,4175,E15-483,2016-12-05T00:16:16.384Z,['tod@media.mit.edu'],True,brain-opera-vienna,1997-12-31,[],Brain Opera Vienna,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:17.534Z,"Breathing Window is a tool for non-verbal dialogue that reflects on your own breathing while also offering a window on another person's respiration. This prototype is an example of shared human experiences (SHEs) crafted to improve the quality of human understanding and interactions. Our work on SHEs focuses on first encounters with strangers. We meet strangers every day, and without prior background knowledge of the individual we often form opinions based on prejudices and differences. In this work, we bring respiration to the foreground as one common experience of all living creatures.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/files/Display/cloud.jpg,4215,--Choose Location,2016-12-05T00:16:16.294Z,"['tod@media.mit.edu', 'rebklein@media.mit.edu']",True,breathing-window,2015-01-01,[],Breathing Window,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:17.616Z,"We created a computational construction kit for the blind and visually impaired by modifying the user interface of our Cricket programmable bricks and adding capabilities such as speech recognition and speech synthesis. We provided this ""Bricket"" system to a small population of visually-impaired children to use in their homes, and studied how and what the children create with this new technology.",2002-09-01,"['gray-matters', 'lifelong-kindergarten']",,4223,E15-020D,2016-12-05T00:16:16.517Z,"['mres@media.mit.edu', 'rahulb@media.mit.edu']",True,brickets,2001-01-01,[],Brickets,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:17.711Z,"Universal Serial Bus (USB) enables true plug-and-play for  peripherals. Simply plug a device in and the computer recognizes it and  downloads the drivers--and that's it. Oddly enough, while there are  millions of PC's out there with USB ports, many of the ports will never get  used, since there are few USB devices. To counter this, we wanted to make a  non-traditional object USB compatible, and chose a 16 million color LED  lamp that has no moving parts. USB has great potential for making toys easy  to hook up to a computer, and the lamp has great potential for changing the  way kids play with their toys by introducing entertainment lighting to the  home.",1999-12-30,['personal-information-architecture'],,4231,,2016-12-05T00:16:16.446Z,['mike@media.mit.edu'],True,bright-light,1998-12-31,[],Bright Light,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:17.795Z,"The relationship between breathing and self-reported stress is bidirectional. Respiration pattern is an indicator of stress, but it can also be manipulated to induce calmness. In this project we explore this relationship via novel means of interaction. BrightBeat is a set of seamless visual, auditory, and tactile interventions that mimic a calming breathing oscillation, with the aim of influencing physiological syncing and consequently bringing a sense of focus and calmness. The animation above shows an exaggerated version of BrightBeat. These interventions are designed to run easily on commonplace personal electronic devices, respect the user's privacy, and not to require constant focus or attention in order to be effective. ",,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/2016/10/14/experiment_mZN8GTF.gif,4239,--Choose Location,2019-04-19T14:59:31.588Z,"['picard@media.mit.edu', 'asma_gh@media.mit.edu']",True,brightbeat,2015-09-01,"['health', 'wearable-computing', 'affective-computing', 'technology', 'wellbeing']",BrightBeat: Effortlessly influencing breathing for cultivating calmness and focus,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:17.882Z,"People form dynamic groups focused on topics that emerge serendipitously during everyday life. They can be long-lived or of short duration. Examples include people interested in buying the same product, those with similar expertise, those in the same location, or any collection of such attributes. We call this the Human Discovery Protocol (HDP). Similar to how computers follow well-established protocols like DNS in order to find other computers that carry desired information, HDP presents an open protocol for people to announce bits of information about themselves, and have them aggregated and returned back in the form of a group of people that match against the user�s specified criteria. We are experimenting with a web-based implementation (brin.gy) that allows users to join and communicate with groups of people based on their location, profile information, and items they may want to buy or sell.",2013-01-01,"['information-ecology', 'viral-communications']",,4247,--Choose Location,2016-12-05T00:17:08Z,"['holtzman@media.mit.edu', 'ypod@media.mit.edu', 'lip@media.mit.edu']",True,bringy-what-brings-us-together,2011-09-01,[],Brin.gy: What Brings Us Together,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:17.971Z,"We are building a set of free, open-source web and mobile tools to support informal face-to-face meetings and organizing.  The first of these tools we've developed provides a lightweight solution for group brainstorming and decision making.",2011-01-01,['civic-media'],,4255,,2016-12-05T00:17:08.022Z,"['ethanz@media.mit.edu', 'borovoy@media.mit.edu']",True,brownbag-toolkit,2010-01-01,[],BrownBag Toolkit,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:18.213Z,"We have built a microfluidic logic family and architecture based on two-phase flow. In this scheme, presence or absence of a bubble or a droplet in a fluidic network represents a bit. Logic operations are executed based on direct or indirect bubble interactions. The system is sufficiently nonlinear to exhibit universal logic. The nonlinearity is derived from boundary conditions of two-phase flow. With the emergence  of very large-scale integrated microfluidic systems, there is an immediate need to build local flow control systems. Bubble Logic can be used for various applications including bubble display, combinatorial synthesis, and control architecture for lab-on-chip devices.",2007-01-01,['physics-and-media'],,4279,E15-023,2022-06-10T14:12:10.558Z,['neilg@media.mit.edu'],True,bubble-logic,2005-01-01,[],Bubble Logic,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:18.290Z,"The Buddy Compass, a watch-like device that points in the direction of the wearer's friend, explores an alternative user interface for location-specific information display. Developed on top of the Location Linked Information infrastructure, the Buddy Compass is a lightweight example that combines physical presence with virtual information.",2003-01-01,['smart-cities'],,4287,NE18-4FL,2016-12-05T00:17:08.050Z,[],True,buddy-compass,2003-01-01,[],Buddy Compass,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:18.366Z,"Build in Progress is a platform for sharing the story of your design process. With Build in Progress, makers document as they develop their design processes, incorporating iterations along the way and getting feedback as they develop their projects.",2016-08-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/bip_logo_hi_res.png,4295,--Choose Location,2016-12-05T00:16:17.917Z,"['mres@media.mit.edu', 'ttseng@media.mit.edu']",True,build-in-progress,2013-09-01,['makers'],Build in Progress,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:18.467Z,"This project helps a community tell its collective story through collaborative hypertext. Members of the community contribute many short anecdotes, and the system helps identify relationships of times, people, places, etc. The relationships are used to present the scattered memories of many people in a logical fashion and to elicit contributions of further memories. Our first prototype, Building 20 Memories, is specialized for the people who worked in the famous MIT Building 20.",2000-12-30,['software-agents'],,4303,,2016-12-05T00:17:08.071Z,['lieber@media.mit.edu'],True,building-20-memories-agent-assisted-hypertext-for-capturing-community-history,1998-12-31,[],Building 20 Memories: Agent-Assisted Hypertext for Capturing Community History,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:18.708Z,"MEMS have been built by patterning nanoparticle colloids of metals, insulators, and release materials. Nanoparticle colloids permit greater solubility and far-suppressed melting temperature compared to the bulk material, which allows liquid to be ""printed"" on plastic substrates and then sintered at plastic-compatible temperatures to form fully functional devices. Using two patterning techniques, ink-jet deposition and liquid embossing, we have created thermal actuators (heatuators), linear and rotary electrostatic motors, and mechanical test structures with characteristic feature sizes ranging from 200 nm to 20 mm.",2005-01-01,['molecular-machines'],,4327,E15-015,2016-12-05T00:16:07.111Z,['jacobson@media.mit.edu'],True,building-mems-with-nanoparticles,2000-01-01,[],Building MEMS with Nanoparticles,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:18.788Z,"GPS is transforming how we live globally, but not locally. To bring the  same capability for three-dimensional positioning indoors, we are  developing RF systems that can make picosecond timing measurements on  signals with a wavelength on the order of the size of a building, thereby  providing coupling without being perturbed by the internal structure.",2000-12-30,['physics-and-media'],,4335,E15-023,2016-12-05T00:17:08.104Z,['neilg@media.mit.edu'],True,building-positioning-system,1997-12-31,[],Building Positioning System,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:18.866Z,"With the LEGO Group and Hasbro, we looked at the emotional experience of playing with games and LEGO bricks. We measured participants' skin conductance as they learned to play with these new toys. By marking the stressful moments, we were able to see what moments in learning should be redesigned. Our findings suggest that framing is key: how can we help children recognize their achievements? We also saw how children are excited to take on new responsibilities but are then quickly discouraged when they aren't given the resources to succeed. Our hope for this work is that by using skin conductance sensors, we can help companies better understand the unique perspective of children and build experiences fit for them.",2014-12-31,['affective-computing'],,4343,--Choose Location,2022-05-12T17:15:56.059Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",True,building-the-just-right-challenge-in-games-and-toys,2013-01-01,[],Building the Just-Right-Challenge in Games and Toys,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:18.943Z,"Time-lapse photographs reveal how an observed physical object sustains itself in an environment. Captured as two-dimensional images over time, they reveal facets of how man-made artifacts, such as buildings, stand with and against natural wonders. We are capturing photographically Frank O. Gehry's  Stata Center on the MIT campus, exploring facets that reveal the interaction of the building with light and human encounters, as well as observing any characterization of the rhythm and flow of spaces that may resist a human story. From this data collection, we aim to incorporate technological means to extend attitude, so as to realize a building's expression of attitude.",2005-01-01,['media-fabrics'],,4351,E15-368,2016-12-05T00:16:16.661Z,['gid@media.mit.edu'],True,building-with-an-attitude,2004-09-01,[],Building with an Attitude,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:08:19.035Z,"We developed a new approach to computer programming, creating new languages with low thresholds but high ceilings, that enabled kids (and other novice programmers) to make a smooth transition from simple commands to complex programs. Using our LogoBlocks languages, kids created programs by snapping virtual LEGO bricks together on the screen.",2003-09-01,['lifelong-kindergarten'],,4359,E15-001,2016-12-05T00:16:16.678Z,"['mres@media.mit.edu', 'bss@media.mit.edu']",True,building-block-programming,1996-01-01,[],Building-Block Programming,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:19.137Z,"How can additive fabrication technologies be scaled to building-sized construction? We introduce a novel method of mobile swarm printing that allows small robotic agents to construct large structures. The robotic agents extrude a fast-curing material which doubles as both a concrete mold for structural walls and as a thermal insulation layer. This technique offers many benefits over traditional construction methods, such as speed, custom geometry, and cost. As well, direct integration of building utilities such as wiring and plumbing can be incorporated into the printing process. This research was sponsored by the NSF EAGER award: Bio-Beams: FGM Digital Design & Fabrication.",2017-05-31,['mediated-matter'],,4367,--Choose Location,2021-05-21T13:05:20.266Z,"['stevenk@media.mit.edu', 'j_klein@media.mit.edu', 'neri@media.mit.edu']",True,building-scale-3d-printing,2011-09-01,[],Building-Scale 3D Printing,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:19.410Z,"Experiments in animating butterflies and moths with Shape Memory Metal, as part of a collaboration with the Punch Drunk theater group.",2012-09-01,"['opera-of-the-future', 'high-low-tech']",https://dam-prod2.media.mit.edu/x/files/Display/butterflies_logo.png,4391,,2016-12-05T00:16:16.804Z,"['leah@media.mit.edu', 'tod@media.mit.edu', 'jieqi@media.mit.edu']",True,butterflies-sleep-no-more,2012-01-01,[],Butterflies Sleep No More,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:19.586Z,"BUZZwatch distills and tracks trends, themes, and topics within collections of texts across time (e.g. Internet discussions, newspaper archives, and Web pages). This system works by combining new natural language processing techniques with text analysis and retrieval methods, and the novel application of statistical time series analysis. Applications for the system include data mining and visualization of topic dynamics within texts and Internet chatrooms, as well as improved retrieval for Web search engines.",2001-12-30,"['software-agents', 'edevelopment']",,4407,E15-391,2016-12-05T00:17:08.188Z,[],True,buzzwatch-tracking-themes-within-text,1995-12-31,[],BUZZwatch: Tracking Themes Within Text,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:19.663Z,"BYOB is a computationally enhanced modular textile system that makes available a new material from which to construct ""smart"" fabric objects (bags, furniture, clothing). The small modular elements are flexible, networked, input/output capable, and interlock with other modules in a reconfigurable way. The object built out of the elements is capable of communicating with people and other objects, and of responding to its environment. ",2005-09-01,['object-based-media'],,4415,E15-368,2016-12-05T00:16:16.870Z,['vmb@media.mit.edu'],True,byob-build-your-own-bag,2004-01-01,[],BYOB: Build Your Own Bag,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:19.743Z,"C2 (and its derivatives) is the core software tool kit being developed by Synthetic Characters to support all of our work. C2 is an integrated tool kit for building autonomous animated characters that ""do what they ought to do"" ""learn what they ought to learn"" and ""move the way they ought to move."" The tool kit provides an integrated approach to perception, motivation and emotion, learning, action-selection, and motor control.",2002-12-30,['synthetic-characters'],,4423,E15-320,2016-12-05T00:16:16.896Z,[],True,c2-and-beyond,1999-12-31,[],C2-and-Beyond,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:08:19.822Z,"C5 is our fourth-generation behavior architecture. It has been re-architected and fully documented. Our intention is to use C5 as a toolkit to continue to explore learning and training for interactive animated characters, as well as computational models of development for interactive characters. As always, the design of the toolkit (written almost entirely in Java, and designed to run on Mac OS X) is heavily inspired by what we know about animal behavior, learning, and development. C5 is being developed in conjunction with the Robotic Life group.",2004-01-01,['personal-robots'],,4431,E15-489,2016-12-05T00:17:08.211Z,[],True,c5-development-and-learning,2003-01-01,[],C5: Development and Learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:08:20.039Z,"Our bodies continue beyond our flesh and bones. Humans have constantly augmented their bodies with tools like clothing or automobiles, and now our bodies also extend into virtual space. An identity includes online identity, which extends from cell phones and laptops into cyberspace. How do we regard our selves when the boundary between self and world is fading? Cach� is a project that aims to extend online gaze into real space. When a photograph of a body is viewed online, it manifests the gaze offline by means of sound localized on the body. Users know exactly when and where they are being seen. How does revealing online activity affect wearers? If data is neutral and equally accessible, how do we distinguish between personal space and neutral grounds?",2010-01-01,['computing-culture'],,4447,E15-020D,2022-06-10T13:40:30.269Z,"['csik@media.mit.edu', 'peek@media.mit.edu']",True,cach,2009-01-01,[],Cach�,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:08:20.211Z,CADcast is a system for projecting instructions into 3-D workspaces. It supports users in constructing three-dimensional structures with greater efficiency and more accuracy. The system also supports improved coordination between the design and construction teams involved in architectural scale building projects.,2002-12-30,['tangible-media'],,4455,E15-441,2016-12-05T00:16:17Z,['ishii@media.mit.edu'],True,cadcast,2000-12-31,[],CADcast,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:20.653Z,"Call to Action is an open-source web platform for creating telephone-based services such as hotlines, voice petitions, and phone blogging. The platform, currently under development, provides an easy-to-use graphical interface that enables the user to plan the flow of calls, record custom audio, and make use of all the input and output features offered by a regular telephone. The service requires no software programming experience, and users can build a service in under half an hour.",2014-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/c2a_logo.png,4479,--Choose Location,2016-12-05T00:16:17.050Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,call-to-action,2012-09-01,[],Call to Action,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:21.063Z,"Calliope was designed by building on the lessons learnt from  the NeverEnding Drawing Machine. Rather than a static system that lives indoors,  Calliope was inspired by the portability of ""en plein air"" painting and the pochade box that made it possible.  Thinking of ""the world as your palette,"" Calliope is a portable,  paper-based platform for interactive, networked story making which allows physical editing of shared digital media at a distance. With Calliope, we shrank the size and cost by using a system of mirrors and the availability of pocket projectors. We were also interested in exploring the difference between a system that allowed ""from many to many"" collaboration to a more intimate ""one to one"" design. Like the Never-Ending Drawing Machine, Calliope is composed of networked ""creation-stations"" that seamlessly blend analog and digital media which  uses the page-turning book format to synchronize networked co-creation.When using the Never-Ending Drawing Machine, we noticed people had trouble pressing the ""big red button"" since their hands were mostly busy. Calliope substituted the button for a pedal to be pressed. Unlike the Never-Ending Drawing Machine, Calliope uses human-readable tags, designed as dominoes, which can be drawn directly onto the paper with a marker by the user.One of the most valuable outcomes of blending analog and digital media, was the ability to save every version, allowing to then explore the process of creation. The NEDM though did not have a way that the user could access this without having to interface with the computer's file system. For Calliope, we designed a tag which, upon placing it over the desired page, lets you see all the versions that came before the last one. Furthermore, Calliope now can record audio! When the ""rooster"" tag is placed, one can record onto that page and play back, extending the palette to the aural realms. The intention remains: to offer opportunities for cross-cultural and cross-generational collaboration among peers with expertise in different media.",2018-06-01,['object-based-media'],,4503,--Choose Location,2020-06-10T15:52:55.768Z,"['vmb@media.mit.edu', 'edwinapn@media.mit.edu']",True,calliope,2011-01-01,[],Calliope,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:21.240Z,The CalTech/MIT Voting Technology Project was established by Caltech president David Baltimore and MIT president Charles Vest in December 2000 to prevent a recurrence of the problems that threatened the 2000 U.S. Presidential Election. Specific tasks of the project include evaluating the current state of reliability and uniformity of U.S. voting systems; establishing uniform attributes and quantitative guidelines for performance and reliability of voting systems; and proposing specific uniform guidelines and requirements for reliable voting systems.,2003-09-01,[],,4511,E15-301,2016-12-05T00:17:02.832Z,[],True,caltechmit-voting-technology-project,2000-09-01,[],CalTech/MIT Voting Technology Project,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:21.396Z,"A camera for exploring and experimenting with the urban environment. The camera doesn�t sense light, but rather things that the eye can�t see like C02, temperature, or wind.",2008-01-01,['lifelong-kindergarten'],,4519,E15-001,2016-12-05T00:16:17.097Z,"['mres@media.mit.edu', 'silver@media.mit.edu']",True,camera-for-the-invisible,2007-01-01,[],Camera for the Invisible,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:21.580Z,"CAMIT is a web-based front-end to milling machines like the Model-A and for our own in-house $75 milling machine, the Mantis Machine. It allows drag-and-drop ""printing"" of circuit boards in seconds. It also captures all uploaded designs and allows anyone to optimize, modularize, change, or simply re-print them.  CAMIT is a circuit-board design anyone can use.",2011-01-01,['information-ecology'],,4527,,2016-12-05T00:16:17.114Z,['holtzman@media.mit.edu'],True,camit,2010-01-01,[],CAMIT,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:21.755Z,"We present CAPS, wearable on-body capsules which produce repulsive odor to deter sexual abuse. The capsules can be triggered by self-actuation, i.e. by pressure or when an act of forceful removal of clothing is identified.  The formulation of odor involves compounds like civet reconstruction, 1-4, butadiene and more.",2022-06-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/11/12/DSC01756_86hlEYD.jpg,4535,--Choose Location,2022-06-10T12:31:22.204Z,"['manisham@media.mit.edu', 'geek@media.mit.edu']",True,caps-curbing-assault-to-protect-society,2016-01-01,"['human-machine-interaction', 'wearable-computing']",CAPS: Curbing Assault to Protect Society,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:21.922Z,"Modern web presentations such as Youtube feature videos with commentary appended at the bottom. In our new imagining of Videotext, we put the two together: comments appear as active bubbles along the playback time line. We thereby associate the commentary with the place in the video to which it refers. It gains context. This project is in the early test stage and is presented for discussion and further development in summer 2016.",,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/Screenshot%202016-02-18%2014.27.59.png,4543,--Choose Location,2016-12-05T00:17:03.626Z,"['weller@media.mit.edu', 'lip@media.mit.edu']",True,captions,2016-01-01,"['communications', 'social-media']",Captions++,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:22.216Z,"We are building  a system that can watch for certain signs of  stress in drivers, specifically stress related to talking on the car phone, as may be caused by increased mental workload. To gather data for training and testing our system, subjects were asked to 'drive' in a simulator past several curves while keeping their speed close to a predetermined desired constant value. In some cases they were simultaneously asked to listen to random  numbers from a speech-synthesis software and to perform simple mathematical tasks over a telephone headset. Several measures drawn from the subjects' driving behavior were examined as possible indicators of  the subjects' performance and of their mental  workload. When subjects were instructed (by a visible sign) to brake, most braked within 0.7-1.4 seconds after the sign came into view.  However, in a  significant number of incidents, subjects never braked or braked 1.5-3.5 seconds after the message; almost all of these incidents were when subjects were on the phone.  On average, we found that drivers on the phone braked 10% slower than when not on the phone; additionally, the variance in their braking time was four times higher -- suggesting that although delayed driver reactions were infrequent, when delays happened they could be large and potentially dangerous.  Furthermore, their infrequency could create a false sense of security.   In future experiments, subjects' physiological data will be analyzed jointly with measures of workload, stress and performance.",1999-12-30,"['cc', 'affective-computing']",,4559,,2016-12-05T00:17:08.156Z,['picard@media.mit.edu'],True,car-phone-stress,1997-12-31,[],Car Phone Stress,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:22.355Z,"PAF (paroxysmal atrial fibrillation) is a dangerous form of cardiac arrhythmia that poses severe health risks, sometimes leading to heart attacks, the recognized number-one killer in the developed world. The technical challenges for detecting and predicting PAF include accurate sensing, speedy analysis, and a workable classification system. To address these issues, electrocardiogram (ECG) data from the PhysioNet Online Database will be analyzed using new spectrum estimation techniques to develop a program able to predict, as well as recognize, the onset of specific cardiac arrhythmias such as PAF. The system could then be incorporated into wearable/mobile medical devices, allowing for interventions before cardiac episodes occur, and potentially saving many lives.",2003-09-01,['affective-computing'],,4567,--Choose Location,2016-12-05T00:16:17.177Z,['picard@media.mit.edu'],True,cardiac-paf-detection-and-prediction,2002-01-01,[],Cardiac PAF Detection and Prediction,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:22.482Z,"We are designing interfaces to enhance driver self-awareness through subliminal visual feedback and shape-changing materials. Advancements in sensing technologies make it possible to measure physiological data in the car environment, opening up the possibility of harnessing such data for just-in-time feedback to drivers. ",2015-01-01,"['advancing-wellbeing', 'fluid-interfaces']",,4575,--Choose Location,2016-12-05T00:16:17.267Z,"['pattie@media.mit.edu', 'amores@media.mit.edu', 'changzj@media.mit.edu', 'rboldu@media.mit.edu']",True,cardio,2014-09-01,[],CarDio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:22.631Z,"Cardiocam is a low-cost, non-contact technology for measurement of physiological signals such as heart rate and breathing rate using a basic digital imaging device such as a webcam. The ability to perform remote measurements of vital signs is promising for enhancing the delivery of primary healthcare.",2013-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/cardiocam_PLDB.jpg,4583,--Choose Location,2016-12-05T00:16:17.291Z,"['picard@media.mit.edu', 'zher@media.mit.edu']",True,cardiocam,2009-09-01,[],Cardiocam,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:22.758Z,A study in the roles of different facial features as used by social groups for communication. The goal is to be able to render faces as caricatured against the average face of different societies.,2001-12-30,['sociable-media'],,4591,,2016-12-05T00:16:17.319Z,['judith@media.mit.edu'],True,caricatures,2000-12-31,[],Caricatures,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:08:22.872Z,"Carousel was inspired by Robert Morris's ""The Finch College Project."" In his installation, a film camera was placed on a platform that rotated at one revolution per minute. One side of the room had a life-sized, black and white panorama of an audience. On the opposite side, a crew of people were constructing and deconstructing a wall of mirrored tiles. The sequence of activities in the room was recorded by the camera. The room was then emptied, leaving only dots from the mirror grid alignment. The projector (playing the footage captured by the camera) was placed on a platform rotating at the same rate as the original film camera. The  images of the audience appeared and disappeared as the mirror wall was built and unbuilt, projected onto the walls of the same room. Carousel uses this same visual technique to create a communication link between two public spaces. The center of each room will have a rotating podium. A camera, microphone, speakers, and a projector sit on each podium. The podiums rotate slowly at the same rate. As they rotate, the images captured as the camera rotates in one room are projected around the space in the other room and vice versa. The result is a moving snapshot-in-time along the periphery of the room as people, both physically present and projected, are moving about the space.",2004-01-01,['sociable-media'],,4599,E15-468,2016-12-05T00:16:17.354Z,['judith@media.mit.edu'],True,carousel,2003-01-01,[],Carousel,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:22.985Z,"Carpal Skin is a prototype for a protective glove to protect against Carpal Tunnel Syndrome, a medical condition in which the median nerve is compressed at the wrist, leading to numbness, muscle atrophy, and weakness in the hand. Night-time wrist splinting is the recommended treatment for most patients before going into carpal tunnel release surgery. Carpal Skin is a process by which to map the pain-profile of a particular patient – its intensity and duration – and to distribute hard and soft materials to fit the patient's anatomical and physiological requirements, limiting movement in a customized fashion. The form-generation process is inspired by animal coating patterns in the control of stiffness variation.",2016-05-31,[],,4607,--Choose Location,2021-05-21T13:15:50.690Z,['neri@media.mit.edu'],True,carpal-skin,2010-09-01,[],Carpal Skin,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:23.086Z,"Cartagen is a set of tools for mapping, enabling users to view and configure live streams of geographic data in a dynamic, personally relevant way. Today's mapping software is largely based on static data sets, and neither incorporates the time dimension in its display nor provides for real-time data streams.",2011-01-01,['civic-media'],,4615,E15-301,2016-12-05T00:16:17.405Z,['ethanz@media.mit.edu'],True,cartagen,2009-01-01,[],Cartagen,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:23.188Z,"Case and Molly is a prototype for a game inspired by (and in homage to) William Gibson's novel Neuromancer. It's about the coordination between virtual and physical, ""cyberspace"" and ""meat."" We navigate the tension between our physical surroundings and our digital networks in a state of continuous partial attention; Case and Molly uses the mechanics and aesthetics of Neuromancer to explore this quintessential contemporary dynamic. The game is played by two people mediated by smartphones and an Oculus Rift VR headset. Together, and under time pressure, they must navigate Molly through physical space using information that is only available to Case. In the game, Case sees Molly's point of view in immersive 3D, but he can only communicate a single bit of information to her. Meanwhile, Molly traverses physical obstacles hoping Case can solve abstract puzzles in order to gain access to the information she needs.",2022-06-01,['playful-systems'],,4623,--Choose Location,2022-06-10T12:33:45.288Z,"['slavin@media.mit.edu', 'gregab@media.mit.edu']",True,case-and-molly,2013-09-01,[],Case and Molly,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:23.578Z,"We introduce a novel interactive method to assess cataracts in the human eye by crafting an optical solution that measures the perceptual impact of forward scattering on the foveal region. Current solutions rely on highly trained clinicians to check the back scattering in the crystallin lens and test their predictions on visual acuity tests. Close-range parallax barriers create collimated beams of light to scan through sub-apertures, scattering light as it strikes a cataract. User feedback generates maps for opacity, attenuation, contrast, and local point-spread functions. The goal is to allow a general audience to operate a portable, high-contrast, light-field display to gain a meaningful understanding of their own visual conditions. The compiled maps are used to reconstruct the cataract-affected view of an individual, offering a unique approach for capturing information for screening, diagnostic, and clinical analysis.",2015-09-15,['camera-culture'],,4655,--Choose Location,2016-12-05T00:17:08.240Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'elawson@media.mit.edu']",True,catra-mapping-of-cataract-opacities-through-an-interactive-approach,2010-09-01,"['bioengineering', 'consumer-electronics', 'developing-countries', 'health', 'imaging', 'public-health']",CATRA: Mapping of Cataract Opacities Through an Interactive Approach,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:23.840Z,"In collaboration with the Early Childhood Cognition Center at MIT BCS, we are developing sensor-enabled toys and infant affect sensors with the goal to understand how children on the autism spectrum use patterns of evidence to learn causal relationships and the extent with which this is state-dependent. We investigate in what respects, if any, causal learning is different in comparison to typically developing children. The results of this research will inform the design of new object-based technologies for language and communication learning.",2008-09-01,['affective-computing'],,4679,E15-450,2016-12-05T00:16:17.452Z,['picard@media.mit.edu'],True,causal-learning-and-autism,2007-09-01,[],Causal Learning and Autism,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:24.508Z,"Expressive encoding of spatial ambience allows us to embed technological narratives in objects so as to provoke a re-thinking of our relationship to objects and spaces. The nature of this project addresses how objects may be expressive of attitude; here we explore the space above us. Hanging from the ceiling, organically shaped pods interface as a portal to another imaginative space. Passersby or visitors engage in an interaction with an encountered entity. This entity imparts a day-to-day dependent narrative, expressive and reactive to an accumulation of past interactions. This research explores both the dichotomy of tension and the engagement of affective states and historical data collected from interactions between the real-world visitor and the encountered imaginative character.",2004-01-01,['media-fabrics'],,4743,E15-368,2016-12-05T00:17:08.328Z,"['gid@media.mit.edu', 'monster@media.mit.edu']",True,ceiling-with-an-attitude,2003-09-01,[],Ceiling with an Attitude,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:08:24.595Z,"Why do our advanced cell phones still ring at completely inappropriate times, such as at the theatre, at the movies, or during family dinners? How would a person contact me in an emergency if I have turned off my cell phone because I am in a meeting? In this work, we explore ways to make mobile communication devices socially intelligent, both in their internal reasoning and in how they interact with people. We propose the concept of an Autonomous Interactive Intermediary that helps the user manage her mobile communication devices in a socially appropriate way.",2007-09-01,['living-mobile'],,4751,E15-368,2016-12-05T00:17:08.352Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,cellular-squirrel-autonomous-interactive-intermediaries,2002-01-01,[],Cellular Squirrel: Autonomous Interactive Intermediaries,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:24.756Z,"RESTful services and the Web provide a framework and structure for content delivery that is scalable, not only in size but, more importantly, in use cases. As we in Responsive Environments build systems to collect, process, and deliver sensor data, this project serves as a research platform that can be shared between a variety of projects both inside and outside the group. By leveraging hyperlinks between sensor data clients can browse, explore, and discover their relationships and interactions in ways that can grow over time.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/Internet_map_1024_-_transparent.png,4767,--Choose Location,2016-12-05T00:16:17.588Z,"['gershon@media.mit.edu', 'bmayton@media.mit.edu', 'sfr@media.mit.edu', 'joep@media.mit.edu']",True,chain-api,2013-01-01,[],Chain API,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:24.834Z,"ChainFORM is a modular hardware system for designing linear shape-changing interfaces. Each module is developed based on a servo motor with added flexible circuit board, and is capable of touch detection, visual output, angular sensing, and motor actuation. Moreover, because each module can communicate with other modules linearly, it allows users and designers to adjust and customize the length of the interface. Using the functionality of the hardware system, we propose a wide range of applications, including line-based shape changing display, reconfigurable stylus, rapid prototyping tool for actuated crafts, and customizable haptic glove. We conducted a technical evaluation and a user study to explore capabilities and potential requirements for future improvement.",,"['tangible-media', 'responsive-environments']",,4775,--Choose Location,2018-05-04T15:33:24.390Z,"['ishii@media.mit.edu', 'artemd@media.mit.edu', 'ken_n@media.mit.edu', 'joep@media.mit.edu']",True,chainform,2015-09-01,['human-computer-interaction'],ChainFORM,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:24.918Z,"You leave for work in the morning saluting your dog resting on the floor, wondering what he might be up to in the hours you don't see him. Returning home later, you find him where you left him�but your sofa tells a story of playful traces of paw prints and body prints! Chairs and other seating are a large part of our daily lives, from car seats to desk chairs. What ""sense of chairness"" can a chair hold beyond its formal and functional qualities? This research explores the idea of anthropocentric illusions, which enable a chair�or any object�to reflect, share, and reveal its stories of experience. The system accumulates a multifaceted log of occurrences and encounters with humans and their contexts, and formulates unique expressions that reflect ""attitude"" back to the human. We are realizing a chair's expression by designing and engineering an electronic fabric called TechStyle, a next-generation electronic textile that incorporates input and output capability in the woven structure. The fabric was developed in collaboration with Collins & Aikman, whose expertise includes textile and chemical engineering. We are currently developing the system board that drives both the input and output capability of the fabric. The design and engineering is driven by the vision that electronic input/output will enhance the design of many everyday objects. ",2005-01-01,['media-fabrics'],,4783,E15-368,2016-12-05T00:16:17.554Z,['gid@media.mit.edu'],True,chair-with-an-attitude,2003-09-01,[],Chair with an Attitude,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:25.064Z,"The Chameleon Car is a vehicle that extends its wheelbase width and length as it increases its speed. The project demonstrates an approach that allows a car to transform from a small, maneuverable vehicle to a longer, more aerodynamic vehicle. Included are baffled airbags for passenger safety and a longer, wider wheelbase for added efficiency, safety, and style.",2004-01-01,['context-aware-computing'],,4791,E15-320,2016-12-05T00:16:17.606Z,[],True,chameleon-car,2000-01-01,[],Chameleon Car,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:25.233Z,"How can traditional values be embedded into a digital object? We explore this concept by implementing a special guitar that combines physical acoustic properties with virtual capabilities. The acoustical values will be embodied by a wooden heart�a unique, replaceable piece of wood that will give the guitar a unique sound. The acoustic signal created by this wooden heart will be digitally processed in order to create flexible sound design.",2013-09-01,['responsive-environments'],,4799,E15-320,2016-12-05T00:17:08.375Z,['joep@media.mit.edu'],True,chameleon-guitar-physical-heart-in-a-virtual-body,2008-01-01,[],Chameleon Guitar: Physical Heart in a Virtual Body,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:25.552Z,"We are studying the communication capacities of unitary two-party unitary quantum interactions.  We have found expressions for their one-way classical and quantum capacities assisted by an arbitrary amount of entanglement per use of the gate.  More importantly, classical communication using unitary methods has turned out to be a useful abstraction for studying other problems in quantum information theory.  This abstraction, called ""coherent classical communication,"" has led to short proofs of many of the central results in the field, and allowed the unification of several problems in quantum information theory.",2004-09-01,['quanta'],,4815,E15-427,2016-12-05T00:16:17.677Z,['ike@media.mit.edu'],True,chaniltonians,2001-01-01,[],Chaniltonians,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:25.905Z,"CharmMe is a mobile social discovery application that helps people meet each other during events. The application blends physical and digital proximity to help you connect with with other like-minded individuals. Armed with RFID sensors and a model of how the Lab works, CharmMe determines who you should talk to using information including checking in to conference talks or ""liking"" projects using QR codes. In addition, possible opening topics of conversation are suggested based on users' expressed similar interests. ",2016-08-31,['digital-intuition'],,4831,,2021-07-14T15:34:41.229Z,['havasi@media.mit.edu'],True,charmme,2011-09-01,[],CharmMe,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:26.052Z,"Chat Circles is an abstract graphical interface for synchronous text conversation. Here, color and form are used to convey social presence and activity, and proximity-based filtering is used intuitively to break large groups into conversational clusters. The system also includes an integrated history interface, which visualizes archival Chat Circle logs. Our goal in this work is to create a richer environment for online discussions. We are currently revamping several of the chatroom's features to give our users more flexibility when setting up their own Chat Circles server.",2017-05-01,['sociable-media'],,4839,E15-383,2022-06-10T14:41:29.732Z,['judith@media.mit.edu'],True,chat-circles,1998-01-01,[],Chat Circles,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:26.204Z,"We describe a study of the impact of visual traces on the behavior of groups of users in Chat Circles, an abstract graphical chat room. The results indicate that traces, which appear in the chat space when users move or speak, change the way people position themselves relative to others and the way they move around the space. Traces also facilitate more expressive uses of participants� avatars, such as intimidation or showing agreement. Additionally, we consider differences in positioning and communicating between dyadic and larger groups in the chat room.",2004-01-01,['sociable-media'],,4847,E15-383,2016-12-05T00:16:17.718Z,['judith@media.mit.edu'],True,chat-circles-user-study,2003-09-01,[],Chat Circles User Study,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:26.338Z,"ChatScape is a chat environment in which user-programmable autonomous behaviors are part of the communication repertoire.  We are developing tools that will allow users to combine a number of simple actions into a complex set of behaviors that respond to events in the environment, user commands, and the activity of others. Our goal with this project is to develop a more expressive and engaging environment for online conversation.",2001-12-30,['sociable-media'],,4855,E15-449,2016-12-05T00:17:08.396Z,['judith@media.mit.edu'],True,chatscape,1999-12-31,[],Chatscape,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:26.476Z,"Chatterprint creates visual and textual fingerprints of one or more instant message conversations. These sorts of visualization techniques are typically used for browsing archives and exposing behavioral patterns. Instead, we are working to build static visualizations that capture a facet of someone's IM identity. Chatterprints will be easy to create and share, communicating someone's interests and style in a concise visual way. Using Chatterprints as a tool, we can explore what makes someone interested in sharing a personal visualization, what people see in other people's personal visualizations, and what kinds of information people are comfortable sharing from their IM histories.",2007-01-01,['sociable-media'],,4863,--Choose Location,2016-12-05T00:16:17.800Z,['judith@media.mit.edu'],True,chatterprint,2006-09-01,[],Chatterprint,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:26.640Z,"Dating practices have been indelibly marked by the pace and communication opportunities available online. We are examining this phenomenon to see what it suggests about decision-making, attraction, and human attachment. To do so, we are using a few converging approaches: data analysis of online behavior, data visualizations, and direct experiments. Unlike other decision-making domains, online behaviors each result in a trace: button clicks, pages visited. Even from anonymous histories, we can begin to reconstruct the decision-making processes and variables that inform such decisions. Our laboratory experiments complement our descriptive data analysis through direct examination of the trends and patterns we see occurring. Through these approaches, we hope to understand predictors of successful relationships and to develop methods to improve the process.",2005-01-01,['erationality'],,4871,E15-318,2016-12-05T00:17:01.678Z,[],True,checkmate,2002-01-01,[],CheckMate,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:26.833Z,"Conventional Web interfaces respond to and consider only mouse clicks when defining a user model. We have extended this and take into account all mouse movements on a page as an additional layer of information for inferring user interest. We have developed a straightforward way to record all mouse movements on a page, and conducted a user study to analyze and investigate mouse behavior trends. We have found certain mouse behaviors, common across many users, which are useful for content providers in increasing the effectiveness of their interface design.",2003-01-01,['context-aware-computing'],,4879,E15-320,2016-12-05T00:16:17.824Z,[],True,cheese,2001-01-01,[],Cheese,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:27.014Z,"People use text to communicate in online spaces because of its directness and ease of use. However, we are unable to convey many social cues and communication nuances by text. We explore how to enhance the richness and expressiveness of text, so that we can have an expressive and intuitive online environment for textual communication. Cheiro is a chat space offering a novel form of gesture-enhanced textual communication over the Internet. It provides an intuitive interface for turning the form of text into an expressive visual medium; it is an experimental arena for exploring the relationships between gestures, emotions, and visuals.",2017-05-01,['sociable-media'],,4887,E15-390,2022-06-10T14:41:29.746Z,['judith@media.mit.edu'],True,cheiro,2005-01-01,[],Cheiro,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:27.526Z,"ChessMates is a collaborative chess game. Building on the TeleAction framework, ChessMates allows two communities of players to play chess against one another. This project focuses on supporting the social aspects of playing chess by providing a social interaction space for players to talk about the game, as well as a voting system to allow community-based decisions about which moves should be made.",2001-12-30,"['broadercasting', 'sociable-media']",,4911,E15-450,2016-12-05T00:16:17.875Z,['judith@media.mit.edu'],True,chessmates,2000-12-31,[],ChessMates,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:27.905Z,"The Chit Chat Club is an experiment in bringing people together in a mixed physical and virtual environment. Online chatrooms and real world cafes are both venues for social interaction, but with significant differences, e.g., the participants' knowledge of each other's expressions and identity and the more governing introductions and turn-taking. Our goal is to create, through careful design of the physical environment and computer interface, a place that gracefully combines these two cultures. The analysis of how well this space actually functions will further our understanding of social interaction, both online and in person.",2002-12-30,['sociable-media'],,4927,E15-449,2016-12-05T00:16:17.897Z,"['judith@media.mit.edu', 'monster@media.mit.edu']",True,chit-chat-club,1999-12-31,[],Chit Chat Club,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:28.344Z,"Makeup has long been used as a body decoration process for self-expression and for the transformation of one's appearance. While the material composition and processes for creating makeup products have evolved, they still remain static and non-interactive. But our social contexts demand different representations of ourselves; thus, we propose ChromoSkin, a dynamic color-changing makeup system that gives the wearer ability to alter seamlessly their appearance. We prototyped an interactive eye shadow tattoo composed of thermochromic pigments activated by electronics or ambient temperature conditions. We present the design and fabrication of these interactive cosmetics, and the challenges in creating skin interfaces that are seamless, dynamic, and fashionable.",,"['living-mobile', 'responsive-environments']",https://dam-prod2.media.mit.edu/x/2016/10/20/Katia_cover_dark.png,4951,--Choose Location,2018-02-08T20:50:38.533Z,"['cindykao@media.mit.edu', 'katiav@media.mit.edu', 'manisham@media.mit.edu', 'geek@media.mit.edu', 'joep@media.mit.edu']",True,chromoskin,2015-12-01,"['fashion', 'wearable-computing']",ChromoSkin,PUBLIC,https://vimeo.com/155460417,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:28.508Z,"Cicadence is an interactive soundscape. Created through a bio-mimetic process, it examines and is inspired by the auditory experience produced by a cicada.",2011-01-01,['opera-of-the-future'],,4959,,2016-12-05T00:16:17.934Z,['tod@media.mit.edu'],True,cicadence,2011-01-01,[],Cicadence,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:28.691Z,"In nature, hair has numerous functions such as providing warmth, adhesion, locomotion, sensing, and a sense of touch, as well as its well-known aesthetic qualities. This work presents a computational method of 3D printing hair structures. It allows us to design and generate hair geometry at 50 micrometer resolution and assign various functionalities to the hair. The ability to fabricate customized hair structures enables us to create superfine surface texture, mechanical adhesion properties, new passive actuators, and touch sensors on a 3D-printed artifact. We also present several applications to show how the 3D-printed hair can be used for designing everyday interactive objects.",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2016/10/26/cillia-edit.jpg,4967,--Choose Location,2019-11-13T20:55:24.351Z,"['ishii@media.mit.edu', 'gershon@media.mit.edu', 'jifei@media.mit.edu']",True,cilllia-3d-printed-micro-pillar-structures-for-surface-texture-actuation-and-sensing,2015-01-01,"['design', 'environment', 'fashion', 'human-machine-interaction', 'interfaces', '3d-printing', 'technology']","Cilllia: 3D-printed micro pillar structures for surface texture, actuation, and sensing",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:29.127Z,"The goal of the Cinema Fabriqu� system is to create a software environment, complementary wearable devices, and a usage methodology for producing engaging cinematic experiences in real time for live audiences through natural-language control. Current multimedia performance packages suffer from input bandwith bottlenecks that restrict the scope of user control and audience engagement. Our proposed alternative aims to couple a high degree of user control through gestural and speech input with intelligent software to create rich audiovisual output.",2003-01-01,['aesthetics-computation'],,4991,E15-301,2016-12-05T00:17:08.480Z,[],True,cinema-fabriqu,2003-01-01,[],Cinema Fabriqu�,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:29.265Z,"With the proliferation of cheap video cameras that are always on, and always ready to record, distinctions between the amateur and professional videographer become blurred. When we can record at any moment, how will we decide what we want to record, and when and where we need to be in order to capture what we want? How can we best index the recorded material to support later communication needs? In this research, we use a common-sense approach to knowledge and inference to support a partnership between camera and human videographer. We present a new paradigm for producing common-sense video metadata and we show how the metadata can have a positive impact on video content capture, representation, and presentation.",2005-01-01,['media-fabrics'],,4999,E15-368,2016-12-05T00:16:18.113Z,"['gid@media.mit.edu', 'barbara@media.mit.edu']",True,cinematic-common-sense,2001-01-01,[],Cinematic Common Sense,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:29.404Z,"Cinemaware is a custom-built capture, annotation, editing, and compression tool for preparing video content for integration with multiple display and distribution applications. Implemented in Java, the application provides four modes of operation: live capture, keyword annotation, rudimentary editing, and batch processing. The capture mode provides an interface for capturing video through a USB port, while the annotation mode presents an extensible interface for adding freeform text annotations, time-code associated key-words, and general movie properties such as title, character, location, and time. The annotation schema can be modified to incorporate appropriate input fields depending on the needs of external retrieval and display applications. Captured video clips can be refined using a simple editing interface, and the batch-processing mode enables the export of multiple video, image, or audio files according to a specified compression output.",2003-09-01,['interactive-cinema'],,5007,E15-368,2016-12-05T00:16:17.979Z,['gid@media.mit.edu'],True,cinemaware,2002-09-01,[],Cinemaware,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:29.529Z,"Circuit Stickers is a toolkit for crafting electronics using flexible and sticky electronic pieces. These stickers are created by printing traces on flexible substrates and adding conductive adhesive. These lightweight, flexible, and sticky circuit boards allow us to begin sticking interactivity onto new spaces and interfaces such as clothing, instruments, buildings, and even our bodies.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2016/07/16/re-circuit.jpg,5015,--Choose Location,2019-12-19T19:50:49.848Z,"['leah@media.mit.edu', 'nanwei@media.mit.edu', 'jieqi@media.mit.edu', 'joep@media.mit.edu']",True,circuit-stickers,2013-01-01,"['art', 'learning-teaching', 'storytelling', 'engineering']",Circuit Stickers,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:29.647Z,"The Circuit Sticker Activity Book is a primer for using circuit stickers to create expressive electronics. Inside are explanations of the stickers, and circuits and templates for building functional electronics directly on the pages of the book. The book covers five topics, from simple LED circuits to crafting switches and sensors. As users complete the circuits, they are also prompted with craft and drawing activities to ensure an expressive and artistic approach to learning and building circuits. Once completed, the book serves as an encyclopedia of techniques to apply to future projects.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/four_pages.JPG,5023,--Choose Location,2016-12-05T00:16:18.006Z,"['leah@media.mit.edu', 'jieqi@media.mit.edu', 'joep@media.mit.edu']",True,circuit-stickers-activity-book,2012-01-01,[],Circuit Stickers Activity Book,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:29.762Z,"An interactive picture book that explores storytelling techniques through paper-based circuitry. Sensors, lights, and microcontrollers embedded into the covers, spine, and pages of the book add electronic interactivity to the traditional physical picture book, allowing us to tell new stories in new ways. The current book, ""Ellie,"" tells the adventures of an LED light named Ellie who dreams of becoming a star, and of her journey up to the sky.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/Ellie_storybook.JPG,5031,--Choose Location,2022-06-10T13:53:08.111Z,"['slavin@media.mit.edu', 'jieqi@media.mit.edu', 'joep@media.mit.edu']",True,circuit-storybook,2015-01-01,[],Circuit Storybook,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:29.888Z,"CircuiTUI is an application for circuit design and modeling, intended to aid novice circuit designers or students in gaining intuition into the behavior of circuit components. Projected images of circuit components can be bound to physical objects and reorganized in a virtual circuit as the physics of the circuit is modeled and overlaid quantitatively on the components.",2004-01-01,['tangible-media'],,5039,E15-344,2016-12-05T00:16:07.244Z,['ishii@media.mit.edu'],True,circuitui,2002-01-01,[],circuiTUI,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:30.030Z,"Planning a city is a complex task requiring collaboration between multiple stakeholders with different, and often conflicting, goals and objectives. Researchers have studied the role of technology in group collaboration for many years. It has been noted that when the task between collaborators increases in complexity, such as in a decision-making process, the use of computer technology could either enhance, or disturb, the collaboration process. City Game evaluates the impact of computer interfaces on a multi-objective negotiation problem. Using a tangible user interface (TUI) is more effective for multi-objective group decision-making than a graphical or multitouch user interface; this project will focus on designing and developing a TUI and a serious game for an urban planning scenario. We will test the game on different computer interfaces to evaluate the decision-making process between different collaborators with conflicting objectives.",2016-12-01,['city-science'],,5047,--Choose Location,2017-10-11T13:06:14.048Z,"['kll@media.mit.edu', 'mkh@media.mit.edu']",True,city-game,2015-09-01,[],City Game,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:30.389Z,"The City Symphony project by the Opera of the Future group brings creative musical participation to everyone while encouraging collaboration between artists and amateurs, with symphony orchestras (and many other organizations) as the principal galvanizers. City Symphonies invite the citizens of a particular place to listen to the world around them, to discover the ""music"" in that place, and to work together to create a sonic portrait of that city that reveals its essential qualities and most important issues and questions to audiences locally and around the world. Going beyond crowd-sourcing, City Symphonies propose a new model of collaboration, where people of all ages and backgrounds work together to make beautiful, meaningful music that none of them—including the highest-level professionals—could have made alone.Tod Machover and Opera of the Future launched the City Symphony project in 2012, and since then have created collaborative symphonies with the cities of Toronto (Toronto Symphony Orchestra, 2013), Edinburgh (Edinburgh International Festival, Royal Scottish National Orchestra, 2013), Perth (Perth International Festival, West Australian Symphony Orchestra, 2014), Lucerne (Lucerne Festival, Lucerne Festival Academy Orchestra, 2015), and Detroit (Detroit Symphony Orchestra with Knight Foundation, 2015). Machover and his research group collaborated with these cities to explore new relationships between author/audience, composition/improvisation, music/noise, and online/onsite while emphasizing the potential of each locale to inspire its citizens to engage with their community through music in a profound way. One of the most rewarding aspects of the City Symphonies project is that the processes employed to achieve the final work are designed to grow naturally out of each particular city and context. For this reason, the five City Symphonies realized to date by Tod Machover and the MIT Media Lab have differed widely in terms of use of imagery/video, interactive performance elements, incorporation of local musicians, and the balance of acoustic/electronic sounds.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/12/12/Toronto_Skyline_at_night_-b.jpg,5063,--Choose Location,2019-04-17T19:59:05.709Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'benb@media.mit.edu', 'sovsey@media.mit.edu', 'akito@media.mit.edu', 'rebklein@media.mit.edu', 'dnunez@media.mit.edu', 'holbrow@media.mit.edu', 'platte@media.mit.edu']",True,city-symphonies-massive-musical-collaboration,2012-01-01,"['design', 'art', 'artificial-intelligence', 'crowdsourcing', 'music', 'storytelling', 'sensors', 'ethics', 'performance', 'creativity', 'technology', 'voice']",City Symphonies: Massive musical collaboration,PUBLIC,http://opera.media.mit.edu/projects/city_symphonies/,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:30.563Z,"The CityCar electric automobile, developed and prototyped by Smart Cities, is designed to meet the demand for enclosed personal mobility – with weather protection, climate control and comfort, secure storage, and crash protection – in the cleanest and most economical way possible. It weighs less than a thousand pounds, parks in much less space than a Smart Car, and is expected to get the equivalent of 150 to 200 miles per gallon of gasoline. Since it is battery-electric, it produces no tailpipe emissions.The architecture of the CityCar is radical. It does not have a central engine and traditional power train, but is powered by four in-wheel electric motors. Each wheel unit contains drive motor (which also enables regenerative braking), steering, and suspension, and is independently digitally controlled. This enables maneuvers like spinning on its own axis (an O-turn instead of a U-turn), moving sideways into parallel parking spaces, and lane changes while facing straight ahead.",2013-01-01,['city-science'],https://dam-prod2.media.mit.edu/x/files/Display/CityCar.jpg,5071,E15-001,2017-08-24T12:02:37.347Z,"['kll@media.mit.edu', 'rchin@media.mit.edu']",True,citycar,2001-01-01,"['environment', 'transportation', 'urban-planning']",CityCar,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:31.606Z,"By 2030, nine billion people will populate the globe and six out of every 10 will live in cities. The future of global food production will mandate a paradigm shift to resource-leveraged and environmentally sound urban food-growing solutions. The CityFARM project explores building-integrated agriculture and environmentally optimized growing. We are exploring what it means–technologically, environmentally, and socially–to design industrially scalable agricultural systems in the heart of urban areas. Through innovative research, and through development of hydroponic and aeroponic systems, diagnostic and networked sensing, building integration, and reductive energy design, CityFARM methodology reduces water consumption by 90 percent, eliminates chemical pesticides, and reduces embodied energy in produce by a factor of four. By fundamentally rethinking ""grow it THERE and eat it HERE,"" we can eliminate environmental contaminants and increase access to nutrient-dense produce in our future cities.",2015-09-01,['city-science'],https://dam-prod2.media.mit.edu/x/files/Display/cityfarmlogo.png,5119,--Choose Location,2017-10-18T01:14:51.880Z,"['kll@media.mit.edu', 'elplatt@media.mit.edu', 'calebh@media.mit.edu']",True,cityfarm,2013-01-01,[],CityFARM,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:31.977Z,"We demonstrate how the CityHome, which has a very small footprint, can function as an apartment two to three times that size. This is achieved through a transformable wall system which integrates furniture, storage, exercise equipment, lighting, office equipment, and entertainment systems. One potential scenario for the CityHome is where the bedroom transforms to a home gym, the living room to a dinner party space for 14 people, a suite for four guests, two separate office spaces plus a meeting space, or an a open loft space for a large party. Finally, the kitchen can either be open to the living space, or closed off to be used as a catering kitchen. Each occupant engages in a process to personalize the precise design of the wall units according to his or her unique activities and requirements.",2016-12-01,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/17/v59dawmfmkhhpuormqzw_ZSHWRNn.gif,5135,--Choose Location,2017-10-18T01:06:53.526Z,"['kll@media.mit.edu', 'hlarrea@media.mit.edu']",True,OLD_cityhome2,2011-01-01,"['architecture', 'urban-planning', 'alumni', 'startup']",CityHome,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:32.656Z,"The ""Barcelona"" demo is an independent prototype designed to model and simulate human interactions within a Barcelona-like urban environment. Different types of land use (residential, office, and amenities) are configured into urban blocks and analyzed with agent-based techniques.",2016-01-01,['changing-places'],https://dam-prod2.media.mit.edu/x/files/Display/IMG_6418.JPG,5167,--Choose Location,2018-04-27T14:41:51.014Z,"['kll@media.mit.edu', 'jiw@media.mit.edu', 'gowharji@media.mit.edu', 'csmuts@media.mit.edu', 'ryanz@media.mit.edu']",True,cityscope-barcelona,2015-09-01,[],CityScope Barcelona,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:32.853Z,"The development of transportation improvements in any city poses a range of challenges, including meaningfully involving relevant stakeholders. With many members of the public generally skeptical of government’s ability to generate solutions that will work for them, transport agencies and community organizations are looking for better ways to engage each other and the general public in developing project ideas. One problem is that existing representational tools are not well suited for allowing diverse stakeholders to understand, evaluate, and provide feedback on the geographically distributed benefits and tradeoffs of potential transport decisions. These decisions range widely, from local pedestrian flows around public transport stations, to parking provision on streets, to corridor alignment and priority schemes that can affect regional connectivity.In recent years, however, new cooperative planning tools have emerged, made possible by the rapidly growing availability of open-source data platforms and interactive computing technology. These technologies promise to facilitate the inclusion of local knowledge in a way that could transform public participation. Starting from the premise that meaningful public engagement is fundamental to doing transit right, together with the Mobility Futures Collaborative at MIT Department of Urban Studies and Planning (DUSP), we developed several interactive planning tools to see if they can enable inclusive and authentic dialogue. Open dialogue is a cornerstone of meaningful engagement and learning in collaborative planning settings. We designed the interactive tools to allow individuals to explore impacts and alternatives at the regional, neighborhood, and street scales. With support from the Barr Foundation, and in partnership with Nuestra Comunidad, a local community development organization, the tools were deployed in a series of public workshops held in October 2015 in Boston’s Roxbury neighborhood. These pilot workshops focused on the potential for implementing Bus Rapid Transit (BRT). While the tools were tested using the case of BRT corridors in the Boston area, we believe they have applicability to planning for a broader range of transportation alternatives in a variety of settings. These tools include the CityScope—an interactive platform that utilizes physical models (built from LEGO bricks) and 3-D projection—to enable community members to engage in neighborhood and street-level decisions including alternative bus corridor designs and station-level variations (such as pre-pay boarding). The second tool, CoAXs, is a new interactive platform for collaborative transit planning that builds on open-source urban analytics tools such as Conveyal Transport Analyst.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/08/28/ezgif.com-optimize.gif,5175,--Choose Location,2019-09-03T18:32:43.123Z,"['kll@media.mit.edu', 'rchin@media.mit.edu', 'jiw@media.mit.edu', 'noyman@media.mit.edu', 'ptinn@media.mit.edu']",True,CityscopeBostonBRT,2015-01-01,[],Boston Bus Rapid Transit (BRT) Community Engagement,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:33.032Z,"MIT City Science is working with HafenCity University to develop CityScope for the neighborhood of Rothenburgsort in Hamburg, Germany. The goal is to create an interactive stakeholder engagement tool that also serves as the platform for joint research of modules for city simulation. Researchers are developing modules for walkability, neighborhood connectivity, energy efficiency, and economic activity, among others.Read more about this project here.",,['city-science'],https://dam-prod2.media.mit.edu/x/2016/12/13/Ariel Noyman_5_N4YYJzE.jpg,5183,--Choose Location,2023-07-26T20:54:12.056Z,"['kll@media.mit.edu', 'noyman@media.mit.edu', 'agrignar@media.mit.edu', 'alonsolp@media.mit.edu', 'mdchurch@media.mit.edu']",True,cityscope-hamburg,2015-01-01,"['computer-vision', 'augmented-reality', 'civic-technology', 'open-source', 'urban-planning', 'mapping']",City Science Lab @ Hamburg,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:33.210Z,"Real-time geospatial data is visualized on an exhibition-scale 3D city model. The model is built of LEGO bricks, and visualization is performed by an array of calibrated projectors. Through computation, GIS data is ""LEGO-tized"" to create a LEGO abstraction of existing urban areas. Data layers include mobility systems, land use, social media, business activity, windflow simulations, and more.",2016-01-01,['changing-places'],https://dam-prod2.media.mit.edu/x/2017/10/15/twitter_MJCiC0o.jpg,5191,--Choose Location,2018-01-08T21:22:13.710Z,"['kll@media.mit.edu', 'jiw@media.mit.edu', 'mkh@media.mit.edu', 'csmuts@media.mit.edu']",True,cityscope-mark-i-real-time-data-observatory,2013-09-01,[],CityScope Mark I: Real-Time Data Observatory,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:33.394Z,"The CityScope ""Scout"" prototype integrates augmented reality with real-time mathematical modeling of geospatial systems. In practice, the technology transforms any tabletop into a canvas for land-use planning and walkability optimization. Users perform rapid prototyping with LEGO bricks and receive real-time simulation and evaluation feedback.",,['changing-places'],https://dam-prod2.media.mit.edu/x/files/Display/1528557_10201545291019831_1108752879_n-1.jpg,5199,--Choose Location,2017-10-16T03:13:05.718Z,"['kll@media.mit.edu', 'jiw@media.mit.edu']",True,OLD_cityscope-mark-ii-scout,2014-01-01,[],CityScope Mark II: Scout,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:33.550Z,"The Dynamic 3D prototype allows users to edit a digital model by moving physical 3D abstractions of building typologies. Movements are automatically detected, scanned, and digitized so as to generate inputs for computational analysis. 3D information is also projected back onto the model to give the user feedback while edits are made.",,['changing-places'],https://dam-prod2.media.mit.edu/x/files/Display/projection_mapping.JPG,5207,--Choose Location,2017-10-16T03:03:54.492Z,"['kll@media.mit.edu', 'jiw@media.mit.edu']",True,OLD_cityscope-mark-iii-dynamic-3d,2014-01-01,[],CityScope Mark III: Dynamic 3D,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:33.698Z,"This project depicts the design, deployment and operation of a Tangible Regulation Platform, a physical-technological apparatus made for the distilment of regulations. The platform is set to exemplify the effects of regulations on a designated territory, allowing planners, designers, stakeholders and community members a common ground for discussion and decision making. An accessible and self-explanatory tool, this platform illustrates the relationship between urban form and regulations, offering a seamless and transparent process of regulation-based urban design. Lastly, projecting on the foreseen future of law and urbanism, this project proposes an alternative data and performance-based approach for the making of new regulations. Beyond excelling the processes of design under regulations, this platform and other new tools are offered to help facilitate a discussion on the way future regulations will be devised, improving both the design processes and their final outcome.",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/01/06/IMG_0334.jpg,5215,--Choose Location,2019-04-09T14:40:07.692Z,"['kll@media.mit.edu', 'jiw@media.mit.edu', 'noyman@media.mit.edu']",True,cityscope_playground,2014-09-01,"['computer-vision', 'architecture', 'augmented-reality', 'civic-technology', 'data', 'open-source', 'urban-planning', 'mapping']",CityScope PlayGround: MIT East Campus,PUBLIC,http://ArielNoyman.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:33.853Z,"We recently led a workshop in Saudi Arabia, with staff from the Riyadh Development Authority, to test a new version of our CityScope platform. With only an hour to work, four teams of five professionals competed to develop a redevelopment proposal for a neighborhood near the city center. The platform evaluated their designs according to energy, daylighting, and walkability.",2016-09-01,['city-science'],https://dam-prod2.media.mit.edu/x/files/Display/CityScope%20Riyadh.jpg,5223,--Choose Location,2018-06-26T14:28:55.162Z,"['kll@media.mit.edu', 'jiw@media.mit.edu', 'mkh@media.mit.edu']",True,cityscope-mark-iva-riyadh,2014-09-01,"['computer-vision', 'augmented-reality', 'open-source', 'mapping']",CityScope Riyadh,PUBLIC,http://irawinder.com/blog/riyadh,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:33.994Z,"CityScope MarkIVb is programmed to demonstrate and model the relationship between land use (live and work), population density, parking supply and demand, and traffic congestion. ",,['changing-places'],https://dam-prod2.media.mit.edu/x/files/Display/Toronto.JPG,5231,--Choose Location,2017-10-16T03:05:09.935Z,"['kll@media.mit.edu', 'jiw@media.mit.edu', 'csmuts@media.mit.edu']",True,OLD_cityscope-mark-ivb-land-usetransportation,2014-09-01,[],CityScope Mark IVb: Land Use/Transportation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:34.118Z,"The Civic Crowdfunding project is an initiative to collect data and advance social research into the emerging field of civic crowdfunding, the use of online crowdfunding platforms to provide services to communities. The project aims to bring together folks from across disciplines and professions—from research and government to the tech sector and community organizations—to talk about civic crowdfunding and its benefits, challenges, and opportunities. It combines qualitative and quantitative research methods, from analysis of the theory and history of crowdfunding to fieldwork-based case studies and geographic analysis of the field.",2015-01-01,['civic-media'],,5239,--Choose Location,2016-12-05T00:17:08.672Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,civic-crowdfunding-research-project,2013-01-01,"['data', 'social-science']",Civic Crowdfunding Research Project,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:34.903Z,"For various reasons, some communities have good connectivity amongst themselves, but limited or inadequate contact with a broader community. The CLAP (Cross-Layer Algorithms for Phealth) project is exploring ways to monitor quality-of-life parameters. We are developing low-cost, low-bandwith sensor networks that support applications for capturing, interpreting, and communicating various aspects of both community data (e.g., water, air, or soil quality), and individual data (e.g., heath, hygiene, nutrition), in settings where there is better connectivity among group members than to a core network. Examples of such communities may be found, for example, in OLPC deployment scenarios.",2010-01-01,[],,5279,--Choose Location,2016-12-05T00:16:18.664Z,"['mbletsas@media.mit.edu', 'pantelis@media.mit.edu']",True,clap,2009-01-01,[],CLAP,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:35.595Z,"The automatic classification of marine mammal sounds is very attractive as a means of assessing massive quantities of recorded data, freeing humans and offering rigorous and consistent output. Calculations on a set of vocalizations of Northern Resident killer whales using dynamic time warping have been reported recently. Since this method requires the time-consuming pre-processing measurement of frequency contours, we have explored the use of Gaussian Mixture Models (GMM) and Hidden Markov Models (HMM). These methods can be applied directly to time-frequency decompositions of the recorded signals. Calculations have been made on a set of 75 calls previously classified perceptually into seven call types. With cepstral coefficients as features both HMM�s and GMM�s give over 90% agreement with the perceptual classification, with the
HMM over 95% for some cases.",2009-09-01,['music-mind-and-machine'],,5327,--Choose Location,2016-12-05T00:16:07.280Z,"['bv@media.mit.edu', 'brown@media.mit.edu']",True,classification-of-killer-whale-sounds-with-gmm-and-hmm,2007-09-01,[],Classification of Killer Whale Sounds with GMM and HMM,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:35.950Z,Click fraud is an important problem plaguing online advertisers. A click fraud is an attempt by an agency (such as a competitor) to increase the amount charged to an advertiser. Motivations for click fraud range from the urge to deplete a competitor's budget (to ensure that the competitor's advertisement is not aired to consumers) to an attempt to receive cheaper advertising. We are looking at statistical methods of measuring click fraud in order to control this problem.,2007-01-01,['erationality'],,5351,E15-313,2016-12-05T00:16:18.758Z,[],True,click-fraud-detection,2006-09-01,[],Click-Fraud Detection,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:36.116Z,"The practical demands of modern life obligate people to spend time physically separated from the people they care about. To cope, people turn to communications technology like the telephone, email, and instant messaging (IM) to maintain a connection with social contacts. These communication modalities are limited in their ability to provide social-connectedness by their failure to balance expressiveness, overhead, and social obligation. Clique Here is a mobile communications platform that attempts to address this limitation and support a higher degree of social-connectedness by complementing mobile telephone capability with media-rich awareness and multiple lightweight communication modes. The Clique Here system consists of a mobile client (implemented on a camera-embedded mobile handset) and a home client (implemented on a wireless web tablet); communication between clients is facilitated by an application server.",2004-01-01,['living-mobile'],,5359,E15-368,2016-12-05T00:16:18.955Z,['geek@media.mit.edu'],True,clique-here,2003-09-01,[],Clique Here,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:36.819Z,"Imagine opening your eyes and being awake for only half an hour at a time. This is the life that robots traditionally live. This is due to a number of factors, such as battery life and wear on prototype joints. Roboticists have typically muddled though this challenge by crafting handmade perception and planning models of the world, or by using machine learning with synthetic and real-world data, but cloud-based robotics aims to marry large distributed systems with machine learning techniques to understand how to build robots that interpret the world in a richer way. This movement aims to build large-scale machine learning algorithms that use experiences from large groups of people, whether sourced from a large number of tabletop robots or a large number of experiences with virtual agents. Large-scale robotics aims to change embodied AI as it changed non-embodied AI.",2017-05-30,['personal-robots'],,5399,--Choose Location,2017-05-31T18:45:21.362Z,"['cynthiab@media.mit.edu', 'ndepalma@media.mit.edu']",True,cloud-hri,2010-09-01,[],Cloud-HRI,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:36.958Z,"The Clubhouse Village is an online community that connects people at The Clubhouse Network after-school centers around the world. Through the Village, Clubhouse members and staff at more than 100 Clubhouses in 19 countries can share ideas with one another, get feedback and advice on their projects, and work together on collaborative design activities.",2016-08-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/village-logo-square2016.png,5407,LEGO Learning Lab,2016-12-05T00:17:08.759Z,"['mres@media.mit.edu', 'chrisg@media.mit.edu', 'nrusk@media.mit.edu', 'sylvan@media.mit.edu']",True,clubhouse-village,2003-01-01,[],Clubhouse Village,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:37.193Z,"CNSILK explores the design and fabrication potential of silk fibers–inspired by silkworm cocoons–for the construction of woven habitats. It explores a novel approach to the design and fabrication of silk-based building skins by controlling the mechanical and physical properties of spatial structures inherent in their microstructures using multi-axis fabrication. The method offers construction without assembly, such that material properties vary locally to accommodate for structural and environmental requirements. This approach stands in contrast to functional assemblies and kinetically actuated facades which require a great deal of energy to operate, and are typically maintained by global control. Such material architectures could simultaneously bear structural load, change their transparency so as to control light levels within a spatial compartment (building or vehicle), and open and close embedded pores so as to ventilate a space.",2019-05-31,['mediated-matter'],,5423,--Choose Location,2021-05-21T13:08:20.159Z,['neri@media.mit.edu'],True,cnsilk-computer-numerically-controlled-silk-cocoon-construction,2010-01-01,[],CNSILK: Computer Numerically Controlled Silk Cocoon Construction,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:37.413Z,"Collaborating and media creation are difficult tasks, both for people and for network architectures. CoCam is a self-organizing network for real-time camera image collaboration. Like with all camera apps, just point and shoot; CoCam then automatically joins other media creators into a network of collaborators. Network discovery, creation, grouping, joining, and leaving is done automatically in the background, letting users focus on participation in an event. We use local P2P middleware and a 3G negotiation service to create these networks for real-time media sharing. CoCam also provides multiple views that make the media experience more exciting�such as appearing to be in multiple places at the same time. The media is immediately distributed and replicated in multiple peers; thus if a camera phone is confiscated or lost, other users have copies of the images.",2013-09-01,"['information-ecology', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/icon-small.png,5439,--Choose Location,2016-12-05T00:16:18.896Z,"['holtzman@media.mit.edu', 'lip@media.mit.edu']",True,cocam,2012-01-01,[],CoCam,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:37.521Z,"Coco is an alternative applications development system for PDAs based on PalmOS and PocketPC. Using Coco's development environment on a desktop machine, users will be able to write programs in a specialized Logo language and download them to run on PDAs. The goal is to increase the range of people who can develop solutions for PDAs, including standard PalmOS and PocketPC applications such as Calendar or games such as Ms. Pacman. In particular, we plan to enable PDALogo communication with the serial port, thereby allowing integration with the Tower prototyping system.",2004-01-01,[],,5447,E15-344,2016-12-05T00:16:18.926Z,[],True,coco,2002-01-01,[],Coco,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:37.606Z,Coda is a collaborative musical-knowledge database in which articles are accessed by selecting graphic entities in a musical score. Coda is meant to serve as the center for a community of learners sharing music-theory knowledge and musical ideas through musical pieces with personal meaning.,2012-09-01,['opera-of-the-future'],,5455,,2016-12-05T00:17:08.816Z,['tod@media.mit.edu'],True,coda,2012-09-01,[],Coda,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:37.810Z,"Code4Rights promotes human rights through technology education. By facilitating the development of rights-focused mobile applications in workshops and an online course, Code4Rights enables participants to create meaningful technology for their communities in partnership with local organizations. For example, Code4Rights, in collaboration with It Happens Here, a grassroots organization focused on addressing sexual violence, created the First Response Oxford App to address sexual violence at Oxford University. Over 30 young women contributed to the creation of the app, which provides survivors of sexual violence and friends of survivors with information about optional ways to respond, essential knowledge about support resources, critical contact details, and answers to frequently asked questions.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/c4r-logo.png,5471,--Choose Location,2022-06-10T13:35:29.881Z,"['ethanz@media.mit.edu', 'joyab@media.mit.edu']",True,code4rights,2014-09-01,[],Code4Rights,PUBLIC,http://www.code4rights.org,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:37.900Z,"Codeable Objects is a library for processing that allows people to design and build objects using geometry and programming. Geometric computation offers a host of powerful design techniques, but its use is limited to individuals with a significant amount of programming experience or access to complex design software. In contrast, Codeable Objects allows a range of people, including novice coders, designers, and artists to rapidly design, customize, and construct an artifact using geometric computation and digital fabrication. The programming methods provided by the library allow the user to program a wide range of structures and designs with simple code and geometry. When users compile their code, the software outputs tool paths based on their specifications, which can be used in conjunction with digital fabrication tools to build their objects. ",2013-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/04/04/scott_lamps.jpg,5479,--Choose Location,2017-04-04T15:19:21.631Z,"['mres@media.mit.edu', 'leah@media.mit.edu', 'jacobsj@media.mit.edu']",True,codeable-objects,2012-01-01,[],Codeable Objects,PUBLIC,http://media.mit.edu/~jacobsj,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:38.039Z,"Computational photography is an emerging multi-disciplinary field at the intersection of optics, signal processing, computer graphics and vision, electronics, art, and online sharing in social networks. The first phase of computational photography was about building a super-camera that has enhanced performance in terms of the traditional parameters, such as dynamic range, field of view, or depth of field. We call this Epsilon Photography. The next phase of computational photography is building tools that go beyond the capabilities of this super-camera. We call this Coded Photography. We can code exposure, aperture, motion, wavelength, and illumination. By blocking light over time or space, we can preserve more details about the scene in the recorded single photograph.",,['camera-culture'],,5487,E15-320,2016-12-05T00:17:08.841Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,coded-computational-photography,2008-01-01,[],Coded Computational Photography,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:38.211Z,"We present coded focal stack photography as a computational photography paradigm that combines a focal sweep and a coded sensor readout with novel computational algorithms. We demonstrate various applications of coded focal stacks, including photography with programmable non-planar focal surfaces and multiplexed focal stack acquisition. By leveraging sparse coding techniques, coded focal stacks can also be used to recover a full-resolution depth and all-in-focus (AIF) image from a single photograph. Coded focal stack photography is a significant step towards a computational camera architecture that facilitates high-resolution post-capture refocusing, flexible depth of field, and 3D imaging.",2015-09-15,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/1.png,5495,--Choose Location,2016-12-05T00:17:04.011Z,"['raskar@media.mit.edu', 'gordonw@media.mit.edu', 'naik@media.mit.edu']",True,coded-focal-stack-photography,2013-01-01,[],Coded Focal Stack Photography,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:38.388Z,"We propose Coded Lens, a novel system for lensless photography. The system does not require highly calibrated optics, but instead, utilizes a coded aperture for guiding lights. Compressed sensing (CS) is used to reconstruct scene from the raw image obtained through the coded aperture. Experimenting with synthetic and real scenes, we show the applicability of the technique and also demonstrate additional functionality such as changing focus programmatically. We believe this will lead to cheaper, more compact, and even more versatile imaging systems.",2014-01-01,['tangible-media'],,5503,--Choose Location,2016-12-05T00:16:19.062Z,['ishii@media.mit.edu'],True,coded-lens,2013-09-01,[],Coded Lens,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:38.580Z,A broad series of studies is being executed in the domain of computational machinery with intrinsically visible properties. Such experiments seek to culminate in a superior redesign of the programming paradigm as a process more oriented toward the visual thinker.,2001-12-30,['aesthetics-computation'],,5511,E15-443,2016-12-05T00:16:19.086Z,[],True,codefocus,2000-12-31,[],codefocus,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:38.762Z,"Involving communities in the design process results in products that are more responsive to a community's needs, more suited to accessibility and usability concerns, and easier to adopt. Civic media tools, platforms, and research work best when practitioners involve target communities at all stages of the process: iterative ideation, prototyping, testing, and evaluation. In the codesign process, communities act as codesigners and participants, rather than mere consumers, end-users, test subjects, or objects of study. In the Codesign Studio, students practice these methods in a service learning project-based studio, focusing on collaborative design of civic media with local partners. The Toolkit will enable more designers and researchers to utilize the co-design process in their work by presenting current theory and practices in a comprehensive, accessible manner.",2014-01-01,['civic-media'],,5519,--Choose Location,2016-12-05T00:16:36.084Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'msauter@media.mit.edu']",True,codesign-toolkit,2011-09-01,[],Codesign Toolkit,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:38.951Z,"This research examines the use of new music technologies as clinical interventions for individuals with autistic spectrum disorders. The technological application mediates and supports group music making. The intervention is designed to affect the domains where autistic individuals show symptomatic behaviors: communication, psychosocial interaction, and repetitive behavior. Furthermore, through working with the interface, a user generates quantifiable information pertaining to their cognitive performance on domain-specific cognitive tasks. This information is valuable for correlation with emergent social behavior as measured by the system. The primary contribution of the work will be to develop the design principles and methodology to track cognitive features over the course of a complex music interaction.",2006-01-01,['opera-of-the-future'],,5527,E15-445,2016-12-05T00:16:19.131Z,['tod@media.mit.edu'],True,cognitive-composing,2005-09-01,[],Cognitive Composing,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:39.142Z,"While we have learned much about human behavior and neurobiology, there is arguably no field that studies the mind itself. We want to overcome the fragmentation of the cognitive sciences. We aim to create models and concepts that bridge between methodologies, and can support theory-driven research. Among the most interesting questions: How do our minds construct the dynamic simulation environment that we subjectively inhabit, and how can this be realized in a neural substrate? How can neuronal representations be compositional? What determines the experiential qualities of cognitive processes? What makes us human?",2017-06-01,"['synthetic-neurobiology', 'playful-systems']",,5535,--Choose Location,2022-06-10T14:17:21.093Z,"['esb@media.mit.edu', 'slavin@media.mit.edu', 'amarbles@media.mit.edu', 'joscha@media.mit.edu']",True,cognitive-integration-the-nature-of-the-mind,2016-01-01,[],Cognitive Integration: The Nature of the Mind,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:08:39.331Z,"There is a wide cultural belief in the power of the Internet and social media as enablers of collective intelligence. They help us spread information rapidly, and learn useful information from each other. But there are fundamental limits to the capabilities of those networks. Understanding these limits is essential to improving social media and allowing society to make the most of it.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/03/22/dynamics copy.jpg,5543,--Choose Location,2020-07-23T19:29:13.667Z,"['irahwan@media.mit.edu', 'lorenzoc@media.mit.edu']",True,cognitive-limits-of-social-networks,2015-01-01,[],Cognitive Limits of Social Networks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:08:39.496Z,"With CogNotes, new music composition tools become platforms for cognitive assessment. CogNotes users engage their memory as they emerge as composers and participants in their own health process. Together with partners the Lincoln Park Performing Arts School and the Yamaha Corporation, a group of seniors are undertaking an extensive multi-month music composition workshop built around Tod Machover's Hyperscore program. The program is outfitted with cognitive measures sensitive to the earliest transition to Alzheimer's Disease, validated as part of post-doc Adam Boulanger's research with Harvard Medical School and the Alzheimer's Association. Disease assessment can be part of your everyday, creative, and rewarding life. You can be the manager of your own health information as part of the activities you love.",2012-01-01,['opera-of-the-future'],,5551,,2016-12-05T00:16:27.456Z,['tod@media.mit.edu'],True,cognotes-cognitive-assessment-in-social-media-enabled-creativity-tools,2011-01-01,[],CogNotes: Cognitive Assessment in Social Media-Enabled Creativity Tools,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:39.895Z,"To foster and better understand collaboration in the Scratch Online Community, we created Collab Camp, a month-long event in which Scratch community members form teams (�collabs�) to work together on Scratch projects. Our goals include: analyzing how different organizational structures support collaboration in different ways; examining how design decisions influence the diversity of participation in collaborative activities; and studying the role of constructive feedback in creative, collaborative processes. (Collab Camp was a collaboration with Yasmin Kafai's research group at University of Pennsylvania.)",2014-01-01,['lifelong-kindergarten'],,5575,--Choose Location,2016-12-05T00:17:08.935Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'ria@media.mit.edu']",True,collab-camp,2011-01-01,[],Collab Camp,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:40.143Z,"Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? We use machine learning algorithms to develop  Companion AI  to make robots learn to be helpful to young learners. Our robots learn actions that improve children's storytelling and keep them engaged.  We are also interested in how a social robot can personalize its interaction with each child over multiple encounters, because every child learns and engages differently. In Fall 2019, we sent 18 Jibo robots to kindergarten classrooms in Atlanta. Most children in these schools come from less privileged neighborhoods, and the main focus is in prepping children with basic literacy skills, so they're ready to learn when they get to each grade level. This ongoing project will last for the whole school year, during which time our robots will provide one-on-one, personalized story-time interaction for the young readers. Please inspect our website for teachers and parents to find detailed information about the activities.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2017/09/06/Screen Shot 2017-09-06 at 10.01.40 PM.jpg,5591,--Choose Location,2020-02-11T12:30:52.469Z,"['cynthiab@media.mit.edu', 'haewon@media.mit.edu', 'igrover@media.mit.edu', 'nikhita@media.mit.edu', 'samuelsp@media.mit.edu', 'hchen25@media.mit.edu', 'randiw12@media.mit.edu', 'pcuellar@media.mit.edu', 'xiajie@media.mit.edu', 'jon@media.mit.edu', 'jhgray@media.mit.edu']",True,collaborative-robot-storyteller,2015-09-01,"['robotics', 'artificial-intelligence', 'human-machine-interaction', 'kids', 'learning-teaching', 'storytelling', 'machine-learning', 'social-robotics', 'nonverbal-behavior', 'long-term-interaction', 'language-learning', 'technology', 'science', 'diversity', 'women', 'covid19']",Personalized Robot Storytelling Companion,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:40.386Z,"CollaboRhythm is a platform that enables patients to be at the center of every interaction in their healthcare with the goal of empowering them to be involved, reflective, and proactive. Care can be coordinated securely through cell phones, tablets, televisions, and computers so that support can be provided in real-time in the real world instead of through inconvenient doctor's office visits. We are currently developing and demonstrating applications for diabetes and hypertension management. A number of third parties have also developed exciting applications using CollaboRhythm. Please visit http://newmed.media.mit.edu to learn about how you can build a project with us using CollaboRhythm.",2013-01-01,['new-media-medicine'],https://dam-prod2.media.mit.edu/x/files/Display/collaborhythm.JPG,5607,E15-320,2020-07-29T16:20:41.800Z,"['jom@media.mit.edu', 'fmoss@media.mit.edu']",True,collaborhythm,2008-01-01,[],CollaboRhythm,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:40.609Z,"Collections is an application for the management of digital pictures according to their intended audiences. The goal is to create a graphical interface that supports the creation of fairly complex privacy decisions concerning the display of digital photographs. Simple graphics are used to enable the collector to create a wide range of audience arrangements for her digital photographs. The system allows users to express their preferences in sharing their personal pictures over a disembodied environment such as the Web. The system also introduces an original approach to the presentation interface of photographic collections on the Web: a viewing application that takes into account the viewing history of the photographs and the integration of text comments to images. 
",1999-12-30,['sociable-media'],,5623,E15-449,2016-12-05T00:16:19.202Z,['judith@media.mit.edu'],True,collections,1998-12-31,[],Collections,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:40.703Z,"The choices we make about diet, environment, medications, or alternative therapies constitute a massive collection of ""everyday experiments."" These data remain largely unrecorded and are underutilized by traditional research institutions. Collective Discovery leverages the intuition and insight of patient communities to generate datasets about everyday experiments.  We support the patient's process by simplifying tracking and assessment of lifestyle changes in their bodies and lives.  This model is embodied in the free-for-the-public website Personal Experiments (http://personalexperiments.org) and is used to power a clinical ""N-of-1"" experiment platform called MyIBD at the Cincinnati Children's Hospital.",2013-01-01,[],,5631,E15-320,2020-07-29T16:20:26.855Z,"['lieber@media.mit.edu', 'eslick@media.mit.edu', 'fmoss@media.mit.edu']",True,collective-discovery,2008-01-01,[],Collective Discovery,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:41.159Z,"Collective Power  employs a new type of human-computer interaction by visualizing data in a socially and psychologically aware way. The typical display of energy usage where a customer is informed, via various data visualization means, of their own use, is broadened to include a public element. The information for physically adjacent buildings is posted on nearby lamp posts in a vertical meter-like light display that can be viewed for long distances on the urban scale at night. The poles read a comparison to a global standard so that the energy collective participants can tell their collective standing in energy use, percapita, at a glance. More detailed information about individual participants in the energy collective is provided through an iPhone application accessible through a projection from each pole, enabling members of the public and the collective to improve their energy use habits.",2011-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/Thumbnail-1.jpg,5663,,2016-12-05T00:16:19.344Z,['holtzman@media.mit.edu'],True,collective-power,2011-09-01,[],Collective Power,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:41.454Z,"With Color Code, you can create computer programs that respond to colors of objects in the physical world. You can stack up LEGO bricks to form an obstacle in a video game, integrate a crayon-drawn picture into a virtual story, or use M&Ms to create a musical score. ",2012-01-01,['lifelong-kindergarten'],,5679,,2016-12-05T00:16:19.391Z,"['mres@media.mit.edu', 'silver@media.mit.edu', 'ericr@media.mit.edu']",True,color-code,2010-09-01,[],Color Code,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:41.590Z,"Comm.unity is a platform that implements a wireless, device-to-device information system that bypasses the need for any centralized servers, coordination, or administration. A key feature of the platform is that it combines knowledge, awareness, and learning of the user's social relationships and integrates this information into the communication protocols and network services. Comm.unity is designed to work on as many devices as possible, and with as many different radios as possible (WiFi, Bluetooth, IR). It is designed as a platform over which many different networked applications could be developed with ease. SnapN�Share and additional applications in development are intended to be used in upcoming field studies to collect information about user behavior and their social interactions, and aid in fine-tuning the platform�s learning capabilities.",2009-09-01,['viral-communications'],,5687,E15-483,2016-12-05T00:16:19.417Z,"['nadav@media.mit.edu', 'lip@media.mit.edu']",True,community,2008-09-01,"['communications', 'networks', 'social-media', 'sensors']",Comm.unity,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:41.869Z,"Comment Flow is a flexible tool for the content-driven exploration and visualization of a social network. Building upon a traditional force-directed network layout, our system shows the activity and the information exchange (postings in the comment box) between nodes, taking the sequence and age of messages into account. This project serves both as an illustration of one approach to the general problem of individuated network visualization and as an example of the practical uses of such representations. By going beyond the �skeleton� of network connectivity and looking at the flow of information between the individual actors, we can create a far more accurate portrait of online social life.",2017-05-01,['sociable-media'],,5703,E15-390,2022-06-10T14:41:29.783Z,['judith@media.mit.edu'],True,comment-flow,2007-01-01,[],Comment Flow,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:08:42.025Z,"CommenTV is a social commenting system for audiovisual content, able to take and display texts, images, and related videos as social comments. ",2012-01-01,['information-ecology'],,5711,,2016-12-05T00:16:19.500Z,['holtzman@media.mit.edu'],True,commentv,2011-01-01,[],CommenTV,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:42.380Z,"ARIA (Annotation and Retrieval Integration Agent) is a software agent that acts as an assistant to a user writing email or Web pages. As the user types a story, it does continuous retrieval and ranking on a photo database. It can use descriptions in the story text to semi-automatically annotate pictures based on how they are used. This project is improving ARIA's automatic annotation capabilities through world-aware semantic understanding of the text; making photo retrieval more robust by using Open Mind Common Sense to make semantic connections between the story text and annotations; and learning personal common sense through the text that can then be used to improve photo retrieval by enabling personalized semantic connections.",2007-01-01,['software-agents'],,5727,E15-383,2016-12-05T00:16:27.796Z,['lieber@media.mit.edu'],True,common-sense-and-semantic-understanding-for-aria,2001-01-01,[],Common-Sense and Semantic Understanding for ARIA,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:42.535Z,"This project aims to develop an intelligent personal-finance advisory agent that bridges the gap between the novice user and the expert model of the finance domain. The agent uses common-sense reasoning and inference for associating the user's personal life, financial situation, and goals with the attributes of the expert domain model and vice versa. The agent interface provides a natural-language interface for elicitation and explanations of design and process rationale. The architecture of the system is domain-independent and consequently can be used for any novice-expert domain model.",2010-09-01,['software-agents'],,5735,E15-383,2016-12-05T00:16:19.530Z,['lieber@media.mit.edu'],True,common-sense-investing,2003-09-01,[],Common-Sense Investing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:42.674Z,"A long-standing dream of artificial intelligence has been to put common-sense knowledge into computers�enabling machines to reason about everyday life. Some projects, such as Cyc, have begun to amass large collections of such knowledge. However, it is widely assumed that the use of common sense in interactive applications will remain impractical for years, until these collections can be considered sufficiently complete, and common-sense reasoning sufficiently robust. Recently we have had some success in applying common-sense knowledge in a number of intelligent interface agents, despite the admittedly spotty coverage and unreliable inference of today's common-sense knowledge systems.",2014-09-01,['software-agents'],,5743,E15-383,2022-06-10T14:18:57.370Z,['lieber@media.mit.edu'],True,common-sense-reasoning-for-interactive-applications,2003-01-01,[],Common-Sense Reasoning for Interactive Applications,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:42.818Z,"Common sense enables us to build innovative recommendation systems that are more interactive and user-friendly than traditional collaborative filtering systems. By applying ideas from blending and PerspectiveSpace to recommendations, we discover the characteristics of products that drive user ratings. We can use these characteristics to build intelligent recommendation agents and effective product exploration tools.",2010-09-01,['software-agents'],,5751,--Choose Location,2016-12-05T00:17:08.986Z,['lieber@media.mit.edu'],True,common-sense-recommendations,2008-09-01,[],Common-Sense Recommendations,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:42.955Z,"Common Consensus is a fun, self-sustaining web-based game that both collects and validates commonsense knowledge about everyday goals. Goals are a key element of commonsense knowledge; in many of our inferface agents, we need to recognize goals from user actions (plan recognition), and generate sequences of actions that implement goals (planning). We also often need to answer more general questions about the situations in which goals occur, such as when and where a particular goal might be likely, or how long it is likely to take to achieve.",2014-09-01,['software-agents'],,5759,E15-320,2022-06-10T14:18:57.386Z,['lieber@media.mit.edu'],True,commonconsensus-a-game-for-collecting-commonsense-goals,2005-09-01,[],CommonConsensus: A Game for Collecting Commonsense Goals,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:08:43.112Z,"A CAPTCHA is a ""Completely Automated Public Turing Test to tell Computers and Humans Apart."" CAPTCHAs are tools used frequently to stop spammers from using computer programs to register for Web accounts or to deface wiki pages with advertisements. CAPTCHAs are most familiar as a picture of garbled or distorted text that humans are asked to decode on a Website. This project uses a new type of CAPTCHA that asks a series of common-sense questions; this new CAPTCHA is accessible to vision-impaired users and seems capable of distinguishing between humans and computers effectively. Most importantly, it allows individuals, in the process of their normal actions, to make easy and constructive contributions to AI research by helping to verify shared common-sense databases.",2007-09-01,['software-agents'],,5767,E15-020C,2016-12-05T00:17:09.049Z,['walter@media.mit.edu'],True,commonsense-captcha,2005-01-01,[],Commonsense CAPTCHA,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:43.446Z,"comMotion is a location-aware, mobile communication system that delivers timely, travel condition-related information to the mobile user. How many times have you gone to the grocery store but left the shopping list on the fridge door? Wouldn't it make more sense to have the list delivered to you when you were about to drive by the store?",1999-01-01,"['e-markets', 'cc', 'living-mobile']",,5783,E15-344,2016-12-05T00:17:09.074Z,['geek@media.mit.edu'],True,commotion,1997-01-01,[],comMotion,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:43.973Z,"Providing computers and Internet access to rural communities can help overcome barriers of geographic isolation, lack of services, and poor infrastructure. However, the technologies themselves can create an entirely new set of barriers around language, literacy, and usability. Community Knowledge Sharing aims to provide a single multi-literate environment for communities to share experiences and learn from each other.",2002-12-30,"['e-markets', 'edevelopment']",,5807,E15-391,2016-12-05T00:16:27.822Z,[],True,community-knowledge-sharing,1999-12-31,[],Community Knowledge Sharing,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:44.125Z,"We explore secure, cooperative storage mechanisms in peer-to-peer networks, where a group of peer nodes form a contributory storage infrastructure. In this collective storage platform, each peer node can access a large pool of information by only contributing a small portion of the total required storage. The basic idea is that we fragment a given data-object into several segments and store them in the peer nodes based on DHT (distributed hash table). Each segment is associated with a partial key, and the master key can be retrieved if the number of assembled partial keys reaches a given threshold. In practice, this means one can enlist one's peer group to help with one's storage needs, and vice versa; it is also secure even when one loses one's password.",2009-09-01,['viral-communications'],,5815,E15-483,2016-12-05T00:16:19.629Z,['lip@media.mit.edu'],True,community-storage,2006-09-01,[],Community Storage,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:44.299Z,"What you own can sometimes tell what you need and how you feel. We have built a complete portable inventory of the possessions of one person and we are making that available in place and at the moment. Our system extracts affect information from our context (what we're wearing, what we're doing, where we're doing it), and matches it with a music playlist to provide a soundtrack, which makes it easy to enjoy and discover music that fits your lifestyle. It can also inform our financial interactions with the world and serve as guidance for our community.",2010-01-01,['viral-communications'],,5823,E15-491,2016-12-05T00:16:19.658Z,['lip@media.mit.edu'],True,como,2009-09-01,[],CoMo,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:44.621Z,"Design and modeling processes in architecture and the automotive industry have evolved along completely separate paths. This research project compares these processes in order to discover areas of potential translation or cross-fertilization. Areas such as aesthetics, software, tools, manufacturing, and fabrication have the highest likelihood of translatability.",2009-09-01,['smart-cities'],,5839,E15-001,2016-12-05T00:17:09.120Z,['rchin@media.mit.edu'],True,comparative-design-processes-architecture-vs-automotive,1999-01-01,[],Comparative Design Processes: Architecture vs. Automotive,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0
False,2016-07-15T00:08:44.959Z,"How are people's interests related? By exploring people's interests from LinkedIn profiles, we have created an interest graph to understand these relationships. The graph nodes are interests (hiking, painting, design), color-coded by category. Links between nodes appear when interests co-occur in multiple profiles. By looking at the graph, we can see which interests are more commonly shared among people, and whether these interests are in the same category (that is, similar to each other). This allows us to form hypotheses about how and why certain interests co-occur more often than others, and measure the diversity of a person's interests.",2012-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/IS_logo.png,5855,,2016-12-05T00:17:09.323Z,['lip@media.mit.edu'],True,compound-interest,2012-01-01,[],Compound Interest,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:08:45.231Z,"Consumer photography is undergoing a paradigm shift with the development of light field cameras. Commercial products such as those by Lytro and Raytrix have begun to appear in the marketplace with features such as post-capture refocus, 3D capture, and viewpoint changes. These cameras suffer from two major drawbacks: major drop in resolution (converting a 20 MP sensor to a 1 MP image) and large form factor. We have developed a new light-field camera that circumvents traditional resolution losses (a 20 MP sensor turns into a full-sensor resolution refocused image) in a thin form factor that can fit into traditional DSLRs and mobile phones.",2015-09-15,['camera-culture'],,5871,--Choose Location,2016-12-05T00:17:09.150Z,"['raskar@media.mit.edu', 'gordonw@media.mit.edu', 'bandy@media.mit.edu']",True,compressive-light-field-camera-next-generation-in-3d-photography,2012-01-01,[],Compressive Light-Field Camera: Next Generation in 3D Photography,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:45.365Z,"Our hearing mechanism is very good in ignoring redundant sounds and parsing complex auditory scenes. This is a subject that has been extensively studied, but most of the work is in the heuristic level thus impractical for machine implementation. By redefining listening theories in a more rigorous and mathematical framework we can come closer to constructing machines capable of auditory consciousness.  When successful, we can apply these principles to engineering problems to obtain significant results, one such application has been source separation.",2004-09-01,['music-mind-and-machine'],,5879,--Choose Location,2016-12-05T00:17:09.177Z,['bv@media.mit.edu'],True,computational-auditory-scene-analysis,1994-01-01,[],Computational Auditory Scene Analysis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:45.491Z,"Fashion Design is a domain that offers exciting possibilities when combined with programming and digital fabrication. Soft Objects is an extension of the Codeable Objects software that allows novice programmers to create and build personal clothing and garments through computational design. The programming methods provided by the software allow the user to program a wide range of structures and designs with simple code and geometry. The software includes a graphical user interface that allows users to preview their designs once they have compiled their code.  Users can also import existing patterns and forms to incorporate pre-drawn elements into their shapes and patterns.  Soft Objects supports a variety of digital fabrication output devices by including ink-jet printers, vinyl cutters, laser cutters, and computationally controlled embroidery machines. ",2013-09-01,[],,5887,--Choose Location,2017-04-04T15:04:14.224Z,"['leah@media.mit.edu', 'jacobsj@media.mit.edu']",True,computational-clothing-design,2012-01-01,[],Computational Clothing Design,PUBLIC,http://media.mit.edu/~jacobsj,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:45.739Z,"This research continues to explore technology and computation as expressive elements on the body, while also addressing the larger issue of enabling design in software, firmware, and hardware simultaneously. By blurring the distinction between screen-based applications and behavioral physical devices, we hope to establish a contiguous space for creating computational objects. We are developing an all-purpose system for building small hardware modules that can act as interfaces, output devices, or independent computational units, specifically geared towards quick implementation of custom designed projects.",2001-12-30,['aesthetics-computation'],,5903,E15-443,2016-12-05T00:17:09.213Z,[],True,computational-garments-jetset,1999-12-31,[],Computational Garments / Jetset,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:45.865Z,"Technological advances in the past decade have allowed us to take a close look at the proteomes of living organisms. As a result, more than 120,000 solved protein structures are readily available, and we are still on an exponential growth curve. By looking at the proteomes of current living organisms, we are essentially taking snapshots of the successful results in this evolutionary process of continuous adaptation to the environment. Could we process the information available to us from nature to design new proteins, without the need for millions of years of Darwinian evolution?To answer this question, we are developing an integrated Deep Learning framework for the evolutionary analysis, search, and design of proteins, which we call Evolutron. Evolutron is based on a hierarchical decomposition of proteins into a set of functional motif embeddings. Two of our strongest motivations for this work are gene therapy and drug discovery. In both cases, protein analysis and design play a fundamental role in the implementation of safe and effective therapeutics.",,['molecular-machines'],,5911,--Choose Location,2019-04-17T19:37:56.781Z,"['jacobson@media.mit.edu', 'karydis@media.mit.edu', 'kfirs@media.mit.edu']",True,evolutron,2016-01-01,"['synthetic-biology', 'machine-learning']",Evolutron: Deep Learning for Protein Design,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:45.992Z,"As part of our motivation to expand the classic Montessori curriculum and to address contemporary proficiencies, we are working closely with Montessori experts and computer scientists to develop a scope and sequence for computational thinking that will contribute to the Montessori classroom. This curriculum outlines the key concepts behind computer science, along with the corresponding materials and their lessons. ",2017-05-01,['social-computing'],https://dam-prod2.media.mit.edu/x/2016/10/20/CSScopeSequence.jpg,5919,--Choose Location,2022-06-10T14:40:02.001Z,"['sdkamvar@media.mit.edu', 'yonatanc@media.mit.edu', 'smithkim@media.mit.edu']",True,computational-scope-and-sequence-for-a-montessori-learning-environment,2015-09-01,['learning-teaching'],Computational Scope and Sequence for a Montessori Learning Environment,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:46.122Z,"The Computational Textiles Curriculum is a collection of projects that leverage the creativity and beauty inherent in e-textiles to create an introductory computer-science curriculum for middle- and high-school students. The curriculum is taught through a sequence of hands-on project explorations of increasing difficulty, with each new project introducing new concepts in computer science, ranging from basic control flow and abstraction to more complex ideas such as networking, data processing, and algorithms. Additionally, the curriculum introduces unique methods of working with the LilyPad Arduino, creating non-traditional projects such as a game controller, a networked fabric piano, an activity monitor, and a gesture recognition glove. The projects are validated, calibrated, and evaluated through a series of workshops with middle- and high-school youth in the Boston area. ",2014-01-01,['high-low-tech'],https://dam-prod2.media.mit.edu/x/files/Display/IMG_0087-square.jpg,5927,,2022-06-10T13:57:55.504Z,"['leah@media.mit.edu', 'kanjun@media.mit.edu']",True,computational-textiles-curriculum,2012-01-01,[],Computational Textiles Curriculum,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:46.242Z,"After meeting someone for the first time, we come away with an intuitive sense of how much we can trust that person. Nonverbal behaviors, such as gaze patterns, body language, and facial expressions, have been explored as honest or �leaky� signals that are salient cues in gaining trust insights. We are working to create a computational model for recognizing interpersonal trust in social interactions. By observing the trust-related nonverbal cues expressed in the social interaction, we aim to design a machine-learning algorithm that is capable of differentiating whether a person finds his or her socially assistive robot to be a trustworthy or untrustworthy partner. We aim to enable robots to understand our nonverbal signals. With so much of our communication being passed in these nonverbal streams, we hope that by enabling robots to understand these signals, we can design robots that can communicate with us more effectively.",2014-01-01,['personal-robots'],,5935,--Choose Location,2017-10-15T14:37:39.275Z,"['cynthiab@media.mit.edu', 'jinjoo@media.mit.edu']",True,computationally-modeling-interpersonal-trust-using-nonverbal-behavior-for-human-robot-interactions,2011-01-01,[],Computationally Modeling Interpersonal Trust Using Nonverbal Behavior for Human-Robot Interactions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:46.391Z,"The Clubhouse provides a creative and safe out-of-school learning environment where young people from underserved communities around the world work with adult mentors to explore their own ideas, develop new skills, and build confidence in themselves through the use of technology. The first Clubhouse was established in 1993, as a collaboration between the Lifelong Kindergarten group and The Computer Museum (now part of the Boston Museum of Science). Four guiding principles were created to empower youth from all backgrounds to become more capable, creative, and confident learners. The four principles are: learning by designing, following one's interests, building a community, and fostering respect and trust.  Since then the network has expanded to more than 120 centers in 20 countries, serving more than 30,000 young people annually.The Lifelong Kindergarten group continues to develop new technologies, introduce new educational approaches, and lead professional-development workshops for Clubhouses around the world. Learn more: https://theclubhousenetwork.org/",,"['lifelong-kindergarten', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2016/10/20/feyalegria-girls.jpg,5943,LEGO Learning Lab,2023-09-07T23:36:22.587Z,"['mres@media.mit.edu', 'chrisg@media.mit.edu', 'nrusk@media.mit.edu', 'jaleesat@media.mit.edu', 'rupal@media.mit.edu']",True,computer-clubhouse,1993-01-01,"['human-computer-interaction', 'learning-teaching', 'technology', 'diversity', 'women']",The Clubhouse Network,PUBLIC,http://www.computerclubhouse.org/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:46.559Z,"Quilts have served as a cultural symbol for centuries and represent the diversity of people, places, ideas, and events from every area of a community. The Clubhouse Quilt was a vehicle for kids in the Computer Clubhouse Network to share their diverse projects (artwork, music, movies, poetry, and 3-D animation) with the rest of the Clubhouse community. 
The Quilt consisted of patches, each of which represented a single project with links to information about the project designer(s), their clubhouse(s) and local community. This project provided an outlet for Clubhouse members to share the uniqueness of their Clubhouse and community and learn about others.",2001-09-01,['lifelong-kindergarten'],,5951,E15-001,2016-12-05T00:17:09.362Z,"['mres@media.mit.edu', 'leob@media.mit.edu']",True,computer-clubhouse-quilt,2001-01-01,[],Computer Clubhouse Quilt,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:46.734Z,"Imagine that a full computer (with touch screen, sensors, keyboard, and everything else) was simply just another craft on your craft table. How would you use it? Computer crafting weaves together full computers with regular paper and markers, textiles, and everyday objects. By using computers as just another craft, the everyday world can be programmed and combined with computers.",2010-01-01,['lifelong-kindergarten'],,5959,LEGO Learning Lab,2016-12-05T00:17:09.382Z,"['mres@media.mit.edu', 'silver@media.mit.edu', 'kbrennan@media.mit.edu']",True,computer-crafting,2009-01-01,[],Computer Crafting,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:46.914Z,"Our novel platform automates experiments in genetic engineering to bring large-scale moonshot projects within reach. Too often, lab experiments are limited in scale by human fatigue and costs associated with manual labor. In particular, the process of delivering genetic materials via manual microinjection remains a long-standing bottleneck. We  developed a computer-assisted microinjection platform to streamline the production of transgenic organisms. Briefly, organisms are immobilized in a gel and microinjections are performed using precision robotics using computer vision algorithms. This platform demonstrated high-throughput gene editing in an animal model (C. elegans) for the first time. We use this technology to refine and create safeguards for our gene drive and population editing technologies.",2018-12-31,['sculpting-evolution'],,5967,--Choose Location,2020-06-15T12:43:10.145Z,"['esvelt@media.mit.edu', 'jmin01@media.mit.edu', 'erikad@media.mit.edu', 'codyg@media.mit.edu']",True,computer-assisted-transgenesis,2016-01-01,[],Computer-Assisted Transgenesis,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:47.208Z,"Computer workstations can provide a much-needed user interface to advanced telephony functions, provided a path exists between the workstation and switch. Controlling call set-up from a user�s workstation allows a greater degree of personalization and dynamic call-handling, both for outgoing and incoming calls. This project is being implemented in the ISDN environment of MIT�s campus telephone network, using PhoneServer, a computer network interface to Basic Rate ISDN switching.",1999-12-30,['living-mobile'],,5983,,2016-12-05T00:16:27.645Z,['geek@media.mit.edu'],True,computers-and-telephony,1999-12-31,[],Computers and Telephony,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:47.488Z,"Computing in Context is a distributed computing environment tailored to host agile, opprtunistic processes on a collection of heterogeneous computer systems and devices. The environment lets each user leverage the diversity of resources made available in a given neighborhood to provide rich computational abilities for distributed applications. Processes deployed on the substrate opportunistically aggregate and consume ambient resources at runtime. Applications are written in a high-level language providing a simple and extensible semantic to express complex distributed behaviors.
",2008-01-01,['viral-communications'],,5999,E15-483,2016-12-05T00:16:10.345Z,['lip@media.mit.edu'],True,computing-in-context,2008-01-01,[],Computing in Context,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:47.602Z,"The ComTouch project explores interpersonal communication by use of haptic technology. Touch as an augmentation to existing communication may provide and enhance existing audio and video media. ComTouch is a handheld device that allows the squeeze under each finger to be represented as vibration. This haptic device will enable users to transmit thoughts, feelings, and concepts to each other remotely. The form factor is designed to fit on the back of a cellular phone; as users talk on the cell phone, they can squeeze and transmit a representation of their touch. Through this research, we aim to describe more accurately the language of touch-based communication. We also hope to devise a set of guidelines for designing touch-based communication devices.",2002-12-30,['tangible-media'],,6007,E15-441,2020-06-09T20:22:43.247Z,['ishii@media.mit.edu'],False,comtouch-haptic-communication-medium,2000-12-31,[],ComTouch: Haptic Communication Medium,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:47.718Z,"Imparting common-sense knowledge to computers enables a new class of intelligent applications better equipped to make sense of the everyday world and assist people with everyday tasks. Our approach to this problem is ConceptNet, a freely available commonsense knowledge base that possesses a great breadth of general knowledge that computers should already know, ready to be incorporated into applications. ConceptNet 5 is a semantic network with millions of nodes and edges, built from a variety of interlinked resources, both crowd-sourced and expert-created, including the Open Mind Common Sense corpus, WordNet, Wikipedia, and OpenCyc. It contains information in many languages including English, Chinese, Japanese, Dutch, and Portuguese, resulting from a collaboration of research projects around the world. In this newest version of ConceptNet, we aim to automatically assess the reliability of its data when it is collected from variously reliable sources and processes.",2016-08-31,[],https://dam-prod2.media.mit.edu/x/files/Display/cnet-logo.png,6015,E15-383,2021-07-14T15:33:07.050Z,"['havasi@media.mit.edu', 'eslick@media.mit.edu']",True,conceptnet,2003-01-01,[],ConceptNet,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:47.951Z,"Audio conferences have a number of striking downsides�we lose many of the familiar physical signals from other people that help us judge what other people want to do. In this project, we explore how a new currency for conversation might be added to audio conferences to provide a new channel for non-verbal communication. We're experimenting with different uses for this currency, such as managing the agenda, conversation turn-taking, or length of speaking. Adding this currency provides participants with meaningful, non-verbal ways to communicate through giving currency to people in the meeting you want to support, or spending that currency to guide the meeting in directions in which you want it to move. Mediated environments typically lack these rich backchannels, and we hope that adding new kinds of channels will change the power structures in meetings in a positive way.",2010-01-01,['living-mobile'],,6031,E15-368,2016-12-05T00:16:12.337Z,['geek@media.mit.edu'],True,conch,2009-01-01,[],Conch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:08:48.297Z,"Americans have a negative personal savings rate; this indicates budgeting and saving can be very hard to do�money is abstract, fungible, and difficult to think about, and current software for planning budgeting and savings does not remove this difficulty. However, research from psychology and behavioral economics suggests ways to make personal budgeting easier, more natural, and more successful, by making trade-offs and the effect of risk more concrete.  This project aims to use these principles to create a more effective tool for personal money management.",2008-01-01,['erationality'],,6047,E15-320A,2016-12-05T00:17:09.552Z,[],True,concrete-budgeting-for-personal-savings,2006-09-01,[],Concrete Budgeting for Personal Savings,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:48.464Z,"Concrete Financial Sim aims to anticipate probable outcomes of different decisions across time. Life consistently presents choices that require a rational balance between instant gratification and long-term consequences. Should I buy the sunglasses now or should I save? Should I buy a house, or should I rent a room? What if I do it next year instead of next month? Intertemporal components of choices complicate the decision-making process. The complexity comes not in just a one-to-one immediate tradeoff decision, but in its long-term implications. Based on one�s past financial behavior and current plans, we are designing a decision environment that visualizes the future values of present choices. The goal is to create a reality-based model that informs decision makers of their probable rewards and penalties over time, and will serve as a �cognitive prosthesis� for people to externalize their mental model of intertemporal choices.",2010-01-01,['social-machines'],,6055,--Choose Location,2016-12-05T00:16:13.202Z,"['dkroy@media.mit.edu', 'aithpao@media.mit.edu']",True,concrete-financial-sim,2009-01-01,[],Concrete Financial Sim,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:48.612Z,"While instant messaging clients are frequently and widely used for interpersonal communication, they lack the richness of face-to-face conversations. Without the benefit of eye contact and other non-verbal ""back-channel feedback,"" text-based chat users frequently resort to typing ""emoticons"" and extraneous punctuation in an attempt to incorporate contextual affect information in the text communication. Conductive Chat is an instant messenger client that integrates users' changing skin conductivity levels into their typewritten dialogue. Skin conductivity level (also referred to as galvanic skin response) is frequently used as a measure of emotional arousal, and high levels are correlated with cognitive states such as high stress, excitement, and attentiveness. On an expressive level, Conductive Chat communicates information about each user's arousal in a consistent, intuitive manner, without needing explicit controls or explanations. On a communication-theory level, this new communication channel allows for more ""media rich"" conversations without requiring more work from the users.",2003-09-01,"['electronic-publishing', 'affective-computing', 'sociable-media']",,6063,E15-320,2016-12-05T00:16:13.264Z,"['judith@media.mit.edu', 'walter@media.mit.edu']",True,conductive-chat,2002-01-01,[],Conductive Chat,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:48.785Z,"This new version of the original online Confectionary project is open and extensible. Designed for constructing and exchanging mediated personal stories, the software invites participants to create story compositions by assembling video, text, image, and audio content within a two-dimensional collage interface. Participants can make use of flexible privacy controls. They publish their stories to a collaborative library and can create a sequence of multiple Confectionaries using the make-path feature. New work involves adaptations for multicultural and international use in schools.",2008-09-01,[],,6071,E15-368,2016-12-05T00:17:09.588Z,['gid@media.mit.edu'],True,confectionary-20,2004-01-01,[],Confectionary 2.0,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:48.933Z,"We have built a configurable infrastructure to protect users' dynamic levels of�privacy in a pervasive sensor network. Our system is based around a badge that can alert the user to the presence of participating sensor networks, plus emit an RF beacon with which the network can gauge the level of privacy desired. Badges either periodically emit an ""opt out"" signal, blocking sensing within their RF (and sensor perceptual) range, or allow the users' desired level of privacy to be preconfigured online. This privacy level depends on user location, and can eliminate or ""blur"" the data and calculated features available from various sensors and sensor nodes. The privacy level can also be dependent on the status of the client browsing the sensor network�the badge user can assign different levels of privacy to different groups of people.",2010-01-01,['responsive-environments'],,6079,E15-331,2016-12-05T00:16:13.682Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,configurable-dynamic-privacy-for-pervasive-sensor-networks,2008-09-01,[],Configurable Dynamic Privacy for Pervasive Sensor Networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:49.274Z,"Connectibles are small ""keepsake"" objects that are exchanged by individuals, establishing a communication portal between them. As an ecology of objects, the Connectibles system physically embodies a social network; as an ecology of ""smart"" objects, it can automatically build rich representations of social networks. Any gift from one person to another contains information about the relationship between the giver and the receiver, but this information remains locked away in each object. If we could capture this information, we might expose social networks that have both more detail and greater consonance with a user's ""reality-based"" social network than current virtual, Internet-based social network applications. We might also open up a new, tangible way to interact with social networks, using as phicons the very same sentimental objects that represent people in the user's social network.",2008-09-01,['object-based-media'],,6095,Garden Conference Room,2016-12-05T00:16:13.904Z,['vmb@media.mit.edu'],True,connectibles,2006-01-01,[],Connectibles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:50.114Z,"Constant Crit encourages Media Lab researchers to post their work in its earliest form, a concise one- to two-sentence statement. The system then displays these ideas throughout the Media Lab, offering others a chance to critique the work by suggesting readings and comments. It also offers a way for others to simply 'like' a project, or to go further and follow it or collaborate with the author.",2010-01-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/constantcrit-logo.png,6143,,2016-12-05T00:16:15.100Z,['holtzman@media.mit.edu'],True,constant-crit,2010-01-01,[],Constant Crit,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:08:50.347Z,"We are developing ""Constructionism"" as a theory of learning and education. Constructionism is based on two different senses of ""construction."" It is grounded in the idea that people learn by actively constructing new knowledge, rather than by having information ""poured"" into their heads. Moreover, constructionism asserts that people learn with particular effectiveness when they are engaged in constructing personally meaningful artifacts (such as computer programs, animations, or robots).",2007-01-01,[],,6159,E15-320,2016-12-05T00:16:16.199Z,"['papert@media.mit.edu', 'mres@media.mit.edu', 'cavallo@media.mit.edu']",True,constructionism,2000-01-01,[],Constructionism,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:50.458Z,"Starting with a simple rectangular box, we are exploring possibilities for ""extracting"" virtual form with the help of minimal constructs that are, in a way, hidden within its volume. The process is not unlike a (subtractive) mode of sculpting, although the notion of interactivity poses new challenges to an otherwise fundamental concept in visualization. Our current goal is to explore form-making at the basic level in order to provide a comprehensive framework for more complex analyses in the future.",2002-12-30,['aesthetics-computation'],,6167,E15-443,2016-12-05T00:17:09.703Z,[],True,constructive-geometries,1999-12-31,[],Constructive Geometries,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:50.553Z,"The Constructopedia was a browsable, interactive database, designed to help children build structures, mechanisms, and computer programs as well as to make connections to the mathematical and scientific ideas underlying those constructions. Whereas encyclopedias focus on ""what is,"" the Constructopedia focused on ""how to"" and ""what if."" The Constructopedia was intended as a communal resource in which children not only get access to building tips, but also contribute their own ideas and designs.",2005-01-01,['lifelong-kindergarten'],,6175,E15-001,2016-12-05T00:16:16.488Z,['mres@media.mit.edu'],True,constructopedia,1996-01-01,[],Constructopedia,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:50.655Z,"The goal of this project, building upon work begun by Stephen Benton and the Spatial Imaging group, is to enable consumer devices such as tablets, phones, or glasses to display holographic video images in real time, suitable for entertainment, engineering, telepresence, or medical imaging. Our research addresses real-time scene capture and transmission, computational strategies, display technologies, interaction models, and applications.",2019-10-01,"['ultimate-media', 'ce-20', 'future-storytelling', 'object-based-media']",,6183,Garden Conference Room,2020-06-10T15:55:15.587Z,"['vmb@media.mit.edu', 'sjolly@media.mit.edu', 'nsavidis@media.mit.edu', 'bdatta@media.mit.edu']",True,consumer-holo-video,2005-01-01,[],Consumer Holo-Video,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:08:50.877Z,"When browsing the Web, what quality information does the user have about the information underlying the links? The only hint behind links is that they change their color when visited. But a great deal of relevant knowledge possessed by colleagues is not exploited. The aim of this project is to develop a system for enhancing links in Web pages, based on the experience of the community. Using a mediator based on IBM WBI intermediary [1] as a platform, we developed a system that behaves like a proxy with some changes: first, it logs user requests, and builds a repository of access statistics and page metadata; second, it keeps user profiles based on their homepages; third, it ranks pages based on these access statistics and a matches with the expert profiles (based on the first two points); fourth, it uses this information to augment browsing via a unique user interface.",2002-09-01,"['software-agents', 'context-aware-computing']",,6199,E15-320,2016-12-05T00:17:09.754Z,['lieber@media.mit.edu'],True,context-aware-annotation-proxy-based-system-caps,2001-01-01,[],Context-Aware Annotation Proxy Based System (CAPS),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:51.004Z,"Current biological research workflows make use of disparate, poorly integrated systems that impose a large mental burden on the scientist, leading to mistakes, often on long, complex, and costly experimental procedures. The lack of open tools to assist in the collection of distributed experimental conditions and data is largely responsible for making protocols difficult to debug, and laboratory practice hard to learn. In this work, we describe an open Protocol Descriptor Language (PDL) and system to enable a context-rich, quantitative approach to biological research. We detail the development of a closed-loop pipetting technology and a wireless sample-temperature sensor that integrate with our Protocol Description platform, enabling novel, real-time experimental feedback to the researcher, thereby reducing mistakes and increasing overall scientific reproducibility.",2015-12-01,['molecular-machines'],,6207,--Choose Location,2016-12-05T00:17:09.781Z,"['jacobson@media.mit.edu', 'fracchia@media.mit.edu']",True,context-aware-biology,2014-01-01,[],Context-Aware Biology,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:51.180Z,"The robotic facade is conceived as a mass-customizable module that combines solar control, heating, cooling, ventilation, and other functions to serve an urban apartment. It attaches to the building ""chassis"" with standardized power, data, and mechanical attachments to simplify field installation and dramatically increase energy performance. The design makes use of an articulating mirror to direct shafts of sunlight to precise points in the apartment interior. Tiny, low-cost, easily installed wireless sensors and activity recognition algorithms allow occupants to use a mobile phone interface to map activities of daily living to personalized sunlight positions. We are also developing strategies to control LED luminaires to turn off, dim, or tune the lighting to more energy-efficient spectra in response to the location, activities, and paths of the occupants. ",2016-12-01,['changing-places'],,6215,--Choose Location,2016-12-08T20:40:51.604Z,['kll@media.mit.edu'],True,OLD_context-aware-dynamic-lighting,2010-01-01,[],_Context-Aware Dynamic Lighting,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:51.370Z,"Pipettes are the equivalent in biology of the keyboard for computer science: a key tool that enables interface with the subject matter. In the case of the pipette, it enables the scientist to move precise amounts of liquids. Pipette design hasn't changed in over 30 years. We've designed a new type of pipette that allows wireless, context-aware operation.",2015-12-01,['molecular-machines'],,6223,--Choose Location,2016-12-05T00:16:59.612Z,"['jacobson@media.mit.edu', 'fracchia@media.mit.edu']",True,context-aware-pipette,2014-01-01,[],Context-Aware Pipette,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:51.534Z,"Digital music libraries are constantly expanding, thus making it increasingly difficult to recall a particular song in the library, let alone create a playlist for a specific event. By using context-aware data, one can automatically generate a playlist to suit one's current activity or mood. Songs corresponding to a certain tempo can be selected from one's music library by converting a rate of step to a tempo value. Alternatively, a musical playlist can be generated based on the affective value of text entered by the user. Thus, music can be tailored to fit a specific context by using alternative classification methods and by monitoring a user�s actions.",2007-01-01,['music-mind-and-machine'],,6231,E15-484,2016-12-05T00:16:18.035Z,['bv@media.mit.edu'],True,context-aware-playlist-generation,2005-09-01,[],Context-Aware Playlist Generation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:51.672Z,"These tables are aware of their height and of neighboring tables and respond accordingly. When they are lowered, the high-resolution surface presents pictures you would find in a coffee-table book. When raised, they become bar menus. Adjusting the height also allows drums to be tuned from bongo to kettle drums to a cowbell. When connected to a neighboring table, the two can be used for collaboration and table-to-table interactions. A low-cost, capacitive sensor integrated into the table's piston enables these transformations. The capacitive piston-sensor consists of an evaporative, deposited, metallic layer on a thin dielectric that is wrapped around the piston shaft of a pneumatic cylinder. The capacitor is held in place and pressed against the moving shaft with a plastic retaining clip. These capacitors can tell the height of a chair or table.",2001-09-01,"['counter-intelligence', 'context-aware-computing']",,6239,E15-320,2016-12-05T00:17:09.828Z,[],True,context-aware-tables,2000-01-01,[],Context-Aware Tables,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:52.386Z,"ContextController is a second screen social TV application that augments linear broadcast content with related contextual information. By utilizing existing closed-captioning data, ContextController gathers related explanatory video content, displaying this in real-time synchronized to the original content.",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/contextcontroller_icon.png,6279,--Choose Location,2016-12-05T00:16:19.293Z,['holtzman@media.mit.edu'],True,contextcontroller,2012-01-01,[],ContextController,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:52.518Z,"The Jukebox understands its listeners and attempts to select music based on past performances. A user model is created by looking at a history of responses. A positive response is generated by dropping a coin into the jukebox, and a negative response can be given by a kick.",2002-01-01,['context-aware-computing'],,6287,E15-320,2016-12-05T00:16:19.554Z,[],True,contextual-jukebox,2000-01-01,[],Contextual Jukebox,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:52.734Z,"Motivated by applications in rehabilitation and robotics, we are developing methodologies to control muscle-actuated systems via electrical stimulation. As a demonstration of such potential, we are developing centimeter-scale robotic systems that utilize muscle for actuation and glucose as a primary source of fuel. This is an interesting control problem because muscles: a) are mechanical state-dependent actuators; b) exhibit strong nonlinearities; and c) have slow time-varying properties due to fatigue-recuperation, growth-atrophy, and damage-healing cycles. We are investigating a variety of adaptive and robust control techniques to enable us to achieve trajectory tracking, as well as mechanical power-output control under sustained oscillatory conditions. To implement and test our algorithms, we developed an experimental capability that allows us to characterize and control muscle in real time, while imposing a wide variety of dynamical boundary conditions.",,['biomechatronics'],,6303,E15-418,2021-09-17T19:11:32.615Z,['hherr@media.mit.edu'],True,control-of-muscle-actuated-systems-via-electrical-stimulation,2004-01-01,[],Control of muscle-actuated systems via electrical stimulation,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:52.965Z,"A socially intelligent communication agent that assists the user in her telecommunication tasks needs ""communication intelligence."" One way to acquire such intelligence is to rely on the human social intelligence of co-located people to decide on whether, when, and how to alert a user of an incoming communication. As a prerequisite, the agent needs to know who is part of an ongoing conversation; a network of small, body-worn, wireless sensors helps the agent to determine this. A novel, fully distributed decision-making process is used to detect conversations; these nodes have binary speech detectors and low-range radio transceivers, and communicate asynchronously with each other on a single channel. They independently come to a conclusion about who is part of the user's current conversation by looking at alignment and non-alignment of the speaking parties, and can be queried wirelessly for this list of people.",2005-01-01,['living-mobile'],,6319,E15-368,2016-12-05T00:16:19.728Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,conversation-finder-for-socially-aware-communication,2002-01-01,[],Conversation Finder for Socially Aware Communication,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:53.059Z,"This work uses combined speech input and output to converse with a user seeking to control a computer, or to access information from it. Conversational techniques allow the computer gracefully to limit the vocabulary that is likely to be spoken, facilitating speech recognition. Discourse techniques aid error detection and correction. Domain knowledge and learning about user preferences allow for determining a user�s needs more efficiently. Listening for various forms of back-channel response from the listener permits a talking computer to gauge user interest better, and determine at what level of detail to describe the requested information.",2001-12-30,['living-mobile'],,6327,,2016-12-05T00:16:05.765Z,['geek@media.mit.edu'],True,conversational-interfaces,1984-12-31,[],Conversational Interfaces,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:53.247Z,CONVIVO Communicator is a synchronous voice over IP (VoIP) communication system designed to respond to poor or varying network performance. CONVIVO adapts to a network's performance both by configuring in real-time voice sampling parameters as well as by provisioning a variety of innovative user interfaces. User interface modalities facilitate turn-taking given a very high latency connection and basic messaging given an extremely low bandwidth connection. CONVIVO is designed to work given the network realities of many rural communities in the developing world.,2002-12-30,['edevelopment'],,6343,E15-391,2016-12-05T00:16:19.856Z,[],True,convivo-communicator,2000-12-31,[],CONVIVO Communicator,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:53.647Z,"Cooperative communication represents a paradigm shift from current point-to-point, client-server interaction to a fully decentralized, symmetric model of communication. In order to realize the gains of this model of communication, a new class of protocols is required. In this project, we design protocols and architectures for multipoint-to-multipoint communication, ranging from media distribution to real-time communication.",2005-01-01,['viral-communications'],,6367,E15-483,2016-12-05T00:17:09.945Z,['lip@media.mit.edu'],True,cooperative-transport-protocols,2002-01-01,[],Cooperative Transport Protocols,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:53.820Z,"CopyCAD is an augmented milling machine that allows users to design new objects directly on the material they wish to cut using projected feedback. CopyCAD empowers users to become creators by allowing them to copy geometry from existing objects, directly and tangibly, and remix it by adding geometry from other objects, or by sketching new geometry. CopyCAD attempts to bring remix culture to the design of objects.",2012-01-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/copythumb.jpg,6375,--Choose Location,2016-12-05T00:16:19.907Z,['ishii@media.mit.edu'],True,copycad,2010-01-01,[],CopyCAD,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:53.992Z,"Cord UIs are sensorial augmented cords that allow for simple, metaphor-rich interactions to interface with their connected devices. Cords offer a large, under-explored space for interactions, as well as unique properties and a diverse set of metaphors that make them potentially interesting tangible interfaces. We use cords as input devices and explore different interactions like tying knots, stretching, pinching, and kinking to control the flow of data and/or power. We also look at ways to use objects in combination with augmented cords to manipulate data or properties of a device. For instance, placing a clamp on a cable can obstruct the audio signal to the headphones. Using special materials such as piezo copolymer cables and stretchable cords, we built five working prototypes to showcase these interactions.",2014-09-01,"['tangible-media', 'fluid-interfaces']",,6383,--Choose Location,2016-12-05T00:17:09.974Z,"['ishii@media.mit.edu', 'pattie@media.mit.edu', 'sangwon@media.mit.edu']",True,cord-uis-controlling-devices-with-augmented-cables,2014-01-01,[],Cord UIs: Controlling Devices with Augmented Cables,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:54.193Z,"Named for, and inspired by, the medieval practice of erecting barriers to prevent the spread of disease, Cordon Sanitaire is a collaborative, location-based mobile game in which players seek to isolate an infectious ""patient zero"" from the larger population. Every day, the game starts abruptly, synchronizing all players at once, and lasts for two minutes. In 60 seconds, players must choose either to help form the front line of a quarantine, or remain passive. Under pressure, the ""uninfected"" attempt to collaborate without communication, seeking to find the best solution for the group. When those 60 seconds end, a certain number of players are trapped inside with patient zero, and the score reflects the group's ability to cooperate under duress.",2017-06-01,['playful-systems'],,6391,--Choose Location,2022-06-10T14:14:48.798Z,['slavin@media.mit.edu'],True,cordon-sanitaire,2013-01-01,['gaming'],Cordon Sanitaire,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:54.542Z,"How can a knowledge base learn from the Internet, when you shouldn't trust everything you read on the Internet? CORONA is a system for building a knowledge base from a combination of reliable and unreliable sources, including crowd-sourced contributions, expert knowledge, games with a purpose (GWAP), automatic machine reading, and even knowledge that is imperfectly derived from other knowledge in the system. It confirms knowledge as reliable as more sources confirm it or unreliable when sources disagree, and then by running the system in reverse it can discover which knowledge sources are the most trustworthy.",2013-01-02,['digital-intuition'],,6407,,2022-06-10T13:46:47.114Z,[],True,corona,2012-01-01,[],Corona,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:08:54.699Z,"How best to govern society and promote cooperation is a centuries-old debate: is cooperation best maintained by a central authority, or is it better handled by more decentralized forms of governance? Using mathematical models, we show that when some actors can bribe a powerful centralized authority, they can completely undermine cooperation in society. Counterintuitively, a weaker centralized authority is more effective because it allows peer punishment to restore cooperation in the presence of corruption. Our results help explain why citizen participation is a fundamental necessity for policing the commons.Scientific writings:S. Abdallah, R. Sayed, I. Rahwan, B. LeVeck, M. Cebrian, A. Rutherford, J. Fowler (2014). Corruption Drives the Emergence of Civil Society. Journal of the Royal Society Interface. 11(93).Selected Media: Anti-Corruption Research Network (part of Transparency International)",2015-09-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/03/22/corruption.JPG,6415,--Choose Location,2017-03-23T01:07:23.288Z,[],True,corruption-resistant-cooperation-institutions-vs-crowds,2015-01-01,[],Corruption-Resistant Cooperation: Institutions vs. Crowds,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:08:54.850Z,"We present the design of a cost-effective, wearable sensor to detect and indicate the strength and other characteristics of the electric field emanating from a laptop display. Our bracelet can provide an immediate awareness of electric fields radiated from a frequently used object, thus supporting awareness of ambient background emanation beyond human perception. We discuss how detection of such radiation might help to ""fingerprint"" devices and aid in applications that require determination of indoor location.",2009-09-01,['tangible-media'],,6423,E15-350,2016-12-05T00:17:09.996Z,['ishii@media.mit.edu'],True,cost-effective-wearable-sensor-to-detect-emf,2008-01-01,[],Cost-Effective Wearable Sensor to Detect EMF,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:55.003Z,"CoSync builds the ability to create and act jointly into mobile devices. This mirrors the way we as a society act both individually and in concert. CoSync device ecology combines multiple stand-alone devices and controls them opportunistically as if they are one distributed, or diffuse, device at the user�s fingertips. CoSync includes a programming interface that allows time-synchronized coordination at a granularity that will permit watching a movie on one device and hearing the sound from another. The open API encourages an ever-growing set of such finely coordinated applications.",2013-09-01,"['information-ecology', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/synced_camera.png,6431,--Choose Location,2016-12-05T00:16:20.101Z,"['holtzman@media.mit.edu', 'lip@media.mit.edu']",True,cosync,2012-01-01,[],CoSync,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:55.144Z,"Coterie is an emerging series of experiments in the dynamic visualizations of online communities. Coterie reintroduces a human element to an abstract data source by creating visual representations that evoke the underlying activity and interactions among the members of an online community. The first experiment in this series portrays individuals as ovals, using a bouncing motion to show current individual activity. A person's saturation and opacity reflect their participation, over time, in the group.",2000-12-30,['sociable-media'],,6439,E15-450,2016-12-05T00:16:20.138Z,['judith@media.mit.edu'],True,coterie,1999-12-31,[],Coterie,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:55.422Z,"How can the kitchen of the future help you instead of replace you?  That's the idea behind the CounterActive recipe counter.  It may look like an ordinary Formica kitchen surface, but this counter has been augmented with a overhead projection system and a field sensing array under the counter.  With it, we provide a new medium for teaching people how to cook. The project as a whole explores how moving the computer to the kitchen fundamentally changes the style of interaction.",2001-12-30,"['counter-intelligence', 'personal-information-architecture']",,6455,E15-068,2016-12-05T00:16:20.172Z,['mike@media.mit.edu'],True,counteractive,1998-12-31,[],CounterActive,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:55.552Z,"Computer-assisted crafting is one type of software that can prove relevant for non-industrial crafting communities, such as Santa Clara del Cobre in Mexico. We developed artisanal pattern CAD software, based on the Adolfo Best Maugard drawing technique (Best Maugard, 1923, 1927), to teach Mexican-style drawing to children. Developed with sculptors James Metcalf and Ana Pellicier and the artisans of Santa Clara, this software enabled users to create primitive  graphic motifs and combine them into patterns. This environment provided alternate ways of exploring variations in the creation of motifs and their arrangement into patterns. The software was developed using a vector drawing program, Sketch, as a platform. It was designed not to replace traditional ways of crafting but instead to offer new ways to explore the creation and combination of traditional patterns.",2002-09-01,['lifelong-kindergarten'],,6463,E15-001,2016-12-05T00:16:48.837Z,['mres@media.mit.edu'],True,craft-computing,2001-01-01,[],Craft Computing,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:55.776Z,"We worked with the residents at Camfield Estates, a housing development in Roxbury, MA, to investigate the role of community technology for the purpose of community building in low- to moderate-income and minority communities. In particular, examined how new technology-supported activities can help to increase social capital and to activate cultural capital within the housing development. Our larger goal was to develop new approaches for bridging the digital divide.",2001-09-01,['lifelong-kindergarten'],,6479,E15-001,2016-12-05T00:17:10.135Z,['mres@media.mit.edu'],True,creating-community-connections,1999-01-01,[],Creating Community Connections,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:56.233Z,"We are developing a new set of movement experiences titled Crenulations and Excursions. This work consists of two aspects: a public installation that allows visitors to explore a rich sonic space through their expressive movement, and a short dance performance that allows a trained performer to explore the expressive capabilities of the installation environment. With a tiny, energetic gesture or with a fluid and sweeping movement, a performer can create and shape layers of sound around herself. Both the installation and performance will explore the body as a subtle and powerful instrument, providing continuous control of continuous expression. This piece is related to Elena Jessop's doctoral work on new high-level analysis frameworks for recognition and extension of expressive physical performance.",2013-09-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/files/Display/crenulations_square_crop.jpg,6503,--Choose Location,2016-12-05T00:16:06.254Z,"['tod@media.mit.edu', 'ejessop@media.mit.edu']",True,crenulations-and-excursions,2012-09-01,[],Crenulations and Excursions,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:56.402Z,"Crickets are small programmable devices that can make things spin, light up, and play music. With Crickets, kids can create musical sculptures, interactive jewelry, dancing creatures, and other artistic inventionsand learn important math, science, and engineering ideas in the process. Lifelong Kindergarten researchers previously collaborated with LEGO on the development of the LEGO MindStorms robotics kits, now used by millions of people around the world. Crickets grow out of this same tradition, but with greater emphasis on artistic expression. Crickets are now sold as a product through the Playful Invention Company (www.picocricket.com).",2008-09-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/cricklogo-new.jpg,6511,E15-001,2016-12-05T00:16:20.268Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'bss@media.mit.edu']",True,crickets,2006-01-01,[],Crickets,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:56.558Z,Critical Cartography develops mapping tools for urban activists. The current prototype is a data-collection and route-planning application for wireless devices. This system is currently in use by privacy-rights advocates in New York and Amsterdam to monitor the proliferation of closed-circuit television (CCTV) cameras surveilling public spaces.,2003-01-01,['smart-cities'],,6519,E15-001,2016-12-05T00:17:10.161Z,[],True,critical-cartography,2002-01-01,[],Critical Cartography,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:08:56.735Z,"Cronicas de Heroes documents acts of extraordinary and everyday heroism in communities that sometimes seem devoid of heroes�cities on the US-Mexico border, where drug violence has destroyed communities and silenced many forms of community dialog. Cronicas de Heroes invites people to share their (signed or anonymous) reports of heroism and visualizes them on maps of the city. Currently operating in four Mexican cities, Cronicas de Heroes is now expanding on both sides of the border, and in other communities around the world.",2012-01-01,['civic-media'],,6527,,2016-12-05T00:17:10.196Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,cronicas-de-heroes-hero-reports,2011-09-01,[],Cronicas de Heroes (Hero Reports) ,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:08:56.896Z,"It takes us years to learn our own musical tradition, and it is rare to find people who have become musically multilingual. However, by learning to compose in different cultural styles, we can expand our compositional palette and communicate more effectively across cultural boundaries. We are designing a computer-assisted compositional tool that can help composers to begin composing melodies in other cultural styles by dynamically analyzing the musical context and presenting as musical analogies melodic materials from various cultures. These melodic patterns address questions such as: In a target cultural style, how does one develop a musical idea? What are the idiomatic melodic progressions? How does one establish a structural pitch? What are the possible continuations to an unfinished melody and how does one cadence? This tool makes it more accessible for composers to transform and render their musical ideas in other musical languages.  ",2008-09-01,['music-mind-and-machine'],,6535,E15-484,2016-12-05T00:17:10.060Z,['bv@media.mit.edu'],True,cross-cultural-melodic-transformation,2006-09-01,[],Cross-Cultural Melodic Transformation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:57.160Z,"We can use sensor networks as a way of building a comprehensive picture of a physical space to transmit a sense of that space into a virtual environment. In this project, we use the Spinner sensor nodes to capture video, audio, and environmental information. This information is used to build an interactive three-dimensional space in Second Life that makes it easy to explore the history of any sensor node, as well as interact live using text and audio with people at any of the nodes. Avatars� virtual presence near nodes is also shown on the physical nodes, so visitors to the physical space know when there are virtual visitors nearby.",2009-09-01,['responsive-environments'],,6551,E15-344,2016-12-05T00:17:10.080Z,['joep@media.mit.edu'],True,cross-reality-demonstration,2008-09-01,[],Cross-Reality Demonstration,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:57.422Z,"CrossTalk is an interactive keyboard designed to allow two or more people to play with language and communication in a social setting. It was designed as part of the emergent Literary Salon project, in which cafe tables function both as traditional tables and as ways to communicate with  other people in the same physical space, enhancing and enabling face-to-face social interactions, rather than detracting from them. A primary goal of this project is to encourage people to play with language; another goal is to create a feeling of community within the cafe.",2000-12-30,['gesture-and-narrative-language'],,6567,,2016-12-05T00:16:20.380Z,[],True,crosstalk,1996-12-31,[],Crosstalk,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:57.533Z,"People often say that we live in a small world. In a brilliant experiment, legendary social psychologist Stanley Milgram proved the six degrees of separation hypothesis: that everyone is six or fewer steps away, by way of introduction, from any other person in the world. But how far are we, in terms of time, from anyone on Earth? Our team won the Tag Challenge, a social gaming competition, showing it is possible to find a person, using only his or her mug shot, within 12 hours.",2016-09-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2016/10/12/tn_cam.jpg,6575,--Choose Location,2016-12-05T00:16:05.988Z,"['irahwan@media.mit.edu', 'dsouza@media.mit.edu']",True,crowdsourcing-a-manhunt,2015-01-01,"['crowdsourcing', 'social-media', 'social-science', 'collective-intelligence', 'behavioral-science']",Crowdsourcing a Manhunt,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:08:57.644Z,"In 2009, DARPA launched the Network Challenge to explore the roles the Internet and social networking play in the timely communication, wide-area team-building, and urgent mobilization required to solve broad-scope, time-critical problems. The challenge was to be the first to locate 10 moored, 8-foot, red weather balloons at 10 random locations in the continental United States. A team from MIT won by locating all balloons in under 9 hours. We helped analyze the factors behind the team's success. We then quantified the limits of this kind of mobilization, and introduced techniques for improving information verification in mass collaboration.",2014-09-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2016/10/12/tn_rbc.jpg,6583,--Choose Location,2016-12-05T00:17:10.099Z,"['sandy@media.mit.edu', 'irahwan@media.mit.edu', 'dsouza@media.mit.edu']",True,crowdsourcing-search-the-red-balloon-challenge,2010-01-01,"['crowdsourcing', 'social-media', 'social-science', 'collective-intelligence', 'behavioral-science']",Crowdsourcing Search: The Red Balloon Challenge,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:08:57.751Z,"We are crowdsourcing the creation of socially rich, interactive characters by collecting data from thousands of people interacting and conversing in online multiplayer games, and mining recorded gameplay to extract patterns in language and behavior. The tools and algorithms we are developing allow non-experts to automate characters who can play roles by interacting and conversing with humans (via speech or typed text), and with each other. The Restaurant Game recorded over 16,000 people playing the roles of customers and waitresses in a virtual restaurant. Improviso is recording humans playing the roles of actors on the set of a sci-fi movie. This approach will enable new forms of interaction for games, training simulations, customer service, and HR job applicant screening systems.",2014-01-01,['social-machines'],https://dam-prod2.media.mit.edu/x/files/Display/customer_waitress_300_300.jpg,6591,E15-483,2016-12-05T00:17:10.118Z,['dkroy@media.mit.edu'],True,crowdsourcing-the-creation-of-smart-role-playing-agents,2006-09-01,[],Crowdsourcing the Creation of Smart Role-Playing Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:08:57.856Z,"The Internet has unleashed the capacity for planetary-scale collective problem solving (also known as crowdsourcing). However, the very openness of crowdsourcing makes it vulnerable to sabotage by rogue or competitive actors. To explore the effect of errors and sabotage on the performance of crowdsourcing, we analyze data from the DARPA Shredder Challenge, a prize competition for exploring methods to reconstruct documents shredded by a variety of paper shredding techniques.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/03/22/chaffsscat.png,6599,--Choose Location,2020-07-23T18:51:55.639Z,['irahwan@media.mit.edu'],True,crowdsourcing-under-attack,2014-01-01,[],DARPA Shredder Challenge: Crowdsourcing under attack,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:08:58.007Z,"The cry detector running on the iPAQ under Linux is an implementation of the baby monitor, one of the applications announced in the Impromptu project. Unlike the other Impromptu applications, the baby monitor doesn't need the bandwidth for sound transmission all the time. It opens a bi-directional audio channel only when the baby starts crying. One can think of detecting different types of interesting sound patterns. In many cases, the detection requires few computational resources. Simple sound-pattern detectors of this kind are highly scalable and can be used in a variety of applications.",2003-01-01,['living-mobile'],,6607,--Choose Location,2016-12-05T00:16:20.290Z,['geek@media.mit.edu'],True,crybaby,2001-01-01,[],CryBaby,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:08:58.179Z,"A physical interface designed for simultaneous social interaction with visual material. We built a hemispherical, multi-person, interactive touch display that allows a small group of people in the same place or in equivalently equipped ones to jointly interact on the same surface. We created an application that runs on this platform and presents a selection of visual media and offers recommendations for common viewing.",2015-01-01,"['ultimate-media', 'object-based-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/crystal-ball.png,6615,--Choose Location,2017-06-25T00:47:05.431Z,"['vmb@media.mit.edu', 'novysan@media.mit.edu', 'lip@media.mit.edu', 'amirl@media.mit.edu']",True,crystal-ball,2013-09-01,[],Crystal Ball,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:08:58.369Z,"The city of the future will provide a high degree of connectivity to its inhabitants. Targeting the fast-paced urban traffic environment, the smart meter revolutionizes curbside parking by integrating digital network technologies with existing parking meter infrastructure. Sensors integrated into existing parking meters detect occupancy; parking meters talk with each other in a multihope scheme; drivers will be able to �locate� and �reserve� vacant parking spaces; and the parking space can email you or post a vacancy on a Web page, reducing traffic and saving time and fuel. These new meter functions will lead to new driver interfaces. This project re-thinks the relationship between the users of transit systems (mass-transit and private vehicles) and their surrounding context. A system of communication nodes throughout the urban landscape and within the car forms our vision for a networked city. (Based on previous work by Jeffrey Tsui, Joseph Ho, and Edwin Lau.)",2004-09-01,['smart-cities'],,6623,Cube,2016-12-05T00:16:20.352Z,['rchin@media.mit.edu'],True,curbside-sensor,2001-01-01,[],Curbside Sensor,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:08:58.712Z,"Can robots learn to be social? Can they do that in a structured way? This project uses the DragonBot platform and state-of-the-art artificial curiosity algorithms to explore the possibility of robots learning to behave socially, similar to children. The robot reacts to people and receives internal rewards whenever the social interaction succeeds. Initially, the robot learns which behavior best initiates social interaction and later learns which behavior maintains that interaction for the longest period. The goal is to build a brain-inspired hierarchical curiosity-driven social behavior architecture, in which the robot autonomously learns a growing repertoire of social skills.",2015-08-31,['personal-robots'],https://dam-prod2.media.mit.edu/x/files/Display/20140416_185151.jpg,6639,--Choose Location,2017-05-31T18:36:24.278Z,"['cynthiab@media.mit.edu', 'ggordon@media.mit.edu']",True,curious-social-robot,2014-01-01,[],Curious Social Robot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:08:58.882Z,"curlybot is an autonomous two-wheeled toy that can record and play back physical motion. As one plays with it, it remembers its change in position and can replay its movement forward or backward with all the intricacies of the original gesture. Every pause, and even the shaking in the user's hand, is recorded.",2000-12-30,['tangible-media'],,6647,E15-441,2016-12-05T00:16:20.494Z,['ishii@media.mit.edu'],True,curlybot,1997-12-31,[],curlybot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:59.033Z,The CurlyCart is a modification of a Power Wheels toy that can record and play back your motions.,2001-12-30,"['cc', 'sociable-media']",,6655,E15-450,2016-12-05T00:16:20.403Z,"['judith@media.mit.edu', 'mike@media.mit.edu']",True,curlycart,2000-12-31,[],CurlyCart,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:59.181Z,"Curtains blends the aesthetics of everyday home design with sensing technology. Whereas today a sensor commonly augments a device, with Curtains, the sensing is inherent in the nature of the material. The metallic threads woven into the fabric allow for the sensing of touch and proximity. The curtain then becomes both sensor and ornament.
",2000-12-30,['sociable-media'],,6663,E15-449,2016-12-05T00:16:20.429Z,['judith@media.mit.edu'],True,curtains,1998-12-31,[],Curtains,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:59.314Z,"We are exploring innovative use of cell-phone Bluetooth technologies for consumer research and customer measurement.  We have developed a small, portable, Bluetooth base station that can monitor consumer activity in a retail space and also enable new interactive services.  This Bluetooth hub also serves as a network gateway for other wireless sensors in the local area.",2009-09-01,['affective-computing'],,6671,E15-449,2016-12-05T00:16:07.608Z,"['picard@media.mit.edu', 'fletcher@media.mit.edu']",True,customer-measurement-using-bluetooth,2008-09-01,[],Customer Measurement Using Bluetooth,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:08:59.449Z,"We developed a music control surface which enables integration between any musical instruments via a versatile, customizable, and inexpensive user interface. This sensate surface allows capacitive sensor electrodes and connections between electronics components to be printed onto a large roll of flexible substrate unrestricted in length. The high-dynamic, range-capacitive sensing electrodes can not only infer touch, but near-range, non-contact gestural nuance in a music performance. With this sensate surface, users can ""cut"" out their desired shapes, ""paste"" the number of inputs, and customize their controller interfaces, which can then send signals wirelessly to effects or software synthesizers. We seek to find a solution for integrating the form factor of traditional music controllers seamlessly on top of one's instrument, while adding expressiveness to performance by sensing and incorporating movements and gestures to manipulate the musical output.",2014-09-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/uke.JPG,6679,--Choose Location,2016-12-05T00:16:28.069Z,"['nanwei@media.mit.edu', 'nanzhao@media.mit.edu', 'pragun@media.mit.edu', 'joep@media.mit.edu']",True,customizable-sensate-surface-for-music-control,2012-01-01,"['music', 'interfaces']",Customizable Sensate Surface for Music Control,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:59.568Z,"Individuals diagnosed with autism spectrum disorder (ASD) often have intense, focused interests. These interests, when harnessed properly, can help motivate an individual to persist in a task that might otherwise be too challenging or bothersome. For example, past research has shown that embedding focused interests into educational curricula can increase task adherence and task performance in individuals with ASD. However, providing this degree of customization is often time-consuming and costly and, in the case of computer-mediated interventions, high-level computer-programming skills are often required. We have recently designed new software to solve this problem. Specifically, we have built an algorithm that will: (1) retrieve user-specified images from the Google database; (2) strip them of their background; and (3) embed them seamlessly into Flash-based computer programs.",2012-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/customize.jpg,6687,,2016-12-05T00:16:28.100Z,"['picard@media.mit.edu', 'rmorris@media.mit.edu']",True,customized-computer-mediated-interventions,2010-01-01,[],Customized Computer-Mediated Interventions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:08:59.889Z,"This project was a weekend exploration of gyroscopic stabilization with application to vehicle control and user interface.  Using the well known inverted pendulum drive system, a unicycle scooter was made from low-cost components. It's like a Segway, but more dangerous/fun!",2012-09-01,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/solowheel.png,6711,,2016-12-05T00:16:20.573Z,['neri@media.mit.edu'],True,cyclopscooter,2012-01-01,[],Cyclopscooter,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:08:59.993Z,"Dance is an expressive activity that combines music and movement. We are interested in both encouraging people to dance, and in measuring the emotional experience of musical expression through movement. The Dance Remixer is a program that transforms any piece of music into something that people can dance to. The program remixes music to add customizable rhythmic elements�for instance, those typical to Latin dance music. The Dance Remixer is a first step toward personalizing how people interact with music, by giving the user the ability to modify its function and emotional content. We are also interested in quantitatively measuring the emotional experience of dancing. What makes dancing enjoyable? How is our enjoyment of music reflected in dance? Specifically, we examine the psychological factors behind Argentine Tango, an improvisational dance that prioritizes the interpretation of music.",2012-01-01,['opera-of-the-future'],,6719,,2016-12-05T00:16:20.603Z,['tod@media.mit.edu'],True,dance-emotion-and-expression,2011-09-01,[],"Dance, Emotion, and Expression",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:00.084Z,"Professional ballroom dancer Adrianne Haslet-Davis lost her natural ability to dance when her left leg was amputated below the knee following the Boston Marathon bombings in April 2013. Hugh Herr was introduced to Adrianne while meeting with bombing survivors at Boston's Spaulding Rehabilitation Hospital. For Professor Herr, this meeting generated a research challenge: build Adrianne a bionic ankle prosthesis, and restore her ability to dance. The research team for this project spent some 200 days studying the biomechanics of dancing and designing the bionic technology based on their investigations. The control system for Adrianne was implemented on a customized BiOM bionic ankle prosthesis.",2013-03-31,['biomechatronics'],,6727,--Choose Location,2016-12-05T00:16:43.994Z,"['hherr@media.mit.edu', 'bevinlin@media.mit.edu']",True,dancing-control-system-for-bionic-ankle-prosthesis,2014-01-01,[],Dancing Control System for Bionic Ankle Prosthesis,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:00.182Z,"Cristiano Ronaldo can famously volley a corner kick in total darkness. The magic behind this remarkable feat is hidden in Ronaldo's brain, which enables him to use advance cues to plan upcoming actions. Darkball challenges your brain to do the same, distilling that scenario into its simplest form�intercept a ball in the dark. All you see is all you need.",2015-07-01,['playful-systems'],,6735,--Choose Location,2016-12-05T00:16:20.623Z,"['slavin@media.mit.edu', 'cwwang@media.mit.edu']",True,darkball,2014-09-01,[],Darkball,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:00.369Z,"We study different methods to depict the ways that people are spending their time in their lived environments. We look at people's relationships with each other in physical space through a variety of visualization techniques. By combining different sources of data from sensors, mobile phone logs, and other social media, we build rich and meaningful histories from the less-recognizable details of our lives to understand how our identities are shaped in time.",2017-05-01,['sociable-media'],,6751,E15-391,2022-06-10T14:41:29.799Z,['judith@media.mit.edu'],True,data-portrait-study-series-social-maps-of-time-and-space,2005-09-01,[],Data Portrait Study Series: Social Maps of Time and Space,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:00.459Z,"Data portraits are a series of visualizations of information traces collected from various social application feeds. Using salient words and phrases from an individual's postings or collection, we visualize the topical and temporal patterns to create an abstract portrait of the author. Displayed as a group, these portraits both feature the individuality of each subject and highlight the external events that unite them.",2017-05-01,['sociable-media'],,6759,E15-383,2022-06-10T14:41:29.839Z,['judith@media.mit.edu'],True,data-portraits,2008-01-01,[],Data Portraits,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:00.534Z,"As part of our larger effort to build out a suite of tools for community organizers, we are helping to build their capacity to do their own creative data visualization and presentation. New computer-based tools are lowering the barriers of entry for making engaging and creative presentations of data. Rather than encouraging partnerships with epidemiologists, statisticians, or programmers, we see an opportunity to build capacity within small community organizations by using these new tools. This work involves workshops, webinars, and writing about how to pick more creative ways to present their data stories.datatherapy.org",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/20/IMG_3542.jpg,6767,,2022-06-10T13:35:29.893Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,data-therapy,2011-01-01,"['data', 'learning-teaching', 'data-visualization']",Data Therapy,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:00.846Z,"Our glass building lets us see across spaces�through the walls that enclose us and beyond. Yet invisibly, networks of sensors inside and out capture the often imperceivable dimensions of the built and natural environment. Our project uses multi-channel spatial sound to bring that data into the utilitarian experience of riding the glass elevator. In the past, we've mixed live sound from microphones throughout the building with sonification of sensor data, using a pressure sensor to provide fine-grained altitude for control. In its present form, the elevator is displaying data from the Living Observatory, a wetland restoration site 60 miles away. Each string pluck represents a new data point streaming in; its pitch corresponds to the temperature at the sensor and its timbre reflects the humidity. Live and recorded sound reflect the real ambience of the remote wetland.",2014-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/Escher's_Relativity.jpg,6799,--Choose Location,2016-12-05T00:17:10.401Z,"['gershon@media.mit.edu', 'bmayton@media.mit.edu', 'sfr@media.mit.edu', 'joep@media.mit.edu']",True,data-driven-elevator-music,2011-01-01,[],Data-Driven Elevator Music,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:00.929Z,"Data-Pop Alliance is a joint initiative on big data and development with a goal of helping to craft and leverage the new ecosystem of big data--new personal data, new tools, new actors--to improve decisions and empower people in a way that avoids the pitfalls of a new digital divide, de-humanization, and de-democratization. Data-Pop Alliance aims to serve as a designer, broker, and implementer of ideas and activities, bringing together institutions and individuals around common principles and objectives through collaborative research, training and capacity building, technical assistance, convening, knowledge curation, and advocacy. Our thematic areas of focus include official statistics, socio-economic and demographic methods, conflict and crime, climate change and environment, literacy, and ethics.",,['human-dynamics'],,6807,--Choose Location,2016-12-05T00:16:20.681Z,['sandy@media.mit.edu'],True,data-pop-alliance,2014-01-01,[],Data-Pop Alliance,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:01.006Z,"DataBasic is a suite of web-based tools that give people fun and relevant ways learn how to work with data. Existing tools focus on operating on data quickly to create some output, rather than focusing on helping learners understand how to work with data. This fails the huge population of data literacy learners, who are trying to build their capacity in various ways. Our tools focus on the user as learner. They provide introductory activities, connect to people with fun sample datasets, and connect to other tools and techniques for working with data. We strongly believe in building tools focused on learners, and are putting those ideas into practice on these tools and activities. Visit databasic.io today to try it out!",2020-08-01,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2016/10/17/Opening Shot_OuxqB04.png,6815,--Choose Location,2022-06-10T13:29:45.208Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'dignazio@media.mit.edu']",True,databasic,2015-09-01,"['data', 'data-visualization']",DataBasic,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:01.083Z,"A picture may be worth a thousand words, but are they the words we're looking for? The dataCam introduces the notion of associating non-visual data with  every image we capture. For every click of the shutter, the dataCam records relevant, non-visual information, such as where and when the picture was  taken, who is in it, the environmental conditions, and any other details the photographer decides are important. After processing (or downloading), the  dataCam format makes it possible to build a large database of images, searchable in non-traditional ways, such as by altitude or by the blood pressure of  the people pictured.",1999-12-30,['personal-information-architecture'],,6823,E15-468,2016-12-05T00:16:28.130Z,['mike@media.mit.edu'],True,datacam-taking-pictures-in-a-whole-new-way,1997-12-31,[],dataCam: Taking Pictures in a Whole New Way,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:01.162Z,"Providing people with information about their own physical state has been used for a long time under the general term ""bio-feedback."" In this project, we are trying to expand the general ideas of bio-feedback to include the monitoring of day-to-day health information such as diets, exercise, and stress. We are particularly interested in compliance, and in health improvements as a function of both the hardware used (personal computers vs. PDAs) and the type of information and feedback that is given back to the users.",2004-09-01,['erationality'],,6831,--Choose Location,2016-12-05T00:16:20.712Z,[],True,day-to-day-monitoring-for-e-health,2001-01-01,[],Day-to-Day Monitoring for e-Health,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:01.233Z,"We all use systems for organizing our cluttered schedules, from the day planner, to to-do lists, to methods such as Getting Things Done. Daydar is a framework that makes this process social: Can you learn from the working styles of others? Can you collaboratively create an environment of healthy competition by being aware of your friends' daily accomplishments? Can this help you to find a better balance between work and play? Within this framework, we are experimenting with physical and digital artifacts that enable you to reflect on your own goals through your peers' work habits, get motivated, and externalize your tasks in order to improve the process of accomplishing projects.",2010-01-01,['information-ecology'],,6839,E15-301,2016-12-05T00:17:10.425Z,"['holtzman@media.mit.edu', 'dsmall@media.mit.edu', 'jkestner@media.mit.edu']",True,daydar-framework-for-socially-motivated-goal-fulfillment,2009-01-01,[],Daydar: Framework for Socially Motivated Goal Fulfillment,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:01.311Z,"DbDb (pronounced DubDub) is a collaborative, visually based analysis and simulation platform. We promote open distribution of experimental data by allowing researchers to present a graphical representation of their data and processing techniques that collaborators can build on and augment. This helps test the reproducibility of results and allows others to learn and apply their own techniques. Our intention is for the research community as a whole to benefit from a growing body of open, analytical techniques. DbDb provides an interface for archiving data, executing code, and visualizing a tree of forked analyses. It is part of the Viral initiative on open, author-driven publishing, collaboration, and analysis. It is intended to be linked to PubPub, the main project.",,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/dbdbLogo.png,6847,--Choose Location,2016-12-05T00:16:20.776Z,"['trich@media.mit.edu', 'lip@media.mit.edu']",True,dbdb,2015-01-01,"['communications', 'networks']",DbDb,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:01.690Z,"Death and the Powers is a groundbreaking opera that brings a variety of technological, conceptual, and aesthetic innovations to the theatrical world. It is a one-act, full-evening work that tells the story of Simon Powers, a successful and powerful businessman and inventor, reaching the end of his life and facing the question of his legacy. He is now conducting his final experiment, passing from one form of existence to another in an effort to project himself into the future. Simon Powers is himself now a System. His family, friends, and associates must decide what this means, whether or not he is actually alive, how it affects them, and whether to follow.Death and the Powers was composed by Tod Machover and developed at the MIT Media Lab along with Diane Paulus (director) and Alex McDowell (production designer). The opera uses the techniques of tomorrow to address age-old human concerns of life and legacy. The unique performance environment, including autonomous robots, expressive scenery, new Hyperinstruments, and human actors, blurs the line between animate and inanimate. The opera premiered in Monte Carlo in Fall 2010, with additional performances in Boston and Chicago in 2011 and a new production with a global, interactive simulcast in Dallas in February 2014. The DVD of the Dallas performance of Powers was released in April 2015.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/10/20/DatP-Paula-Aguilera-IMG_5307-7867.jpg,6887,Swatch Lab,2021-09-15T19:46:26.169Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'benb@media.mit.edu', 'sovsey@media.mit.edu', 'ejessop@media.mit.edu', 'akito@media.mit.edu']",True,death-and-the-powers-redefining-opera,2005-09-01,"['multimodal-representation', 'performance-technologies', 'digital-opera', 'theater', 'mediatization', 'performance-technology', 'spatialized-audio', 'opera', 'disembodied-performance', 'interactive-graphics', 'robotics', 'show-control-systems', 'performance-representation', 'mapping-system', 'disembodied-performance-system', 'performance-systems', 'onstage-representation', 'control-system-computer', 'art', 'entertainment', 'music', 'storytelling', 'sensors', 'performance', 'technology']",Death and the Powers: Redefining Opera,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:01.773Z,"How would you translate the word ""samosa"" for someone who has never experienced this food item? One way would be to map the word onto names of similar foods that you think the person does know, and then explain ways in which the foods are similar and different. We are developing a system called TLC that will learn to translate food terms in this way. An understanding of food will be created in two ways. First, the system will acquire a set of data structures that capture the similarities between basic food ingredients such as sugar, eggs, or chicken. This level of representation will model aspects of the human taste and olfactory systems. Second, TLC will compile a massive collection of recipes from the Internet into a structured database. With these two sources of knowledge, the system will be able to compare foods with potentially complex underlying structures and act as a language translator. Although TLC is focused on the domain of food, the underlying data representations and algorithms can be applied to numerous other terminology translation problems.",2003-09-01,['social-machines'],,6895,E15-483,2016-12-05T00:17:10.480Z,['dkroy@media.mit.edu'],True,deep-semantic-representations-for-language-translation,2003-01-01,[],Deep Semantic Representations for Language Translation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:01.849Z,"A picture is more than a thousand words. As cameras become smarter, imagery will be enhanced by meta-data of all sorts. Some of our early field projects, like the exploratory use of the dataCam on Mount Everest, pointed out the acute need to fuse image and sensor data, and perhaps more importantly, to construct a new image bank that can really leverage these new collections of data-enhanced images.  Deep View addresses this latter need. As a first step, we are digitizing a large backlog of photographs (two years of expeditionary images) and building the new file system and relational framework to leverage this and future data-rich imagery.  With this basis, the system will be able to intelligently create overview presentations or slide shows.",1999-12-30,['personal-information-architecture'],,6903,E15-468,2016-12-05T00:16:07.642Z,['mike@media.mit.edu'],True,deep-view-an-image-and-data-bank,1998-12-31,[],Deep View: An Image and Data Bank,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:01.928Z,"The recent availability of quantitative behavioral data provides an opportunity to study human behavior at unprecedented scale. Using large-scale financial transaction data, we propose a novel deep learning framework for understanding human purchase patterns and testing the link between them and the existence of individual financial troubles. Our work opens new possibilities in studying human behavioral traits using state-of-the-art machine learning techniques, without the need for hand-engineered features.",,['human-dynamics'],,6911,--Choose Location,2019-04-19T14:43:35.517Z,"['sandy@media.mit.edu', 'xdong@media.mit.edu', 'suhara@media.mit.edu']",True,deepshop-understanding-purchase-patterns-via-deep-learning,2016-01-01,[],DeepShop: Understanding purchase patterns via deep learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:02.004Z,"Citizens and journalists are increasingly choosing to live stream civic events. But live streams are currently hard to find and lack in-depth information about the events being documented. DeepStream seeks to increase participation in this emergent form of media by creating tools for live stream curation. Users can add relevant news stories, images, tweets, and other media to almost any live or on-demand video to create more informative and engaging viewing experiences. To help find relevant videos, Deepstream includes a search engine that lets you find live streams across multiple platforms with a single search query.By lowering the technical barriers to creating enhanced live and on-demand videos, Deepstream makes it possible for newsrooms or individuals to curate the chaos of live streams from major global events, add media to video in real-time like fact-checking live political debates, or create enhanced version of documentaries with extra footage and related stories that appear at specific times. Our goal is to connect viewers to global events in a way that emphasizes local perspectives and deeper engagement, while maintaining the experience of immediacy and authenticity that is an essential part of live streaming.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/20/memberweek_Qa3HCVq.png,6919,--Choose Location,2022-06-10T13:35:29.905Z,"['ethanz@media.mit.edu', 'wgmangum@media.mit.edu']",True,deepstream,2014-09-01,"['civic-media', 'social-media']",DeepStream,PUBLIC,http://www.deepstream.tv,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:02.085Z,"Competitive chess is an exciting spectator sport. It is fast-paced, dynamic, and deeply psychological. Unfortunately, most of the game's drama is only visible to spectators who are themselves expert chess players. DeepView seeks to use computational tools to make the drama of high-level chess accessible to novice viewers. There is a long tradition of software trying to beat human players at chess; DeepView takes advantage of algorithmic tools created in the development of advanced chess engines such as Deep Blue, but instead uses them to understand and explain the styles of individual players and the dynamics of a given match. It puts into the hands of chess commentators powerful data science tools that can calculate player position preferences and likely game outcomes, helping commentators to better explain the exciting human story inside every match.",2015-07-01,['playful-systems'],,6927,--Choose Location,2016-12-05T00:16:27.884Z,"['slavin@media.mit.edu', 'gregab@media.mit.edu']",True,deepview-computational-tools-for-chess-spectatorship,2014-01-01,[],DeepView: Computational Tools for Chess Spectatorship,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:02.163Z,"deFORM is a deformable input device that supports 2.5D touch gestures, tangible tools, and arbitrary objects through real-time structured light scanning of a malleable surface of interaction. deFORM captures high-resolution surface deformations and 2D grey-scale textures of a gel surface through a three-phase structured light 3D scanner. This technique can be combined with IR projection to allow for invisible capture, providing the opportunity for co-located visual feedback on the deformable surface",2012-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/beautiful.png,6935,--Choose Location,2016-12-05T00:16:20.867Z,['ishii@media.mit.edu'],True,deform,2011-01-01,[],deFORM,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:02.334Z,"We are working with sponsor Schneider Electric to deploy a dense, low-power wireless sensor network aimed at environmental monitoring for smart energy profiling. This distributed sensor system measures temperature, humidity, and 3D airflow, and transmits this information through a wireless Zigbee protocol. These sensing units are currently deployed in the lower atrium of E14. The data is being used to inform CFD models of airflow in buildings, explore and retrieve valuable information regarding the efficiency of commercial building HVAC systems, energy efficiency of different building materials, and lighting choices in novel architectural designs.",2013-09-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/logo.jpg,6951,,2016-12-05T00:16:07.663Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,dense-low-power-environmental-monitoring-for-smart-energy-profiling,2010-01-01,[],"Dense, Low-Power Environmental Monitoring for Smart Energy Profiling",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:02.408Z,"The Department of Play (DoP) is a working group of researchers, students, and community practitioners who share a common passion: designing appropriate technology and methods to empower youth and their communities. In particular, the Department of Play initiative aims to develop an easy-to-use, open-source digital toolkit to foster youth participation, social inclusion and local civic engagement. Among other things, we are implementing a multi-channel neighborhood communication system that combines email, SMS and regular touchtone phones to help young people organize and promote block parties, games, performances and other events in the places where they live. ",2012-09-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/dop-logo.png,6959,,2016-12-05T00:16:20.806Z,"['ethanz@media.mit.edu', 'leob@media.mit.edu']",True,department-of-play,2010-01-01,[],Department of Play,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:02.728Z,"We are at last approaching a time when low-cost computers will make it technically feasible to provide real education to ALL the children of the world. It will be absurd if content and pedgagogy copies the past. This project aims to mobilize people, ideas, and resources to develop a radical new methodology and content. ",2007-09-01,['future-of-learning'],,6991,--Choose Location,2016-12-05T00:16:20.928Z,"['papert@media.mit.edu', 'cavallo@media.mit.edu']",True,design-for-global-education,2005-01-01,[],Design for Global Education,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:02.809Z,"DesignBlocks is a derivative of the Scratch project that focuses on 2-dimensional digital design. With DesignBlocks, artists control lines, shapes, colors and images to create generative and interactive artworks.  DesignBlocks uses the same visual grammar as Scratch, but uses a vocabulary more suited for graphic design. Inspired by Processing, DesignBlocks aims to make programming more accessible and suited to artists.",2010-01-01,['lifelong-kindergarten'],,6999,LEGO Learning Lab,2016-12-05T00:16:20.967Z,"['mres@media.mit.edu', 'ericr@media.mit.edu', 'bss@media.mit.edu']",True,designblocks,2009-01-01,[],DesignBlocks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:02.972Z,"Food offers a rich multi-modal experience that can deeply affect emotion and memory. We're interested in exploring the artistic and expressive potential of food beyond mere nourishment, as a means of creating memorable experiences that involve multiple senses. For instance, music can change our eating experience by altering our emotions during the meal, or by evoking a specific time and place. Similarly, sight, smell, and temperature can all be manipulated to combine with food for expressive effect. In addition, by drawing upon people's physiology and upbringing, we seek to create individual, meaningful sensory experiences. Specifically, we are exploring the connection between music and flavor perception.",2017-06-01,['playful-systems'],https://dam-prod2.media.mit.edu/x/files/Display/sushi%20headphones.jpg,7015,--Choose Location,2022-06-10T14:14:48.822Z,['slavin@media.mit.edu'],True,designing-immersive-multi-sensory-eating-experiences,2012-09-01,[],Designing Immersive Multi-Sensory Eating Experiences,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:03.114Z,"This project explores software architectures and user interfaces to voice as a computer data-type, as well as a command channel. Its goal is to make speech ubiquitous to a range of applications (for instance, editing a telephone message to include annotation of a text document). Related issues include object-oriented manipulation of multiple media ""selection"" (or ""clipboard"") data between processes, and a client-server architecture allowing multiple applications to share audio resources.",1999-12-30,['living-mobile'],,7023,,2016-12-05T00:16:20.994Z,['geek@media.mit.edu'],True,desktop-audio,1999-12-31,[],Desktop Audio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:03.397Z,We are using consumer-grade inkjet printers with shrinkable print media in order to enable the rapid printing of extremely inexpensive holograms and diffractive optics.,2009-01-01,['object-based-media'],,7039,Garden Conference Room,2016-12-05T00:16:07.688Z,['vmb@media.mit.edu'],True,desktop-printed-holograms,2008-09-01,[],Desktop Printed Holograms,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:03.524Z,"Driving is an ideal test bed for detecting stress in natural situations. Four types of physiological signals (electrocardiogram, electromyogram, respiration, and skin conductivity related to autonomic nervous system activation) were collected in a natural driving situation under various driving conditions. The occurrence of natural stressors was designed into the driving task and validated using driver self-report, real-time, third-party observations, and independently coded video records of road conditions and facial expression. Features reflecting heart-rate variability derived from the adaptive Bayesian spectrum estimation, the rate of skin-conductivity orienting responses, and the spectral characteristics of respiration were extracted from the data. Initial pattern-recognition results show separation for the three types of driving states: rest, city, and highway, and some discrimination within states for cases in which the state precedes or follows a difficult turn-around or toll situation. These results yielded from 89-96 percent accuracy in recognizing stress level. We are currently investigating new, advanced means of modeling the driver data.",2004-09-01,['affective-computing'],,7047,E15-001,2016-12-05T00:16:28.156Z,['picard@media.mit.edu'],True,detection-and-analysis-of-driver-stress,1998-01-01,[],Detection and Analysis of Driver Stress,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:03.655Z,"Some auction sites have ""deterministic-ending"" rules, which means the auction ends at a known time. Other auction sites have ""contingent-ending"" rules, which means they end once a certain criterion has been met (e.g., when no one has increased a bid for 30 minutes). In this project, we examine the role of ending rules on bidders' competitiveness and final bids.",2005-01-01,['erationality'],,7055,--Choose Location,2016-12-05T00:16:07.707Z,[],True,deterministic-vs-contingent-ending-rules-in-online-auctions,2001-01-01,[],Deterministic vs. Contingent Ending Rules in Online Auctions,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:03.767Z,We are developing technologies and community methodologies in electronic commerce for social and economic development. We study e-commerce technologies and systems software that are appropriate and appropriable by the majority of the world. Research and development activities integrate with activist field projects that are community-centered. We are keenly interested in first-class technologies that are cost-appropriate for people who make less than two dollars a day.,2001-12-30,"['e-markets', 'software-agents', 'edevelopment']",,7063,E15-391,2016-12-05T00:17:10.617Z,[],True,development-e-commerce,1998-12-31,[],Development E-commerce,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:04.172Z,"Dialup Radio is an audiocentric, mobile-phone-based independent media service. It has been designed and tested with activists in sub-Saharan Africa, and is expected to be deployed in several locations in 2008.",2008-01-01,['smart-cities'],,7095,E15-001,2016-12-05T00:16:21.101Z,[],True,dialup-radio,2006-09-01,[],Dialup Radio,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:04.272Z,"Today, algorithms drive our cars, our economy, what we read, and how we play. Modern-day computer games utilize weighted probabilities to make games more competitive, fun, and addicting. In casinos, slot machines--once a product of simple probability--employ similar algorithms to keep players playing. Dice++ takes the seemingly straight probability of rolling a die and determines an outcome with algorithms of its own.",2017-06-01,['playful-systems'],https://dam-prod2.media.mit.edu/x/2016/12/14/dice.gif,7103,--Choose Location,2022-06-10T14:14:48.844Z,"['slavin@media.mit.edu', 'jbobrow@media.mit.edu']",True,dice,2014-09-01,[],Dice++,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:04.372Z,"The Display for Indirect Collaboration Environments (DICE) system collected dynamic electronic artwork and displayed it in shared spaces while reacting to people in the surrounding environment. DICE was designed especially for use at Computer Clubhouses, where there is a tradition of displaying static artwork (such as Photoshop projects) on the walls, but few ways for members to publicly display their animations, videos, and other dynamic creations. DICE helped facilitate idea sharing, feedback, and collaboration processes in a community by persistently displaying members' saved work. DICE audience members, who previously may have had limited interactions with others, became aware of the capabilities and interests of their peers. DICE also leveraged sensor-driven awareness of the environment which indicated when the author of a displayed project was in the room and welcomed potential collaborators.",2003-09-01,['lifelong-kindergarten'],,7111,E15-020F,2016-12-05T00:16:28.039Z,"['mres@media.mit.edu', 'millner@media.mit.edu']",True,dice-display-for-indirect-collaboration-environments,2003-09-01,[],DICE: Display for Indirect Collaboration Environments,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:04.650Z,"What do customers really think about your company or brand? Using skin conductance sensors, we measure what excites and frustrates customers when discussing topics relevant to your brand. For example, with the National Campaign to Prevent Teenage Pregnancy, we saw conversations about empowerment and abortion upset conservative families. However, talking about the importance of strong families excited and engaged them. Rather than rely on self-reports, physiological measurements allow us to pinpoint what words and concepts affect your customers. We hope work like this will help companies better reflect on how their actions and messaging affect their customer's opinion in more detailed and accurate ways.",2014-01-01,['affective-computing'],,7135,--Choose Location,2016-12-05T00:17:10.573Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",True,digging-into-brand-perception-with-psychophysiology,2013-01-01,[],Digging into Brand Perception with Psychophysiology,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:04.733Z,"This project aims to reliably spot digit strings in voicemail messages. This can be very useful since it allows easy access to telephone numbers that have been left within a long message, eliminating the need to listen to the message in its entirety. The project involves running speech recognition in dictation mode on incoming voicemail messages, and further text processing to determine the boundaries of digit strings. This project can plug into voicemail systems seamlessly.",2005-01-01,['living-mobile'],,7143,E15-368,2016-12-05T00:16:21.125Z,['geek@media.mit.edu'],True,digit-spotter,2003-09-01,[],Digit Spotter,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:04.903Z,"The Digital Aura project deals with ways to give end users better control over their personal information, leveraging principles of social and physical proximity. People are quite good at establishing a social style and using it in different communications contexts, but they do less well when the communication is mediated by computer networks. It is hard to control what information is revealed and how one's digital persona will be presented or interpreted. In this project, we ameliorate this problem by creating a ""Virtual Private Milieu,"" a ""VPM,"" that allows networked devices to act on our behalf and project a ""digital aura"" to other people and devices around us in a manner analogous to the way humans naturally interact with one another. The dynamic aggregation of the different auras and facets that the devices expose to one another creates social spheres of interaction between sets of active devices, and consequently between people. We demonstrate our prototype of the ""Social Dashboard,"" a readily usable control for one's aura. Finally, we present ""Comm.unity,"" a software package that allows developers and researchers easy implementation and deployment of local and distant social applications, and present several applications developed over this platform.",2009-01-01,['viral-communications'],,7159,E15-483,2016-12-05T00:16:32.623Z,"['nadav@media.mit.edu', 'lip@media.mit.edu']",True,digital-aura,2008-09-01,[],Digital Aura,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:04.986Z,"The Digital Cigarette is an exploration in motivation. It addresses the design of a device that can operate in multiple modalities, on behalf of its user and society.  It is a small electronic device with a microchip that senses when it has been lit, when its user has inhaled or exhaled, when it is tapped, and when it is in the proximity of other digital cigarettes.  Responding with sound and light it engages its user. For the individual user it is a device that strives to provide a break, a reminder of an agenda, procrastination control, meditation, stress relief, and focusing.  For a group it strives to facilitate social interaction through gift giving and has some of the attributes of  pocket pets, cell phones, text messaging, candy, cards, and actual cigarettes.",2004-09-01,['context-aware-computing'],,7167,E15-320,2016-12-05T00:17:10.659Z,[],True,digital-cigarette,2003-09-01,[],Digital Cigarette,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:05.138Z,"Conventional 3-D printing processes are material-dependent and irreversible. We are working on an alternative approach based on 3-D assembly of mass-produced 2-D components of digital material. This significantly enlarges the available material set, allows reversible disassembly, and imposes constraints that reduce the accumulation of local positioning errors in constructing a global shape. Experimental work on material properties and dimensional scaling of the digital material leads to application in assembling functional structures. We propose that assembling digital material will be the future of 3-D free-form fabrication of functional materials.",2007-01-01,['physics-and-media'],,7183,E15-023,2022-06-10T14:12:10.585Z,['neilg@media.mit.edu'],True,digital-fabrication,2005-01-01,[],Digital Fabrication,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:05.213Z,"The Internet has disrupted the aid sector like so many other industries before it. In times of crisis, donors are increasingly connecting directly with affected populations to provide participatory aid. The Digital Humanitarian Marketplace aggregates these digital volunteering projects, organizing them by crisis and skills required to help coordinate this promising new space.",2014-04-01,['civic-media'],,7191,,2016-12-05T00:16:44.562Z,"['ethanz@media.mit.edu', 'stempeck@media.mit.edu']",True,digital-humanitarian-marketplace,2012-09-01,[],Digital Humanitarian Marketplace,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:05.290Z,"We are adding digital capabilities to the traditional toys of childhood, and, in the process, redefining how and what children learn. When children play with traditional toys (such as beads, blocks, and balls), they can gain an understanding of concepts such as number, size, and shape. With our new digital versions of these toys, children can learn concepts (such as process, probability, and emergence) that were previously seen as too complex for children.",2007-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/dimalogo.jpg,7199,E15-001,2016-12-05T00:17:10.596Z,"['mres@media.mit.edu', 'borovoy@media.mit.edu', 'bss@media.mit.edu']",True,digital-manipulatives,1995-01-01,[],Digital Manipulatives,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:05.459Z,"This project explores ways to use images as qualitative data for health maintenance/improvement. Currently, many health patients (such as diabetics) use handheld devices to get, for example, quantitative readings about blood sugar or heart rates. We are developing imaging tools to help people photograph their daily behaviors, and use these images to interpret these quantitative data. The goal is to use imagery as another diagnostic tool for patients and their physicians.",2004-09-01,['erationality'],,7215,E15-309,2016-12-05T00:16:21.299Z,[],True,digital-mirror-3,2002-01-01,[],Digital Mirror,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:05.968Z,"We are developing a printer that builds functional, 3-D structures by reversible assembly of a discrete set of ""digital materials."" This approach uses the components rather than a control system to impose the spatial and functional constraints. Printing can be performed as a parallel rather than a linear process; the printing process is reversible for re-use of the pieces or error correction at any point in the object�s life. Error detection, error-reduction, and error-tolerance during assembly allow for reliable, high-throughput printing. We are presenting development approaches to such a printing device.",2007-01-01,['physics-and-media'],,7239,E15-023,2022-06-10T14:12:10.602Z,['neilg@media.mit.edu'],True,digital-printing-of-digital-materials,2005-09-01,[],Digital Printing of Digital Materials,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:06.114Z,"Low-resolution printing has many applications in the countries of the developing world. Just as the onset of digital desktop printers gave rise to a revolution in desktop publishing, we are concieving similar ""tools for tomorrow"" for handicraft workers in parts of rural India. Since the devices need to be locally fabricated and affordable, we are exploiting open-source hardware and software tool kits for the same.",2005-01-01,['physics-and-media'],,7247,E15-023,2016-12-05T00:16:21.249Z,['neilg@media.mit.edu'],True,digital-stamp,2001-01-01,[],Digital Stamp,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:06.246Z,"For centuries, luthiers have failed to duplicate a Stradivarius. We're now passing a remarkable threshold when sensors and computers can exceed the performance of nature at this level of description, so that a mathematical description of an instrument can become a playable instrument. In this project we are creating the instrumentation and algorithms to let computers develop real-time models to emulate the observed performance of complex driven physical systems.",2001-12-30,['physics-and-media'],,7255,E15-023,2016-12-05T00:17:10.700Z,['neilg@media.mit.edu'],True,digital-stradivarius,1996-12-31,[],Digital Stradivarius,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:06.380Z,"Digital Synesthesia looks to evolve the idea of human-computer interfacing and give way for human-world interacting. It aims to find a way for users to experience the world by perceiving information outside of their sensory capabilities. Modern technology already offers the ability to detect information from the world that is beyond our natural sensory spectrum; however, there is still no real way for our brains and body to incorporate this new information as a part of our sensory toolkit, so that we can understand our surrounding world in new and undiscovered ways. The long-term vision of this work is to give users the ability to turn senses on and off depending on the desired experience. This project is part of the Ultimate Media initiative and will be applied to the navigation and discovery of media content.",2014-01-01,"['ultimate-media', 'object-based-media']",,7263,--Choose Location,2019-05-17T14:21:46.728Z,"['vmb@media.mit.edu', 'talfaro@media.mit.edu']",True,digital-synesthesia,2012-01-01,"['human-computer-interaction', 'cognition', 'sensors']",Digital Synesthesia,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:06.510Z,"Working with Juan Alberto Belloch, the mayor of Zaragoza, Spain, MIT developed a programmable water wall composed of digitally controlled solenoids that release water droplets to create a dynamic, urban-scale intervention for Expo 2008. This project focuses on the potential of advanced communications and media technology in the public realm of the Milla Digital in the center of the old city of Zaragoza, rather than in its buildings and private development. The result is an urban design and digital framework, and specific proposals for digitally enhanced environments that will serve the learning, skill development, and social interests of Zaragoza's citizens, as well as making the city more attractive for business growth and tourism.",2009-01-01,['smart-cities'],,7271,E15-001,2016-12-05T00:16:28.255Z,"['rchin@media.mit.edu', 'susanne@media.mit.edu']",True,digital-water-wall-zaragoza-spain,2005-09-01,[],"Digital Water Wall: Zaragoza, Spain",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:06.743Z,"The digitally reconfigurable surface is a pin matrix apparatus for directly creating rigid 3D surfaces from a computer-aided design (CAD) input. A digital design is uploaded into the device, and a grid of thousands of tiny pins, much like the popular pin-art toy, are actuated to form the desired surface. A rubber sheet is held by vacuum pressure onto the tops of the pins to smooth out the surface they form; this strong surface can then be used for industrial forming operations, simple resin casting, and many other applications. The novel phase-changing electronic clutch array allows the device to have independent position control over thousands of discrete pins with only a single motorized ""push plate,"" lowering the complexity and manufacturing cost of this type of device. Research is ongoing into new actuation techniques to further lower the cost and increase the surface resolution of this technology.",2012-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/Logo.jpg,7287,--Choose Location,2021-05-21T13:04:33.954Z,['neri@media.mit.edu'],True,digitally-reconfigurable-surface,2011-01-01,[],Digitally Reconfigurable Surface,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:06.854Z,"The Digital Seed is a microworld for learning about plant growth, life cycles, and the origin of seeds. The Digital Seed is a virtual alter-ego of a real seed, he lives in a cube. To grow up he needs help from outside, the user must take care of the seed: watering the cube, keeping the cube the right temperature, and exposing it to the right amount of light. The physical actions on the cube will affect the inner virtual world where the seed lives and grows. The user must care for the seed throughout its life cycle until the end, with the birth of a new seed.",2003-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/DigiSeed.jpg,7295,,2016-12-05T00:16:21.415Z,[],True,digitalseed,2002-09-01,[],DigitalSeed,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:06.967Z,"Digito is a gesturally controlled virtual musical instrument, controlled through a number of intricate hand gestures which provide both discrete and continuous control of its sound engine. The hand gestures are captured using a 3D depth sensor and recognized using computer vision and machine learning algorithms. Digito is currently being used to evaluate the possible strengths and limitations of gesturally controlled virtual musical instruments and to assist in uncovering new questions regarding the design of gestural musical interfaces.",2013-01-01,['responsive-environments'],,7303,,2016-12-05T00:17:10.768Z,['joep@media.mit.edu'],True,digito-a-fine-grained-gesturally-controlled-virtual-musical-instrument,2011-09-01,[],"Digito: A Fine-Grained, Gesturally Controlled Virtual Musical Instrument",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:07.058Z,"With The Diorama Project we are putting virtual objects in real world  spaces. Our goal is to create a system that allows people to build an  imaginative parallel universe superimposed on their everyday space.  The  research challenges are technical (precise location sensing), perceptual  (creating a coherent hybrid space) and social (developing new interface  metaphors for physically located virtual objects). 
",1999-12-30,['sociable-media'],,7311,,2016-12-05T00:16:21.439Z,['judith@media.mit.edu'],True,diorama,1997-12-31,[],Diorama,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:07.149Z,"Photorefractive polymer has many attractive properties for dynamic holographic displays; however, the current display systems based around its use involve generating holograms by optical interference methods that complicate the optical and computational architectures of the systems, and limit the kinds of holograms that can be displayed. We are developing a system to write computer-generated diffraction fringes directly from spatial light modulators to photorefractive polymers, resulting in displays with reduced footprint and cost, and potentially higher perceptual quality.",2012-09-01,['object-based-media'],,7319,--Choose Location,2016-12-05T00:17:10.795Z,"['vmb@media.mit.edu', 'sjolly@media.mit.edu']",True,direct-fringe-writing-of-computer-generated-holograms,2011-09-01,[],Direct Fringe Writing of Computer-Generated Holograms,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:07.344Z,"Early in the opera ""Death and the Powers,"" the main character, Simon Powers, is subsumed into a technological environment of his own creation. The set comes alive through robotic, visual, and sonic elements that allow the actor to extend his range and influence across the stage in unique and dynamic ways. This environment assumes the behavior and expression of the absent Simon; to distill the essence of this character, we recover performance parameters in real time from physiological sensors, voice, and vision systems. Gesture and performance parameters are then mapped to a visual language that allows the off-stage actor to express emotion and interact with others on stage. To accomplish this, we developed a suite of innovative analysis, mapping, and rendering software systems.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/10/20/008_dps_fs-8883-1024x576.jpg,7335,Swatch Lab,2017-03-31T20:27:08.548Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'ejessop@media.mit.edu']",True,disembodied-performance,2008-09-01,"['performance-representation', 'mapping-system', 'disembodied-performance-system', 'performance-systems', 'onstage-representation', 'control-system-computer', 'music', 'sensors', 'performance']",Disembodied Performance,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:09:07.516Z,"We are creating thirty-dollar, sunlight-readable, low-power, and high-resolution displays for laptop computers. We are focusing on the replacement of color filters with LED back- and front-lights, and, additionally, on <$100 projectors for the classroom.",2008-01-01,[],,7351,--Choose Location,2016-12-05T00:17:10.839Z,['mlj@media.mit.edu'],True,displays-for-the-100-laptop,2005-09-01,[],Displays for the $100 Laptop,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:07.583Z,"A human disruption model is used to develop a disruption manager that mediates instant message interruptions as people navigate the Web. The system is designed to reduce disruption and increase overall user satisfaction. The disruption manager is based on research investigating people�s reactions to interruptions and factors involved in the interruption process, such as interruption relevancy and task priority. The manager supports monitoring ongoing behaviors using implicit metrics to control possible disruptive outcomes given the user and system state (mouse and keyboard behaviors, interruption type, task request or notification, concepts surrounding the user�s goals, interruption relevancy, and concept priority). The disruption manager selects appropriate interruption timing and selects whether or not instant message interruptions should be presented to the user.",2007-01-01,['context-aware-computing'],,7359,E15-320,2016-12-05T00:16:21.501Z,[],True,disruption-manager,2005-01-01,[],Disruption Manager,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:07.661Z,"Disruption is an important issue in the design of self-adaptive interfaces. This project attempts to make a computer intelligent enough to select the appropriate sensorial modality. This project investigates which modality is the most efficient and at the same time, the least disruptive. Two interruption modalities are studied: heat and light. This research adds to previous research by showing that there is an effect on performance caused by interruption modalities: for example, thermal modality produced a larger decrease in performance than visual modality; this modality has a greater disruptive effect on interrupted tasks than light. Disruptiveness and performance measures agree that heat causes more of a detrimental effect than light when used as an interruption.",2004-09-01,['context-aware-computing'],,7367,E15-320,2016-12-05T00:16:21.525Z,[],True,disruptive-interruptions,2001-01-01,[],Disruptive Interruptions,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:07.737Z,"Almost any communications system makes best use of scarce bandwidth by making the messages sent appear to be as random as possible. This is optimally done with spread-spectrum techniques based on Linear Feedback Shift Registers; this entails a significant amount of processing for a receiver to acquire and track a signal. We have found that simple physical systems can be designed that perform these functions through their dynamics, promising to make spread-spectrum communications significantly cheaper and faster.",2001-12-30,"['personal-fabrication', 'toys-of-tomorrow', 'physics-and-media']",,7375,E15-023,2016-12-05T00:16:21.549Z,['neilg@media.mit.edu'],True,dissipative-communications,1999-12-31,[],Dissipative Communications,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:07.894Z,"This project explores different applications for tapered-aperture audio transducers, where incoming acoustic wavefronts correlate with spatial patterns that we deposit onto large-area distributed acoustic transducers.  Our initial efforts focused on developing very simple sensors that determine the bearing of incoming sound waves, giving the computer easy access to directional audio information. The initial devices were broadband, passive monopulse acoustic receivers made from PVDF foil. By properly shaping the electrode patterns on the PVDF and mounting the foil appropriately, a simple ratio of two signals determines acoustic bearing over a wide frequency range without dispersive effects. The transducers that we have developed have responded well into the audio band, and provided directional cues for nearby acoustic transients resulting from different kinds of activity. We are now exploring other interesting audio applications of distributed-aperture transducers, and developing different ways to fabricate them.",2000-12-30,"['personal-fabrication', 'cc', 'responsive-environments']",,7391,,2016-12-05T00:16:28.277Z,['joep@media.mit.edu'],True,distributed-aperture-acoustic-transducers,1997-12-31,[],Distributed Aperture Acoustic Transducers,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:08.147Z,"Exploring path diversity via multipath routing adds more degrees of freedom and reliability for wireless communications, in particular in wireless ad hoc networks. We present approaches based on smart traffic allocation and novel scheduling algorithms to achieve distributed security in wireless communication systems, where the reception from one path at the receiver gets a reasonable result and the receptions from more paths lead to better results. ",2005-01-01,['viral-communications'],,7407,E15-483,2016-12-05T00:16:07.751Z,['lip@media.mit.edu'],True,distributed-security-via-multi-path-routing-in-wireless-networks,2004-09-01,[],Distributed Security via Multi-path Routing in Wireless Networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:08.290Z,"We consider the problem of compressing an image into packets sets that are mutually refinable. This problem is applicable to distributed storage of data: each user owns a different  compressed version of the same original picture. Each version by itself is a coarse approximation of the original picture. Then, people can decide to share their versions. The more versions people contribute, the better the resulting approximation becomes, independently of which people are actually involved: only the number of users that collaborate matters. We will address this problem by using multiple description techniques combined with sub-band coders.",2003-01-01,['media-and-networks'],,7415,E15-483,2016-12-05T00:17:10.927Z,['lip@media.mit.edu'],True,distributed-storage-of-data-using-multiple-description-techniques,2002-01-01,[],Distributed Storage of Data Using Multiple Description Techniques,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:08.438Z,"We can direct users through a building to the least crowded destination using cues that indicate the next step in a path; providing next-step directions to users is a well-known technique. Often, the path planning is static, occurring before users start off to their destinations. This project will direct users through the Media Lab to specific destinations using real-time status information in an ad hoc network. We can infer the crowd size at locations such as demo areas using sensors. The sensors forward on that crowd data to nodes in hallways and doorways whose displays direct users to the least-crowded destination. As crowd sizes change, the nodes adapt and redirect users to different areas without any centralized control.",2006-01-01,['viral-communications'],,7423,E15-495,2016-12-05T00:17:10.885Z,['lip@media.mit.edu'],True,distributed-in-distributed-out-sensor-networks,2004-01-01,[],"Distributed-In, Distributed-Out Sensor Networks",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:09.068Z,"A library of Python 2.7 code for facilitating commonsense reasoning with ConceptNet4. This makes high-level features of the ConceptNet4 code base more accessible to application developers. It does not require that users understand matrix math or many underlying concepts. The code is extensively documented, including example calls and lots of default arguments.",2014-09-01,['software-agents'],,7455,--Choose Location,2022-06-10T14:18:57.406Z,"['lieber@media.mit.edu', 'cfry@media.mit.edu']",True,divisi-for-dummies-d4d,2012-09-01,[],Divisi For Dummies (D4D),PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:09.262Z,"We have developed technology that enables easy analysis of semantic data, blended in various ways with common-sense world knowledge. The results support reasoning by analogy and association. A packaged library of code is being made available to all sponsors.",2016-08-31,['digital-intuition'],,7463,E15-383,2021-07-14T15:34:55.841Z,['havasi@media.mit.edu'],True,divisi-reasoning-over-semantic-relationships,2007-01-01,[],Divisi: Reasoning Over Semantic Relationships,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:09.466Z,"An exploration into the possibilities for individual construction and customization of the most ubiquitous of electronic devices, the cellphone. By creating and sharing open-source designs for the phone's circuit board and case, we hope to encourage a proliferation of personalized and diverse mobile phones. Freed from the constraints of mass production, we plan to explore diverse materials, shapes, and functions. We hope that the project will help us explore and expand the limits of do-it-yourself (DIY) practice. How close can a homemade project come to the design of a cutting-edge device? What are the economics of building a high-tech device in small quantities? Which parts are even available to individual consumers? What's required for people to customize and build their own devices?",2015-01-01,['lifelong-kindergarten'],,7471,--Choose Location,2016-12-05T00:16:21.623Z,"['mres@media.mit.edu', 'leah@media.mit.edu', 'mellis@media.mit.edu']",True,diy-cellphone,2012-01-01,[],DIY Cellphone,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:09.625Z,"Using digital fabrication and embedded computation to allow individuals to make their own devices. This effort started by creating open-source DIY versions of common devices (speakers, radios, mice, and cellphones) each combining a custom electronic circuit board and digitally-fabricated enclosure. The current focus is on creating devices with unique functionality, aesthetics, or production processes. One early prototype is of a special-purpose internet-connected device, whose behavior can be customized by the person creating it. Another experiment explores the possibilities of automated circuit board assembly services and their implications for open-source hardware. Most importantly, we're beginning to develop resources to enable others to design and build custom devices through meaningful and educational creative processes. These efforts are still in an early stage, but we're interested in finding ways to transition from reproducing existing devices to helping people create a diverse set of new ones.",2014-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/DIY%20Devices.jpg,7479,--Choose Location,2016-12-05T00:16:21.735Z,"['mres@media.mit.edu', 'mellis@media.mit.edu']",True,diy-devices,2014-01-01,[],DIY Devices,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:09.779Z,"DJ I, Robot is the world's first fully robotic hip-hop DJ, able to cut, scratch, and mix records with three high-speed servo-controlled motors; it functions both as a tool for composing original collaged music and as a novel performance interface, the latter being the current focus of our research. The cultural phenomenon of DJing began in the mid-1970s as a ghetto misappropriation of a delicate consumer technology; this project attempts to intervene in this history by creating a novel technology for the field, and has been met by professional DJs with a mixture of respect, envy, and loathing. DJ I, Robot premiered in 2001 at the Berlin Film Festival, and has performed from Boston to Spliit, Croatia, while appearing in publications from Artbyte to Wired News.",2004-09-01,['computing-culture'],,7487,E15-001,2016-12-09T19:40:33.814Z,['csik@media.mit.edu'],True,dj-i-robot,2001-01-01,[],"DJ I, Robot",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:09:10.555Z,"The ability to learn is a potentially compelling and important quality for interactive synthetic characters; this project investigates a practical approach to real-time learning for them. The implementation is grounded in techniques of reinforcement learning, and informed by insights from animal training. It simplifies the learning task for characters because it enables them to take advantage of predictable regularities in their world; allows them to make maximal use of any supervisory signals; and makes them easy to train. We have built an autonomous, animated dog that can be trained by a technique used with real dogs called ""clicker training."" Capabilities demonstrated include training to recognize and use acoustic patterns as cues for actions, as well as synthesizing new actions from novel paths through its motion space. This work demonstrates that by addressing the problems of state, action, and state-action space discovery at the same time, the solution for each becomes easier. Finally, we articulate heuristics and design principles that make learning practical for synthetic characters.",2002-01-01,[],,7543,E15-441,2016-12-05T00:16:28.566Z,[],True,dobie-integrated-learning-for-interactive-synthetic-characters,2002-01-01,[],Dobie: Integrated Learning for Interactive Synthetic Characters,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:10.662Z,"Dollars & Scents explores ways to display abstract information using scent. A device is installed above the lower-lobby rotating door that periodically outputs scent depending on the state of the NASDAQ index. The smell of mint indicates that the market has gone up since the day's opening, and the smell of lemon indicates it has gone down. While these scents have convenient verbal mnemonics (you're either ""making money"" or have ""picked lemons""), they are abstract smell icons, or ""smicons,"" an olfactory equivelant of the ""earcons"" that have been developed in the field of auditory research.",2001-12-30,"['counter-intelligence', 'personal-information-architecture']",,7551,E15-068,2016-12-05T00:16:21.816Z,['mike@media.mit.edu'],True,dollars-scents,1999-12-31,[],Dollars & Scents,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:10.779Z,"This installation features the animated character Max T. Rat (Dobie's cousin). Max is the first autonomous, animated character that can be teased, that changes its behavior in response to the teasing, and that remembers the participant and behaves appropriately should the participant return to the installation at a later time. The user teases Max with a piece of physical ""cheese,"" and can also verbally tease him. Based on the participant�s actions, Max forms expectations�can he trust the participant to give him the cheese?  is the participant a good person or not?�and ultimately modifies his behavior to find the best strategy for getting the cheese. The installation demonstrates a number of technologies: Max is built using the next generation of our behavioral toolkit that includes a new framework for learning in a long-term behavioral context, and specialized learning mechanisms for performing social learning and expectation-based learning. In addition, the system incorporates a new Java-based graphics library and novel applications of sensing technology. The �cheese� emits IR, allowing us to track, precisely and robustly, the position of the cheese via a stereo-vision system. An RF link allows us to change the state of the cheese in response to Max�s actions. Participants are identified using retro-reflective markers on their badges as well as via acoustic pattern-matching. Pitch tracking is employed to detect if the participant is verbally teasing Max.",2004-01-01,['synthetic-characters'],,7559,E15-489,2016-12-05T00:16:18.581Z,[],True,dont-tease-the-animals,2003-09-01,[],Don't Tease the Animals,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:11.166Z,"Homes and offices are being filled with sensor networks to answer specific queries and solve pre-determined problems, but no comprehensive visualization tools exist for fusing these disparate data to examine relationships across spaces and sensing modalities. DoppelLab is a cross-reality virtual environment that represents the multimodal sensor data produced by a building and its inhabitants. Our system encompasses a set of tools for parsing, databasing, visualizing, and sonifying these data; by organizing data by the space from which they originate, DoppelLab provides a platform to make both broad and specific queries about the activities, systems, and relationships in a complex, sensor-rich environment. ",,[],https://dam-prod2.media.mit.edu/x/2016/07/16/re-doppel.jpg,7591,--Choose Location,2020-05-19T16:34:54.972Z,"['gershon@media.mit.edu', 'bmayton@media.mit.edu', 'joep@media.mit.edu']",True,doppellab-experiencing-multimodal-sensor-data,2009-01-01,"['human-computer-interaction', 'learning-teaching', 'music', 'community', 'technology', 'covid19']",DoppelLab: Experiencing multimodal sensor data,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:11.255Z,"In DoppelLab, we are developing tools that intuitively and scalably represent the rich, multimodal sensor data produced by a building and its inhabitants. Our aims transcend the traditional graphical display, in terms of the richness of data conveyed and the immersiveness of the user experience. To this end, we have incorporated 3D spatialized data sonification into the DoppelLab application, as well as in standalone installations. Currently, we virtually spatialize streams of audio recorded by nodes throughout the physical space. By reversing and shuffling short audio segments, we distill the sound to its ambient essence while protecting occupant privacy. In addition to the sampled audio, our work includes abstract data sonification that conveys multimodal sensor data.",2013-09-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2016/07/16/clearDop.jpg,7599,--Choose Location,2016-12-05T00:17:10.977Z,"['gershon@media.mit.edu', 'bmayton@media.mit.edu', 'joep@media.mit.edu']",True,doppellab-spatialized-sonification-in-a-3d-virtual-environment,2011-01-01,[],DoppelLab: Spatialized Sonification in a 3D Virtual Environment,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:11.344Z,"Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2017/05/17/sensor_vision_AB.jpg,7607,--Choose Location,2019-04-19T14:24:11.949Z,"['gershon@media.mit.edu', 'bmayton@media.mit.edu', 'eflynch@media.mit.edu', 'sfr@media.mit.edu', 'ddh@media.mit.edu', 'joep@media.mit.edu']",True,doppelmarsh-cross-reality-environmental-sensor-data-browser,2014-09-01,"['environment', 'sensors']",Doppelmarsh: Cross-reality environmental sensor data browser,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:11.436Z,"The ""Nominal Group Technique"" is a popular way to brainstorm, often executed with Post-it notes and voting stickers. We're reimagining and reimplementing this technique for online use, for things such as hackathons, design workshops, and brainstorms across multiple geographies. The best part: everyone can take the results of the brainstorm with them, and embed it in blogs or websites.",2012-09-01,['living-mobile'],,7615,,2016-12-05T00:16:21.859Z,['geek@media.mit.edu'],True,dotstorm,2012-01-01,[],Dotstorm,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:11.517Z,"DragonBot is a new platform built to support long-term interactions between children and robots. The robot runs entirely on an Android cell phone, which displays an animated virtual face. Additionally, the phone provides sensory input (camera and microphone) and fully controls the actuation of the robot (motors and speakers). Most importantly, the phone always has an Internet connection, so a robot can harness cloud-computing paradigms to learn from the collective interactions of multiple robots. To support long-term interactions, DragonBot is a ""blended-reality"" character: if you remove the phone from the robot, a virtual avatar appears on the screen and the user can still interact with the virtual character on the go. Costing less than $1,000, DragonBot was specifically designed to be a low-cost platform that can support longitudinal human-robot interactions ""in the wild.""",2015-01-01,['personal-robots'],,7623,--Choose Location,2017-05-31T18:40:07.639Z,['cynthiab@media.mit.edu'],True,dragonbot-android-phone-robots-for-long-term-hri,2011-09-01,[],DragonBot: Android Phone Robots for Long-Term HRI,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:11.795Z,Drawdio is a pencil that draws music. You can sketch musical instruments on paper and play them with your finger. Touch your drawings to bring them to life�or collaborate through skin-to-skin contact. Drawdio works by creating electrical circuits with graphite and the human body. ,2012-09-01,['lifelong-kindergarten'],,7655,LEGO Learning Lab,2016-12-05T00:16:21.896Z,"['mres@media.mit.edu', 'silver@media.mit.edu']",True,drawdio,2007-09-01,[],Drawdio,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:11.873Z,"DressCode is a computer-aided design and fabrication tool that combines programming with graphic drawing and manipulation, allowing novice programmers to create computationally-generated, physical artifacts. The software consists of a programming environment and a graphic-user interface design tool, as well as a custom programming language. The GUI tools allow for a unique combination of graphic drawing and computational manipulation, because the software automatically generates editable code in the programming environment that reflects the designer's drawing actions. DressCode exports designs that are compatible with digital fabrication machines, allowing for the creation of physical artifacts. We have introduced DressCode to amateur programmers with a series of craft activities that allow them to produce functional, beautiful, and unique objects including t-shirts, jewelry, and personal accessories.",2015-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/logo_white.png,7663,--Choose Location,2017-04-04T14:49:37.761Z,"['mres@media.mit.edu', 'leah@media.mit.edu', 'jacobsj@media.mit.edu']",True,dresscode,2013-01-01,[],DressCode,PUBLIC,http://media.mit.edu/~jacobsj,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:11.955Z,"This project steps beyond data visualizations to create data experiences. It aims to engage not only the analytic mind, but also the artistic and emotional self. In this project, chemicals found in people's bodies and homes are turned into a series of fashions. Quantities, properties, and sources of chemicals are represented through various parameters of the fashion, such as fabric color, textures, and sizes. Wearing these outfits allows people to live the data—to experience tangibly the findings from their homes and bodies. This is the first project in a series of works that seek to create aesthetic data experiences that prompt researchers and laypeople to engage with information in new ways.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2016/10/20/laceYellow_TOvPdJI.jpg,7671,--Choose Location,2022-06-10T14:09:49.185Z,['perovich@media.mit.edu'],True,dressed-in-data,2013-01-01,"['data', 'fashion']",Dressed in Data,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:09:12.041Z,"How can emotions be conveyed, expressed, and felt? Drift Bottle is a project exploring interfaces that allow users to ""feel"" others� emotions to promote their communication. We have developed a voice message-exchange web service. Based on that, we design and develop several terminals with different interfaces which convey emotions via media such as light, smell, and motion. One solution is to convey the emotions in voice messages via different colors of light. Our latest effort is conveying emotions via smells, with the intention of arousing the same emotions in the receivers.",2014-09-01,['object-based-media'],,7679,--Choose Location,2016-12-05T00:16:38.605Z,"['vmb@media.mit.edu', 'slysun@media.mit.edu']",True,drift-bottle,2014-01-01,[],Drift Bottle,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:12.123Z,"Email is a tool that people use daily, making an implicit statement about their relationships with other people, and providing an opportunity for a computer to learn about their social network. While people have come to depend on email in their daily lives, the tool has hardly changed to help people deal with an overwhelming amount of information. Many of the social cues that allow people to function naturally within their social network are not inherent or obvious in Computer Mediated Communication (CMC). This work uses automatic social network analysis to bring these cues to CMC and foster the user�s coherent understanding of the people and resources of their communication network. The goal of this work is to demonstrate that Artificial Intelligence can help people in the realm of social decisions. Using AI of Social Networks, this work improves human-human communication, recognizing the social characteristics of human relations in order to achieve a more natural online communication interface.",2003-01-01,['context-aware-computing'],,7687,E15-320,2016-12-05T00:16:21.967Z,[],True,driftcatcher,2002-01-01,[],DriftCatcher,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:12.204Z,"Droplet is a tangible interface that explores the movement of information between digital and physical representations. Through light-based communication, the project allows information to be easily extracted from its digital form behind glass and converted into mobile, tangible representations,  altering its form and our perception of the information.",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/droplet2.png,7695,--Choose Location,2016-12-05T00:16:21.994Z,['holtzman@media.mit.edu'],True,droplet,2011-09-01,[],Droplet,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:12.284Z,"The Drum Network provides players with a collaborative playing experience where participants can manipulate, share, and shape each others' music in real time. The drums in the network serve as controllers, sensing hitting and pressure that is then sent via a central system to other players. The drums also serve as speakers by using an attached actuator, which provides acoustic and tactile feedback.",2001-12-30,['opera-of-the-future'],,7703,E15-483,2016-12-05T00:16:22.061Z,['tod@media.mit.edu'],True,drum-network,2000-12-31,[],Drum Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:12.446Z,"The Dual Reality Lab is an experiment aimed at an interface between distributed sensor networks (which collect information about the real world), and online worlds (which allow people to interact and create in a virtual environment). In this case, we are connecting multimodal sensor data taken throughout the Media Lab to events happening in a virtual version of the Media Lab in the online world of Second Life. We hope to discover how data from sensor networks can be used as a creative medium to enhance virtual worlds, and how virtual worlds can act as an intuitive means for browsing sensor data.",2007-09-01,['responsive-environments'],,7719,E15-344,2016-12-05T00:16:28.449Z,['joep@media.mit.edu'],True,dual-reality-lab-using-sensor-networks-to-merge-real-and-virtual,2006-09-01,[],Dual Reality Lab: Using Sensor Networks to Merge Real and Virtual,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:12.552Z,Dual-space drawing supports creative drawing and reflective learning experiences using dual layers: a screen display and a transparent display. Dual-space drawing users can reflect themselves and embody their ideas while designing scenes and drawing objects.,2012-01-01,['information-ecology'],,7727,,2016-12-05T00:16:21.943Z,['holtzman@media.mit.edu'],True,dual-space-drawing,2010-09-01,[],Dual-Space Drawing ,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:12.637Z,"The Duct Tape Network (DTN) is a series of fun, hands-on maker clubs that encourage young children (ages 7-10) to use cardboard, tape, wood, fabric, LED lights, motors, and more to bring their stories and inventions to life. We are designing an educational framework and toolkit to engage kids in the creation of things that they care about before they lose their curiosity or get pulled in by more consumer-oriented technology. Work on DTN started in 2014 as part of a collaboration with Autodesk and is now expanding to communities all around the world.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/02/09/IMG_20150219_160123 copy.jpg,7735,--Choose Location,2022-09-22T01:20:14.226Z,"['leob@media.mit.edu', 'alishap@media.mit.edu']",True,duct-tape-network,2015-01-01,['kids'],Duct Tape Network,PUBLIC,http://ducttapenetwork.org/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:12.799Z,"We are in the third year of a multi-year project to see how close we can come to building a synthetic canine whose everyday common sense, ability to learn, and ability to empathize approaches that of a real canine. The past year has been largely focused on dog learning and training. In particular, Duncan can be trained to do many of the same things that real dogs can do using a technique borrowed from real dog training, called ""clicker-training."" Technically, this requires him to do state-space discovery, behavioral adaptation, and motor learning in an integrated, goal-driven fashion. While we are continuing this work, we are also broadening our focus to examine issues of social behavior and learning as well as spatial learning. Two installations currently feature Duncan: sheep/dog: Trial By Eire, and Clicker By Eire.",2001-12-30,['synthetic-characters'],,7751,E15-320,2016-12-05T00:16:28.477Z,[],True,duncan-the-highland-terrier,1998-12-31,[],Duncan the Highland Terrier,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:12.895Z,"DUSK was created as part of the Media Lab's Advancing Wellbeing initiative (supported by the Robert Wood Johnson Foundation) to create private, restful spaces for people in the workplace. DUSK promotes a vision of a new type of ""nap pod,"" where workers are encouraged to use the structure on a daily basis for regular breaks and meditation. The user is provided with the much-needed privacy to take a phone call, focus, or rest inside the pod for short periods during the day. The inside can be silent, or filled with binaural beats audio; pitch black, or illuminated by a sunlamp; whatever works for users to get the rest and relaxation needed to continue to be healthy and productive. DUSK is created with a parametric press-fit design, making it scalable and suitable for fabrication customizable on a per-user basis.",2018-06-01,"['advancing-wellbeing', 'object-based-media']",https://dam-prod2.media.mit.edu/x/files/Display/side1.png,7759,--Choose Location,2020-06-10T15:59:12.658Z,"['vmb@media.mit.edu', 'bdatta@media.mit.edu']",True,dusk,2014-09-01,[],DUSK,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:13.138Z,"This project explores the use of passive acoustic sensing in automobiles. We are building acoustic sensing equimpment suitable for automotive use, and developing algorithims and heuristics for dynamically extracting driver-relevant information from the acoustic data stream (i.e. location, trajectory, and acceleration of nearby vehicles).",1999-12-30,"['cc', 'responsive-environments']",,7783,E15-023,2016-12-05T00:17:11.049Z,['joep@media.mit.edu'],True,dynamic-acoustic-tracking-and-characterization-of-powered-vehicles,1998-12-31,[],Dynamic Acoustic Tracking and Characterization of Powered Vehicles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:13.223Z,"This project builds a real-time, scalable network where each node cooperates in the distribution of data. This cooperation frees the mobile nodes from any fixed base stations and enables advances in efficient use of power and bandwidth without adding latency.  Effectively it operates like a cellular network where the cells are created on-the-fly instead of by base stations. The protocol is being developed via an infrared simulation on an air hockey table where each node is a puck.",2003-01-01,['media-and-networks'],,7791,E15-483A,2016-12-05T00:16:08.002Z,['lip@media.mit.edu'],True,dynamic-cellularization,2002-01-01,[],Dynamic Cellularization,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:13.375Z,"Garments that embody unique aesthetic properties�for instance, light, sound, or motion rather than color, pattern, or cut�provide the possibility of enhancing and enriching our daily interactions with people, places, schedules, and self. Saturnpants and Iris are two such garments, creating interactive relationships with their owners and the people around them through the tight integration of electrical components into the aesthetic framework of the garments themselves.",2003-01-01,['aesthetics-computation'],,7807,E15-301,2016-12-05T00:16:28.306Z,[],True,dynamic-clothing-saturnpants-and-iris,2002-01-01,[],Dynamic Clothing: Saturnpants and Iris,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:14.300Z,"One of the biggest challenges for the digital economy is what to do when things go wrong. Orders get misplaced, numbers mistyped, requests misunderstood: then what? Consumers are frustrated by long waits on hold, misplaced receipts, and delays to problem resolution; companies are frustrated by the cost of high-quality customer service. Online companies want customers� trust, and how a company handles problems directly impacts that. We are exploring how software agents and other technologies can help with this issue. Borrowing ideas from software debugging, we can have agents help to automate record-keeping and retrieval, track dependencies, and provide visualization of processes. Diagnostic problem-solving can generate hypotheses about causes of errors, and seek information that allows hypotheses to be tested. Agents act on behalf of both the consumer and the vendor to resolve problems more quickly and at lower cost.",2014-09-01,['software-agents'],,7855,E15-383,2022-06-10T14:18:57.424Z,['lieber@media.mit.edu'],True,e-commerce-when-things-go-wrong,2001-01-01,[],E-Commerce When Things Go Wrong,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:14.467Z,"Most e-commerce sites today are little more than electronic catalogs of product offerings. Consumer input is limited to requirements questionnaires, search engines, and accepting or rejecting particular offerings. But in complex purchases, such as real estate, cars, or computers, it is often difficult to specify exactly what you want, and priorities and preferences often change in the process of exploration. We are investigating software agents that use machine learning, context sensitivity, and predictive interfaces. We would like these agents to act as advisors to consumers much as a real-estate agent or travel agent would, implicitly inferring general preferences from a history of relatively unconstrained reactions to specific examples.",2007-01-01,['software-agents'],,7863,E15-383,2016-12-05T00:16:28.625Z,['lieber@media.mit.edu'],True,e-commerce-when-you-dont-know-what-you-want,2000-01-01,[],E-Commerce When You Don't Know What You Want,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:14.640Z,"e-MotionInfo enables users to explore the harmonization of their movements, digital information, and responsive objects. e-MotionInfo creates links between motions, digital content, and associated objects to improve upon expressive and natural user interactions.",2012-01-01,['information-ecology'],,7871,,2016-12-05T00:17:11.236Z,['holtzman@media.mit.edu'],True,e-motioninfo,2010-09-01,[],E-MotionInfo,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:14.816Z,"To celebrate the construction of the new building, E14, the public installation E14 1/2 seeks to capture the Media Lab community's vision for our new space and our future directions via audience participation with a large projection on the facade of the new building. We consider the Lab's role within the School of Architecture and Planning, as well as MIT, and how the new community of E14-E15 will be a center of learning, collaboration, and innovation across all disciplines.",2009-01-01,['design-ecology'],,7879,--Choose Location,2016-12-05T00:16:22.380Z,['dsmall@media.mit.edu'],True,e14-12,2009-01-01,[],E14 1/2,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:14.988Z,"E15 is an experimental architecture that places the power of the presentation of Web content into the hands of those that use it. Based on a dynamic, interactive, OpenGL-based scripting engine, E15 exposes an entirely new face to Web content, freely modifiable by each individual user.",2008-01-01,['information-ecology'],,7887,E15-301,2016-12-05T00:16:22.353Z,['holtzman@media.mit.edu'],True,e15,2007-01-01,[],E15,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:15.176Z,"E15:oGFx is a dynamic openGL texture engine. It provides an interface to a dynamic procedural texture generation context that can be modified at runtime. Using Python as the scripting language, E15:oGFx can be used for procedural animations and data visualizations. One goal of E15:oGFx is to increase the level of end user engagement with existing programs and foster additional creativity on top of scripts written by others within the E15 community. In contrast to traditional 2D graphics environments, E15:oGFx leverages openGL to reveal more than the standard 2D view of the script execution. In particular, the 2D canvas can be scaled in vector graphics sense, and the history of the script execution can be visualized. In addition to many graphics environments, E15:oGFx also supports dynamic loading of GLSL-based shaders, as well as procedural manipulation of shader parameters.",2008-01-01,['information-ecology'],,7895,E15-301,2016-12-05T00:16:22.408Z,['holtzman@media.mit.edu'],True,e15ogfx,2007-01-01,[],E15:oGFx,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:15.344Z,"Echologue is a new kind of public space media for sensing and displaying socio-cultural characteristics of a place based on its sonic features. It can reflect its surroundings like a ""smart mirror,"" highlight the salient details and patterns in the environment, and contribute to our understanding of the perception of social places. The interface senses ambient sound and deliberate user input and displays a visualization of the activity in that space as its output. The design explores the utility of sound for envisioning new social, cultural, and entertainment uses of public places and helps us shape our relationships with each other with new social interfaces embedded in urban settings.",2017-05-01,['sociable-media'],,7903,E15-383,2022-06-10T14:41:29.857Z,['judith@media.mit.edu'],True,echologue,2007-09-01,[],Echologue,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:15.606Z,"Decades ago development scholars argued that the productive structure of a country (i. e. the mix of industries operating in the country) constrains its ability to generate and distribute income. They were correct! It was recently shown that the mix of products that a country exports is predictive of its future pattern of diversification and economic growth. But what is the link between a country's productive structure and its ability to distribute income? Here, we combine methods from econometrics, network science, and economic complexity, together with data on income inequality and world trade, to show that countries exporting complex products have lower levels of income inequality than countries exporting simpler products. Using multivariate regression analysis, we show that economic complexity is a significant and negative predictor of income inequality and that this relationship is robust to controlling for aggregate measures of income, institutions, export concentration, and human capital. Moreover, we introduce a measure that associates a product to a level of income inequality equal to the average GINI of the countries exporting that product (weighted by the share the product represents in that country’s export basket). The Product-GINI index, or PGI, can provide important insights on the constraints to inequality imposed by a country's productive structure. Finally, we integrate our results to the Observatory of Economic Complexity, an online resource that allows its users to visualize the structural transformation of over 150 countries and their associated changes in income inequality during 1963–2008.",2019-08-31,[],https://dam-prod2.media.mit.edu/x/2017/03/27/32637952522_a79feed480_o.jpg,7919,--Choose Location,2020-08-28T19:04:43.249Z,"['hidalgo@media.mit.edu', 'hartmado@media.mit.edu', 'crisjf@media.mit.edu', 'arista@media.mit.edu']",True,economic-complexity-and-income-inequality,2014-09-01,[],Inequality and the impact of industrial structures,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:15.864Z,"Cooperative media distribution provides a solution to the scalability and efficiency problem of real-time media distribution and information dissemination in general. The flip-side of this is that collaborative media distribution is a destabilizing issue for entities that rely on a centralized architecture for revenue. In order to reconcile the economic realities of media distribution with the technical necessity of network scalability, a suitable economic model is required. We are developing licensing and payment schemes to enable and encourage cooperative distribution, while providing viable revenue schemes for intellectual property holders.",2005-01-01,['viral-communications'],,7935,E15-483,2016-12-05T00:16:22.592Z,['lip@media.mit.edu'],True,economic-models-for-cooperative-media-distribution,2002-01-01,[],Economic Models for Cooperative Media Distribution,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:16.078Z,"Electrodermal Activity (EDA) is a physiological indicator of stress and strong emotion. While an increasing number of wearable devices can collect EDA, analyzing the data to obtain reliable estimates of stress and emotion remains a difficult problem. We have built a graphical tool that allows anyone to upload their EDA data and analyze it. Using a highly accurate machine learning algorithm, we can automatically detect noise within the data. We can also detect skin conductance responses, which are spikes in the signal indicating a ""fight or flight"" response. Users can visualize these results and download files containing features calculated on the data to be used in their own analysis. Those interested in machine learning can also view and label their data to train a machine learning classifier. We are currently adding active learning, so the site can intelligently select the fewest possible samples for the user to label. ",2018-12-31,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/files/Display/Logo.png,7951,--Choose Location,2022-05-12T17:23:57.081Z,"['picard@media.mit.edu', 'sataylor@media.mit.edu', 'jaquesn@media.mit.edu', 'akanes@media.mit.edu', 'sfedor@media.mit.edu', 'cvx@media.mit.edu']",True,eda-explorer,2015-01-01,['wearable-computing'],EDA Explorer,PUBLIC,http://eda-explorer.media.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:16.191Z,"Conventional holograms require illuminators to be mounted on walls or ceilings near the hologram. Edge-lit holograms are a new type of white-light hologram that allow the light source to be included within the mount itself, ensuring a compact and carefully aligned illumination. This project explores the fundamental diffraction and imaging properties of these holograms, with a view toward making their images deeper, brighter, and clearer.",2004-01-01,[],,7959,E15-441,2016-12-05T00:16:22.568Z,[],True,edge-lit-holograms,2000-01-01,[],Edge-Lit Holograms,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:16.286Z,"When a die is picked up, it attempts to teach patterns, progressions of patterns, numbers, counting, arithmetic, and probability depending on user successes. It gives instructions: ""turn dice to find four"" or ""turn dice to add two."" The tasks increase and decrease in difficulty based upon the user's success. The dice are adaptive educational toys for a range of ages and skill levels (from infant to third grade). This adaptive toy demonstrates a novel capacitive 3-D accelerometer. The water-filled vessel accelerometer is an inexpensively manufactured, low-technology way to measure orientation. Water within a cavity sloshes to be close to capacitive plates on various sides of the accelerometer. A micro-computer compares capacitances of the sides of the tetrahedron or cube cavity to determine its orientation. The dice use this novel capacitive, tetrahedral, position sensor, as well as voice feedback and adaptive user modeling to teach mathematical relations in a progressive manner.",2001-09-01,['context-aware-computing'],,7967,E15-320,2016-12-05T00:16:22.655Z,[],True,educational-dice,2000-01-01,[],Educational Dice,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:16.385Z,"Lower-extremity amputees face a series of potentially serious post-operative complications. Among these are increased risk of further amputations, excessive stress on the unaffected and residual limbs, and discomfort at the human-prosthesis interface. Currently, conventional, passive prostheses have made strides towards alleviating the risk of experiencing complications, but we believe that the limit of ""dumb"" elastic prostheses has been reached; in order to make further strides we must integrate ""smart"" technology in the form of sensors and actuators into lower-limb prostheses. This project compares the elements of shock absorption and socket pressure between passive and active ankle-foot prostheses. It is an attempt to quantitatively evaluate the patient's comfort.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2018/08/01/BiOM-Augmenting-Runner-1024x688.jpg,7975,,2023-01-30T23:58:57.566Z,"['hherr@media.mit.edu', 'dhill24@media.mit.edu']",True,effect-of-a-powered-ankle-on-shock-absorption-and-interfacial-pressure,2011-01-01,"['bioengineering', 'bionics']",Effect of a powered ankle on shock absorption and interfacial pressure,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:16.475Z,"It has been theoretically proven that a wireless diversity scheme that utilizes a relay node to assist in conveying information is more efficient in throughput and transmission power. This project aims to construct and demonstrate such a system and its feasibility in a viral architecture. Optimal transmitters, relays, and receivers are built and their collaboration is measured and compared to theoretical results. Methods to exploit the benefits of such a scheme are explored.",2005-01-01,['viral-communications'],,7983,E15-483,2016-12-05T00:16:28.650Z,['lip@media.mit.edu'],True,efficient-wireless-antenna-sharing,2003-09-01,[],Efficient Wireless Antenna Sharing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:16.710Z,"A transgenic chicken commercial for ovulating womenEsgtrogen Farms is a fictional company that raises genetically modified chickens that produce ovulation hormones in their egg whites. The eggs are marketed towards women who are either trying to get pregnant, or work as egg donors for the fertility industry. The slogan reads, ""One egg a day is the fertility way."" The project highlights a connection between women and chickens as raw commodities for the biotech industry, performing ways in which women are targeted for bio-consumerism. What is the rhetoric and imagery used in birth management products? Moreover, as avian transgenic technologies become further developed, is it possible to imagine a confluence of the poultry industry with the pharmaceutical health industry?",2010-05-01,['design-fiction'],https://dam-prod2.media.mit.edu/x/2016/10/24/Egstrogen.jpg,8007,--Choose Location,2022-06-10T13:44:47.794Z,['maggic@media.mit.edu'],True,egstrogen-farms,2015-09-07,"['genetics', 'storytelling']",Egstrogen Farms,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:16.943Z,"During the Ejewels workshops, participants used a combination of basic electronics materials and basic craft materials to create jewelry with lights that glow, flash, and change color. These workshops enabled participants to become simultaneously engaged in a diverse set of investigations into: the nature of materials (for example, how light reflects against and refracts through different objects); the workings of basic electronics (gaining a practical, hands-on experience with serial and parallel circuits, short-circuiting, and the concepts underlying Ohm's law); and issues of personal identity (engaging in discussions of how  jewelry reveals aspects of the wearer).",2005-09-01,['lifelong-kindergarten'],,8031,E15-020A,2016-12-05T00:16:22.810Z,"['mres@media.mit.edu', 'sylvan@media.mit.edu']",True,ejewels-workshops,2004-01-01,[],Ejewels Workshops,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:17.018Z,"Can you feel empathy for a robot? The electric parrot project is about designing a robot that can engender empathy. For this we are constructing a novel zoomorphic robot that can create its own life story: that is to say it can experience the world, be changed by the experience, and communicate the experience to us. We aim to show through psychometric tests that giving the robot an implicit life story will invoke empathy. Such a robot can subsequently be used for empathy intervention. ",2016-08-31,['personal-robots'],,8039,--Choose Location,2017-05-31T18:44:43.634Z,"['cynthiab@media.mit.edu', 'palash@media.mit.edu']",True,electric-parrot,2015-01-01,[],Electric Parrot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:09:17.092Z,"Electric Price Tags are a realization of a mobile system that is linked to technology in physical space. The underlying theme is that being mobile can mean far more than focusing on a portable device�it can be the use of that device to unlock data and technology embedded in the environment. In its current version, users can reconfigure the price tags on a store shelf to display a desired metric (e.g., price, unit price, or calories). While this information is present on the boxes of the items for sale, comparisons would require individual analysis of each box. The visualization provided by Electric Price Tags allows users to view and filter information in physical space in ways that were previously possible only online.",2013-01-01,['viral-communications'],,8047,--Choose Location,2016-12-05T00:16:22.895Z,"['borovoy@media.mit.edu', 'lip@media.mit.edu']",True,electric-price-tags,2011-01-01,[],Electric Price Tags,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:17.253Z,"Books with printed pages are unique in that they embody the simultaneous, high-resolution display of hundreds of pages of information. The representation of information on a large number of physical pages, which may be physically turned and written on, constitutes a highly preferred means of information interaction. An obvious disadvantage of the printed page, however, is its immutability once typeset. We are currently developing electronically addressable paper-page displays that use real paper substrates. This effort includes the development of novel, electronically addressable contrast media, microencapsulation chemistry, and desktop printing technologies to print functional circuits, logic, display elements, and actuators on paper or paper-like substrates.",2005-01-01,['molecular-machines'],,8063,--Choose Location,2016-12-05T00:17:11.359Z,['jacobson@media.mit.edu'],True,electronic-ink-and-electronic-paper,2000-01-01,[],Electronic Ink and Electronic Paper,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:17.327Z,"The Electronic Necklace demonstrates that sensors can be responsive and interactive devices, even without a computer. The necklace is activated mechanically, without batteries or electronics, simply by the user playing with it. For example, turning one of the beads turns all the beads. If the user turns on an LED, the necklace will light up. It can be illuminated in various ways depending on how the beads are turned. The necklace can also be augmented to include sound, so that the beads squeak when turned. This project sets a baseline for other work in context-aware computing, where sensors and computers with memory are used to add explicit task, system, and user models to objects in our everyday environment.",2003-01-01,['context-aware-computing'],,8071,E15-320,2016-12-05T00:16:22.913Z,[],True,electronic-necklace,2000-01-01,[],Electronic Necklace,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:17.402Z,"Electronic Popables is an interactive pop-up book that sparkles, sings, and moves. The book integrates traditional pop-up mechanisms with thin, flexible, paper-based electronics; the result is an artifact that looks and functions much like an ordinary pop-up book, but has added elements of dynamic interactivity. ",2009-09-01,['high-low-tech'],,8079,E15-368,2016-12-05T00:17:11.292Z,"['leah@media.mit.edu', 'jieqi@media.mit.edu']",True,electronic-popables-an-interactive-pop-up-book,2009-01-01,[],Electronic Popables: An Interactive Pop-Up Book,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:17.481Z,"The pioneering research of the Electronic Publishing group has explored a ""constructionist"" approach to the development of personal and community publishing tools in order to help increase the level of personal and community participation and appropriation. In the early days of the Internet we built systems to fine-tune and prioritize information based on criteria such as timeliness, importance, and relevance. Still, the expectations of the Internet consumer closely aligned with those of the traditional media consumer—the need for an editor, whether human or machine, to reduce complexity and expose the essence of available information. But we are beginning to expect more than just efficient access to ideas. Our goal is to expand scope rather than restrict it; to that end, we are exploring a unique feature of electronic media: you can tinker under the hood because every Web browser allows any content to reveal its inner structure. We are attempting to build representations of domain knowledge, but also local knowledge about people, local cultures, and norms. We aim to make the means of expression accessible without diminishing quality or complexity.",2006-09-01,[],,8087,--Choose Location,2019-02-06T21:44:53.165Z,['walter@media.mit.edu'],True,electronic-publishing,2006-09-01,[],Electronic Publishing,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:17.546Z,"We are designing an �electronic lens� that provides pedestrians with immediate, on-the-spot, geographically and temporally contextualized information about the attractions and resources that a city offers. The Electronic Lens explores and creates new paradigms of civic ubiquitous networking with mobile technologies. The eLens matches electronic information with the physical environment in an innovative way; for example, eLens users can post lasting messages in physical locations, tag buildings and places, or create social networks based on interest and social affinities. eLens interactions combine the physical environment with formal and institutional information and the annotations from users� personal experiences.",2006-09-01,['smart-cities'],,8095,E15-001,2016-12-05T00:17:11.324Z,"['sajid@media.mit.edu', 'federico@media.mit.edu']",True,elens-the-electronic-lens,2005-09-01,[],eLens: The Electronic Lens,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:17.631Z,"Imagine the full power of a theater lighting system, complete with an expert human lighting controller, at your disposal to light your home. Such a system could put light where you need it, when you need it. You could talk to the controller, who would then translate your wishes into lighting configurations, taking into account numerous contextual factors such as who is in the room, where they are, what other sources of light are effecting the room, and so forth. Elvis is a conversational, robotic chandelier designed with these goals in mind. By combining elements of computer vision, robotic control, and grounded language understanding, Elvis demonstrates a new kind of device that is aware of its surroundings and able to communicate with people using natural communication.",2004-01-01,['social-machines'],,8103,E15-483,2016-12-05T00:16:28.501Z,['dkroy@media.mit.edu'],True,elvis-a-situation-aware-conversational-chandelier,2003-09-01,[],Elvis: A Situation-Aware Conversational Chandelier,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:17.708Z,"Email lists are useful for communicating with groups, but it is difficult to refine the intended audience. When planning a surprise party, for example, it is time consuming to delete the guest of honor's address from the multiple e-mail lists one wants to contact. We have built a tool that makes it easy to modify e-mail lists and, thus, contact specific audiences. The tool provides the computational mechanism for solving 'e-mail equations'. An equation for the aforementioned example could be ((partyPeople@mit.edu & dancers@mit.edu) - joe@mit.edu). This equation would resolve to address all recipients of two e-mail lists except for joe@mit.edu. E-mail list addition, subtraction, intersection and xor can be computed with our tool.",2002-12-30,['electronic-publishing'],,8111,E15-311,2019-02-06T21:47:58.684Z,['walter@media.mit.edu'],True,email-equations,2001-12-31,[],Email Equations,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:17.856Z,"High-quality sound in professional audio production has required large ASICs or power-hungry DSPs to get the work done. Even the MPEG-4 audio standard has unrealistic demands in its core technology to be ported to low-power handhelds and consumer products. This project has brought the same high audio standards to low-power processors embedded in hand-held formats. This is a redesign of the Structured Audio core that Media Lab contributed to the MPEG-4 standard. SA's floating-point processing has now been redesigned to suit a low-power 16-bit processor, and the first port to an Analog Devices 16-bit Blackfin processor is now running and demonstrable. While ring tones and MIDI-encoded melodies are trivial, the power of this system is revealed not only in its 64-voice synthetic orchestras with audio-post effects (reverb, chorusing) but also in simultaneous voice processing with automatic harmonization and tempo tracking. Try doing that on your mobile!",2007-01-01,['music-mind-and-machine'],,8127,E15-484,2016-12-05T00:16:23.042Z,['bv@media.mit.edu'],True,embedded-audio-systems,2005-01-01,[],Embedded Audio Systems,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:17.925Z,"Embedded Networks are digital networks designed to form links among everyday objects. By design, embedded network links are wireless, self-organizing, and low-power. Control of the network is decentralized. There is no base station or centralized point of command, and distributed, each node shares in the overall maintenance of the network and routing of data packets. The goal of this research is to create networks that are exceptionally easy to set up and maintain. Providing such ""instant infrastructure"" requires a major rethinking of network architectures. Research in Embedded Networks focuses on novel techniques for control, routing and power management to realize low-cost, packet-switched digital networks.",2001-12-30,['personal-information-architecture'],,8135,E15-468,2016-12-05T00:16:22.973Z,['mike@media.mit.edu'],True,embedded-networks,1998-12-31,[],Embedded Networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:18.076Z,"Light that is imprisoned within the CRT can only be interpreted as being of and pertaining to everyday information processing. By recontextualizing the synthetic light into other vehicles, such as wearable displays or light sculpture, we can examine the potential of organic graphics in everyday spaces and situations.",2000-12-30,['aesthetics-computation'],,8151,E15-448,2016-12-05T00:16:38.637Z,[],True,embodied-light,1999-12-31,[],Embodied Light,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:18.227Z,"EmBuddy is a digital creature that senses its user's stress levels via an EEG headset. When EmBuddy's user is stressed, EmBuddy will wiggle in empathy. EmBuddy's user can calm this wiggling either by petting EmBuddy's stroke sensors and/or by calming his mind. This prototype was built by Sen Ando, Aydin Arpa, Sam Leuscher, and me for Hiroshii Ishii's Tangible Interfaces class. EmBuddy's motorized body was constructed by Sam, its skin and stroke sensors were constructed by Sen and me, and its software was written by Aydin, based on my MindRider sketch.",2011-09-01,"['tangible-media', 'information-ecology', 'camera-culture']",https://dam-prod2.media.mit.edu/x/files/Display/Thumbnail-2.jpg,8167,,2016-12-05T00:16:23.124Z,"['holtzman@media.mit.edu', 'ishii@media.mit.edu', 'raskar@media.mit.edu']",True,embuddy,2011-09-01,[],EmBuddy,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:18.826Z,"The Emonator is a new musical instrument for amateurs and professionals alike. It gives users a three-dimensional tactile interface to control music using their hands, and can be used as either a stand-alone gestural input device or in conjunction with a traditional musical keyboard or microphone. The Emonator generates audio output by mapping the perfomer's expressive gestures to different musical parameters. The sculptable surface of the Emonator can control a variety of sonic parameters, ranging from the audible harmonics of an additive synthesis engine to the activity level of algorithmically generated music.",2000-12-30,['opera-of-the-future'],,8239,E15-492,2016-12-05T00:16:23.146Z,['tod@media.mit.edu'],True,emonator-2,1999-12-31,[],Emonator,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:18.900Z,"The Emonator project is aimed at affective information browsing and exchange. We are using an innovative gesture controller as our input interface, and music/video as our media. The applications include real-time sound synthesis, MIDI control, live video browsing, and more. The multipurpose gesture interface allows for interactive control of music and video, and is controlled by hand gestures. The device, which consists of 144 rods arranged in a square pattern, derives its position from 12 optical boards, and is controlled by a field programmable gate array. The Emonator is also the first input interface for the Emonic Environment project. The hardware for the Emonator project was collaboratively designed by Dan Overholt, a recent master's candidate in the Lab's Opera of the Future group, and Andrew Yip, who constructed the interface.",2002-09-01,['interactive-cinema'],,8247,E15-368,2016-12-05T00:16:23.167Z,['gid@media.mit.edu'],True,emonator,2000-01-01,[],Emonator,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:18.983Z,"Many of us enjoy making, changing, and sharing digital media, but we are often unsure how to use or present our collections. This project allows users to focus on the higher-level aspect of media manipulation: controlling the structure by which images and sounds come together. Working with its users, the Emonic Environment structures audio, video, and text into a network, while continuously providing suggestions for manipulations that can be applied to elements of this network. It operates in real time, is capable of operating with or without human guidance, and participants can contribute and edit media, or interact solely on the structural network level, leaving the low-level control to the system's algorithms. The system and its content are controllable by mouse and keyboard, microphones, cameras, cell phones, MIDI controllers, sensors, and third-party interfaces.",2006-01-01,['media-fabrics'],,8255,E15-368,2016-12-05T00:16:08.063Z,['gid@media.mit.edu'],True,emonic-environment,2001-01-01,[],Emonic Environment,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:19.065Z,"The Emonique project is a collaboration between the Interactive Cinema group and the Story Networks group (Media Lab Europe) to develop an improvisational installation performance using the Emonic Environment. The goal of this event is to show how, using the Emonic Environment, the mobile phone can be transformed into an instrument for audiovisual improvisation. The installation aims to entice the members of the audience to use their phones as recorders and controllers to create structures of sonic samples in a collective manner, thus making the audience creative, improvisational artists and actors within a surrounding informational landscape.",2003-09-01,['interactive-cinema'],,8263,E15-368,2016-12-05T00:16:23.103Z,['gid@media.mit.edu'],True,emonique,2003-01-01,[],Emonique,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:19.140Z,"EmoteMail is an email client that is augmented to convey aspects of the state of the writer during the composition of email to the recipient. The client captures facial expressions and typing speed and introduces them as design elements. These contextual cues provide extra information that can help the recipient decode the tone of the email. Moreover, the contextual information is gathered and automatically embedded as the sender composes the email, allowing an additional channel of expression.",2005-09-01,['affective-computing'],,8271,E15-001,2016-12-05T00:16:23.189Z,['picard@media.mit.edu'],True,emotemail,2004-01-01,[],EmoteMail,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:19.219Z,"Have you ever wondered what makes an ad memorable? We have performed a comprehensive review of literature concerning advertising, memory, and emotion. A summary of results are available.",2012-01-01,['affective-computing'],,8279,,2016-12-05T00:16:28.819Z,['picard@media.mit.edu'],True,emotion-and-memory,2011-09-01,[],Emotion and Memory,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:09:19.295Z,"The Emotion Bottles are tangibly enticing objects that embody three emotions:  angry, happy, and sad. When a bottle is opened, a vocal output is generated as if the emotion that was stored within the bottle is released. The bottles are placed near each other and represent a person in three possible emotional states. Varying degrees of these emotions are ""bottled up"" inside. The three bottles were chosen to maintain the simplicity of exploring the combination of distinct emotional states (eight possibilities). While not completely representative of the possible emotional state of a person, the bottles explore the interface in accessing emotions, the interaction between conflicting emotions, and the meaning of transition between clear emotional states as a person empathizes with or projects their feelings onto the bottles.",2007-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/onebottle.jpg,8287,E15-443,2016-12-05T00:17:11.415Z,['picard@media.mit.edu'],True,emotion-bottles,2005-01-01,[],Emotion Bottles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:19.370Z,"People who have difficulty communicating verbally (such as many people with autism) sometimes send nonverbal messages that do not match what is happening inside them. For example, a child might appear calm and receptive to learning�but have a heart rate over 120 bpm and be about to meltdown or shutdown. This mismatch can lead to misunderstandings such as ""he became aggressive for no reason."" We are creating new technologies to address this fundamental communication problem and enable the first long-term, ultra-dense longitudinal data analysis of emotion-related physiological signals. We hope to equip individuals with personalized tools to understand the influences of their physiological state on their own behavior (e.g., ""which state helps me best maintain my attention and focus for learning?""). Data from daily life will also advance basic scientific understanding of the role of autonomic nervous system regulation in autism.",2010-01-01,['affective-computing'],,8295,East Lab,2016-12-05T00:16:23.256Z,"['picard@media.mit.edu', 'rmorris@media.mit.edu', 'mgoodwin@media.mit.edu']",True,emotion-communication-in-autism,2007-09-01,[],Emotion Communication in Autism,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:19.453Z,"You can test whether a website is usable by making wire frames, but how do you know if that site, product, or store is emotionally engaging? We build quick, iterative environments where emotions can be tested and improved. Emphasis is on setting up the right motivation (users always have to buy what they pick), pressures (can you buy the laptop in 10 minutes?), and environment (competitors� products better be on the shelf too). Once we see where customers are stressed or miss the fun part, we change the space on a daily, iterative cycle. Within two to three weeks, we can tell how to structure a new offering for a great experience. Seldom do the emotions we hope to create happen on the first try; emotion prototyping delivers the experience we want. We hope to better understand the benefits of emotion prototyping, especially while using the skin conductance sensor.",2015-09-01,['affective-computing'],,8303,--Choose Location,2016-12-05T00:16:08.083Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",True,emotion-prototyping-redesigning-the-customer-experience,2013-01-01,[],Emotion Prototyping: Redesigning the Customer Experience,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:19.528Z,"The technology in this project changes facial expressions in videos without the system knowing anything in particular about the person's face ahead of time. There are a few reasons to create something like this: first, it provides an artistic tool with which to alter photos or videos; second, it could be set up to let people open-endedly explore their facial communication and expressiveness by playing with a real-time video of their own current face; finally, E-DJ demonstrates an unexpected way in which we can't always trust the video information we love to consume. ",2006-09-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/2005spring/mas630/05.projects/silver/emotionalDJ3web.jpg,8311,Cube,2016-12-05T00:16:23.393Z,"['picard@media.mit.edu', 'silver@media.mit.edu']",True,emotional-dj,2005-01-01,[],Emotional DJ,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:19.603Z,"Social-emotional communication difficulties lie at the core of autism spectrum disorders, making interpersonal interactions overwhelming, frustrating, and stressful. We are developing the world's first wearable affective technologies to help the growing number of individuals diagnosed with autism�approximately 1 in 150 children in the United States�learn about nonverbal communication in a natural, social context. We are also developing technologies that build on the nonverbal communication that individuals are already using to express themselves, to help families, educators, and other persons who deal with autism spectrum disorders to better understand these alternative means of nonverbal communication.",2012-09-01,['affective-computing'],,8319,E15-443,2016-12-05T00:16:23.293Z,"['picard@media.mit.edu', 'mehoque@media.mit.edu', 'micahrye@media.mit.edu']",True,emotional-social-intelligence-toolkit,2005-09-01,[],Emotional-Social Intelligence Toolkit,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:19.771Z,"The design process is no longer limited to one group of individuals, as number, level, and cost make tools ever more accessible. As we move towards tools that allow us to create our own materials, having a set of rules with which to evaluate, interpret, and design them will become increasingly important. One way of approaching this problem is by unpacking the ways in which materials create meaning. This project explores the more emotive aspects of materials, such as haptic responses to, cognitive evaluation of, and emotive perception of materials to understand how materials communicate meaning.The development of an effective methodology aims to lower the barriers of fabrication of engaging objects. By incorporating qualities that were not previously quantifiable, we aim to encourage a more interactive design process that allows for the production of experiences tailored to individual preference, and a framework for conversations around material issues.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2016/10/17/IMG_9130.jpg,8335,--Choose Location,2022-06-10T14:09:49.214Z,['bdatta@media.mit.edu'],True,emotive-materials,2015-09-01,"['design', 'interfaces', 'behavioral-science']",Emotive Materials,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:19.868Z,"Whether or not we're experts in the design language of objects, we have an unconscious understanding of the emotional character of their forms. EmotiveModeler integrates knowledge about our emotive perception of shapes into a CAD tool that uses descriptive adjectives as an input to aid both expert and novice designers in creating objects that can communicate emotive character.",2019-10-30,['object-based-media'],https://dam-prod2.media.mit.edu/x/2016/10/23/EM bottles on shelf.jpg,8343,--Choose Location,2020-05-27T22:26:00.464Z,"['vmb@media.mit.edu', 'pip@media.mit.edu']",True,emotivemodeler-an-emotive-form-design-cad-tool,2013-09-01,"['design', '3d-printing']",EmotiveModeler: An emotive form design CAD tool,PUBLIC,http://emotivemodeler.media.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:19.944Z,"We have an unconscious understanding of the meaning of different physical objects through our extensive interactions with them. Designers can extend and adapt the existing symbolic meanings through the design of these objects, adding a layer of emotive expression by manipulating their forms. Tactile Allegory explores the physical design language encoded into objects and asks: how can objects be computationally designed to communicate specific information through their very forms? This research explores the underlying design ""grammar"" of the form of objects, particularly how objects can communicate information to us through their form. This framework is used to create a computational design tool to help people design expressively shaped objects that can express higher-level sentiments of their ideas via aesthetic forms.",2014-09-01,['object-based-media'],,8351,--Choose Location,2016-12-05T00:17:11.379Z,"['vmb@media.mit.edu', 'pip@media.mit.edu']",True,emotivemodeler-tactile-allegory-design-framework,2012-09-01,[],EmotiveModeler: Emotive Form Design Taxonomy,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:20.023Z,"EMotoPhone adds a new dimension to cell-phone interaction, augmenting verbal communication by allowing users to send personalized emoticons over the phone to show how they are feeling. The EMotoPhone icons can also be sent independently, outside of the context of a conversation. For example, if you are too busy to engage in a conversation, but you want to let someone know that you are thinking of them, you can send them an EMotoPhone wink.  Using real faces for the EMotoPhone icons makes cell-phone interaction richer, more personalized, and more fun.",2007-01-01,['living-mobile'],,8359,E15-384C,2016-12-05T00:16:23.414Z,['geek@media.mit.edu'],True,emotophone,2004-09-01,[],EMotoPhone,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:20.098Z,"Story understanding is a notoriously difficult problem in AI. Broad-spectrum, common-sense knowledge about the world is a good resource, but current common-sense knowledge bases are far from human-level story understanding. We examine affective story understanding in order to perceive the broad emotional overtones of a story at the sentence level, using both a common-sense perspective and the observation that much of the way we emote in response to everyday situations is part of a shared human experience and therefore a part of common sense. With a corpus of common-sense knowledge, we create a semantic network of everyday situations and the emotions associated with them, which, when combined with our linguistic processing, lets our system classify story sentences into six primitive emotions. We then explore how this technology enables innovations in emotional UIs such as EmpathyBuddy, or in prosody, emotional TTS, gaming, story evaluation, and emotional indexing of documents.",2010-09-01,['software-agents'],,8367,E15-383,2016-12-05T00:17:11.444Z,['lieber@media.mit.edu'],True,emotus-ponens-affective-story-understanding-for-agents,2002-01-01,[],Emotus Ponens: Affective Story Understanding for Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:20.175Z,"Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.",,['opera-of-the-future'],,8375,--Choose Location,2019-04-17T19:59:42.795Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'benb@media.mit.edu', 'sovsey@media.mit.edu', 'akito@media.mit.edu', 'rebklein@media.mit.edu', 'dnunez@media.mit.edu', 'holbrow@media.mit.edu', 'platte@media.mit.edu']",True,empathy-and-the-future-of-experience,2015-01-01,[],Empathy and the future of experience,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:20.338Z,"Empowering Minds proposes a constructionist, networked model for teacher learning. The focus of the work will be to explore a new constructionist model for professional development of teachers. The design of effective and meaningful learning environments will be informed by a technologically rich support structure that facilitates reflection, technological fluency, and collaboration. (Funded by the Higher Education Authority of Ireland.)",2005-01-01,['future-of-learning'],,8391,E15-441,2016-12-05T00:16:23.446Z,"['papert@media.mit.edu', 'gid@media.mit.edu']",True,empowering-minds,2002-01-01,[],Empowering Minds,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:20.485Z,"In the EME project we worked with a quadraplegic individual with severe cerebral palsy to develop the hardware and software necessary for an individual with extremely limited movement to fully interpret�as a soloist in a live performance�his own music composed in Hyperscore. Our technological development has incorporated an adaptive input device and accompanying software to allow the user�s symptomatic movement to provide sufficient accuracy, precision, and expressivity for the demands of a performance. By working with the subject as a collaborator, we have designed an interface that not only meets his very specific physical needs, but also unveils novel insights into the design of expressive interfaces that can be customized and personalized for any user.",2007-09-01,['opera-of-the-future'],,8407,E15-445,2016-12-05T00:16:08.109Z,['tod@media.mit.edu'],True,enabling-musical-expression,2007-01-01,[],Enabling Musical Expression,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:20.560Z,"""Encephalodome"" (working title) is an art+science game under development for the dome projection (planetarium) setting of the Lower Eastside Girls Club. Players will wear inexpensive Electroencephalography (EEG) devices to both control and contribute to the game. They can expressively explore science through activities like concentrating, meditating, closing their eyes, and moving their bodies. By fusing many kinds of science data sets into a vast spatial experience, �Encephalodome� will engage players in natural beauty beyond the scales of human perception. ""Encephalodome"" gameplay focuses on ocean acidification: increased pollution is changing the pH of the oceans, thus affecting the growth of sea vertebrates and shellfish. ""Encephalodome"" will invite its users to interactively role-play prototypical sea organisms like coral, plankton, jellyfish, and lobster through decades of increased carbon emissions. ",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/03.png,8415,--Choose Location,2016-12-05T00:16:23.549Z,['holtzman@media.mit.edu'],True,encephalodome,2013-01-01,[],Encephalodome,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:20.638Z,"We are exploring techniques to integrate digital codes into physical objects. Spanning both the hard and the soft, this work entails incorporating texture patterns into the surfaces of objects in a coded manner. Leveraging advancements in rapid prototyping and manufacturing capabilities, techniques for creating deterministic encoded surface textures are explored. The goal of such work is to take steps towards a self-descriptive universe in which all objects contain within their physical structure hooks to information about how they can be used, how they can be fixed, what they're used for, who uses them, etc. Our motivation is to transform opaque technologies into things that teach and expose information about themselves through the sensing technologies we already, or foreseeably could, carry on us.",2013-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/encodedReality_logo.png,8423,--Choose Location,2016-12-05T00:16:23.690Z,"['trich@media.mit.edu', 'lip@media.mit.edu']",True,encoded-reality,2012-09-01,[],Encoded Reality,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:20.795Z,"Everyone in the city is an expert on their own experience of that city. So how might we integrate new forms of citizen input into the planning and transformation of public spaces around Mexico City, using both digital and non-digital strategies? EncuestaCDMX is a civic technology platform developed with the Laboratorio para la Ciudad that combines in-person surveys and responses from a version of the Action Path location-based survey app to inform city planning decisions. The survey responses power a real-time public dashboard of the feedback available to both city planners and residents for accountability.",2015-10-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/19/ic_launcher.png,8439,--Choose Location,2016-12-05T00:16:34.976Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'erhardt@media.mit.edu', 'emreiser@media.mit.edu']",True,encuestacdmx,2015-09-01,"['civic-media', 'civic-technology']",EncuestaCDMX,PUBLIC,http://erhardtgraeff.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:20.871Z,"Endangered Senses is a wearable that allows people to experience a sense that endangered animals have but is not possessed by humans. The project is an elephant-inspired costume which investigates their ability to detect infrasonic and seismic vibrations. The wearable has long telescoping sleeves which conceal the arms and hands and connect to the floor. Thus the human is asked to sacrifice defining human characteristics (bipedal, with opposable thumbs) in order to experience a supplemental sense. The audio signals are broadcast to create a shared sensory experience.",2006-01-01,['computing-culture'],,8447,E15-020C,2016-12-05T00:16:23.707Z,['csik@media.mit.edu'],True,endangered-senses,2005-01-01,[],Endangered Senses,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:20.958Z,"We are developing methods of controlling the genetic and cellular composition of microbial communities in the gut. Stably colonized microbes could be engineered to sense disease, resist pathogen invasion, and release appropriate therapeutics in situ.",,['sculpting-evolution'],https://dam-prod2.media.mit.edu/x/2017/03/23/DSC_0011 copy_dJoCrfD.jpg,8455,--Choose Location,2019-10-17T19:17:14.074Z,"['esvelt@media.mit.edu', 'erikad@media.mit.edu', 'ave@media.mit.edu']",False,engineering-microbial-ecosystems,2016-01-01,[],Engineering Microbial Ecosystems,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:21.040Z,"As the population ages, acuity in one or more sensory channels often diminishes or may be totally lost. Augmenting or compensating for loss in the perceptual system by taking advantage of sensory data outside the normal human range and mapping it to meaningful perceptual information has the potential of giving an ordinary person enhanced sensory perception (ESP).
Sensory deficiency is not restricted to any particular segment of the population, however.  For example, we tend to be myopic about ourselves, and thus can benefit from psychological mirrors in the form of trainers or therapists who can assess and guide our physical and/or mental development. In this spirit, ""Reflective Biometrics"" is a novel approach to analyzing and interpreting biometric sensory information for self monitoring and examination.  It is self-examination via technology as a mirror. Biometric technologies in service of the individual can serve as reflectors that enhance our self-awareness, self-understanding, and health, and they can facilitate our interaction with computers and with each other by augmenting our perceptual system.",2007-01-01,['affective-computing'],,8463,E15-443D,2016-12-05T00:16:23.620Z,['picard@media.mit.edu'],True,enhanced-sensory-perception,2006-09-01,[],Enhanced Sensory Perception,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:21.116Z,"A peer-to-peer network, enabling different parties to jointly store and run computations on data while keeping the data completely private. Enigma's computational model is based on a highly optimized version of secure multi-party computation, guaranteed by a verifiable secret-sharing scheme. For storage, we use a modified distributed hashtable for holding secret-shared data. An external blockchain is utilized as the controller of the network, manages access control and identities, and serves as a tamper-proof log of events. Security deposits and fees incentivize operation, correctness, and fairness of the system. Similar to Bitcoin, Enigma removes the need for a trusted third party, enabling autonomous control of personal data. For the first time, users are able to share their data with cryptographic guarantees regarding their privacy.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/files/Display/logo_dark,8471,--Choose Location,2018-02-12T20:37:17.158Z,"['sandy@media.mit.edu', 'guyzys@media.mit.edu']",True,enigma,2015-01-01,[],Enigma,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:21.193Z,"In physics education, virtual simulations have given us the ability to show and explain phenomena that are otherwise invisible to the naked eye. However, experiments with analog devices still play an important role. They allow us to verify theories and discover ideas through experiments that are not constrained by software. What if we could combine the best of both worlds? We achieve that by building our applications on a projected augmented reality system. By projecting onto physical objects, we can paint the phenomena that are invisible. With our system, we have built ""physical playgrounds"": simulations that are projected onto the physical world and that respond to detected objects in the space. Thus, we can draw virtual field lines on real magnets, track and provide history on the location of a pendulum, or even build circuits with both physical and virtual components.",,['fluid-interfaces'],,8479,--Choose Location,2016-12-05T00:16:23.642Z,"['pattie@media.mit.edu', 'kubat@media.mit.edu', 'linder@media.mit.edu']",True,enlight,2014-01-01,[],Enlight,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:09:21.438Z,"eRadio: Empowerment Through Community Electronic Radio is a set of logistics and electronic tools designed to help underserved communities that suffer from multiple communication gaps (lack of content, audience, means, and motivation, stemming from underestimation of their resources and capabilities). The logistics intend to develop a community-specific iterative process of self-discovery and empowerment, bringing together eTools, applications, procedures, and people, and making use of inquiry, discovery, and ethno-methodological strategies and techniques to get the community to generate content ""a voice"" and to broadcast and Webcast it so as to multiply participation, generate more feedback, and further the iterative process. Inter-community and diaspora-community involvement is also foreseen. The tool, named ""eAudioKit,"" is a low-cost, user-friendly data and audio processor, fitted with specific software and open-source applications (eTools), organized, integrated, and customized for recording, editing, encoding, archiving, and uploading small audio pieces and audio projects or radio programs.",2004-01-01,['electronic-publishing'],,8503,E15-320B,2019-02-06T21:47:11.178Z,['walter@media.mit.edu'],True,eradio-empowerment-through-community-electronic-radio,2003-09-01,[],eRadio: Empowerment Through Community Electronic Radio,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:21.516Z,"Erase the Border is a web campaign and voice petition platform. It tells the story of the Tohono O'odham people, whose community has been divided along 75 miles of the US-Mexico border by a fence. The border fence divides the community, prevents tribe members from receiving critical health services, and subjects O'odham to racism and discrimination. This platform is a pilot that we are using to research the potential of voice and media petitions for civic discourse.",2014-01-01,['civic-media'],,8511,,2016-12-05T00:16:23.811Z,"['ethanz@media.mit.edu', 'dignazio@media.mit.edu']",True,erase-the-border,2013-01-01,[],Erase the Border,PUBLIC,,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:21.591Z,"Using machine learning, computer vision, and wrist-worn, smaller, time-of-flight cameras, we can recover hand pose and micro-gesture (small movements of the fingers and thumb). It is clear that ubiquitous wearables will need a similar eyes-free user interface�but how should this interface be designed? We are examining interaction through user tests�what gesture set designs work well for text entry or focus selection? How can we predict user experience and the usability of such systems? We hope to answer such questions through the EMGRIE system and experimental application design.",2014-01-01,['responsive-environments'],,8519,--Choose Location,2016-12-05T00:17:11.526Z,['joep@media.mit.edu'],True,ergonomic-micro-gesture-recognition-and-interaction-evaluation,2012-01-01,[],Ergonomic Micro-Gesture Recognition and Interaction Evaluation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:21.791Z,"The ESP team is designing and building embedded sensor packs for expeditionary projects. Earlier efforts pioneered human vital-sign monitoring (with marathon runners, cyclists, Army rangers, and members of Everest expeditions). This is a new DARPA-funded effort aimed at developing a breakthrough in new sensor system architecturessensor packs that are ultrasmall, robust, and capable of recording and transmitting a wide variety of bio-, geo-, and environmental data. We are currently planning an ambitious series of expeditions to begin this spring.",2001-12-30,['personal-information-architecture'],,8543,E15-468,2016-12-05T00:16:23.833Z,['mike@media.mit.edu'],True,esp-embedded-sensor-packs,1998-12-31,[],ESP: Embedded Sensor Packs,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:21.872Z,"The sense of smell is perhaps the most pervasive of all senses, but it is also one of the least understood and least exploited in HCI. We present Essence, the first olfactory computational necklace that can be remotely controlled through a smartphone and can vary the intensity and frequency of the released scent based on biometric or contextual data.",,"['advancing-wellbeing', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2017/04/27/essence-web.jpg,8551,--Choose Location,2022-06-10T14:56:50.331Z,"['pattie@media.mit.edu', 'amores@media.mit.edu']",True,essence,2016-09-01,"['design', 'fashion', 'perception', 'sensors', 'interfaces', 'wellbeing']",Essence,PUBLIC,http://www.judithamores.com/essence,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:22.105Z,"This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.Adoption of self-driving, Autonomous Vehicles (AVs) promises to dramatically reduce the number of traffic accidents, but some inevitable accidents will require AVs to choose the lesser of two evils, such as running over a pedestrian on the road or the sidewalk. Defining the algorithms to guide AVs confronted with such moral dilemmas is a challenge, and manufacturers and regulators will need psychologists to apply methods of experimental ethics to these situations.",2019-12-01,"['ethics', 'scalable-cooperation']",https://dam-prod2.media.mit.edu/x/2016/10/06/scalable-cooperation.jpg,8575,--Choose Location,2020-02-28T17:46:17.234Z,"['irahwan@media.mit.edu', 'awad@media.mit.edu', 'dsouza@media.mit.edu']",True,ethics-of-autonomous-vehicles,2015-01-01,"['artificial-intelligence', 'cryptocurrency', 'social-science', 'blockchain', 'ethics', 'autonomous-vehicles']",Ethics of Autonomous Vehicles,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:22.196Z,"Success in a networked society will require not just new skills and new knowledge, but new ways of thinking. Rather than seeing the world as a clockwork mechanism, people will need to think in more ecological terms, recognizing the importance of adaptation and improvisation, and understanding how patterns can arise from many simple, local interactions. We are developing new technologies and activities to help people develop as ""eThinkers.""",2005-09-01,['lifelong-kindergarten'],,8583,E15-001,2016-12-05T00:16:23.886Z,['mres@media.mit.edu'],True,ethinking,1998-01-01,[],eThinking,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:22.287Z,"Ethos is a decentralized, Bitcoin-like network for storing and sharing valuable information. We provide transparency, control, and ownership over personal data and its distribution. Validation and maintenance is distributed throughout the data community and automatically maintained without needing a safe deposit box or a commercial site. What Bitcoin has done for currency and BitTorrent for media, Ethos does for personal data. Nodes in the network are incentivized by collecting transaction fees, coinbase transactions (""finding blocks""), and proof-of-storage fees to sustain the distribution of personal data. Fees are paid with the underlying cryptocurrency represented by the network, also known as ""PrivacyCoin."" The role of nodes, besides the usual proof-of-work, which protects against ""double spending,"" is to maintain shredded pieces of information and present them to the network on-demand. ",2015-09-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/logo%202-2.jpg,8591,--Choose Location,2017-06-25T16:32:52.708Z,"['guyzys@media.mit.edu', 'lip@media.mit.edu', 'amirl@media.mit.edu']",True,ethos,2014-01-01,"['cryptocurrency', 'data', 'privacy', 'blockchain']",Ethos,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:22.372Z,"To help people improve their reading of faces during natural conversations, we developed a video tool to evaluate this skill. We collected over 100 videos of conversations between pairs of both autistic and neurotypical people, each wearing a Self-Cam. The videos were manually segmented into chunks of 7-20 seconds according to expressive content, labeled, and sorted by difficulty—all tasks we plan to automate using technologies under development. Next, we built a rating interface including videos of self, peers, familiar adults, strangers, and unknown actors, allowing for performance comparisons across conditions of familiarity and expression. We obtained reliable identification (by coders) of categories of smiling, happy, interested, thinking, and unsure in the segmented videos. The tool was finally used to assess recognition of these five categories for eight neurotypical and five autistic people. Results show some autistics approaching the abilities of neurotypicals while several score just above random.",2011-09-01,['affective-computing'],,8599,E15-443,2017-06-02T12:52:36.575Z,['picard@media.mit.edu'],True,evaluation-tool-for-recognition-of-social-emotional-expressions-from-facial-head-movements,2006-09-01,[],Evaluation Tool for Recognition of Social-Emotional Expressions from Facial-Head Movements,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:22.537Z,"EventNet is the common-sense approach to goal planning. By extracting cause-effect and action-goal links from Open Mind Common Sense, we build an associative network of tens of thousands of nodes about human actions and goals. For example, EventNet is able to infer that in order to go from home to the movies you should drive your car. It is a toolkit for the easy use of interactive applications.",2007-01-01,['software-agents'],,8615,E15-383,2016-12-05T00:16:23.958Z,['lieber@media.mit.edu'],True,eventnet,2004-09-01,[],EventNet,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:22.611Z,"Every Sign of Life challenges assumptions about how we might think and feel about personal health monitoring. It is an exploration of how to make information collected by personal health-monitoring devices fun and engaging, and consequently more useful to the non-specialist. The approach is to design and build computer games based on such information. The goal is self-efficacy; to implicitly make people take care of their own health by altering their habits and by health-aware planning of their lives. This work tests the hypothesis that fun (the fun of learning, achieving, competing) is a way to achieve this goal. One research focus explores the basic architecture for personal health monitoring systems, which has led to a new approach to design of sensor peripherals and wearable computer components called ""Extremity Computing."" This approach is used to redefine biosensor monitoring from periodic to continuous (ultimately saving data over a lifetime). Another research focus explores adding implicit biofeedback to computer games, which has led to a new genre of games that straddles the boundary between sports and computer games called ""bio-analytical"" games.",2003-09-01,"['gray-matters', 'electronic-publishing']",,8623,E15-320,2019-02-06T21:47:29.955Z,['walter@media.mit.edu'],True,every-sign-of-life,2000-01-01,[],Every Sign of Life,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:22.692Z,"Following upon work begun in the Graspables project, we are exploring what happens when a wide range of everyday consumer products can sense, interpret into human terms (using pattern recognition methods), and retain memories, such that users can construct a narrative with the aid of the recollections of the ""diaries"" of their sporting equipment, luggage, furniture, toys, and other items.",2019-10-01,['object-based-media'],,8631,Garden Conference Room,2020-06-10T16:02:00.303Z,['vmb@media.mit.edu'],True,everything-tells-a-story,2009-09-01,[],Everything Tells a Story,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:22.845Z,"Drowsy drivers are a danger to themselves and others on the road. Additionally, for many commuters it is hard to get ample time to exercise during the week as commuting time eats away at personal time. The ExcerCar project is an effort to enhance both alertness and physical activity. We are developing an exercise interface to the vehicle that counteracts the effects of fatigue and inactivity while driving. The hypothesis is that exercise will mitigate the effects of fatigue and increase a drowsy driver's alertness. Put another way, can Fred Flintstone's car make him a more alert driver?",2007-01-01,['context-aware-computing'],https://dam-prod2.media.mit.edu/x/files/%7Eaggelos/time.html,8647,E15-320,2016-12-05T00:17:11.625Z,['win@media.mit.edu'],True,excercar-a-solution-to-drowsy-driving,1999-09-01,[],ExcerCar: A Solution to Drowsy Driving?,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:22.918Z,"Are electronic instruments that generate their own power better than those that don't?  Can acoustic and electronic musical instruments be successfully merged?  Can the movement of the sound generation be tightly coupled to the power generation, as opposed to merely modulating a large power reserve, as in traditional instruments?  What useful musical artifacts/affordances can be created through this technology? ",2010-01-01,['computing-culture'],,8655,Lower Atrium,2022-06-10T13:40:30.287Z,['csik@media.mit.edu'],True,exertion-music,2007-01-01,[],Exertion Music,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:23.036Z,"Using exhaled water vapor as the display medium, we explore projecting into ""thin air,"" eliminating the need to carry a bulky display around.",2005-09-01,[],,8663,--Choose Location,2016-12-05T00:16:24.012Z,['mlj@media.mit.edu'],True,exhale-a-display,2005-09-01,[],Exhale-a-Display,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:23.346Z,"The vision of pervasive computing is now mainstream. These connected devices permeate every aspect of our lives. Yet, we remain tethered to arcane user interfaces. Unlike consumer devices, building appliances and utilities perpetuate this outdated vision. Lighting control is a prime example. Here, we show how a data-driven methodology—using people and sensors—enables an entirely new method of lighting control.We are evaluating new methods of interacting and controlling solid-state lighting based on our findings of how participants experience and perceive architectural lighting in our new lighting laboratory (E14-548S). This work, aptly named ""Experiential Lighting,"" reduces the complexity of modern lighting controls (intensity/color/space) into a simple mapping, aided by both human input and sensor measurement. We believe our approach extends beyond general lighting control and is applicable in situations where human-based rankings and preference are critical requirements for control and actuation. We expect our foundational studies to guide future camera-based systems that will inevitably incorporate context in their operation (e.g., Google Glass).",,['responsive-environments'],,8679,--Choose Location,2019-04-19T14:25:05.542Z,"['maldrich@media.mit.edu', 'nanzhao@media.mit.edu', 'joep@media.mit.edu']",True,experiential-lighting-new-user-interfaces-for-lighting-control,2014-01-01,"['energy', 'human-machine-interaction', 'sensors', 'interfaces']",Experiential Lighting: New user interfaces for lighting control,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:23.658Z,"Conversion rates (ratio of visitors to buyers) on the Internet are amazingly low. In our current work, we suggest that one of the most noted advantages of electronic commerce�low search costs�also turns out to be a major reason for low conversion rates. We claim that the low search costs associated with finding products and information about them can cause indecision and procrastination. Moreover, the possibility of finding additional information after a decision has been made has a high potential for regret, which also increases indecision to buy. We are interested in exploring how to overcome these indecision problems. We suggest that imposing deadlines can act as a psychological device that will promote more decisive actions. The mechanism upon which we focus is one where discounts, but not product offers, have a limited lifetime (in the form of expiring discounts).",2007-01-01,['erationality'],,8695,E15-314,2016-12-05T00:16:24.055Z,[],True,expiring-coupons-for-e-markets,2001-01-01,[],Expiring Coupons for e-Markets,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:23.795Z,"In this project we present our exploration of several visualization techniques which use motion as the primary visual element for depicting online discussions. Visual attention appeals most strongly to motion. Since motion changes the immediate environment conditions, it is more likely than static visual cues to attract our attention. Motion is dynamic and vibrant; it is intuitive and suitable for representing large-scale online social data such as online discussions.  Our designs and findings shed light on choosing the mappings between motion and social characteristics for creating intuitive and legible visualizations.",2017-05-01,['sociable-media'],,8703,E15-390,2022-06-10T14:41:29.884Z,['judith@media.mit.edu'],True,exploration-of-motion-to-visualize-large-scale-online-discussions,2005-01-01,[],Exploration of Motion to Visualize Large-Scale Online Discussions,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:09:23.935Z,"We are exploring the methods by which traditional artisans construct new electronic technologies using contextually novel materials and processes, incorporating wood, textiles, reclaimed and recycled products, as well as conventional circuitry. Such artisanal technologies often address different needs, and are radically different in form and function than conventionally designed and produced products.",2013-01-01,['high-low-tech'],,8711,--Choose Location,2016-12-05T00:16:24.075Z,"['leah@media.mit.edu', 'mellis@media.mit.edu']",True,exploring-artisanal-technology,2012-01-01,[],Exploring Artisanal Technology,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:24.067Z,"A smile is a multi-purpose expression. We smile to express rapport, polite disagreement, delight, sarcasm, and often, even frustration. Is it possible to develop computational models to distinguish among smiling instances when delighted, frustrated, or just being polite? In our ongoing work, we demonstrate that it is useful to explore how the patterns of smile evolve through time, and that while a smile may occur in positive and in negative situations, its dynamics may help to disambiguate the underlying state. ",2014-09-01,['affective-computing'],,8719,--Choose Location,2016-12-05T00:16:24.096Z,"['picard@media.mit.edu', 'mehoque@media.mit.edu']",True,exploring-temporal-patterns-of-smile,2010-01-01,[],Exploring Temporal Patterns of Smile,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:24.200Z,"Creating smart 3-D models in the CATIA environment allows designers and engineers to explore and test the feasible envelopes in component-assembly systems. Parametric models are created based on design rules at both the component and master-assembly level. Once the subcomponents are created, assemblies test associative properties such as connection points, packaging, manufacturability, structural integrity, and substitutive affinity. By mapping all the possible solutions in an unbiased and systematic open-tree structure, conventions can be questioned and design opportunities investigated.",2006-01-01,['smart-cities'],,8727,E15-001,2016-12-05T00:17:01.492Z,['rchin@media.mit.edu'],True,exploring-the-solution-space-parametric-design,2003-09-01,[],Exploring the Solution Space: Parametric Design,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:09:24.344Z,"We are developing a new and exciting tool for expression in paint, combining technology and art to bring together the physical and the virtual through the use of robotics, artificial intelligence, signal processing, and wearable technology. Our technology promotes expression in paint not only by making it a lot more accessible, but also by making it flexible, adaptive, and fun, for everyone across the entire spectrum of abilities. With the development of the technology, new forms of art also emerge, such as hyper, hybrid, and collaborative painting. All of these can be extended to remote operation (or co-operation) thanks to the modular system design. For example, a parent and a child can be painting together even when far apart; a disabled person can experience an embodied painting experience; and medical professionals can reach larger populations with physical therapy, occupational therapy, and art therapy, including motor/neuromuscular impaired persons.",,"['advancing-wellbeing', 'fluid-interfaces']",,8735,--Choose Location,2018-05-07T19:44:41.673Z,"['pattie@media.mit.edu', 'achituv@media.mit.edu']",True,express,2015-01-01,[],Express,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:24.486Z,"Expressive musical re-performance is about enabling a person to experience the creative aspects of a playing a favorite song regardless of technical expertise. This is done by providing users with computer-linked electronic instruments that distills the instruments' interface but still allows them to provide expressive gesture. The next note in an audio source is triggered on the instrument, with the computer providing correctly pitched audio and mapping the expressive content onto it. Thus, the physicality of the instrument remains, but requires far less technique. We are implementing an expressive re-performance system using commercially available, expressive electronic musical instruments and an actual recording as the basis for deriving audio. Performers will be able to select a voice within the recording and re-perform the song with the targeted line subject to their own creative and expressive impulse.",2013-09-01,['responsive-environments'],,8743,,2016-12-05T00:16:24.206Z,['joep@media.mit.edu'],True,expressive-re-performance,2011-09-01,[],Expressive Re-Performance,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:24.598Z,"Exquisite Corpus is a collaborative writing application that aims to explore new and more ""indirect"" forms of collaboration similar to those in open-source software or in Wikipedia. EC allows collaborators to contribute short vignettes in traditional ""scene"" (i.e., screenplay or script) form and then, using heuristics and AI techniques, helps to integrate these into larger aggregate works suitable for use as a movie or play script. These aggregate works aim to provide consistent characters and themes through the work. The system provides a platform for investigating and improving computer support of indirect collaboration.",2008-01-01,['computing-culture'],,8751,E15-020C,2016-12-05T00:16:24.232Z,['walter@media.mit.edu'],True,exquisite-corpus,2005-09-01,[],Exquisite Corpus,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:24.819Z,"We propose a set of customizable, easy-to-understand, and low-cost physiological toolkits in order to enable people to visualize and utilize autonomic arousal information. In particular, we aim for the toolkits to be usable in one of the most challenging usability conditions: helping individuals diagnosed with autism. This toolkit includes: wearable, wireless, heart-rate and skin-conductance sensors; pendant-like and hand-held physiological indicators hidden or embedded into certain toys or tools; and a customized software interface that allows caregivers and parents to establish a general understanding of an individual's arousal profile from daily life and to set up physiological alarms for events of interest. We are evaluating the ability of this externalization toolkit to help individuals on the autism spectrum to better communicate their internal states to trusted teachers and family members.",2012-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/miniheart_icon.gif,8767,E15-443D,2016-12-05T00:16:24.144Z,"['picard@media.mit.edu', 'mgoodwin@media.mit.edu']",True,externalization-toolkit,2009-01-01,[],Externalization Toolkit,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:24.929Z,"ExtrAct, a set of Internet-based databasing, mapping, and communications technologies for communities impacted by natural gas development, is a novel platform for community education and civic action. Its objective is to create and distribute open-source, Web-based tools for mapping, analyzing, and intervening in this industry based on supplementing data obtained from state and federal agencies with user-generated reports, complaints, and experiences. All of these tools, though accessible individually, will share information through a unified database. Because these tools will serve both urban and rural populations, we are also developing innovative paper and phone interfaces to the Web services. To develop these tools we are working with a network of lawyers, citizen�s alliances, national activist organizations, and environmental health experts in Colorado, New Mexico, Ohio, New York, Pennsylvania, West Virginia, and Texas.",2011-01-01,['civic-media'],,8775,E15-001,2016-12-05T00:16:24.163Z,['ethanz@media.mit.edu'],True,extract,2008-01-01,[],ExtrACT,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:25.031Z,"A ""society"" of intelligent mobile cameras collaboratively solves calibration and scene-modeling problems through exchange of data. The small prototypes are built starting with StrongARM-based PDAs running Linux. Each has a camera with motorized pan/tilt head, IEEE 802.11 wireless networking, and a motor for traveling along an overhead monorail-style track.",2006-09-01,['object-based-media'],,8783,E15-368,2016-12-05T00:16:24.185Z,['vmb@media.mit.edu'],True,eye-society,2001-01-01,[],Eye Society,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:25.121Z,"Eye-aRe is a system designed to detect and communicate the intentional information conveyed in eye movement. This glasses-mounted, wireless device stores and transfers information based on users' eye motion and external IR devices, thus promoting an enriched experience with their environment. This project describes how the system measures eye motion and utilizes this as an implicit input channel to a sensor system and computer. In the primary scenario, eye motion detection is used to recognize a user's gaze. When the user�s eyes are fixed, the system infers that he is paying attention to something in his environment and then tries to facilitate an exchange of information in either direction on his behalf.",2002-01-01,['context-aware-computing'],,8791,E15-320,2016-12-05T00:16:24.272Z,[],True,eye-are,2000-09-01,[],Eye-aRe,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:25.522Z,"Millions of people worldwide need glasses or contact lenses to see or read properly. We introduce a computational display technology that predistorts the presented content for an observer, so that the target image is perceived without the need for eyewear. We demonstrate a low-cost prototype that can correct myopia, hyperopia, astigmatism, and even higher-order aberrations that are difficult to correct with glasses.",2016-09-01,['camera-culture'],,8823,--Choose Location,2016-12-05T00:16:24.496Z,"['raskar@media.mit.edu', 'gordonw@media.mit.edu', 'naik@media.mit.edu']",True,eyeglasses-free-displays,2014-01-01,[],Eyeglasses-Free Displays,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:25.623Z,"While modern communication technologies mean that we can connect to more people, these connections lack the affective subtleties inherent in situated interactions. EyeJacking is an application for the sharing of experiences in which one or more persons �eyejack� a person�s visual field to share what he or she sees. Using a wearable camera/micorphone system, remote interaction partners can share an experience first-hand and play an active role in shaping the experience. We explore the application of EyeJacking as a tool for situated learning for individuals on the autism spectrum, where parents, caregivers, or peers could �eyejack� and tag the world remotely. We also explore the application of EyeJacking to leverage the power of the masses to bootstrap people-sense abilities in robots. ",2008-09-01,['affective-computing'],,8831,E15-450,2016-12-05T00:16:24.376Z,['picard@media.mit.edu'],True,eyejacking-see-what-i-see,2007-01-01,[],EyeJacking: See What I See,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:25.727Z,"The goal of the project is to build a computational tool that will allow students to explore the realm of image processing, signal processing, matrices and filters.
Very complex mathematical and physical concepts are embedded in these fields of knowledge. We normally believe that only specialists have the technical background to think about them. However, the use of computation offers an intuitive way of exploring these ideas these ideas, through digital image, sound and signal processing.
Why are these fields of knowledge important? We don�t believe in creating tool to teach one specific subject � the idea is not to better explain a derivative or a Fourier transform.
Turtle geometry was not a tool to teach angles and polygons, but rather offered an invaluable and innovative paradigm of mathematical exploration and construction. We believe that extending the Logo aesthetics to the field of matrices, filters, variations and non-conventional spaces of mathematical representation could be a powerful construction tool.
Image processing software is usually opaque: the end user has no idea of the algorithms or the operations that it is performing. EyeLogo would be both a tool to explore and demystify the realm of image and a powerful construction tool that could enhance current Logo-based environments.",2003-01-01,[],,8839,E15-489,2016-12-05T00:16:24.414Z,['cavallo@media.mit.edu'],True,eyelogo,2001-01-01,[],EyeLogo,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:09:25.928Z,"The Seung Lab at MIT's Brain + Cognitive Sciences Department has developed EyeWire, a game to map the brain. To date, it has attracted an online community of over 50,000 ""citizen neuroscientists"" who are mapping the 3D structure of neurons and discovering neural connections. Playful Systems is collaborating with the Seung Lab to reconsider EyeWire as a large scale mass-appeal mobile game to attract 1MM players or more. We are currently developing mobile, collaborative game mechanics, and shifting the focus to short-burst gameplay.",2015-01-01,['playful-systems'],,8855,--Choose Location,2016-12-05T00:16:24.523Z,"['slavin@media.mit.edu', 'gregab@media.mit.edu', 'tjlevy@media.mit.edu', 'cwwang@media.mit.edu']",True,eyewire,2013-09-01,[],EyeWire,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:09:26.098Z,"Fab FM explores the possibilities for personal fabrication of consumer electronic devices.  It is a wood- and fabric-cased FM radio that can be manufactured in small volumes by an individual with access to a laser cutter.  Each radio can be customized with materials (e.g., wood or fabric) provided by the customer.  Because the radio can be produced from its digital design files using minimal infrastructure, it offers a diverse set of possible business models and distribution schemes.  For example, radios could be sold as kits to be assembled by the customer, or produced by individuals in many different cities.  ",2010-09-01,['high-low-tech'],,8871,,2016-12-05T00:16:24.557Z,"['leah@media.mit.edu', 'mellis@media.mit.edu']",True,fab-fm,2009-09-01,[],Fab FM,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:26.184Z,"""Fab Lab"" is an abbreviation for Fabrication Laboratory, a group of off-the-shelf, industrial-grade fabrication and electronics tools, wrapped in open-source software and programs. Fab Labs give users around the world the ability to locally conceptualize, design, develop, fabricate, and test almost anything. The engineering capability for design and fabrication at micron length and microsecond time-scales opens up numerous possibilities for innovative solutions to common problems. Since local communities foster this innovation, it can lead to sustainable solutions. As yet, high-end technological solutions have not been addressing problems faced on the local level; therefore, we believe Fab Labs will provide a thriving incubator for local micro-businesses.",2007-01-01,['physics-and-media'],,8879,E15-023,2023-09-07T16:22:42.185Z,"['neilg@media.mit.edu', 'millner@media.mit.edu']",True,fab-labs,2005-01-01,"['learning-teaching', '3d-printing', 'mechanical-engineering', 'fabrication']",Fab Labs,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:26.350Z,"Fablur explores the limit of the self in its relationship to others through the medium of clothing. The augmented gown uses a rear dome projection system on the surface of the fabric. The system comprises laser projectors and mirror structures talking wirelessly with a computer, within which is contained both content and warp projection mapping software. This novel technological interface presents both a performative element and a seamless integration in a woman's life experience. This wearable project questions the boundary between the self and others, the boundary between the individual and society, and the boundary between the body and nature.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/10/18/DressF_s3o6CgG.png,8895,--Choose Location,2016-12-05T00:16:26.876Z,"['tod@media.mit.edu', 'rebklein@media.mit.edu']",True,fablur,2016-01-01,['fashion'],Fablur,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:09:26.437Z,"To change radically the physical shape and feeling of technology, designers must also be able to change the materials from which it is made. Fabric Interfaces are just one example of what can happen when design materials, like fabric and thread, become electronic materials. The musical ball, a continuous-control, plush musical instrument, uses embroidered conductive thread as pressure sensors. These embroidered pressure sensors allow the ball to be soft and plush, rather than covered with hard buttons or keys. They also make manufacturing cheap and easy, and allow designers to shape their sensors anyway they want. The whimsical electronic table cloths and coaster ID tags are part of a digital game of cocktail Jeopardy. They use appliqued conductive fabric and embroidered threads to create, in the table cloths, a tag reader, and decorative key pad.",2000-12-30,"['counter-intelligence', 'toys-of-tomorrow', 'opera-of-the-future']",,8903,E15-493,2016-12-05T00:17:11.788Z,"['tod@media.mit.edu', 'rehmi@media.mit.edu']",True,fabric-interfaces,1999-12-31,[],Fabric Interfaces,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:26.530Z,"Rapid prototyping technologies speed product design by facilitating visualization and testing of prototypes. However, such machines are limited to using one material at a time; even high-end 3D printers, which accommodate the deposition of multiple materials, must do so discretely and not in mixtures. This project aims to build a proof-of-concept of a 3D printer able to dynamically mix and vary the ratios of different materials in order to produce a continuous gradient of material properties with real-time correspondence to structural and environmental constraints.",2016-05-31,['mediated-matter'],,8911,E15-001,2021-05-21T13:19:00.767Z,['neri@media.mit.edu'],True,fabricology-variable-property-3d-printing-as-a-case-for-sustainable-fabrication,2009-01-01,[],FABRICOLOGY: Variable-property 3D printing as a case for sustainable fabrication,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:26.607Z,"The Face Interface system allows users to select facial gestures to control the Windows interface without a keyboard or mouse. The system recognizes head shaking, nodding, eyes and mouth open/closed, and head position through an avatar, giving users the ability to easily view possibilities and change their approaches. Attention Meter demonstrated the ability to rapidly program interfaces for using these features in a variety of environments, including C++, Flash, and Max-MSP. This previous work taught us the usefulness of the face as an input in human-computer interaction, and using this knowledge, Face Interface can inform and critique the user's design choices. This is an exemplar for a new input approach that allows users to define their interactions without programming and to experience and learn the design parameters of a new and unfamiliar system.",2007-01-01,['context-aware-computing'],,8919,E15-320,2016-12-05T00:16:24.637Z,[],True,face-interface,2007-01-01,[],Face Interface,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:26.918Z,"People express and communicate their mental states–such as emotions, thoughts, and desires–through facial expressions, vocal nuances, gestures, and other non-verbal channels. We have developed a computational model that enables real-time analysis, tagging, and inference of cognitive-affective mental states from facial video. This framework combines bottom-up, vision-based processing of the face (e.g., a head nod or smile) with top-down predictions of mental-state models (e.g., interest and confusion) to interpret the meaning underlying head and facial signals over time. Our system tags facial expressions, head gestures, and affective-cognitive states at multiple spatial and temporal granularities in real time and offline, in both natural human-human and human-computer interaction contexts. A version of this system is being made available commercially by Media Lab spin-off Affectiva, indexing emotion from faces. Applications range from measuring people's experiences to a training tool for autism spectrum disorders and people who are nonverbal learning disabled.",2012-09-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/FaceSenseLogo.png,8951,E15-443,2018-04-04T19:58:17.407Z,"['picard@media.mit.edu', 'mehoque@media.mit.edu', 'mgoodwin@media.mit.edu', 'micahrye@media.mit.edu']",True,facesense-affective-cognitive-state-inference-from-facial-video,2006-01-01,"['cognition', 'communications', 'gesture-interface']",FaceSense: Affective-Cognitive State Inference from Facial Video,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:27.014Z,"This work builds on our earlier work with FaceSense, created to help automate the understanding of facial expressions, both cognitive and affective. The FaceSense system has now been made available commercially by Media Lab spinoff Affectiva as Affdex. In this work we present the first project analyzing facial expressions at scale over the Internet. The interface analyzes the participants' smile intensity as they watch popular commercials. They can compare their responses to an aggregate from the larger population. The system also allows us to crowd-source data for training expression recognition systems and to gain better understanding of facial expressions under natural at-home viewing conditions instead of in traditional lab settings.",2014-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/Forbes_Pic.png,8959,--Choose Location,2016-12-05T00:17:11.889Z,['picard@media.mit.edu'],True,facial-expression-analysis-over-the-web,2011-01-01,[],Facial Expression Analysis Over the Web,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:27.100Z,"Facilitorials are tutorials that try to act more like a facilitator. By mimicking a real facilitator, facilitorials end up prioritizing a healthy learning environment over maximal transfer of information. Facilitorials wait to be pulled rather than pushing themselves, and when they're pulled they try to communicate by ""doing"" more often than by ""saying."" We are focusing especially on video facilitorials for the Scratch  programming language.",2008-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/scratch-dance2.png,8967,Cube,2016-12-05T00:17:11.942Z,"['mres@media.mit.edu', 'silver@media.mit.edu']",True,facilitorials,2005-01-01,[],Facilitorials,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:27.189Z,"""Progress and catastrophe are the opposite faces of the same coin.� (Hannah Arendt, 1965) This research focuses on our perception of technology as both the source of our vulnerabilities and the answer to all of our problems. Large-scale, real-time failure has become a prominent feature of the late 20th century, and fear of it has inaugurated the 21st century. Technology can only be as perfect, precise, and efficient as the weakest people who design, maintain, and run a given system. New, improved techniques are often discovered by accident, or upon the failure of an intended process. This project will conduct an autopsy on an historic technological disaster case, examining the build-up to the accident. The object of the experimental artwork is to examine the inevitability of accidents and highlight to the viewer that risk is intrinsic to our world and that technological disaster has become integral in our lives in the 21st century.",2004-01-01,['computing-culture'],,8975,E15-020C,2016-12-05T00:17:11.968Z,['csik@media.mit.edu'],True,failsafe,2003-09-01,[],Failsafe,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:27.270Z,"FakeID is an OpenID server that gives you control over your online identity. With your existing OpenID account, you can use FakeID to create unique online identities.",2007-01-01,['information-ecology'],,8983,--Choose Location,2016-12-05T00:17:11.999Z,[],True,fakeid,2007-01-01,[],FakeID,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:27.439Z,"In Family Creative Learning, we engage parents and children in workshops to design and learn together with creative technologies, like the Scratch programming language and the MaKey MaKey invention kit. Just as children's literacy can be supported by parents reading with them, children's creativity can be supported by parents creating with them. In these workshops, we especially target families with limited access to resources and social support around technology. By promoting participation across generations, these workshops engage parents in supporting their children in becoming creators and full participants in today's digital society.",2016-08-31,"['ml-learning', 'lifelong-kindergarten']",https://dam-prod2.media.mit.edu/x/2018/06/20/families-01.jpg,8999,--Choose Location,2018-06-20T19:53:55.066Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'ria@media.mit.edu']",True,family-creative-learning,2012-01-01,"['kids', 'learning-teaching']",Family Creative Learning,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:27.599Z,"We explore advanced machine learning and reflective user interfaces to scale the national Crisis Text Line. We are using state-of-the-art probabilistic graphical topic models and visualizations to help a mental health counselor extract patterns of mental health issues experienced by participants, and bring large-scale data science to understanding the distribution of mental health issues in the United States. ",2017-12-31,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/files/Display/image.jpg,9007,--Choose Location,2022-05-12T17:14:41.829Z,"['picard@media.mit.edu', 'lieber@media.mit.edu', 'kdinakar@media.mit.edu']",True,fathom-probabilistic-graphical-models-to-help-mental-health-counselors,2013-01-01,[],Fathom: Probabilistic graphical models to help mental health counselors,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:27.793Z,"Many researchers have considered probabilistic models of music structure representation. However, these models have yet to provide a truly generative framework, capable of elucidating the cognitive processes of induction, concept formation, or categorization. Feature centrality examines dependency networks of harmonic units within short phrases as a possible foundation for this type of inference. We have found that features of music structures do not adhere to the same feature centrality principles as artifacts or essentialized categories. We are continuing to develop this model through subject testing and computational modeling.",2005-09-01,['opera-of-the-future'],,9015,--Choose Location,2016-12-05T00:16:08.304Z,['tod@media.mit.edu'],True,feature-centrality-in-western-harmonic-structure,2005-01-01,[],Feature Centrality in Western Harmonic Structure,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:09:27.961Z,"At present, luminous efficacy and cost remain the greatest barriers to broad adoption of LED lighting. However, it is anticipated that within several years, these challenges will be overcome. While we may think our basic lighting needs have been met, this technology offers many more opportunities than just energy efficiency: this research attempts to alter our expectations for lighting and cast aside our assumptions about control and performance. We will introduce new, low-cost sensing modalities that are attuned to human factors such as user context, circadian rhythms, or productivity, and integrate these data with atypical environmental factors to move beyond traditional lux measurements. To research and study these themes, we are focusing on the development of superior color-rendering systems, new power topologies for LED control, and low-cost multimodal sensor networks to monitor the lighting network as well as the environment.",2014-01-01,['responsive-environments'],,9023,--Choose Location,2016-12-05T00:17:12.176Z,"['maldrich@media.mit.edu', 'nanzhao@media.mit.edu', 'joep@media.mit.edu']",True,feedback-controlled-solid-state-lighting,2009-01-01,[],Feedback Controlled Solid State Lighting,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:28.098Z,"The wide availability of low-cost, wearable, biophysiological sensors enables us to measure how the environment and our experiences impact our physiology. This creates a new challenge: in order to interpret the collected longitudinal data, we require the matching contextual information as well. Collecting weeks, months, and years of continuous biophysiological data makes it unfeasible to rely solely on our memory for providing the contextual information. Many view maintaining journals as burdensome, which may result in low compliance levels and unusable data. We present an architecture and implementation of a system for the acquisition, processing, and visualization of biophysiological signals and contextual information.",2015-12-31,['affective-computing'],,9031,--Choose Location,2022-05-12T17:13:39.014Z,"['picard@media.mit.edu', 'yadid@media.mit.edu']",True,feel-a-cloud-system-for-frequent-event-and-biophysiological-signal-labeling,2011-09-01,[],FEEL: A cloud system for frequent event and biophysiological signal labeling,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:28.235Z,"Fensadense is a new work for 10-piece ensemble composed by Tod Machover, commissioned for the Lucerne Festival in summer 2015. The project represents the next generation of hyperinstruments, involving the measurement of relative qualities of many performers where previous systems only looked at a single performer. Off-the-shelf components were used to collect data about movement and muscle tension of each musician. The data was analyzed using the Hyperproduction platform to create meaningful production control for lighting and sound systems based on the connection of the performers, with a focus on qualities such as momentum, connection, and tension of the ensemble as a whole. The project premiered at the Lucerne Festival, and a spring European tour just concluded this May 2016.Fensadense site created by our former UROPer, Garrett Parrish.Listen to a complete recording of the Lucerne performance here.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/10/20/FENSADENSE-7743-1200x600.png,9039,--Choose Location,2017-04-03T19:36:35.564Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'benb@media.mit.edu']",True,fensadense,2015-01-01,"['design', 'art', 'music', 'performance']",Fensadense,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:28.376Z,"As computers move off the desktop they must be able to detect interaction with a user in their immediate environment. We are developing the instrumentation and algorithms to measure such gestures through the influence of the user's body on weak electric fields. This provides interfaces that are completely unobtrusive (the devices could be built into ordinary furniture), and that can inexpensively and reliably match the limits of human performance in unconstrained environments.",2005-01-01,['physics-and-media'],,9047,E15-023,2016-12-05T00:16:25.129Z,"['neilg@media.mit.edu', 'rehmi@media.mit.edu']",True,field-imaging,2000-01-01,[],Field Imaging,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:28.530Z,"Microfabricated biosensors based on silicon field-effect are being developed for applications where traditional biological methods are not suitable. This is a non-optical tool aimed at label-free detection of DNA hybridization, enzyme activity, and metabolic changes in single cells.",2005-09-01,[],,9055,E15-420,2016-12-05T00:16:25.154Z,['scottm@media.mit.edu'],True,field-effect-biosensors,1999-01-01,[],Field-Effect Biosensors,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:28.686Z,"Figments is a theatrical performance that tells a story inspired by a variety of source texts, including Dante Alighieri's prosimetrum La Vita Nuova. Framed by a woman's accidental discovery of the compelling journals of the Dante-archetype, three inner vignettes reveal the timeless tribulations of the memoir's author(s). Figments was created using Media Scores, a framework in development to facilitate the composition of Gesamtkunstwerk using parametric score-like visual notation. The Media Score for Figments is realized in this production through the performance of actors, light, visuals, and the generation of musical accompaniment in response to the expressive qualities represented in the score. The score served as a reference during the creation and design of the piece, a guide during rehearsals, and as show control for the final production.",2015-01-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/files/Display/figments_pldb.jpg,9063,--Choose Location,2016-12-05T00:17:12.141Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu']",True,figments,2013-01-01,[],Figments,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:28.853Z,"The dream of connecting everything to everything else presupposes a network infrastructure up to the task. Traditionally, connecting a device to such a network has required a whole computer, often costing orders of magnitude more than the devices actually needed. Connecting lightbulbs to toasters to stereos to telephones requires a small and inexpensive network interface. Filament is such a device: it is a serial-to-ethernet interface on a single board three inches long and an inch and a half wide. With Filament, a wide range of devices can be easily connected to a local network, and even to the Internet.",2001-12-30,['physics-and-media'],,9071,E15-022,2016-12-05T00:17:12.209Z,['neilg@media.mit.edu'],True,filament,1996-12-31,[],Filament,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:29.007Z,"Because common-sense knowledge differs culturally, misunderstandings frequently occur. Because differences can be subtle, there has been little work in trying to detect places in text where cultural differences might arise. We explicitly represent the common-sense knowledge of each culture in separate knowledge bases. By analyzing a text, we can find differences between each culture's knowledge concerning its subject. For example, given an invitation to a party, the system is able to infer that in an American cultural context, hip-hop dancing might be expected, but in a Mexican context, salsa dancing might be the norm. We are building an email client that suggests knowledge from multiple cultures that might be relevant, while watching the user's typing.",2010-09-01,['software-agents'],,9079,E15-383,2016-12-05T00:16:08.340Z,['lieber@media.mit.edu'],True,finding-cultural-differences-in-text,2004-01-01,[],Finding Cultural Differences in Text,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:29.288Z,"The FingerSynth is a wearable musical instrument made up of a bracelet and set of rings that enables its players to produce sound by touching nearly any surface in their environments. Each ring contains a small, independently controlled audio exciter transducer. The rings sound loudly when they touch a hard object, and are silent otherwise. When a wearer touches their own (or someone else's) head, the contacted person hears sound through bone conduction, inaudible to others. A microcontroller generates a separate audio signal for each ring, and can take user input through an accelerometer in the form of taps, flicks, and other gestures. The player controls the envelope and timbre of the sound by varying the physical pressure and the angle of their finger on the surface, or by touching differently resonant surfaces. The FingerSynth encourages players to experiment with the materials around them and with one another.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/fingersynth-martini.jpg,9095,--Choose Location,2019-04-19T14:26:25.030Z,"['gershon@media.mit.edu', 'joep@media.mit.edu']",True,fingersynth-wearable-transducers-for-exploring-the-environment-through-sound,2014-01-01,[],FingerSynth: Wearable transducers for exploring the environment through sound,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:29.425Z,"The Musical Fireflies are digital rhythm toys that introduce mathematical concepts in music such as beat, accent, mono- and polyrhythm without requiring users to have any prior knowledge of music theory or instruction. Through simple controls, the Fireflies allow users to input rhythmical patterns and embellish them in real time. The toys' wireless communication system allows players to synchronize these patterns and trade instrument sounds with other players. Since this interaction increases the richness and complexity of the game, the Fireflies also motivate collaboration and social play.",2000-12-30,['opera-of-the-future'],,9103,E15-491,2016-12-05T00:17:12.296Z,['tod@media.mit.edu'],True,fireflies,1999-12-31,[],Fireflies,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:29.543Z,"First Upload is a tool for verifying the authenticity of news imagery. It helps find the first upload of imagery, particularly videos. Finding the person who uploaded a video is a key to determining authenticity, because often it is necessary to contact that person directly. It is being developed with input from YouTube and Bloomberg. Currently we have a working prototype, built for the YouTube site.",2020-08-01,"['future-of-news', 'civic-media']",,9111,--Choose Location,2022-06-10T13:37:16.574Z,"['ethanz@media.mit.edu', 'matt54@media.mit.edu']",False,first-upload,2015-01-01,[],First Upload,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:29.669Z,"A better understanding of the biomechanics of human tissue allows for better attachment of load-bearing objects to people. Think of shoes, ski boots, car seats, orthotics, and more. We are focusing on prosthetic sockets, the cup-shaped devices that attach an amputated limb to a lower-limb prosthesis, which currently are made through unscientific, artisanal methods that do not have repeatable quality and comfort from one individual to the next. The FitSocket project aims to identify the correlation between leg tissue properties and the design of a comfortable socket. The FitSocket is a robotic socket measurement device that directly measures tissue properties. With these data, we can rapid-prototype test sockets and socket molds in order to make rigid, spatially variable stiffness, and spatially/temporally variable stiffness sockets.",,"['biomechatronics', 'mediated-matter']",https://dam-prod2.media.mit.edu/x/files/Display/FitSocketLogoBW1ai.png,9119,--Choose Location,2021-09-17T19:23:24.863Z,"['hherr@media.mit.edu', 'petron@media.mit.edu', 'jfduval@media.mit.edu', 'neri@media.mit.edu']",True,fitsocket-measurement-for-attaching-objects-to-people,2010-09-01,[],FitSocket: Measurement for attaching objects to people,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:30.054Z,"This project proposes both to investigate how a more open and flexible construction toolkit enables rich learning in mechanical and structural engineering as well as to investigate how introducing engineering concepts through an open-ended art/storytelling project provides for a welcoming entry to the hard sciences. Through using raw materials instead of pre-designed parts, we will investigate the exploration by the learner of design from raw material to part, as well as from part to whole. Through construction, we will examine how builders develop a proficiency in externalizing their ideas about motion, structure, and form, and how reflection upon, discussion about, and re-internalization of this ability to externalize ideas changes the way they think about, talk about, and approach future designs. The constructions are centered around the theme of artistic interactive 3-D storytelling. Through this, we will explore how builders express and incorporate their own interests and stories into their designs, and as a result, become engaged in learning new concepts.",2007-09-01,[],,9143,E15-368,2016-12-05T00:16:25.206Z,['cavallo@media.mit.edu'],True,flexible-and-appropriable-materials-for-constructing-knowledge,2007-09-01,[],Flexible and Appropriable Materials for Constructing Knowledge,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:30.215Z,"This research focuses on developing a smart material capable of sensing, distilling, and interpreting environmental stimuli while offering mechanical flexibility.  The device itself is composed of a series of small, interconnectable nodes.  Each node has its own embedded processing and a host of multi-modal sensors.  Physically, the system's nodular design allows for scalability as well as customize-ability.  Computationally, the design allows researchers to experiment with resource allocation, information exchange, power management, and differing scopes of processing in sensor networks.
",2008-09-01,['responsive-environments'],,9151,--Choose Location,2016-12-05T00:16:25.233Z,['joep@media.mit.edu'],True,flexible-high-density-grid-sensor-network,2008-01-01,[],Flexible High-Density Grid Sensor Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:30.592Z,"This project aims to create an electronic controller with many different sensor affordances that adapts to the gestural preferences of the user during the course of the interaction.  Our research includes an electronic music interface; departing from existing music controllers and most current multimodal interfaces, this system allows users to train the system to recognize their own personalized gestures and to establish the mappings from those gestures to sound. This approach turns the paradigm for musical-instrument design on its head, giving the device the ability to adapt to the player. Viewed as a data-collection platform that will be used by many subjects, the controller will be a powerful vantage point from which to study universal patterns in the way people associate gesture and sound. Beyond breaking ground in electronic musical instrument design, this work is investigating important and topical issues in learning systems for multimodal and adaptive user interfaces.",2004-01-01,['responsive-environments'],,9167,E15-344,2016-12-05T00:16:25.312Z,['joep@media.mit.edu'],True,flexigesture,2003-09-01,[],FlexiGesture,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:30.771Z,"Flexor is an arm sleeve with a bend sensor, an annunciator, and electro-luminescent lights. It keeps track of arm movement and uses models of arm motion to create entertaining and useful feedback for physical therapy or exercise. If you move your arm in natural gestures, the electro-luminescent lights sparkle like sequins with every movement. When you bend your arm for exercise, as in a weight-lifting bicep curl, the sleeve counts repetitions, encouraging body awareness. A variable resistor in the sleeve senses body position, while an on-board microprocessor facilitates interaction through electro-luminescence and sound. Therapy can also be coupled with expression, communication, performance, play, jewelry, and tools. This playful exploration could also be used to teach people how to protect their ligaments and muscles from strain and injury while playing tennis, squash, or golf, or even picking up their small children.",2003-01-01,['context-aware-computing'],,9175,E15-320,2016-12-05T00:16:25.269Z,[],True,flexor-exercise-sleeve,2000-01-01,[],Flexor Exercise Sleeve,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:31.148Z,"This project aims to enable fast prototyping of a multi-axis and multi-joint active prosthesis by developing a new modular electronics system. This system provides the required hardware and software to do precise motion control, data acquisition, and networking. Scalability is achieved through the use of a fast industrial communication protocol between the modules, and by a standardization of the peripherals' interfaces: it is possible to add functionalities to the system simply by plugging in additional cards. Hardware and software encapsulation are used to provide high-performance, real-time control of the actuators, while keeping the high-level algorithmic development and prototyping simple, fast, and easy.",,['biomechatronics'],,9191,--Choose Location,2021-09-17T19:29:24.423Z,"['hherr@media.mit.edu', 'jfduval@media.mit.edu']",True,flexsea-flexible-scalable-electronics-architecture-for-wearable-robotics-applications,2013-09-01,[],"FlexSEA: Flexible, scalable electronics architecture for wearable robotics applications",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:31.312Z,"FlickInk is a gesture-sensing pen to support collaborative work and to augment the environment. With a quick ""flick"" of the pen toward a desired destination, analog written content on paper instantly transfers onto the corresponding physical object in the environment. The FlickInk gesture sensing module allows for wireless communication and directional gesture sensing. If multiple surfaces are present, the direction of the pen swing determines which screen the information is transferred to. Furthermore, multiple users can flick their written content to multiple devices, creating a personalized collaborative environment.",,['changing-places'],,9199,--Choose Location,2016-12-08T17:29:16.429Z,"['kll@media.mit.edu', 'aithpao@media.mit.edu']",True,flickink,2012-01-01,[],FlickInk,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:31.474Z,"Inspired by the fact that people are communicating more and more through technology, Flickr This explores ways for people to have emotion-rich conversations through all kinds of media provided by people and technology. By grounding them in shared media, the technology allows remote people to have conversations that are more like face-to-face experiences. Flickr This lets viewable content provide structure for a conversation; conversation can move between synchronous and asynchronous, and evolve into a richer collaborative conversation/media.",2011-09-01,['living-mobile'],,9207,E15-383,2016-12-05T00:16:25.339Z,['geek@media.mit.edu'],True,flickr-this,2010-09-01,[],Flickr This,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:31.628Z,"Flights of Fantasy is an interactive research and installation project that focuses on the acts of receiving and sending video messages. Inspired by an analogy between the carrier pigeon and electronic networks, the installation is designed to occupy two rooms. As visitors move between the spaces, they either signal their wish to receive a fragment by opening a door in a forest of birdcages, or they co-construct story fragments using a construction interface that resembles a child's pocket puzzle. As the visitors move between the two spaces, the installation invites them to reflect on an essential attribute of communication: that one cannot be at once the prime creator/sender and a prime receiver of a message or story. Creation changes the opportunity for the message or story to surprise. A similar construction and receiving interface can be found on the WWW.
                                 
This installation contradicts the passive-receptive reverie traditionally associated with the cinematic experience.  Moving through the space in the DeCordova Museum, or playing on the virtual site, the visitor is confronted with a subtle call to action. Without action, there is no story created and no story received.",2002-12-30,['interactive-cinema'],,9215,,2016-12-05T00:16:25.385Z,"['gid@media.mit.edu', 'barbara@media.mit.edu']",True,flights-of-fantasy,2000-12-31,[],Flights of Fantasy,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:31.796Z,"We developed a robotics programming language, with the goal of empowering children to construct more sophisticated behaviors for their robots, and to learn more by doing so. After initial experiments with Flogo I, a visual language using icons, boxes, and wires to represent real-time dataflow relationships, we developed Flogo II. In Flogo II, programs were written as text, but with two key innovations: (i) the text was ""live:"" it revealed its activity as it runs, and it could be edited while running; (ii) Flogo II's programming paradigm integrated two approaches to programming: a program as a sequence of instructions, and a program as a collection of dynamic rules and relationships. In current piloting work with children, we refined the design of Flogo II, explored the learning challenges for children posed by robotics programming, and began to design higher-level behavior construction tools that could be embedded within the language.",2003-01-01,['lifelong-kindergarten'],,9223,E15-001,2016-12-05T00:17:12.451Z,['mres@media.mit.edu'],True,flogo-robotics-programming-for-children,2000-01-01,[],Flogo: Robotics Programming for Children,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:31.944Z,"Flossing is the phenomenon of drivers travelling back and forth on a designated route to show off their vehicles. The Flosser is a software package designed to be integrated into your car to help pre-determined flossing groups arrange meetings and communicate in real time. The system runs off of GPS data and GPRS networks on a Linux platform. The software provides advanced social rules capable of helping cars and people organize better, as well as connecting the car and city. The project hopes to answer a large number of questions raised by location-based tools and the way the car interacts with the city.",2006-09-01,['smart-cities'],,9231,E15-001,2016-12-05T00:16:25.404Z,['rchin@media.mit.edu'],True,flosser,2004-01-01,[],Flosser,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:32.127Z,"Flow is an augmented interaction project that bridges the divide between our non-digital objects and items and our ecosystem of connected devices. By using computer vision, Flow enables our traditional interactions to be augmented with digital meaning, thus allowing an event in one environment to flow into the next. Through this, physical actions such as tearing a document can have a mirrored effect and meaning in our digital environment, leading to actions such as the deletion of the associated digital file. This project is part of an initial exploration that focuses on creating an augmented interaction overlay for our environment, enabling users to redefine their physical actions.",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/flow_logo.png,9239,--Choose Location,2016-12-05T00:16:25.480Z,['holtzman@media.mit.edu'],True,flow,2012-01-01,[],Flow,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:32.293Z,"Fluid Voice is a group communication platform that allows both real-time and asynchronous group activity participation. Users can form local audio broadcast networks for group discussion, similar to a conference call. Disconnected group members can be included in these discussions asynchronously: the audio is recorded and sent to them opportunistically. In this way, membership of a group persists in the face of disconnection caused by communication failures and mobility. Our platform supports other asynchronous applications: polling and voting, sharing of wish lists, and text messaging.  We anticipate extending Fluid Voice to integrate messaging with environmental displays.",2009-09-01,['viral-communications'],,9247,E15-495,2016-12-05T00:16:25.497Z,['lip@media.mit.edu'],True,fluid-voice,2005-01-01,[],Fluid Voice,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:32.613Z,"Fluxa is a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a transient wearable display to foster richer self-expression and communication in daily life . It can be used to enhance existing social gestures such as handwaving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a decoration device that generates images around dancing bodies.",2017-11-30,"['responsive-environments', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/uuid/10e069bb-a599-4e08-a30f-ce7a729196da,9263,--Choose Location,2018-10-12T16:57:48.806Z,"['pattie@media.mit.edu', 'katiav@media.mit.edu', 'xxxxxxin@media.mit.edu', 'joep@media.mit.edu', 'mingrui@media.mit.edu']",True,fluxa,2016-01-01,[],Fluxa,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:32.767Z,"FocalSpace is a system for focused collaboration utilizing spatial depth and directional audio. We present a space where participants, tools, and other physical objects within the space are treated as interactive objects that can be detected, selected, and augmented with metadata. Further, we demonstrate several scenarios of interaction as concrete examples. By utilizing diminishing reality to remove unwanted background surroundings through synthetic blur, the system aims to attract participant attention to foreground activity.",2013-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202011-09-29%20at%209.41.07%20AM.png,9271,--Choose Location,2016-12-05T00:16:25.453Z,"['ishii@media.mit.edu', 'liningy@media.mit.edu']",True,focalspace,2011-01-01,[],FocalSpace,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:32.911Z,"Some readers require greater context to understand complex stories. FOLD is an open publishing platform with a unique structure that lets writers link media cards to the text of their stories. Media cards can contain videos, maps, tweets, music, interactive visualizations, and more. FOLD is used by journalists, educators, and storytellers around the world. ",2020-05-15,"['future-of-news', 'collective-learning', 'civic-media']",https://dam-prod2.media.mit.edu/x/2016/12/14/DSC00609.JPG,9279,--Choose Location,2022-06-10T12:56:41.388Z,"['hidalgo@media.mit.edu', 'ethanz@media.mit.edu', 'matt54@media.mit.edu', 'ahope@media.mit.edu', 'kzh@media.mit.edu']",False,fold,2014-01-01,[],FOLD,PUBLIC,https://readfold.com/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:33.064Z,"""Folk Computing"" supported face-to-face communication and community, modeled on the communicative process of folklore. Although an abundance of research on technologies that support community existed, very little of it focused on co-present communities whose dominant means of interacting is face-to-face. Folk Computing research included earlier work on computationally augmented name tags for conferences, and work on toys that let children create, trade, and track their own digital playthings.",2001-09-01,['lifelong-kindergarten'],,9287,E15-001,2016-12-05T00:17:12.499Z,"['mres@media.mit.edu', 'borovoy@media.mit.edu', 'bss@media.mit.edu']",True,folk-computing,1999-01-01,[],Folk Computing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:33.310Z,"The rise in wearable devices and the desire to quantify various aspects of everyday activities has provided the opportunity to offer just-in-time triggers to aid in achieving pre-determined goals. While a lot is known about the effectiveness of messaging in marketing efforts, less is known about the effectiveness of these marketing techniques on in-the-moment decision-making. We designed an experiment to determine if a simple solution of using just-in-time persuasive messaging could influence participants' eating habits and what types of messaging could be most effective in this effort. Our solution utilizes a head-mounted display to present health-based messages to users as they make real-time snack choices. We are able show that this method is effective and more feasible than current efforts to influence eating habits.",,"['advancing-wellbeing', 'fluid-interfaces']",,9303,--Choose Location,2016-12-05T00:16:44.086Z,"['pattie@media.mit.edu', 'nfarve@media.mit.edu']",True,food-attack,2015-01-01,[],Food Attack,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:33.670Z,"The OpenAg™ Food Server is a shipping container-sized, controlled environment agriculture technology that can be built to utilize hydroponic or aeroponic technology. It can serve as both a research platform for simulating precise environments at scale (see Flavor, environment, and the phenome) , and a production unit for any specified crop of interest. It is intended to produce larger quantities of food than a Personal Food Computer and appeals to interdisciplinary researchers as well as small-scale cafeterias, restaurants, and boutique operators.",2020-04-30,['open-agriculture-openag'],https://dam-prod2.media.mit.edu/x/2019/06/25/FLIR camera_interior.png,9327,--Choose Location,2020-06-17T15:25:34.480Z,"['calebh@media.mit.edu', 'jrye@media.mit.edu', 'rebekahj@media.mit.edu', 'poitrast@media.mit.edu']",True,food-server,2015-09-01,"['agriculture', 'artificial-intelligence', 'food', 'machine-learning']",Food Server,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:33.886Z,"""For Once In Your Life..."" is a site-specific interactive radio play that uses the various sensors in a smartphone to determine specific details, such as where the user walks within a space, to dynamically affect the story. It's a blend of experiential theatre, modern choice-based interactive fiction, and audio walks such as the work of Janet Cardiff.",2017-09-01,"['future-storytelling', 'playful-systems']",,9343,--Choose Location,2022-06-10T13:54:36.852Z,"['slavin@media.mit.edu', 'mslw@media.mit.edu']",True,for-once-in-your-life,2015-09-01,[],For Once In Your Life...,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:34.078Z,"ForgetAboutIT has become an integrated part of CollaboRhythm. Currently only 50 percent of patients with chronic diseases take their medications. The problem is not simple forgetfulness; it is a complex combination of lack of understanding, poor self-reflection, limited social support, and almost non-existent communication between provider and patient. ForgetAboutIT? is a system to support medication adherence which presupposes that patients engaged in tight, collaborative communication with their providers through interactive interfaces would think it preposterous not to take their medications. Technically, it is an awareness system that employs ubiquitous connectivity on the patient side through cell phones, televisions, and other interactive devices and a multi-modal collaborative workstation on the provider side.",2013-01-01,['new-media-medicine'],,9359,E15-320,2020-07-29T16:19:52.561Z,"['jom@media.mit.edu', 'fmoss@media.mit.edu']",True,forgetaboutit,2009-01-01,[],ForgetAboutIT?,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:34.166Z,"Sometimes learners have to focus while experiencing strong emotions (e.g., family problems).�They may also face challenges in perservering when encountering repeated failures in problem solving. The ability to know what one is feeling (e.g., worried, frustrated) and rise above it and handle the situation productively involves meta-affective skills. With such skills, a learner feeling ""I can't do this; I want to quit,"" might instead think, ""I am frustrated, but this is OK�it happens to experts. I should look for a different way to solve this.""�This research develops theory and technology to help learners develop meta-affective skills. Two recent achievements are development of (1) a technology with machine ""common-sense"" emotion�reasoning for enabling teenage girls to reflect on emotions in stories that they've constructed and improve their affect awareness; and (2) a technology to help students become stronger learners even when they feel like quitting.",2008-01-01,['affective-computing'],,9367,E15-443,2016-12-05T00:17:12.588Z,"['picard@media.mit.edu', 'win@media.mit.edu']",True,fostering-affect-awareness-and-regulation-in-learning,2003-09-01,[],Fostering Affect Awareness and Regulation in Learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:34.353Z,"Frame It is an interactive, blended, tangible-digital puzzle game intended as a play-centered teaching and therapeutic tool. Current work is focused on the development of a social-signals puzzle game for children with autism that will help them recognize social-emotional cues from information surrounding the eyes. In addition, we are investigating if this play-centered therapy results in the children becoming less averse to direct eye contact with others. The study uses eye-tracking technology to measure gaze behavior while participants are exposed to images and videos of social settings and expressions. Results indicate that significant changes in expression recognition and social gaze are possible after repeated uses of the Frame It game platform.",2012-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/FrameItLogo.png,9383,E15-443,2016-12-05T00:16:25.626Z,"['picard@media.mit.edu', 'micahrye@media.mit.edu']",True,frame-it,2009-01-01,[],Frame It,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:34.516Z,"This checklist is designed to help projects that include an element of data collection to develop appropriate consent policies and practices. The checklist can be especially useful for projects that use digital or mobile tools to collect, store, or publish data, yet understand the importance of seeking the informed consent of individuals involved (the data subjects). This checklist does not address the additional considerations necessary when obtaining the consent of groups or communities, nor how to approach consent in situations where there is no connection to the data subject. This checklist is intended for use by project coordinators, and can ground conversations with management and project staff in order to identify risks and mitigation strategies during project design or implementation. It should ideally be used with the input of data subjects.",2015-01-01,['civic-media'],,9399,--Choose Location,2016-12-05T00:17:12.642Z,"['ethanz@media.mit.edu', 'bl00@media.mit.edu']",True,framework-for-consent-policies,2014-01-01,[],Framework for Consent Policies,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:34.611Z,"The Free City project promotes awareness and facilitates access to free and low-cost events, services and  opportunities that are locally available.  By using the technologies in our What's Up toolkit, we aim to reduce the information gap, foster social  connectivity and unleash the learning potential of urban centers,  contributing to the development of cities that more educated, sustainable, inclusive and democratic.  ",2012-09-01,['civic-media'],,9407,,2016-12-05T00:16:25.643Z,"['ethanz@media.mit.edu', 'leob@media.mit.edu']",True,free-city,2011-01-01,[],Free City,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:34.687Z,"What if you could not only see but also feel virtual objects as you interacted with them? This would enable richer and more realistic user experiences. We have designed a low-cost air-vortex generator to provide midair haptic feedback when a user touches virtual objects displayed on holographic, aerial, and other 3D displays. The system consists of a 3D-printed chamber and nozzle, five low-frequency transducers, and a custom-designed driver board. The air-vortex generator can provide localized haptic feedback to a range of over 100cm. With increased driving power and a more optimized nozzle design, this range could be extended to several meters.",,"['ultimate-media', 'terrestrial-sensing', 'object-based-media']",,9415,--Choose Location,2018-05-01T01:37:57.436Z,"['vmb@media.mit.edu', 'alims@media.mit.edu']",True,free-space-haptic-feedback-for-3d-displays,2016-01-01,[],Free-Space Haptic Feedback for 3D Displays,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:34.774Z,"The FreeD is a hand-held, digitally controlled milling device that is guided and monitored by a computer while still preserving the craftsperson's freedom to sculpt and carve. The computer will intervene only when the milling bit approaches the planned model. Its interaction is either by slowing down the spindle speed or by drawing back the shaft; the rest of the time it allows complete freedom, letting the user to manipulate and shape the work in any creative way.",2014-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/logo.jpg,9423,--Choose Location,2016-12-05T00:16:25.664Z,['joep@media.mit.edu'],True,freed,2011-01-01,[],FreeD,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:34.849Z,"Freedom Flies is an inexpensive, open-sourced Unmanned Aerial Vehicle for free press and human rights applications. Made from off-the-shelf parts (bicycle rims, weed whacker, water bottle) it can carry a payload of 30 pounds for over three hours, surveying locations off-limits or too dangerous for journalists.",2008-01-01,['computing-culture'],,9431,E15-001,2016-12-05T00:17:12.667Z,['csik@media.mit.edu'],True,freedom-flies,2005-09-01,[],Freedom Flies,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:34.922Z,"With the arrival of mobile phones, the concept of calling has moved from calling a place to calling a person, which has quite distinctive affordances. Frontdesk proposes a place-based communication tool that is accessed primarily through any mobile device and features voice calls and text chat. The application uses �place� loosely to define a physical space created by a group of people that have a shared context of that place. Examples of places could be different parts of a workspace in a physical building, such as the machine shop, caf�, or Speech + Mobility group area at the Media Lab. When a user calls any of these places, frontdesk routes their call to all people that are �checked-in� to that place.",2012-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/frontdesk_logo.jpg,9439,--Choose Location,2016-12-05T00:16:25.682Z,['geek@media.mit.edu'],True,frontdesk,2011-01-01,[],frontdesk,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:35.082Z,Light field camera have inherent trade offs between spatial and angular resolution. We mathematically model the image capture in light field cameras in spatial domain and prove that full resolution light field capture is possible using the heterodyne camera. We demonstrate refocusing a d full resolution imaging results using heterodyne light field camera.,2010-01-01,['camera-culture'],,9455,,2016-12-05T00:17:12.758Z,['raskar@media.mit.edu'],True,full-resolution-lightfields-using-heterodyne-camera,2010-09-01,[],Full Resolution Lightfields using Heterodyne Camera,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:35.160Z,"Full-Contact Poetry is a digital play space for children's poetic expression. It is a software environment in which children can express their poetic thoughts, create their interpretations of writing by others and also share these expressions. The environment combines ideas from literary theory and analysis with constructionism to extend tools for poetic expression. Children can experience poetry by playing with words as objects, experimenting with typographic effects, moving words through space and navigating into and through the text, while also being able to incorporate and reconfigure sound and image.",2004-01-01,['future-of-learning-2'],,9463,E15-320,2016-12-05T00:16:25.707Z,['cavallo@media.mit.edu'],True,full-contact-poetry,2001-01-01,[],Full-Contact Poetry,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:35.236Z,"Computer-generated holographic images typically sacrifice image variations with up-and-down viewer motion (vertical parallax). However, there are several applications that require both horizontal and vertical parallax; this considerably complicates the design of hologram recording technology. This project studies one- and two-optical-step methods for producing full-parallax, hard-copy holograms of digital data.",2004-01-01,[],,9471,E15-420,2016-12-05T00:16:25.733Z,[],True,full-parallax-synthetic-holograms,2000-01-01,[],Full-Parallax Synthetic Holograms,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:09:35.305Z,"This project pushes our work on the miniaturization of low-power radios and sensors for embedded intelligence into very small form factors that will enable unobtrusive wireless sensor packages to be leveraged in many (e.g., wearable, medical) applications. In addition, the project addresses applications that use a wireless agent platform running on embedded Java processors connected to the same low-power radios; this continues work done by the National Microelectronics Research Centre (NMRC) at University College Cork. The research capitalizes upon the Media Lab, Media Lab Europe, and NMRC's shared interest in emergent behavior arising from the interaction of software agents. (Funded by the Higher Education Authority of Ireland.)",2004-09-01,['responsive-environments'],,9479,MLE,2016-12-05T00:16:37.319Z,['joep@media.mit.edu'],True,functional-integration-for-embedded-intelligence,2002-01-01,[],Functional Integration for Embedded Intelligence,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:35.385Z,"Prosthetic sockets belong to a family of orthoic devices designed for amputee rehabilitation and performance augmentation. Although such products are fabricated out of lightweight composite materials and designed for optimal shape and size, they are limited in their capacity to offer local control of material properties for optimizing load distribution and ergonomic fit over surface and volume areas. Our research offers a novel workflow to enable the digital design and fabrication of customized prosthetic sockets with variable impedance informed by MRI data. We implement parametric environments to enable the controlled distribution of functional gradients of a filament-wound carbon fiber socket.",2019-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/Logo_socket.jpg,9487,--Choose Location,2021-05-21T13:20:21.034Z,"['neri@media.mit.edu', 'cdgu@media.mit.edu']",True,functionally-graded-filament-wound-carbon-fiber-prosthetic-sockets,2014-01-01,[],Functionally graded filament-wound carbon-fiber prosthetic sockets,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:35.548Z,"Funk2 is a novel process-description language that keeps track of everything that it does. Remembering these causal execution traces allows parallel threads to reflect, recognize, and react to the history and status of other threads. Novel forms of complex, adaptive, nonlinear control algorithms can be written in the Funk2 programming language. Currently, Funk2 is implemented to take advantage of distributed grid processors consisting of a heterogeneous network of computers, so that hundreds of thousands of parallel threads can be run concurrently, each using many gigabytes of memory. Funk2 is inspired by Marvin Minsky's Critic-Selector theory of human cognitive reflection.",2013-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/funk2-mmp-on_white_square.png,9503,E15-351,2016-12-05T00:17:12.802Z,['joep@media.mit.edu'],True,funk2-causal-reflective-programming,2007-09-01,[],Funk2: Causal Reflective Programming,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:35.636Z,"The Furp (""Future of Urban Planning"") project exists as a first step toward disseminating the Luminous Room project into the world at large. We are collaborating with MIT Professor Eran Ben-Joseph in using a customized version of the Urp system (see Luminous Room) in his Site and Urban Systems Planning class. The system has recently made its in-class debut, and is now undergoing intensive week-by-week modifications and refinements in response to what's being learned about its real-world usability. Our short-term goal is to transform Urp from a ""proof-of-concept"" system into a tool sufficiently convenient and facile to permit ongoing and casual use in a classroom. Longer-term goals  include extrapolation from these class-centered experiments to a better understanding of which modalities and contexts are appropriate for broader  applications for luminous-tangible interaction techniques.",2002-12-30,['tangible-media'],,9511,E15-441,2016-12-05T00:16:25.842Z,"['ishii@media.mit.edu', 'jh@media.mit.edu']",True,furp,1998-12-31,[],Furp,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:35.722Z,"The Future Music Blender (FMB) is a totally new part of the Brain Opera, which was permanently installed in Vienna in summer 2000. The FMB is the culminating experience of the Brain Opera. Visitors will enter a second room after exploring the Brain Opera's Mind Forest. In this FMB room, they will listen to multiple music and sound samples, culled from the Mind Forest, the Internet, and a pre-processor database. Selected sounds represented as ID-tagged tokens will be submitted to a central ""blender"" sculpture, which will read the ID and immediately incorporate the selected sound into an active performance database. A specially adapted Sensor Chair, with a Multi-Modal Mixer (MMM) for extra left-hand control, will allow one user at a time to select sounds from the active database (organized spatially according to perceptually salient characteristics), to create musical collages by waving a hand in the air, and to mix or ""blend"" these sounds with MMM gestures and commands. A generative algorithm will analyze the sound collage playing at any given moment, and supplement it with an appropriate musical accompaniment. Thus, found sounds will be turned into beautiful music.",1999-12-30,['opera-of-the-future'],,9519,,2016-12-05T00:17:12.821Z,['tod@media.mit.edu'],True,future-music-blender,1997-12-31,[],Future Music Blender,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:35.891Z,"This is a fuzzy logic programming environment for embeddable computing that targets non-technical adults or inexperienced hardware developers, in particular fashion and graphic designers who are interested in using computational elements in their work. Control of the system builds upon fuzzy logic reasoning of small microcontrollers, and programming is done through a server-side Web interface.",2002-01-01,[],,9535,E15-301,2016-12-05T00:16:37.040Z,[],True,fuzz,2002-01-01,[],Fuzz,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:35.961Z,"We are exploring how to navigate large information spaces naturally, quickly, and seamlessly between graphical and physical space using body and gesture. The goal is to create an interface where users can manipulate digital space as if they were using telekinesis.",2010-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/g-stalt_logo.jpg,9543,E15-368,2016-12-05T00:16:25.755Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu']",True,g-stalt,2009-09-01,[],g-stalt,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:36.047Z,"Ancient yet modern, enclosing yet invisible, glass was first created in  Mesopotamia and Ancient Egypt 4,500 years ago. Precise recipes for its  production—the chemistry and techniques—often remain closely guarded  secrets. Glass can be molded, formed, blown, plated or sintered; its  formal qualities are closely tied to techniques used for its  formation. From the discovery of core-forming process for bead-making in  ancient Egypt, through the invention of the metal blow pipe during  Roman times, to the modern industrial Pilkington process for making  large-scale flat glass; each new breakthrough in glass technology  occurred as a result of prolonged experimentation and ingenuity, and has  given rise to a new universe of possibilities for uses of the material.",2018-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2016/10/24/Screen Shot 2016-10-24 at 4.40.54 PM.png,9551,--Choose Location,2022-03-11T16:50:04.661Z,"['m_kayser@media.mit.edu', 'j_klein@media.mit.edu', 'inamura@media.mit.edu', 'dlizardo@media.mit.edu', 'neri@media.mit.edu', 'mlstern@media.mit.edu']",True,g3p,2014-01-01,"['design', 'art', 'biology', 'materials', 'covid19']",Glass I,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:09:36.130Z,"Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.",2018-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2017/04/18/00_FRONTPAGE_12.jpg,9559,--Choose Location,2022-03-11T17:09:04.757Z,"['inamura@media.mit.edu', 'dlizardo@media.mit.edu', 'neri@media.mit.edu', 'giorgiaf@media.mit.edu', 'nassia@media.mit.edu', 'ked03@media.mit.edu', 'weller@media.mit.edu', 'achituv@media.mit.edu', 'mlstern@media.mit.edu']",True,g3p-II,2015-09-01,"['design', 'architecture', 'art', 'construction', 'manufacturing', '3d-printing', 'biology', 'materials', 'covid19']",Glass II,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:36.467Z,"The galvactivator is a glove-like wearable device that senses the wearer's skin conductivity and maps its values to a bright LED display. Increases in skin conductivity across the palm tend to be good indicators of physiological arousal, causing the galvactivator display to glow brightly. The galvactivator has many potentially useful purposes, ranging from self-feedback for stress management, to facilitation of conversation between two people, to new ways of visualizing mass excitement levels in performance situations or visualizing aspects of arousal and attention in learning situations. One of the findings in mass-communication settings was that people tended to ""glow"" when a new speaker came onstage, and during live demos, laughter, and live audience interaction. They tended to ""go dim"" during powerpoint presentations.  In smaller educational settings, students have commented on how they tend to glow when they are more engaged with learning.",2002-09-01,['affective-computing'],,9599,E15-001,2017-01-09T17:48:33.951Z,['picard@media.mit.edu'],True,galvactivator,1999-01-01,[],Galvactivator,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:36.546Z,"GalvaPhone is the expansion of EMotoPhone, an earlier project in the Speech Interfaces group. EMotoPhone allows cell-phone users to choose a face manually, representing their current emotion, and send it over a cell phone to another user. The recipient of the EMotoPhone face can then decide whether to accept a phone call from the sender. With GalvaPhone, information about a user's electrodermal activity (also known as the galvanic skin response) is also sent, in addition to the user's self-selected emotion. The electrodermal activity measure is determined by a galvactivator-a glove that measures skin conductivity.",2007-01-01,['living-mobile'],,9607,E15-384C,2016-12-05T00:17:12.896Z,['geek@media.mit.edu'],True,galvaphone,2005-01-01,[],GalvaPhone,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:36.717Z,"Does how you play reflect who you really are? The Media Lab and Tilburg University are bringing science into the game to figure out the connections between our play style and our cognitive traits. To do that, we are gathering data from League of Legends, World of Warcraft, and Battlefield 4, and Battlefield: Hardline players to gain insights across all the major online game genres (MOBA, MMORPG, and FPS). In return, every participant will get an in-depth GAMR profile that shows their personality, brain type, and gamer type.",2017-06-01,['playful-systems'],https://dam-prod2.media.mit.edu/x/files/Display/gamr-share.jpg,9623,--Choose Location,2022-06-10T14:14:48.864Z,"['slavin@media.mit.edu', 'tekofsky@media.mit.edu']",True,gamr,2015-01-01,[],GAMR,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:36.795Z,"We are exploring new methods of call setup, allowing the originating party to participate in the decision as to whether to alert the called party. A small community of trusted callers will have the ability to ""listen in"" to the called party's office or location (see Nomadic Radio), with this audio modified to convey context and speaker identity, but not content.  The caller can then decide whether to interrupt or leave a message.",2000-09-01,['living-mobile'],,9631,E15-344,2016-12-05T00:16:25.940Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,garblephone,1998-01-01,[],Garblephone,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:09:36.871Z,"Gemini—an acoustical “twin chaise""—spans multiple scales of the  human existence extending from the warmth of the womb to the stretches  of the Gemini zodiac in deep space. It recapitulates a human cosmos: our  body—like the Gemini constellation—drifting in space. ",2016-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2017/10/13/gemini01_6Eir1Pb.jpg,9639,--Choose Location,2022-03-11T15:58:20.795Z,['neri@media.mit.edu'],True,gemini,2014-01-01,"['design', 'architecture', 'art', 'synthetic-biology', '3d-printing', 'engineering']",Gemini,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:36.952Z,"This research aims to classify gene expression data sets into different categories, such as normal vs. cancer. The main challenge is that thousands of genes are measured in the micro-array data, while only a small subset of genes are believed to be relevant for disease classification. We have developed a novel approach called ""predictive automatic relevance determination;"" this method brings Bayesian tools to bear on the problem of selecting which genes are relevant, and extends our earlier work on the development of the ""expectation propagation"" algorithm. In our simulations, the new method outperforms several state-of-the-art methods, including support-vector machines with feature selection and relevance-vector machines.",2004-09-01,['affective-computing'],,9647,E15-001,2016-12-05T00:16:26.009Z,['picard@media.mit.edu'],True,gene-expression-data-analysis,2004-01-01,[],Gene Expression Data Analysis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:37.031Z,Devices to facilitate gene therapy will be of increasing importance in years to come.  We are developing fluidic systems to facilitate viral delivery in complex tissues.,2011-01-01,['synthetic-neurobiology'],,9655,--Choose Location,2016-12-05T00:16:08.489Z,"['esb@media.mit.edu', 'jberns@media.mit.edu']",True,gene-therapy-devices,2009-01-01,[],Gene Therapy Devices,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:37.110Z,"An understanding of the historical development of the car is critical to a ""reinvention"" of the car. This research project tracks the different chronologies and typologies that make up the genealogy of the car. An investigation of the early beginnings of the car such as the transition between horse and carriage and the early automobile reveals many clues about the different forces that have shaped the car. Ships, bicycles, and airplanes have all influenced this development in terms of aesthetics, technologies, manufacturing, and new materials. Other stylistic and technological developments are also documented in images and text in the genealogy in order to compare design from different eras and cultures. The genealogy spans from the ancient horse to the chariots of Greek and Roman times to the horse and carriage of the 18th and 19th centuries to the early automobiles of Henry Ford and Gottlieb Daimler. The genealogy continues with military vehicles of the World Wars to the automobiles of the 1930s and 1940s and eventually arrives at modern cars.",2006-01-01,['smart-cities'],,9663,E15-001,2016-12-05T00:16:17.426Z,['rchin@media.mit.edu'],True,genealogy-of-the-car,2000-01-01,[],Genealogy of the Car,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:37.188Z,"What would you like to ""build with biology""? The goal of the GeneFab project is to develop technology for the rapid fabrication of large DNA molecules, with composition specified directly by the user. Our intent is to facilitate the field of synthetic biology as it moves from a focus on single genes to designing complete biochemical pathways, genetic networks, and more complex systems. Sub-projects include: DNA error correction, microfluidics for high throughput gene synthesis, and genome-scale engineering (rE. coli).",2011-09-01,['molecular-machines'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/2plates.jpg,9671,E15-015,2016-12-05T00:17:00.317Z,['jacobson@media.mit.edu'],True,genefab,2002-09-01,[],GeneFab,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:37.358Z,"There have been many attempts at developing generative music systems. Rule-based methods generate a limited range of consistent but repetitive music. Probabilistic models are more flexible, allowing the generation of diverse music but lacking the clear causal structures and hierarchies found in most music. We are developing a generative system that induces the qualitative characteristics of motion in music by hierarchically analyzing and modeling changes of momentum simultaneously along multiple parameters, and subsequently transforming our model to give rise to variety and surprise.",2005-01-01,['music-mind-and-machine'],,9687,E15-484,2016-12-05T00:16:08.515Z,['bv@media.mit.edu'],True,generative-music-systems,2004-01-01,[],Generative Music Systems,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:37.435Z,"The genieBottles system presents a story that is told by three genies that live in glass bottles. When a bottle is opened, the genie contained inside is released and begins to talk to the user. If several genies are released at once, they converse with each other. The physical bottles are  graspable ""containers"" and ""controls"" for the digital story information. The genieBottles use a simple state transition model for interactive storytelling, in which the system plays back the appropriate segment of audio depending on the state it is in, as well as the appropriate segment(s) of audio to transition from one state to another.
                                 
The overall form of the genieBottles story is quite abstract, and does not have a highly structured narrative progression or plot. When users interact with the system, they capture the genies at a particular moment in time, during which they talk about their state of being in bottles, about their pasts, and about their expectations or desires for the future. The physical interface is tightly linked to, and limits, the form of the story: the play-out (i.e., the time of the interaction) corresponds to a brief segment in the genies' lives in which they chat and allude to a larger and more complex story. The interaction time thus constrains the plot time and hence the narrative.",2001-12-30,['tangible-media'],,9695,E15-441,2016-12-05T00:16:26.044Z,['ishii@media.mit.edu'],True,geniebottles,1999-12-31,[],genieBottles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:37.512Z,"This research is about finding ways to understand dynamic sources of information. Network usage patterns and computational processes are being explored through representations that focus on extracting the qualitative features from these very large data sets. The current focus of this work is in genomics and bioinformatics. In addition to the standard computer screen, alternative display devices are being studied, including handheld computers and large format printing.",2003-01-01,['aesthetics-computation'],,9703,E15-301,2016-12-05T00:17:12.944Z,[],True,genomic-cartography,2000-01-01,[],Genomic Cartography,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:37.611Z,"Have you ever been in the middle of a conversation and needed to share your location with the other party? Geo.gy is a location shortener service. It allows you to easily share your location with your peers by encoding it in a short URL which we call a ""geolink."" It is platform-independent, and based on HTML5, so you can use any device with a modern browser to generate a geolink, simply by visiting the project's page. There are no user accounts, so geolinks remain anonymous. You can use Geo.gy to add location context to a post, SMS, anything you want decorated with location context.",2013-01-01,['viral-communications'],,9711,--Choose Location,2016-12-05T00:16:26.130Z,"['ypod@media.mit.edu', 'lip@media.mit.edu']",True,geogy-location-shortener,2011-09-01,[],Geo.gy: Location Shortener,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:37.687Z,"GeoSCAPE is a reconstructive tool for capturing measurement data in field archaeology and for facilitating a 3-D visualization of an excavation rendered in computer graphics. This project is carried out by extending a recently developed orientation-aware digital measuring tape, HandSCAPE, which has been examined to address the efficiency of bridging measuring and modeling for on-site application areas. GeoSCAPE allows the user to navigate the archaeological excavation site with virtually immediate access to excavation information on-site. The timely and accurate result would verify clear interpretations while the excavator obtains increments of information on-site. This will improve collaboration between on-site and laboratory archaeological research.",2001-12-30,['tangible-media'],,9719,E15-441,2016-12-05T00:16:26.068Z,['ishii@media.mit.edu'],True,geoscape,1998-12-31,[],GeoScape,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:37.763Z,"An open publishing platform for visualization, social sharing, and data analysis of geospatial data.",2013-01-01,['tangible-media'],,9727,--Choose Location,2016-12-05T00:16:26.218Z,['ishii@media.mit.edu'],True,geosense,2012-09-01,[],GeoSense,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:37.839Z," This project is an effort to develop computer vision methodologies to perform behavioral evaluation on dyadic interactions grounded in longitudinal video recordings. The proposed system can be applied on any surveillance output, to analyze or recover event dynamics hidden on multi-agent interactions. Applications include analyzing word-learning dynamics in young children, new kinds of video retail analysis aimed at customer-associate interactions, determining dominant agents in a group, and establishing longitudinal diagnostic means for child developmental disorders.",2010-01-01,['social-machines'],https://dam-prod2.media.mit.edu/x/files/Display/PLDB1_tsourk.png,9735,,2016-12-05T00:17:13.059Z,"['dkroy@media.mit.edu', 'tsourk@media.mit.edu']",True,gestalt-video-analyzer,2009-09-01,[],Gestalt Video Analyzer,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:37.914Z,"We are developing gestural interaction techniques and applications using the g-speak platform from Oblong Industries. This work seeks to blend the value of physical, tangible interfaces with the power of gestural interaction. Our goal is to create seamless interfaces that scale and adapt between ambulatory and manipulatory, physical and graphical, ambient and direct, and representational and abstract�interfaces that can break the boundaries of existing paradigms.",2010-01-01,['tangible-media'],,9743,E15-368,2016-12-05T00:17:12.969Z,"['ishii@media.mit.edu', 'labrune@media.mit.edu', 'daniell@media.mit.edu']",True,gestural-interaction,2008-09-01,[],Gestural Interaction,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:38.003Z,"We are all equipped with two extremely expressive instruments for performance: the body and the voice. By using computer systems to sense and analyze human movement and voices, artists can take advantage of technology to augment the body's communicative powers. However, the sophistication, emotional content, and variety of expression possible through original physical channels is often not captured by the technologies used for analyzing them, and thus cannot be intuitively transferred from body to digital media. To address these issues, we are developing systems that use machine learning to map continuous input data, whether of gesture or voice, to a space of expressive, qualitative parameters. We are also developing a new framework for expressive performance augmentation, allowing users to create clear, intuitive, and comprehensible mappings by using high-level qualitative movement descriptions, rather than low-level descriptions of sensor data streams.",2014-09-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/files/Display/gmf-1%20crop.png,9751,E15-443,2016-12-05T00:16:26.104Z,"['tod@media.mit.edu', 'ejessop@media.mit.edu']",True,gestural-media-framework,2009-01-01,[],Gestural Media Framework,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:38.078Z,"The Gesture Ball explores the area of human-computer interaction by implementing a system wherein the human user will be able to use a gesture-based interface to play music. By creating and selecting motions, the user can manipulate the Gesture Ball's music. The Gesture Ball will learn the user's mental model of how such an interface should work, and modify itself to meet those expectations.",2003-01-01,['context-aware-computing'],,9759,E15-320,2016-12-05T00:16:26.252Z,[],True,gesture-ball,2000-01-01,[],Gesture Ball,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:38.151Z,"Emotions are often conveyed through gesture. Instruments that respond to gestures offer musicians new, exciting modes of musical expression. This project gives musicians wireless, gestural-based control over guitar effects parameters. ",2012-09-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/guit_pldb.jpg,9767,E15-450,2016-12-05T00:17:12.999Z,"['picard@media.mit.edu', 'rmorris@media.mit.edu', 'tod@media.mit.edu']",True,gesture-guitar,2008-01-01,[],Gesture Guitar,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:38.229Z,"The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, c++ machine-learning library that has been specifically designed for real-time gesture recognition. The GRT has been created as a general-purpose tool for allowing programmers with little or no machine-learning experience to develop their own machine-learning based recognition systems, through just a few lines of code. Further, the GRT is designed to enable machine-learning experts to precisely customize their own recognition systems, and easily incorporate their own algorithms within the GRT framework.  In addition to facilitating developers to quickly create their own gesture-recognition systems, the machine-learning algorithms at the core of the GRT have been designed to be rapidly trained with just a few examples examples for each gesture. The GRT therefore allows a more diverse group of users to easily integrate gesture recognition into their own projects.",2013-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/GRTLogo.png,9775,--Choose Location,2016-12-05T00:16:37.060Z,['joep@media.mit.edu'],True,gesture-recognition-toolkit,2012-01-01,[],Gesture Recognition Toolkit,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:38.304Z,"Gestures Everywhere is a multimodal framework for supporting ubiquitous computing. Our framework aggregates the real-time data from a wide range of heterogeneous sensors, and provides an abstraction layer through which other ubiquitous applications can request information about an environment or a specific individual. The Gestures Everywhere framework supports both low-level spatio-temporal properties, such as presence, count, orientation, location, and identity; in addition to higher-level descriptors, including movement classification, social clustering, and gesture recognition.",2014-09-01,['responsive-environments'],,9783,--Choose Location,2016-12-05T00:16:26.302Z,['joep@media.mit.edu'],True,gestures-everywhere,2012-09-01,[],Gestures Everywhere,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:38.379Z,"Gestures [Natasha Sinha and Hugo Sol�s] is composed for six Music Shapers, two trumpets, two trombones, two violas, and one double bass. In this piece, the children use the Shapers to improvise and create audio gestures in response to the sound environment produced by the acoustic instruments. The Shaper's sounds are in turn derived from the acoustic. Gestures is a celebration of collaborative improvisation that conveys excitement, uncertainty and joy, and takes players and listeners on a journey to discover the complex yet simple roots of creativity. The compositional process of Gestures was itself quite unusual in at least two ways. First, the piece is the result of close collaboration between a graduate student specialized in electronic music composition and a 12-year-old composer. Second, the process and techniques involved in its creation were diverse and enjoyable, designed to encourage both creators to think in new ways. Drawing, singing, improvising and formalizing audio gestures were some of the methods employed to build the score. In fact, the unique experiences of two composers from different backgrounds, working and learning from each other, served as a basis for imagining the piece itself. Shared interests were passion to explore non-traditional techniques for the acoustical instruments, the desire to make Shapers an idiomatic instrument, the use of space as compositional element, and ultimately the desire to create an acoustic frame where freedom was the fundamental spirit for composers, musicians, young participants, and audience alike.",2003-09-01,['opera-of-the-future'],,9791,E15-443C,2016-12-05T00:16:08.542Z,['tod@media.mit.edu'],True,gestures-an-interactive-music-piece-for-acoustical-ensemble-and-shapers,2003-01-01,[],Gestures: An Interactive Music Piece for Acoustical Ensemble and Shapers,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:38.460Z,"Getting Hands-On with Soft Circuits is a set of instructional materials which seeks to expose middle and high school students to the creative, expressive, and computationally engaging domain of e-textiles. Engaging in hands-on activities, such as creating soft, electronic textile (e-textile) circuits, is one promising path to building self-efficacy and scientific understanding � both of which can have a dramatic impact on diversity in the field of computing. The instructional materials include a workshop activity guide and an accompanying kit of low-cost craft and electronic components. ",2012-01-01,['high-low-tech'],https://dam-prod2.media.mit.edu/x/files/Display/SClogo.png,9799,,2016-12-05T00:16:26.336Z,['leah@media.mit.edu'],True,getting-hands-on-with-soft-circuits,2010-09-01,[],Getting Hands-On with Soft Circuits,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:38.733Z,"What if you could see what the past looked like from where you are standing? What if you could relive any event that happened at your current location? Rather than just reading about an event, we want to be immersed in it and experience it ourselves. Ghosts of the Past allows you to create, save, and geotag panoramic canopies. Anyone who subsequently visits that space can see what you have seen, joining with you to create time-lapsed socialization. Since each canopy is time-stamped and geotagged, it gives the user an anchor in space while they explore history. Any event, special or mundane, can be captured for anyone in the same location to view. QR codes are posted in building locations with an active canopy.",2012-01-01,"['information-ecology', 'viral-communications']",,9831,,2016-12-05T00:16:26.419Z,"['holtzman@media.mit.edu', 'lip@media.mit.edu']",True,ghosts-of-the-past,2011-01-01,[],Ghosts of the Past,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:38.889Z,"GI Mobile is a mobile companion to the Media Lab Glass Infrastructure system. It incorporates the MessageMe messaging system to deliver a suite of location-aware features that complement the Glass Infrastructure. These include locating others in the Lab, browsing projects physically near you, and sending location-based messages. In addition, GI Mobile will alert you when you pass by projects you may be interested in based on what projects you have ""liked.""",2016-08-31,['digital-intuition'],,9847,,2021-07-14T15:34:08.863Z,['havasi@media.mit.edu'],True,gi-mobile,2013-01-01,[],GI Mobile,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:38.964Z,"An animated GIF is a magical thing. It has the power to compactly convey emotion, empathy, and context in a subtle way that text or emoticons often miss. GIFGIF is a project to combine that magic with quantitative methods. Our goal is to create a tool that lets people explore the world of GIFs by the emotions they evoke, rather than by manually entered tags. A web site with 200,000 users maps the GIFs to an emotion space and lets you peruse them interactively.",2018-06-01,"['viral-communications', 'collective-learning']",https://dam-prod2.media.mit.edu/x/files/Display/GXz4o3rY.png,9855,--Choose Location,2020-08-24T18:07:44.933Z,"['hidalgo@media.mit.edu', 'trich@media.mit.edu', 'kzh@media.mit.edu', 'lip@media.mit.edu']",True,gifgif,2014-01-01,[],GIFGIF,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:39.053Z,"In this research, a proactive emotional health system, geared toward supporting emotional self-awareness and empathy, was built as a part of a long-term research plan for understanding the role digital technology can play in helping people to reflect on their beliefs, attitudes, and values. The system, G.I.R.L.S. (Girls Involved in Real-Life Sharing), allows users to reflect actively upon the emotions related to their situations through the construction of pictorial narratives. The system employs common-sense reasoning to infer affective content from the users' stories and support emotional reflection. Users of this new system were able to gain new knowledge and understanding about themselves and others through the exploration of authentic and personal experiences. Currently, the project is being turned into an online system for use by school counselors.",2010-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/girlstalk.gif,9863,E15-443,2016-12-05T00:17:13.033Z,['picard@media.mit.edu'],True,girls-involved-in-real-life-sharing,2006-09-01,[],Girls Involved in Real-Life Sharing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:39.136Z,"Today's digital typography is more or less based on the format defined by movable type, with only a few parametric innovations. It has gone a long way, and surmounted many obstacles, to have similar visual features as printed type. At the same time, people are writing less by hand, and the ancient art of handwriting�which was used and necessary to express many personal attributes�is dying. We are exploring ways to keep written, personal expression a part of our digital life, as well as methods for our personal communication to continue to inherently express our personality, status, and current emotions. For example, in our digital lives, what will a signature look like, or a quickly scribbled love note? We are working with a new typographic format and experimenting with input devices inherent to human expression, such as gesture, voice, and body motion, to address these questions.",2010-01-01,['design-ecology'],,9871,E15-301,2016-12-05T00:17:13.090Z,['dsmall@media.mit.edu'],True,giving-character-to-characters,2009-01-01,[],Giving Character to Characters,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:39.215Z,"This work will present case studies of how people can build understanding of some ideas in motion control by mediating their experiential knowledge about body motion using interactive sensing and control computational environments. The computational tools are designed to attempt to draw upon how people express their understanding of the physical experience and create bridges to more formal understanding and representation of the phenomena. The goal of this case study is to demonstrate that although the essential importance of formal representation cannot be doubted, it can be shown that expert practices, in fact, involve other non-formal forms of knowledge as well. This awareness, in a learning context, implies that a deep understanding of formal knowledge is developed not only by learning the formal descriptions in isolation, but by incorporating them with other forms of understanding as well.",2005-09-01,[],,9879,E15-368,2016-12-05T00:16:26.506Z,['cavallo@media.mit.edu'],True,giving-the-head-a-hand,2005-09-01,[],Giving the Head a Hand,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:39.377Z,"We address two critical elements of news: that it informs, and that it is trustworthy. Glance creates dynamic, real-time, semantic control over news presentation that reveals the inherent slant that underlies coverage of an event. The goal is to empower readers to understand their news intake through a visualization of metadata that empowers readers to choose their news source based on computed metrics rather than sensationalized headlines. Relevant additional information, such as sentiment of text and public reaction, is gathered on each topic to further give readers a richer news-scape.",2015-01-01,"['ultimate-media', 'viral-communications']",,9895,--Choose Location,2016-12-05T00:16:26.560Z,"['vdiep@media.mit.edu', 'lip@media.mit.edu']",True,glance,2014-09-01,[],Glance,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:39.624Z,"As part of a Google-sponsored Glass developer event, we created a Glass-enabled improv comedy show together with noted comedians from ImprovBoston and Big Bang Improv. The actors, all wearing Glass, received cues in real time in the course of their improvisation. In contrast with the traditional model for improv comedy, punctuated by ""freezing"" and audience members shouting suggestions, using Glass allowed actors to seamlessly integrate audience suggestions. Actors and audience members agreed that this was a fresh take on improv comedy. It was a powerful demonstration that cues on Glass are suitable for performance: actors could become aware of the cues without having their concentration or flow interrupted, and then view them at an appropriate time thereafter.",2015-09-30,['fluid-interfaces'],,9919,--Choose Location,2017-09-05T19:36:08.241Z,"['pattie@media.mit.edu', 'swgreen@media.mit.edu']",True,glassprov-improv-comedy-system,2014-01-01,[],GlassProv Improv Comedy System,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:39.721Z,"Every country has a brand, negative or positive, and that brand is mediated in part by its global press coverage. We are measuring and ranking the perceptions of the 20 most populous countries by crowdsourcing those perceptions through a ""World News Quiz."" Quiz-takers match geographically vague news stories to the countries they think they occurred in, revealing how they positively or negatively perceive them. By illustrating the way these biases manifest among English and Chinese speakers, we hope to help news consumers and producers be more aware of the incomplete portrayals they have internalized and propagated.",2015-09-15,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/20/Global_Brands_img.png,9927,--Choose Location,2016-12-05T00:16:26.667Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'erhardt@media.mit.edu']",True,global-brands,2012-09-01,['civic-media'],Global Brands,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:09:39.808Z,"We are developing a system of early literacy apps, games, toys, and robots that will triage how children are learning, diagnose literacy deficits, and deploy dosages of content to encourage app play using a mentoring algorithm that recommends an appropriate activity given a child's progress. Currently, over 200 Android-based tablets have been sent to children around the world; these devices are instrumented to provide a very detailed picture of how kids are using these technologies. We are using this big data to discover usage and learning models that will inform future educational development.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/files/Display/bigsmile.jpg,9935,--Choose Location,2017-06-05T16:10:26.858Z,"['cynthiab@media.mit.edu', 'dnunez@media.mit.edu', 'tinsley@media.mit.edu']",True,global-literacy-tablets,2011-09-01,[],Global Literacy Tablets,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:39.892Z,GlobalMind is a network of multilingual/multicultural common-sense databases. Now we present how GlobalMind can enhance the communication between different cultures and different languages. Intercultural Assistant is a mobile application of GlobalMind.,2007-01-01,['software-agents'],,9943,E15-309,2016-12-05T00:16:26.690Z,[],True,globalmind-intercultural-assistant,2006-01-01,[],GlobalMind: Intercultural Assistant,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:39.970Z,"Globetoddler is a new project that focuses on remote awareness and shared experiences from children's point of view. The project aims to develop a technology and software solution that makes it easier, as well as more fun and rewarding, for young children (3-6 years) to interact with their remote, traveling parents. Newly conducted interviews indicate a need for a system with very different interfaces and interaction styles on the the two sides: on the parent's side the system consists of a fairly simple mobile application, whereas the child's side consists of a combination of a physically interactive toy interface and a guiding virtual avatar.",2008-01-01,['living-mobile'],,9951,E15-384C,2016-12-05T00:16:26.632Z,['geek@media.mit.edu'],True,globetoddler,2007-09-01,[],Globetoddler,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:40.053Z,"Globuddy is a common-sense-based application that helps English-speakers  to communicate in a foreign language. In addition to translating text, Globuddy provides relevant words and concepts for the traveler's specific situation. Common-sense reasoning is done with OMCSNet. Because users need a translator when they are on the street and away from their computers, we are using handheld devices like cell phones and PDAs.",2006-09-01,['software-agents'],,9959,E15-383,2016-12-05T00:16:26.710Z,['lieber@media.mit.edu'],True,globuddy,2002-09-01,[],Globuddy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:40.220Z,"Glowdoodle is free software for painting with light.  In front of your webcam, just move a a glowing object, or anything brightly colored, and see the traces appear on the screen in real time. Then participate in the worldwide Glowdoodle community by sharing your creations on the web.",2010-01-01,['lifelong-kindergarten'],,9975,LEGO Learning Lab,2016-12-05T00:16:08.568Z,"['mres@media.mit.edu', 'ericr@media.mit.edu']",True,glowdoodle,2009-01-01,[],Glowdoodle,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:40.297Z,"Glume is a computationally enhanced translucent modeling medium which offers a generalized, modular, scalable platform with the physical immediacy of a soft and malleable tangible material. The Glume system consists of soft and translucent augmented interlocking modules (each embedded with a full-spectrum LED) which communicate capacitively to their neighbors to determine a network topology and are responsive to human touch. We envision Glume as a viable tool for modeling, visualization, and simulation of three-dimensional data sets in which users construct and manipulate models whose morphology is determined through the distributed system. The Glume system provides a novel means for expressing and investigating organic forms and processes not possible with existing materials by relaxing the rigidity of structure in previous solid building-block approaches.",2005-09-01,['tangible-media'],,9983,E15-344,2016-12-05T00:16:26.785Z,['ishii@media.mit.edu'],True,glume,2005-01-01,[],Glume,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:40.373Z,"Wearable devices and ambient displays need to atomize video to evocative excerpts as glanceable as a still image. Glyph is a web-based tool for generating expressive GIFs from video. The tool integrates scene detection, video stabilization, video manipulation, and loop detection into a simple, web-based authoring interface. Glyph allows for creating GIFs from video with more editorial control than just choosing a clip's start and end time�diminishing some regions of movement in the clip, and highlighting others; erasing a jarring jump between the start and end of the GIF; imbuing a still image with just enough dynamism to hold our eyes and pique our interest. The result is a subtle, dynamic moving image that's lightweight, transmissible, and immediately engaging.",2015-01-01,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/Screenshot%202015-03-26%2007.50.29.png,9991,--Choose Location,2016-12-05T00:16:26.820Z,['lip@media.mit.edu'],True,glyph,2014-09-01,[],Glyph,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:40.454Z,"Consumer electronics devices are becoming more complicated, intimidating users. These devices do not know anything about everyday life or human goals, and they show irrelevant menus and options. We are building Roadie, a system with knowledge about the user's intentions to help the device to display relevant information to reach the user's goal. For example, a DVD player might suggest a sound configuration based on the movie it is playing. This will lead to more human-like interactions with these devices. We have constructed a Roadie interface to real consumer electronics devices: a television, set top box, and smart phone. The devices communicate over Wi-Fi, and use the UPnP protocols.",2014-09-01,['software-agents'],,9999,E15-384A,2022-06-10T14:18:57.450Z,['lieber@media.mit.edu'],True,goal-oriented-interfaces-for-consumer-electronics,2004-09-01,[],Goal-Oriented Interfaces for Consumer Electronics,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:40.532Z,"Currently each app lives in its own little world, with its own interface. Apps are usually unable to communicate with each other and unable to cooperate to meet users' needs. This project intends to enable end-users to ""program"" their phones using natural language and speech recognition to perform complex tasks. A user, for example, could say: ""Send the song I play most often to Bill."" The phone should realize that an MP3 player holds songs; that the MP3 app has a function to order songs by play frequency; how to send a file to another user; and how to look up the user's contact information. We use state-of-the art natural language understanding, common-sense reasoning, and a partial-order planner.",2014-09-01,['software-agents'],,10007,,2022-06-10T14:18:57.461Z,"['lieber@media.mit.edu', 'kdinakar@media.mit.edu', 'cfry@media.mit.edu']",True,goal-oriented-interfaces-for-mobile-phones,2011-01-01,[],Goal-Oriented Interfaces for Mobile Phones,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:40.616Z,"A novice search-engine user may find searching the Web for information difficult and frustrating because he may naturally express search goals rather than the topic keywords search engines need. GOOSE (Goal-Oriented Search Engine Interface) is an adaptive search-engine interface that uses natural-language processing to parse a user�s search goal, and common-sense reasoning to interpret this goal and reason from it an effective query. For a source of common-sense knowledge, we use the Open Mind Common Sense Project. While we cannot be assured of the robustness of the common-sense inference, in a substantial number of cases GOOSE is more likely to satisfy the user's original search goals than simple keywords or conventional query.",2007-01-01,['software-agents'],,10015,E15-383,2016-12-05T00:17:13.244Z,['lieber@media.mit.edu'],True,goal-oriented-web-search-user-interfaces,2002-01-01,[],Goal-Oriented Web Search User Interfaces,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:40.780Z,"The GoGo board provides a simple interface for computers to interact with their surrounding environments. Projects using the GoGo board include various sensing and control applications, games, and environmental sensing. It allows students to be engaged in the design and implementation of solutions to a problem by augmenting locally found, low-cost materials with digital technology. The GoGo board has been designed and used in schools in S�o Paulo, and a network of teachers, students, and other users of the GoGo board is being created. The simplicity of the GoGo board design opens up possibilities for learners to create or modify the board for their particular needs. The research goal is to study the factors, both technical and social, needed to support communities of not-so-technical people to participate in the process of designing the tools that they want.",2001-01-01,[],,10031,E15-443C,2016-12-05T00:17:13.299Z,['cavallo@media.mit.edu'],True,gogo-board-a-personalized-technology-toolset-for-learning,2001-01-01,[],GoGo Board: A Personalized Technology Toolset for Learning,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:40.855Z,"How do people who lead communities on online platforms join together in mass collective action to influence platform operators? Going Dark analyzes a protest against the social news platform reddit by moderators of 2,278 communities in July of 2015. These moderators collectively disabled their communities, preventing millions of readers from accessing major parts of reddit and convincing the company to negotiate over their demands. This study reveals social factors—including the work of moderators, relations among moderators, relations with platform operators, factors within communities, and the isolation of a community—that can lead to participation in mass collective action against a platform.",2020-08-01,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/files/5788/21282569170_0b901afa6f_c.jpg,10039,--Choose Location,2022-06-10T13:29:45.195Z,['jnmatias@media.mit.edu'],True,going-dark-collective-action-in-the-reddit-blackout,2015-01-01,[],Going Dark: Collective action in the Reddit Blackout,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:09:40.937Z,"When friends give directions, they often don't describe the whole route, but instead provide landmarks along the way that they think will be familiar. Friends can assume we have certain knowledge because they know our likes and dislikes. Going My Way attempts to mimic a friend by learning about where you travel, identifying the areas that are close to the desired destination from your frequent path, and picking a set of landmarks to allow you to choose a familiar one. When you select one of the provided landmarks, Going My Way will provide directions from it to the destination.",2009-09-01,['living-mobile'],,10047,E15-384C,2016-12-05T00:16:26.894Z,['geek@media.mit.edu'],True,going-my-way,2007-01-01,[],Going My Way,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:41.097Z,"GoodApp is a cloud environment for the community development of socially responsible and transparent Web applications. It provides a shared infrastructure and pedagogy for the development of civic or ""good"" apps. This infrastructure includes educational components, developmental tools (editing, collaboration, versioning, and visualization), and simple mechanisms for deployment. Conceptually, we seek to understand the technical similarities of, and the sustainability challenges for, civic applications.",2010-01-01,['computing-culture'],,10055,E15-001,2016-12-05T00:17:13.323Z,['csik@media.mit.edu'],True,goodapp,2009-01-01,[],GoodApp,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:41.265Z,"Got Sleep? is an Android application to help people to be aware of their sleep-related behavioral patterns and tips about how they should change their behaviors to improve their sleep. The application evaluates people's sleep habits before they start using the app, tracks day and night behaviors, and provides feedback about what kinds of behavior changes they should make and whether the improvement is achieved or not. ",2014-09-01,"['advancing-wellbeing', 'affective-computing']",,10063,--Choose Location,2016-12-05T00:16:26.921Z,"['picard@media.mit.edu', 'akanes@media.mit.edu']",True,got-sleep,2012-01-01,[],Got Sleep?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:41.561Z,"GrabIt allows users to collaboratively explore the principles of power generation, transfer, and consumption via  tangible wooden sticks. Pushing on one stick generates power that is transfered to other sticks in real time allowing users to see, and feel, the effects of losses and electrical loading. All energy in the GrabIt system is derived from its users and therefore makes the mapping between input, output, and losses transparent. Variable-color light bulbs can also be connected to the system to serve as simplified voltage indicators during use.",2008-01-01,['tangible-media'],,10079,E15-344,2016-12-05T00:16:26.848Z,['ishii@media.mit.edu'],True,grabit,2007-01-01,[],GrabIt,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:41.706Z,"Graffiti Codes transform the space around you into a mobile-readable environment.   Anyone can draw a simple shape on anything, like graffiti, and the mobile device reads it by simply tracing the outline.  It's a human-created VR code. This work diverges from the camera-scanning model and uses accelerometer-based paths to unlock data. Where a QR code cannot be easily generated in the field, Graffiti Codes only require a marker and a surface.",2014-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/LogoGC.png,10087,--Choose Location,2016-12-05T00:16:26.946Z,"['jlrubin@media.mit.edu', 'lip@media.mit.edu']",True,graffiti-codes,2012-09-01,[],Graffiti Codes,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:41.847Z,"GrandChair elicits, records, and plays back grandparents' stories within an interaction model based on face-to-face conversation. Its environment is designed to be comfortable and story-evoking: tellers sit in a comfortable rocking chair and tell stories with the assistance of an attentive child conversational agent.",2001-12-30,['gesture-and-narrative-language'],,10095,E15-320,2016-12-05T00:17:13.365Z,[],True,grandchair,1999-12-31,[],GrandChair,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:42.110Z,"This project explores how modern graphical interface techniques and explicit support for the user's problem-solving activities can make more productive interfaces for debugging, which accounts for half the cost of software development. Animated representations of code, a reversible control structure, and instant connections between code and graphical output are some of the techniques used.",2014-09-01,['software-agents'],,10111,E15-383,2022-06-10T14:18:57.478Z,['lieber@media.mit.edu'],True,graphical-interfaces-for-software-visualization-and-debugging,2000-01-01,[],Graphical Interfaces for Software Visualization and Debugging,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:42.223Z,"We are working on learning agents that can automate semi-repetitive graphical procedures by watching a user perform the procedures on concrete visual examples. The agent is embedded in a graphical editor that records user actions, and uses machine learning techniques to generalize the procedure. The generated procedure can be used in new situations that are similar to, but not necessarily exactly the same as, the original. The user can give advice to the system by drawing graphical annotations that express information like part/whole relations, or can use speech recognition to explain actions verbally.",2003-12-30,['software-agents'],,10119,,2016-12-05T00:17:13.406Z,['lieber@media.mit.edu'],True,graphics-by-example,1999-12-31,[],Graphics by Example,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:42.334Z,"With the Marketing and the Placebo Effect project, we are investigating how various economic, cultural, and social factors such as pricing, branding, or stigma could affect how people will experience their medications.",2009-09-01,[],,10127,E15-485A,2016-12-05T00:16:27.036Z,[],True,grassroots-mapping,2008-09-01,[],grassroots mapping,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:09:42.437Z,"Grassroots Mapping (grassrootsmapping.org) is a participatory mapping project involving communities in cartographic dispute. This January, we worked with a series of organizations and communities to produce maps with children and adults from several communities in Lima, Peru. Seeking to invert the traditional power structure of cartography, participants used helium balloons and kites to loft their own community satellites made with inexpensive digicams. The resulting images, owned by the residents, are georeferenced and stitched into maps which are 100x higher resolution that those offered by Google, at extremely low cost. In some cases these maps may be used to support residents claims to land title. By creating open-source tools to include everyday people in exploring and defining their own geography, Warren hopes to enable a diverse set of alternative agendas and practices, and to emphasize the fundamentally narrative and subjective aspects of mapping over its use as a medium of control.",,[],https://dam-prod2.media.mit.edu/x/files/Display/4445931166_2a592341e0_b.jpg,10135,,2016-12-05T00:17:13.445Z,['dsmall@media.mit.edu'],True,grassroots-mapping-with-balloons-and-kites,2010-09-01,[],Grassroots Mapping with Balloons and Kites,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:42.632Z,"Wireless sensor systems are currently being deployed in a wide variety of lightweight mobile applications such as detecting degenerative diseases, monitoring remote habitats, and tracking the safety of housebound elders. However, current implementations suffer from short lifespans due to high energy use and limited battery size. To enter the consumer mainstream, these applications must be far more energy efficient. This project explores adaptable sensor-driven power management for wireless sensor systems as a means of increasing efficiency. Rather than fully activate nodes on a schedule or in response to very simple stimuli, this project explores an automated framework that we term ""groggy wakeup,"" where the system is activated at increasing energy levels in response to evolving stimuli. This way, the system only becomes fullly awake when an interesting phenomenon is encountered, and resources are appropriately conserved.",2007-09-01,['responsive-environments'],,10151,E15-344,2016-12-05T00:16:37.343Z,['joep@media.mit.edu'],True,groggy-wakeup-an-automated-framework-for-power-efficient-detection-in-smart-sensor-systems,2002-01-01,[],Groggy Wakeup: An Automated Framework for Power-Efficient Detection in Smart Sensor Systems,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:42.726Z,"Language is grounded in experience. Unlike dictionaries which define words in terms of other words, humans understand many basic words in terms of associations with sensory-motor experiences. People must interact physically with their world to grasp the essence of words like ""red,"" ""heavy,"" and ""above."" Abstract words are acquired only in relation to more concretely grounded terms. Grounding is thus a fundamental aspect of spoken language, which enables humans to acquire and to use words and sentences in context. We are developing an interactive robot which learns and understands spoken langauge via multisensory grounding and robotic embodiment. The robot is designed with six degrees of freedom and has auditory, visual, proprioceptive, tactile, and balance sensors. This system will serve as a test bed for experiments in acquiring and understanding elementary semantics and syntax of spoken language. Our goals are two-fold. First, we are interested in using computational models to gain insights into how humans process language. By building and testing models with realistic data, we are able to test theories which are difficult to assess using traditional methods based on observation and analysis. Second, we hope to build a new generation of spoken language interfaces with richer semantic representations leading to more intelligent machine behavior.",2001-12-30,['social-machines'],,10159,E15-384,2016-12-05T00:17:13.505Z,['dkroy@media.mit.edu'],True,grounded-language-learning-and-understanding,1999-12-31,"['archives', 'history']",Grounded Language Learning and Understanding,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:42.911Z,"Understanding spatial language is a challenging problem that requires the ability to map between language and real-world situations. We are building a spatial language understanding system that bridges this representational gap by computationally modeling the semantics of spatial prepositions. Our model enables a system to retrieve video clips that match natural language queries such as ""Show me people going across the kitchen."" We are also applying it to build robots that can follow natural-language directions such as ""Go through the door near the elevators."" By using corpus-based machine learning techniques, our model is robust to real-world noise and linguistic variation. Exploring the connection between language and the real world in concrete domains enables us to make progress towards computers that understand language in human-like ways.",2010-01-01,['social-machines'],,10175,E15-441,2016-12-05T00:17:13.561Z,['dkroy@media.mit.edu'],True,grounding-spatial-language-for-video-retrieval-and-robotic-direction-following,2008-01-01,[],Grounding Spatial Language for Video Retrieval and Robotic Direction Following,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:43.001Z,"We are developing knowledge representations and natural language processing algorithms that connect spatial language to physical environments. Applications include spatial language understanding for robots, computer games, and natural language interfaces for video retrieval.",2006-01-01,['social-machines'],,10183,E15-483,2016-12-05T00:16:37.240Z,['dkroy@media.mit.edu'],True,grounding-spatial-language-robots-and-maps,2004-09-01,[],Grounding Spatial Language: Robots and Maps,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:43.076Z,"The meanings of simple verbs such as ""pick up"" convey more information than meets the eye. In order to pick up an object, a person much reach towards it, touch it, grasp it, and then lift it. To know what ""pick up"" means is to know all of these things. From this kind of word meaning comes common-sense knowledge such as ""you can't pick something up without touching it."" Machines today lack this depth of understanding of verbs. In this project, we are exploring new ways to represent the semantics of verbs so that machines can process and understand sensory-grounded meanings of natural language in human-like ways. Our basic approach is to connect verbs to non-linguistic representations based on the sensory and motor systems of physical robots. Using Ripley, our 7-degree-of-freedom manipulator robot, we have designed a system that learns to recognize gestures and the relations between gestures. Data is recorded by allowing a human operator to move the compliant robot through motions such as ""pick up"" and ""move toward."" A Hidden Markov Model learning algorithm is then trained on the data generated by these gestures, which results in a structured sequential representation of each motion. Each sequential component can then be related to components of other gestures, thus enabling the system to acquire such relations as ""[pick up] is composed of [move toward], [close gripper], and [retract]."" We are thus able to derive relations between word meanings from relations of underlying sensory-motor structures. We believe that this kind of connection between language and non-linguistic knowledge is an essential step toward intelligent language processing and understanding by machines.",2003-01-01,['social-machines'],,10191,E15-483,2016-12-05T00:17:13.609Z,['dkroy@media.mit.edu'],True,grounding-the-meaning-of-verbs-through-structured-motor-control-representations,2001-01-01,[],Grounding the Meaning of Verbs through Structured Motor Control Representations,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:43.155Z,"GroundTruth is a dynamic online map of changing topographic structures (roads, checkpoints, barriers, and settlements) and human narratives in the Occupied Palestinian Territory. This community-driven map is populated with multiple data layers collected by human-rights organizations and a network of informed inhabitants. This community and crisis mapping effort aims to unite disparate datasets and provide a more comprehensive picture of changing restrictions and events. We believe this �ground truth,� correlating spatial mapping with daily human experience, is crucial to understanding the changing nature of the occupation and enabling cooperative action by organizations and individuals in the region.",2010-01-01,['computing-culture'],,10199,E15-001,2016-12-05T00:16:27.125Z,['csik@media.mit.edu'],True,groundtruth,2009-01-01,[],GroundTruth,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:43.317Z,"GroupLoop is a browser-based, collaborative audio feedback control system for musical performance. Upon logging in, GroupLoop users send their microphone stream to other participants while simultaneously controlling the mix of other users' streams played through their speakers. Complex feedback loops involving several participants are possible by working together—in some cases, multiple feedback paths may overlap and interact. Users are able to shape the feedback sounds in real time by adjusting delay, EQ, and gain, as well as by manipulating the acoustics of their portion of the audio feedback path. GroupLoop is capable of diverse and unexpected sounds, immeasurable reconfigurability, and in some cases, unrepeatable complexity. It creates new topologies for collaboration in performance, and invites thoughtful reflection on future topologies for real-time music collaboration over distance.Try it at https://feedback.davidbramsay.com.",2015-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2016/10/18/grouploop_W5CLNMr.gif,10215,--Choose Location,2016-12-05T00:17:13.639Z,"['dramsay@media.mit.edu', 'joep@media.mit.edu']",True,grouploop-a-collaborative-network-enabled-audio-feedback-instrument,2015-09-01,[],"GroupLoop: A Collaborative, Network-Enabled Audio Feedback Instrument",PUBLIC,https://feedback.davidbramsay.com,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:43.697Z,"In this project we are seeking to define a unique new approach to the challenges of artificial intelligence, one that draws its inspiration from the biological phenomenon of development. Our central contention is that by understanding the processes by which animals ""grow up"" cognitively, we will be able to come much closer to our ultimate goal of creating virtual creatures that are as clever and adaptable as their real-life counterparts. After a careful analysis of the biological literature, we have implemented a novel behavior architecture for our virtual creatures, one that is grounded in what we see as the key computational lessons of development. Foremost amongst these lessons is the concept of Specialized Learning Tools, a domain-specific means of organizing the learning of our creatures so as to efficiently manage and integrate diverse competencies in a unified behavioral context.",2004-01-01,['synthetic-characters'],,10239,E15-468,2016-12-05T00:16:08.650Z,[],True,growing-up-virtual-the-computational-lessons-of-development,2003-01-01,[],Growing Up Virtual: The Computational Lessons of Development,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:09:43.881Z,"The electro-holographic video display (Holovideo) project has spanned over decades, beginning back in 1989, and continues to be a main research topic within the Object-Based Media research group. As new technologies and manufacturing processes have become available, Holovideo has been continuously improved to create a better, more efficient holographic display. In our team’s most recent iteration, we are now utilizing femtosecond laser fabrication to develop flat-panel, transparent holographic video displays suitable for both wearable see-through lenses and monitor sized displays.As with previous versions of Holovideo, the system uses acoustic waves to modulate laser light in order to create a holographic image. In the past, this required specialized, and sometimes bulky, optical components in order to guide the laser light to the viewer in order to give the 3D image effect. Now, using a combination of femtosecond laser fabrication and surface acoustic waves (SAWs), many of the required optics can be “printed” directly within the lithium niobate substrate. These full-color modulators support hundreds of thousands of pixels per scan line, making them suitable for fixed or wearable holographic displays.By using femtosecond laser fabrication, we are now able to perform rapid prototyping of optical components, greatly cutting down fabrication time and costs while also allowing full customization of optical components.Follow the links below to see previous versions of Holovideo.",2019-10-01,"['ultimate-media', 'ce-20', 'future-storytelling', 'object-based-media']",https://dam-prod2.media.mit.edu/x/2019/04/17/Holovideo_laser2.jpg,10247,Garden Conference Room,2020-06-10T16:03:09.552Z,"['vmb@media.mit.edu', 'sjolly@media.mit.edu', 'nsavidis@media.mit.edu', 'bdatta@media.mit.edu', 'tjschoep@media.mit.edu', 'vparth@media.mit.edu']",True,guided-wave-light-modulator-for-holographic-video,2005-09-01,[],Guided-wave light modulator for holographic video,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:44.072Z,"GuideShoes is a wearable system that explores how aesthetic forms of expression can deliver both an aesthetic experience and background information simultaneously while walking around a city. Incorporating GPS, wireless spread-spectrum technology, and a MIDI synthesizer, the system tracks the user, and selects and assembles small musical patterns dynamically dependent on the user's progress. Statistically significant results of user testing have shown that users that listened to the musical cues provided by the GuideShoes system were able to extract navigational signals, such as ""continue"" or ""you made a wrong turn"" from the musical experience.",1999-09-01,['interactive-cinema'],,10255,E15-368,2016-12-05T00:16:27.176Z,['gid@media.mit.edu'],True,guideshoes,1997-01-01,[],GuideShoes,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:44.217Z,"Guiding Light is a navigation-based application that provides directions by projecting them onto physical spaces both indoors and outdoors. It enables a user to get relevant spatial information by using a mini projector in a cell phone. The core metaphor involved in this design is that of a flashlight which reveals objects in and information about the space it illuminates. For indoor navigation, Guiding Light uses a combination of e-compass, accelerometer, proximity sensors, and tags to place information appropriately. In contrast to existing heads-up displays that push information into the user's field of view, Guiding Light works on a pull principle, relying entirely on users' requests and control of information.",2012-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/guidinglight.jpg,10263,E15-368,2016-12-05T00:16:27.328Z,['geek@media.mit.edu'],True,guiding-light,2010-01-01,[],Guiding Light,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:44.351Z,"The goal of this project is to produce a guilt detector. We have created an experiment that is designed to produce feelings of guilt of varying levels in different groups while we record EKG and skin conductivity. By examining the differences in physiology across the conditions, we have
exlored how one might build a classifier to determine which condition, and thus which level of guilt, an individual is experiencing.",2005-09-01,['affective-computing'],,10271,Cube,2016-12-05T00:16:27.574Z,['picard@media.mit.edu'],True,guilt-detection,2004-09-01,[],Guilt Detection,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:09:45.082Z,"The tongue has extremely dense sensing resolution, as well as an extraordinary degree of neuroplasticity�the ability to adapt to and internalize new input. Research has shown that electro-tactile tongue displays paired with cameras can be used as vision prosthetics for the blind or visually impaired; users quickly learn to read and navigate through natural environments, and many describe the signals as an innate sense. However, existing displays are expensive and difficult to adapt. Tongueduino is an inexpensive, vinyl-cut tongue display designed to interface with many types of sensors besides cameras. Connected to a magnetometer, for example, the system provides a user with an internal sense of direction, like a migratory bird. Plugged into weighted piezo whiskers, a user can sense orientation, wind, and the lightest touch. Through Tongueduino, we hope to bring electro-tactile sensory substitution beyond vision replacement, towards open-ended sensory augmentation.",2013-09-01,['responsive-environments'],,10311,--Choose Location,2016-12-05T00:17:13.691Z,"['gershon@media.mit.edu', 'joep@media.mit.edu']",True,hackable-high-bandwidth-sensory-augmentation,2011-09-01,[],"Hackable, High-Bandwidth Sensory Augmentation",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:45.225Z,"Discourse on hackathons tends to emphasize projects and project creators rather than the events as a social practice within existing communities. Hackathons have a history as a community-building method for education and creation. More recently, institutions have used hackathons to invite conversation and design with groups affected by those institutions. This step towards broader participation is obfuscated by stories that focus on the creation of products and the lucky geniuses whose work is appropriated by institutions. Critiques of hackathons often accept the same assumptions, focusing on high-profile events, critiquing the small number of sustained projects, and questioning hackathons as a form of entrepreneurial free labor.",2015-01-01,['civic-media'],,10319,--Choose Location,2016-12-05T00:16:27.595Z,"['ethanz@media.mit.edu', 'jnmatias@media.mit.edu', 'bl00@media.mit.edu']",True,hackathonfaq,2013-09-01,[],HackathonFAQ,PUBLIC,,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:45.368Z,"In this project we investigate how the process of building a circuit can be made more organic, like sketching in a sketchbook. We integrate a rechargeable power supply into the spine of a traditional sketchbook, so that each page of the sketchbook has power connections. This enables users to begin creating functioning circuits directly onto the pages of the book and to annotate as they would in a regular notebook.  The sequential nature of the sketchbook allows creators to document their process for circuit design.  The book also serves as a single physical archive of various hardware designs.  Finally, the portable and rechargeable nature of the book allows users to take their electronic prototypes off of the lab bench and share their creations with people outside of the lab environment.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/sketchbook_pldb.JPG,10327,--Choose Location,2016-12-05T00:16:27.472Z,"['jieqi@media.mit.edu', 'joep@media.mit.edu']",True,hacking-the-sketchbook,2013-01-01,[],Hacking the Sketchbook,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:46.069Z,"Imagine a future where lights are not fixed to the ceiling, but follow us wherever we are. In this colorful world we enjoy lighting that is designed to go along with the moment, the activity, our feelings, and our outfits. Halo is a wearable lighting device created to explore this scenario. Different from architectural lighting, this personal lighting device aims to illuminate and present its user. Halo changes the wearer's appearance with the ease of a button click, similar to adding a filter to a photograph. It can also change the user's view of the world, brightening up a rainy day or coloring a gray landscape. Halo can react to activities and adapt based on context. It is a responsive window between the wearer and his or her surroundings.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2016/10/20/_MG_3047_tdqVyPC.JPG,10367,--Choose Location,2016-12-05T00:16:27.619Z,"['nanzhao@media.mit.edu', 'joep@media.mit.edu']",True,halo-wearable-lighting,2014-01-01,"['design', 'entertainment']",Halo: Wearable Lighting,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:46.337Z,"HandSCAPE is an orientation-aware digital tape measure used as an input device for digitizing field measurements, and visualizing the volume of the resulting vectors with computer graphics. Using embedded orientation-sensing hardware, HandSCAPE captures relevant vectors on each linear measurement and transmits this data wirelessly to a remote computer in real-time. Combining physical measuring and computer modeling as a seamless step enhances the efficiency of on-site measuring tasks with the addition of digital functionality. The potential utility of HandSCAPE is for application areas such as archeological surveys, interior design, and storage-space allocation. Thus, HandSCAPE is a tangible interface that provides seamless relationships between digital and physical space, and preserves human senses and skills.",2001-12-30,['tangible-media'],,10383,E15-441,2016-12-05T00:16:27.913Z,['ishii@media.mit.edu'],True,handscape,1997-12-31,[],HandSCAPE,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:46.472Z,"2D screens, even stereoscopic ones, limit our ability to interact with and collaborate on 3D data. We believe that an augmented reality solution, where 3D data is seamlessly integrated in the real world, is promising. We are exploring a collaborative augmented reality system for visualizing and manipulating 3D data using a head-mounted, see-through display, that allows for communication and data manipulation using simple hand gestures.",,"['terrestrial-sensing', 'fluid-interfaces']",,10391,--Choose Location,2019-04-17T20:10:59.187Z,"['pattie@media.mit.edu', 'kevinw@media.mit.edu']",True,handson-a-gestural-system-for-remote-collaboration-using-augmented-reality,2014-01-01,[],HandsOn: A gestural system for remote collaboration using augmented reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:46.589Z,"HandWave is a small, wireless, networked skin conductance sensor that can be worn or used in many different form factors. Skin conductance is the best known measure of arousal (whether emotional, cognitive, or physical) and this device makes it easy to gather this information from mobile users. Many existing affective computing systems make use of sensors that are inflexible and often physically attached to supporting computers. In contrast, HandWave allows an additional degree of flexibility by providing ad hoc wireless networking capabilities to a wide variety of Bluetooth devices as well as adaptive biosignal amplification. As a consequence, HandWave is useful in games, tutoring systems, experimental data collection, and augmented journaling, among other applications. The Handwave builds on the earlier Galvactivator project.",2006-09-01,['affective-computing'],,10399,Cube,2016-12-05T00:16:28.203Z,['picard@media.mit.edu'],True,handwave,2005-01-01,[],HandWave,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:46.991Z,"Happenstance is flexible storytelling testbed that expands the traditional literary and theatrical notions of place and situation to accommodate interactive, on-the-fly story construction. Important aspects of story content and context are made visible, tangible, and manipulable by systematically couching them within the metaphors of ecology, geology, and weather. Information-rich environments become conceptual landscapes that grow, change, and evolve over time and through use. Current information follows a natural cycle modeled on the Earth's water cycle. Older information, history, and complex conceptual constructs�built up by the flow of data over time�are manifested in the rock and soil cycles. Directed inquiries, explorations of theory, and activities associated with the audience's personal interests are captured and reflected by plant growth. In a process which mimics realistic physical modeling, metaphor is implemented as metaphysical systems operating on metadata. As a result, information itself is imbued with sets of systemic, semi-autonomous behaviors which allow it to move and act intelligently within the story world or other navigable information spaces. Interestingly, the operation of this trope-forming micro-world also resembles the internal psychology of characters.",2000-12-30,['interactive-cinema'],,10431,E15-427,2016-12-05T00:16:28.231Z,"['gid@media.mit.edu', 'beb@media.mit.edu']",True,happenstance,1996-12-31,[],Happenstance,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:47.172Z,"Computing systems and electronic devices often frustrate users by seeming to have a will of their own. Designers and engineers thus generally strive for the most unobtrusive, invisible systems possible; nevertheless, these systems are often inflexible when confronted with the complexities of the real world, and this inflexibility seems willful. The urge to make systems invisible means that opportunities for generative conflict�the kind of give and take that makes dealing with people so stimulating�are often ignored in device design. The Haptic Opposition (HO) project explores what happens when apparatuses claim their physical space, and looks for positive interactions that may happen with uncontrollable devices. Mechanical parts potentially moveable both by human beings and machine control are examined as a nexus of human-machine conflict. In a series of experimental installations, the HO project will consist of technical systems built with increasing free will and the power to enforce it; subtle distortions of a user's manipulation will grow into an autonomous behavior.",2004-01-01,['computing-culture'],,10447,E15-489,2016-12-05T00:16:28.357Z,['csik@media.mit.edu'],True,haptic-opposition,2002-01-01,[],Haptic Opposition,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:09:47.263Z,"
                    We have developed a set of ""optogenetic"" reagents: fully genetically encoded reagents that, when targeted to specific cells, enable their physiology to be controlled via light. To confront the three-dimensional complexity of the living brain, enabling the analysis of the circuits that causally drive or support specific neural computations and behaviors, our lab and our collaborators have developed hardware for delivery of light into the brain, enabling control of complexly shaped neural circuits, as well as the ability to combinatorially activate and silence neural activity in distributed neural circuits. We anticipate that these tools will enable the systematic analysis of the brain circuits that mechanistically and causally contribute to specific behaviors and pathologies. We distribute these tools as freely as possible, and host visitors regularly to learn how to use them .
                ",2014-09-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/files/uploads/optogeneticshardware.small.jpg,10455,E15-435,2016-12-05T00:17:13.769Z,"['esb@media.mit.edu', 'azorzos@media.mit.edu']",True,hardware-and-systems-for-control-of-neural-circuits-with-light,2006-09-01,"['bioengineering', 'health', 'neurobiology']",Optogenetics: Hardware Enabling Neural Control by Light,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:47.944Z,"SilverStringers is a community-centric approach to news coverage and presentation, tapping into the strength of the older generation to develop techniques for the next-generation coverage of cities and towns. Persons over the age of 50 have unparalleled wisdom about the communities where they have lived and/or worked. SilverStringers is intended to train and equip men and women in this age group to be reporters, photographers, illustrators, editors, and designers of a localized computer network. We are interested in enhancing grass-roots communications while at the same time discovering new models for media coverage. Often, communities have only an oral basis, but no other platform for information sharing, storytelling, and exchanging past experiences. Improved communication promotes and tightens relationships, and can even create bridges between generations. HDL is a tool that aims to provide such a platform, by taking advantage of the feedback obtained from the SilverStringers (and other users of its predecessor project, Pluto). Using XML/XSL, HDL offers the ability to deliver and create content for devices other than desktop computers, such as mobile phones, PDAs, and set-top boxes. Another new feature is the ability to include streaming video and audio content in the stories. The development of an HDL audio version is underway. The publications that are created will be regularly broadcast on specific days and times, can be 15 to 30 minutes long, and can have target audiences that are urban or rural.",2005-01-01,"['gray-matters', 'electronic-publishing']",,10535,E15-320B,2019-02-06T21:25:42.074Z,"['holtzman@media.mit.edu', 'walter@media.mit.edu', 'driscoll@media.mit.edu']",True,hdlsilverstringers,2001-01-01,[],HDL/Silverstringers,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:48.034Z,"HeadLock is a semi-automated system for head pose annotation that explores how human-computer interfaces can be combined with computer vision technologies to efficiently extract behavioral information from video recordings.  For images with limited resolution, the orientation of a head is often the best approximation for gaze direction, a crucial component to analyzing the rich interactions and behaviors of humans.  The goal of HeadLock is to reduce the cost of extracting head pose from video by several orders of magnitude by developing machine-perception technologies that can perform robust head pose estimation with minimal constraints on resolution and camera angle.",2008-09-01,['social-machines'],,10543,E15-441,2016-12-05T00:16:08.795Z,"['dkroy@media.mit.edu', 'decamp@media.mit.edu']",True,headlock-video-analysis-for-the-human-speechome-project,2007-09-01,[],HeadLock: Video Analysis for the Human Speechome Project,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:48.112Z,"We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: what billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, big data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.",,"['emerging-worlds', 'camera-culture']",,10551,--Choose Location,2019-04-19T17:52:03.224Z,"['raskar@media.mit.edu', 'ajdas@media.mit.edu']",True,health-tech-innovations-with-tata-trusts-mumbai,2016-01-01,[],"Health-tech innovations with Tata Trusts, Mumbai",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:48.274Z,"HealthMap is a multilingual, real-time disease outbreak tracking and visualization system. Launched in fall 2006, the Web site collects over 300 reports per day in seven languages, from both general news media and public health sources around the world. Updated hourly, the system filters these reports to determine relevance, disease, location, and duplication clustering by means of a series of custom-designed, automated, text-processing algorithms. Relevant reports are then aggregated and displayed on a freely available dashboard where users can tailor the view according to date, disease, location, and source. HealthMap provides an overview of real-time information on emerging infectious diseases, and has particular interest for public health officials and international travelers.",2010-01-01,['new-media-medicine'],https://dam-prod2.media.mit.edu/x/files/Display/logo-sm.png,10567,E15-320,2016-12-05T00:16:28.767Z,['fmoss@media.mit.edu'],True,healthmap,2006-09-01,[],HealthMap,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:48.354Z,"With HealthMap's Outbreaks Near Me application, you have all of HealthMap's latest real-time disease outbreak information at your fingertips. Open the app and see all current outbreaks in your neighborhood, including news about H1N1 influenza (""swine flu""). Search and browse outbreak reports on the interactive map, and set up the app to alert you with a notice automatically whenever an outbreak is occurring in your area. If you know of an outbreak not yet on the map, be the first to report it using the app's unique outbreak reporting feature. You will be credited and your report will be featured on the Website. With this iPhone app, we are launching an exploration of crowd-sourced, user-generated, people-powered disease outbreak tracking and collaboration.",2010-01-01,['new-media-medicine'],https://dam-prod2.media.mit.edu/x/files/Display/ipssphoto4.gif,10575,E15-320,2016-12-05T00:16:28.880Z,['fmoss@media.mit.edu'],True,healthmap-iphone-app-outbreaks-near-me,2009-01-01,[],HealthMap iPhone App: Outbreaks Near Me,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:48.435Z,"Hear&There is an augmented reality system using audio as the primary interface. Using the authoring component of this system, individuals can leave ""audio imprints,"" consisting of several layers of music, sound effects, or recorded voice, at a location outdoors. Using the navigation component, individuals can hear imprints by walking into the area that the imprint occupies. Furthermore, imprints can be linked together, whereby an individual is directed from one imprint to related imprints in the area.",2000-12-30,['sociable-media'],,10583,E15-449,2016-12-05T00:16:37.408Z,['judith@media.mit.edu'],True,hearthere,1997-12-31,[],Hear&There,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:48.522Z,"Hearplay provides a tool for sharing speech-annotated media on networked PocketPCs. It uses asynchronous audio to help us stay aware of what others in our community are doing, what they are interested in, and how available they are for real-time or offline interaction. The system provides a spontaneous and relevant audio ""broadcast"" to an affinity group of mobile users.",2007-01-01,['living-mobile'],,10591,E15-344,2016-12-05T00:16:28.540Z,"['swheeler@media.mit.edu', 'geek@media.mit.edu']",True,hearplay,2001-01-01,[],Hearplay,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:48.613Z,"We have created a system that allows a person's heart rate to be displayed on an ambient orb color display. This system uses a heart-rate sensor developed at the Media Lab. This system has many applications, including baby monitors, home health-care monitoring, company meeting tools, and novel user interfaces or games.",2008-09-01,['tangible-media'],,10599,E15-344,2016-12-05T00:16:28.904Z,"['ishii@media.mit.edu', 'picard@media.mit.edu', 'fletcher@media.mit.edu']",True,heart-rate-ambient-display,2007-01-01,[],Heart-Rate Ambient Display,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:48.705Z,"With our Ubiquitous Sonic Overlay, we are working to place virtual sounds in the user's environment, fixing them in space even as the user moves. We are working toward creating a seamless auditory display, indistinguishable from the user's actual surroundings. Between bone-conduction headphones, small and cheap orientation sensors, and ubiquitous GPS, a confluence of fundamental technologies is in place. However, existing head-tracking systems either limit the motion space to a small area (e.g., Oculus Rift), or sacrifice precision for scale using technologies like GPS. We are seeking to bridge the gap to create large outdoor spaces of sonic objects.",,['responsive-environments'],,10607,--Choose Location,2018-06-07T19:16:45.828Z,"['gershon@media.mit.edu', 'sfr@media.mit.edu', 'joep@media.mit.edu']",True,hearthere-ubiquitous-sonic-overlay,2014-09-01,['sensors'],HearThere: Ubiquitous Sonic Overlay,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:48.801Z,We are developing wearable sensors that measure cardiovascular parameters such as heart rate and heart rate variability (HRV) in real time. HRV provides a sensitive index of autonomic nervous system activity. These sensors will be capable of communication with mobile devices such as the iPhone and iPod Touch.,2010-09-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/heartphones_PLDB.jpg,10615,E15-448,2016-12-05T00:16:28.929Z,"['picard@media.mit.edu', 'zher@media.mit.edu']",True,heartphones,2008-09-01,[],Heartphones,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:48.896Z,Helios provides an automatic way of socializing one's video interactions. It is a Chrome browser plug-in that records user's encounters with embedded videos on the web. This data is contributed to a group collection so that one can readily see what is trending among friends and where the outliers are. In addition the data is processed by Glue for metadata tagging. ,2015-01-01,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202014-03-27%20at%2011.45.39%20AM.png,10623,--Choose Location,2016-12-05T00:16:28.961Z,"['dahlseng@media.mit.edu', 'lip@media.mit.edu']",True,helios,2014-01-01,[],Helios,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:48.996Z,"Hello, Operator! is a vintage telephone switchboard from 1927, refurbished and wired up to a modern computer. It currently runs a time-management game; other games being prototyped are exploring the narrative potential of the system. Overall, the project exists to explore what we gain when we are able to physically engage with the antiquated technology that made the past tick.",2017-09-01,"['future-storytelling', 'playful-systems']",,10631,--Choose Location,2022-06-10T13:54:36.871Z,"['slavin@media.mit.edu', 'mslw@media.mit.edu']",True,hello-operator,2016-09-01,[],"Hello, Operator!",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:49.167Z,"We are developing a spoken-language interface for interacting with mobile robots. In our approach, language understanding is treated as a process of homeostatic control. The interpretation of words is treated as an extension of non-linguistic processes of perceptual interpretation. If successful, this research may result in robust natural-language interfaces for robots which may also be applied to a variety of other application domains including entertainment and assistive technologies.",2004-09-01,['social-machines'],,10647,E15-441,2016-12-05T00:16:37.505Z,['dkroy@media.mit.edu'],True,hermes-homeostatic-control-for-a-conversational-mobile-robot,2004-01-01,[],Hermes: Homeostatic Control for a Conversational Mobile Robot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:49.486Z,"This effort has resulted in a compact, wireless, low-power sensor card that transmits 8 analog sensor channels (4 are conditioned for pressure measurements), plus bi-directional bend, 2-axis low-G acceleration (tilt), 3-axis high-G acceleration (shock), 3-axis DC magnetic field strength (orientation), angular rate about vertical (spin), height above an active floor, and translational position as derived from a sonar pickup. The current system runs at 20 kbits/sec, and is able to provide full 8-bit state updates across the wireless link (up to 100 meters away) at 50 Hz. We have instrumented a dance sneaker with this device to measure many parameters of foot, sole, and toe expression, continuously broadcasting them to a base-station and PC over a wireless link. Our current demonstrations allow dancers and athletes to directly produce musical streams from their performances. Ongoing work explores the identification of gait characteristics from the sensor data in applications that reach beyond interactive performance, encompassing digital athletic coaching, podiatric therapy, and context extraction for wearable computers.",2001-12-30,"['toys-of-tomorrow', 'health', 'responsive-environments']",,10679,E15-357,2016-12-05T00:17:13.828Z,['joep@media.mit.edu'],True,high-density-wireless-sensing-for-expressive-footwear,1996-12-31,[],High-Density Wireless Sensing for Expressive Footwear,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:49.573Z,"We are developing a new type of microfabricated accelerometer based on the optical interferometer. The interferometer consists of surface-micromachined interdigital fingers that are alternately attached to a proof mass and support substrate. Illuminating the fingers with coherent light generates a series of diffracted optical beams. Sub-angstrom displacements between the proof mass and frame are detected by measuring the intensity of a diffracted beam. The structure is fabricated with a two-mask silicon process and detected with a standard laser diode and photodetector. We estimate that the minimum detectable acceleration is six orders of magnitude below the acceleration of gravity (i.e., 2 ug/rootHz in a 1 Hz bandwidth centered at 650 Hz). Current emphasis is on packaging this device within a volume less than 10 cubic centimeters.",2005-09-01,[],,10687,E15-420,2016-12-05T00:16:37.646Z,['scottm@media.mit.edu'],True,high-resolution-interferometric-accelerometer,2000-01-01,[],High-Resolution Interferometric Accelerometer,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:50.203Z,"This project investigates urban metagenomics to reveal the invisible microbiological worlds within our cities. Using honeybees to gather samples and hives modified to capture ""bee debris,"" the project employs genetic sequencing to discern and visualize urban microbiological neighborhoods and render microbiological landscapes of the city. The Holobiont project was first displayed at the Palazzo Mora in the 2016 Venice Architecture Biennale, with an installation that includes a ""metagenomic beehive."" Creative, scientific, development and production collaboration with: Ben Berman, Dr. Elizabeth Henaff, Regina Flores Mir, Dr. Chris Mason, Devora Najjar, Tri-Lox, and Chris Woebken, with contributions from Timo Arnall and Jack Schulze and local beekeepers in Brooklyn, Sydney, and Venice.",2017-06-01,['playful-systems'],,10735,--Choose Location,2022-06-10T14:14:48.885Z,"['slavin@media.mit.edu', 'mperez4@media.mit.edu']",True,holobiont-urbanism-revealing-the-microbiological-world-of-cities,2016-01-01,[],Holobiont Urbanism: Revealing the Microbiological World of Cities,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:50.375Z,"Holographic optics and novel rendering techniques allow for a real-time, off-axis, full-parallax autostereoscopic display based on the fly's-eye lens array. Holographic optical elements (HOEs) allow us to create nested, square arrays. Applications for use include medical imaging, video conferencing, and 3-D survey data.",2004-01-01,[],,10743,E15-441,2016-12-05T00:16:08.821Z,[],True,holographic-flys-eye-autostereoscopic-display,2004-01-01,[],Holographic Fly's-Eye Autostereoscopic Display,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:50.523Z,"Full-color, wide-angle, and large-size computer-generated hard-copy holograms take considerable time to create. A ""holographic laser printer"" will allow hard-copy holograms to be generated in minutes instead of hours, automatically and without wet processing. Research topics include recording materials and processing, optical design, image processing and LCD display, and optical techniques for image noise reduction.",2004-01-01,[],,10751,--Choose Location,2016-12-05T00:16:08.843Z,[],True,holographic-laser-printer,2000-01-01,[],Holographic Laser Printer,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:50.797Z,"Holosuite is an application that simulates holographic rendering on advanced 3D displays, connecting remote users together where they can collaborate, visualize and share 3D information across the internet with full motion parallax and stereoscopic rendering.",2016-09-01,[],,10767,--Choose Location,2016-12-05T00:16:29.322Z,[],True,holosuite,2013-01-01,[],holosuite,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:50.928Z,"Using your personal fabratory, explore a world in which 3D printers cost as little as today's inkjets and are found in every home. We've developed several sub-$100 machines that demonstrate the practicality of this future, and greatly expand the range of items that can be created on your desktop. These new capabilities have far-reaching implications for personalization of products, direct-to-consumer production, and the creation of ""information objects.""",2011-01-01,['information-ecology'],,10775,,2016-12-05T00:16:29.358Z,['holtzman@media.mit.edu'],True,home-fabratory,2010-01-01,[],Home Fabratory,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:51.277Z,"By working in the same public spot over a long period of time, street vendors and panhandlers often amass a large amount of social capital.  We are building tools for street vendors to reify their social networks online. These tools will help them leverage this social capital for upward mobility, and enhanced community building.",2011-01-01,['civic-media'],,10791,,2016-12-05T00:16:29.140Z,"['ethanz@media.mit.edu', 'borovoy@media.mit.edu']",True,homeless-neighbors,2010-01-01,[],Homeless Neighbors,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:51.598Z,"A large-scale art installation that investigates the biological systems that represent and embody human life, and their relationship to the built environment. This synthetic organism, built from interconnected microbiological systems, will be sustained in part through its own feedback and feedforward loops, but also through interactions with the architectural systems (like HVAC). As the different systems react and exchange material inputs and outputs, they move towards homeostasis. In the process, Homeostasis creates a new landscape of the human body, in which we can experience the wonder and vulnerability of its interconnected systems.",2017-06-01,['playful-systems'],,10807,--Choose Location,2022-06-10T14:14:48.899Z,['slavin@media.mit.edu'],True,homeostasis,2014-09-01,[],Homeostasis,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:51.739Z,"The Honest Crowds project addresses shortcomings of traditional survey techniques in the modern information and big data age. Web survey platforms, such as Amazon's Mechanical Turk and CrowdFlower, bring together millions of surveys and millions of survey participants, which means paying a flat rate for each completed survey may lead to survey responses that lack desirable care and forethought. Rather than allowing survey takers to maximize their reward by completing as many surveys as possible, we demonstrate how strategic incentives can be used to actually reward information and honesty rather than just participation. The incentive structures that we propose provide scalable solutions for the new paradigm of survey and active data collection.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/06/06/crowd.jpg,10815,--Choose Location,2020-07-23T18:49:13.936Z,"['irahwan@media.mit.edu', 'sunlijun@media.mit.edu', 'mrfrank@media.mit.edu', 'lorenzoc@media.mit.edu']",True,honest-crowds,2015-09-01,"['government', 'social-media', 'social-science']",Honest Crowds,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:51.883Z,"Honey I'm Home is the simplest smell output device possible: a one-bit system that can be used for any purpose. We present it as a presence-awareness device: a quick spray of a favorite scent or perfume lets you know when a loved one is thinking of you. However, we also present the device as a simple toolkit for introducing smell into other applications.",2001-12-30,"['counter-intelligence', 'personal-information-architecture']",,10823,E15-068,2016-12-05T00:17:13.955Z,['mike@media.mit.edu'],True,honey-im-home,2000-12-31,[],Honey I'm Home,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:52.020Z,"The Hook-Ups system is a set of technologies and activities that enables young people to create interactive experiences by programming connections between physical and digital media. With the Hook-Ups system, young people integrate sensors with a myriad of materials to create their own tangible interfaces. These interfaces control digital images and sounds in computer programs (such as games or responsive art pieces) the young people write. For example, a 10-year-old created a paper-plate-based flying saucer, added a sensor, then wrote a program to control an animation of a flying saucer on her computer screen.",2010-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/hookups-animated.gif,10831,LEGO Learning Lab,2016-12-05T00:16:29.472Z,"['mres@media.mit.edu', 'millner@media.mit.edu']",True,hook-ups,2003-09-01,[],Hook-Ups,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:52.131Z,"HouseFly combines audio-video recordings from multiple cameras and microphones to generate an interactive, 3D reconstruction of recorded events.  Developed for use with the longitudinal recordings collected by the Human Speechome Project, this software enables the user to move freely throughout a virtual model of a home and to play back events at any time or speed. In addition to audio and video, the project explores how different kinds of data may be visualized in a virtual space, including speech transcripts, person tracking data, and retail transactions.",2014-01-01,['social-machines'],,10839,E15-441,2016-12-05T00:16:30.496Z,"['dkroy@media.mit.edu', 'decamp@media.mit.edu']",True,housefly-immersive-video-browsing-and-data-visualization,2009-09-01,[],HouseFly: Immersive Video Browsing and Data Visualization,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:52.358Z,"The ability to author new kinds of digital media content rests upon two constraints: first, are there people that can imagine what is coming next? and second, are there people that can write the computer programs to realize that content? In the last five years, we have released two systems for fostering a new creative culture around graphical programming. The first, Design By Numbers (http://dbn.media.mit.edu), is an introduction to programming for the mathematically uninclined; the second, Processing (http://www.processing.org), is an advanced graphics-programming system suitable for production-class experimentation. The impact areas for this project include graphic design education, new media education, and computer art.",2004-01-01,['information-ecology'],,10855,E15-301,2016-12-05T00:16:29.931Z,[],True,how-to-program-visual-systems,2004-01-01,[],How to Program Visual Systems,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:52.471Z,"For 3D displays to be successful, they must be bright enough to compete with 2D displays and not diminish display resolution. To date, stacked-LCD displays have employed parallax barriers, which use pinhole or stripe patterns to provide view-dependent imagery. We show a prototype that adapts the imagery on both layers to multi-view 3D content, increasing brightness while maintaining display resolution. This promises a future of devices with sharp 2D screens and 3D displays with full horizontal and vertical parallax.",2012-01-01,"['information-ecology', 'camera-culture']",https://dam-prod2.media.mit.edu/x/files/Display/NMF_5x3_15_3x_0.5s.jpg,10863,,2016-12-05T00:16:29.951Z,"['holtzman@media.mit.edu', 'raskar@media.mit.edu']",True,hr3d-glasses-free-3dtv,2010-01-01,[],HR3D: Glasses-Free 3DTV,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:52.820Z,"The Huggable is a new type of robotic companion for health care, education, and social communication applications. The Huggable is much more than a fun, interactive robotic companion; it functions as an essential team member of a triadic interaction. Therefore, the Huggable is not meant to replace any particular person in a social network, but rather to enhance it. The Huggable is being designed with a full-body sensitive skin with over 1,500 sensors, quiet back-drivable actuators, video cameras in the eyes, microphones in the ears, an inertial measurement unit, a speaker, and an embedded PC with 802.11g wireless networking. An important design goal for the Huggable is to make the technology invisible to the user. You should not think of the Huggable as a robot but rather as a richly interactive teddy bear.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/06/huggable.jpg,10895,E15-468,2017-06-05T16:14:52.532Z,['cynthiab@media.mit.edu'],True,huggable-a-robotic-companion-for-long-term-health-care-education-and-communication,2005-01-01,[],"Huggable: A Robotic Companion for Long-Term Health Care, Education, and Communication",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:52.947Z,"The Huggable is a new type of robotic companion for health care, education, and social communication applications. The Huggable is much more than a fun, interactive robotic companion; it functions as an essential team member of a triadic interaction. Therefore, the Huggable is not meant to replace any particular person in a social network, but rather to enhance it.Children and their parents may undergo challenging experiences when admitted for inpatient care at pediatric hospitals. While most hospitals make efforts to provide socio-emotional support for patients and their families during care, gaps still exist between human resource supply and demand. The Huggable project aims to close this gap by creating a social robot able to mitigate stress, anxiety, and pain in pediatric patients by engaging them in playful interactions. In collaboration with Boston Children's Hospital and Northeastern University, we ran an experimental study to compare the effects of the Huggable robot to a virtual character on a screen and a plush teddy bear. We demonstrated that children are more eager to emotionally connect with and be physically activated by a robot than a virtual character, illustrating the potential of social robots to provide socio-emotional support during inpatient pediatric care.",2017-03-31,"['advancing-wellbeing', 'personal-robots']",https://dam-prod2.media.mit.edu/x/2016/10/17/IMG_0769.jpg,10903,--Choose Location,2022-05-13T03:55:55.331Z,"['cynthiab@media.mit.edu', 'sooyeon6@media.mit.edu']",True,huggable-a-social-robot-for-pediatric-care,2010-09-01,"['health', 'social-robotics', 'wellbeing']",Huggable: A social robot for pediatric care,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:53.363Z,"In a series of psychometric experiments, we tested subjects' perception of lighting in a virtual environment to assess the possibility of describing and subsequently controlling lighting in a dimension other than brightness. Our findings suggest that human perception of lighting is also explained by variables other than brightness. These data are used to design a lighting control system that simultaneously maps the spatial and visual characteristics of the room into a more natural and intuitive form of control. ",2014-09-01,[],,10943,--Choose Location,2016-12-05T00:17:14.129Z,"['maldrich@media.mit.edu', 'nanzhao@media.mit.edu', 'joep@media.mit.edu']",True,human-factors-and-lighting,2013-01-01,[],Human Factors and Lighting,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:53.447Z,"What motivates people? What changes do people want in the world? We approach questions of this kind by mining goals and plans from text-based websites: wikiHow, eHow, 43things, to-do lists, and commonsense knowledge bases. 43things tells us about people's long term ambitions. How-to instructions and to-do lists tell us about everyday activities. We've analyzed the corpus to find out which goals are most popular, controversial, and concealed. The resulting goal network can be used for plan recognition, natural language understanding, and building intelligent interfaces that understand why they are being used. Come by and learn about how you can use this knowledge about actions/goals, their properties (cost, duration, location), and their relations in your own applications.",2014-09-01,['software-agents'],https://dam-prod2.media.mit.edu/x/files/Display/goalnet.gif,10951,E15-383,2022-06-10T14:18:57.495Z,['lieber@media.mit.edu'],True,human-goal-network,2009-01-01,[],Human Goal Network,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:53.790Z,"Given motion capture samples of Charlie Chaplin's walk, is it possible to synthesize other motions�say, ascending or descending stairs�in his distinctive style? More generally, in analogy with handwritten signatures, do people have characteristic motion signatures that individualize their movements? If so, can these signatures be extracted from example motions? Furthermore, can extracted signatures be used to recognize, say, a particular individual's walk subsequent to observing examples of other movements produced by this individual?  We are developing an algorithm that extracts motion signatures and uses them in the animation of graphical characters. For example, given a corpus of walking, stair ascending, and stair descending motion data collected over a group of subjects, plus a sample walking-motion for a new subject, our algorithm can synthesize never-before-seen ascending and descending motions in the distinctive style of this new individual. ",2007-01-01,['affective-computing'],,10967,E15-443D,2016-12-05T00:16:30.275Z,['picard@media.mit.edu'],True,human-motion-signatures,2005-09-01,[],Human Motion Signatures,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:53.962Z,"The Human Speechome Project is an effort to observe and computationally model the longitudinal language development of a single child at an unprecedented scale. To achieve this, we are recording, storing, visualizing, and analyzing communication and behavior patterns in over 200,000 hours of home video and speech recordings. The tools that are being developed for mining and learning from hundreds of terabytes of multimedia data offer the potential for breaking open new business opportunities for a broad range of industries�from security to Internet commerce.",2014-01-01,['social-machines'],,10975,E15-441,2016-12-05T00:16:30.317Z,"['dkroy@media.mit.edu', 'decamp@media.mit.edu', 'soroush@media.mit.edu', 'mbf@media.mit.edu']",True,human-speechome-project,2005-01-01,[],Human Speechome Project,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:54.139Z,"We are studying the mechanical behavior of leg muscles and tendons during human walking in order to motivate the design of power-efficient robotic legs. The Endo-Herr walking model uses only three actuators (leg muscles) to power locomotion. It uses springs and clutches in place of other essential tendons and muscles to store energy and transfer energy from one joint to another during walking. Since mechanical clutches require much less energy than electric motors, this model can be used to design highly efficient robotic legs and exoskeletons. Current work includes analysis of the model at variable walking speeds and informing design specifications for a collaborative ""SuperFlex"" exosuit project.",,['biomechatronics'],,10983,E15-054,2021-09-17T19:11:52.295Z,"['hherr@media.mit.edu', 'mfurtney@media.mit.edu']",True,human-walking-model-predicts-joint-mechanics-electromyography-and-mechanical-economy,2007-09-01,[],"Human walking model predicts joint mechanics, electromyography, and mechanical economy",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:54.415Z,"Since Alan Turing envisioned Artificial Intelligence (AI), a major driving force behind technical progress has been competition with human cognition (e.g. beating humans in Chess or Jeopardy!). Less attention has been given to developing autonomous machines that learn to cooperate with humans. Cooperation does not require sheer computational power, but relies on intuition, and pre-evolved dispositions toward cooperation, common-sense mechanisms that are difficult to encode in machines. We develop state-of-the-art machine-learning algorithms that cooperate with people and other machines at levels that rival human cooperation in two-player repeated games.Scientific writings: Jacob Crandall, Mayada Oudah, Tennom, Fatimah Ishowo-Oloko, Sherief Abdallah, Jean-François Bonnefon, Manuel Cebrian, Azim Shariff, Michael A. Goodrich, Iyad Rahwan. Cooperating with Machines. 	arXiv:1703.06207",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/03/22/DSC01447.JPG,10999,--Choose Location,2020-07-23T18:45:42.709Z,['irahwan@media.mit.edu'],True,human-machine-cooperation,2015-01-01,"['artificial-intelligence', 'human-machine-interaction']",Human-Machine Cooperation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:54.677Z,"Car seats, and other seating devices (such as wheel chairs) are the product of a steady evolution that began with regular chairs. Starting from zero, considering the physical, medical, and cognitive demands of driving, can we make a ""seat"" to make driving safer, better, and more fun? That makes us go faster on a race track and safer in a city or when driving on snow? Can we make a ""seat"" that excites the parts of our brain activated by bicycling, skiing, or playing a musical instrument? Based on previous work with the ""Athlete Seat,"" we are rethinking vehicle control in a way that allows for improved performance and driver well-being.",2007-09-01,['smart-cities'],,11015,Cube,2016-12-05T00:16:30.386Z,[],True,humanseat,2006-09-01,[],Humanseat,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:54.813Z,"Cities around the world are striving to improve livability by way of reducing dependency on fossil-fuels cars. How might we leverage the autonomous technology to help fulfill this vision, while ensuring the flow of people and goods across the city? The Persuasive Electric Vehicle (PEV) is a small, on-demand, shared, agile, autonomous, and functionally hybrid tricycle. We believe it will become a critical platform in the constellation of emerging mobility systems. The PEV will be a shared bike platform for people's inner-urban and last-mile travel needs, and for delivering goods on-demand around the clock. To deploy it in the real world, it is necessary to match the fleet supply with its demand. This simulator enables cities around the world to forecast the fleet size based on proxy demands from taxis, shared bikes, shared car services, and Call Detail Records (CDR).",2016-01-01,['changing-places'],,11023,--Choose Location,2016-12-08T17:15:49.102Z,"['kll@media.mit.edu', 'ptinn@media.mit.edu', 'mcllin@media.mit.edu']",True,hybrid-autonomous-shared-bike-fleet-deployment-simulator,2015-01-01,[],Hybrid Autonomous Shared Bike Fleet Deployment Simulator,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:54.953Z,A web technology-based update of Smarter Objects and the reality editor project.,2015-09-01,['fluid-interfaces'],,11031,--Choose Location,2016-12-05T00:16:30.529Z,"['pattie@media.mit.edu', 'heun@media.mit.edu']",True,hybrid-objects,2014-01-01,[],Hybrid Objects,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:55.066Z,"By coupling damped semi-acoustic physical objects with virtual percussion acoustics, we have developed highly controllable hybrid digital/acoustic percussion instruments that can respond not just to the velocity of the hit, but also to how they are struck.  Scraping, stirring with brushes, and very subtle sounds are all possible, greatly extending the richness of digital percussion systems.",2007-01-01,['opera-of-the-future'],,11039,E15-443,2016-12-05T00:16:08.888Z,['tod@media.mit.edu'],True,hybrid-percussion,2005-09-01,[],Hybrid Percussion,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:55.178Z,"We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: What billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, big data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.",,['camera-culture'],,11047,--Choose Location,2019-04-19T17:54:21.753Z,"['raskar@media.mit.edu', 'ajdas@media.mit.edu']",True,hyderabad-eye-health-collaboration-with-lvp,2016-01-01,[],Hyderabad eye health collaboration with LVP,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:55.289Z,"This project is a self-sustaining sculptural installation that depicts the changes in outside weather conditions. By means of a simple mechanical system and smart materials, the sculpture responds to precipitation, light, and temperature to move colored glass panels into a wall of ever-changing designs, additionally creating awareness of the passage of time.",2009-01-01,[],,11055,Upper Atrium,2016-12-05T00:16:30.406Z,[],True,hydrocycle,2009-01-01,[],Hydrocycle,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:55.396Z,"HydroMorph is an interactive display based on shapes formed by a stream of water. Inspired by the membrane formed when a water stream hits a smooth surface (e.g., a spoon), we developed a system that dynamically controls the shape of a water membrane. This project explores a design space of interactions around water shapes, and proposes a set of user scenarios in applications across scales, from the faucet to the fountain. Through this work, we look to enrich our interaction with water, an everyday material, with the added dimension of transformation.",,['tangible-media'],,11063,--Choose Location,2016-12-05T00:17:14.252Z,"['ishii@media.mit.edu', 'thariq@media.mit.edu', 'ken_n@media.mit.edu']",True,hydromorph,2016-01-01,[],HydroMorph,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:55.729Z,"The Hyperinstruments project creates expanded musical instruments and uses technology to give extra power and finesse to virtuosic performers. They were designed to augment a wide range of traditional musical instruments and have been used by some of the world's foremost performers (Yo-Yo Ma, the Los Angeles Philharmonic, Peter Gabriel, and Penn & Teller). Research focuses on designing computer systems that measure and interpret human expression and feeling, exploring appropriate modalities and content of interactive art and entertainment environments, and building sophisticated interactive musical instruments for non-professional musicians, students, music lovers, and the general public. Recent projects involve the production a new version of the ""classic"" Hyperstring Trilogy for the Lucerne Festival, and the design of a new generation of Hyperinstruments, for Fensadense and other projects, that emphasizes measurement and interpretation of inter-player expression and communication, rather than simply the enhancement of solo performance.",,['opera-of-the-future'],,11087,E15-483,2016-12-05T00:16:30.636Z,"['tod@media.mit.edu', 'rebklein@media.mit.edu', 'tristan@media.mit.edu']",True,hyperinstruments,2000-01-01,"['artificial-intelligence', 'music', 'storytelling', 'sensors', 'performance', 'creativity', 'voice']",Hyperinstruments,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:55.853Z,"Hyperproduction is a conceptual framework and a software toolkit that allows producers to specify a descriptive computational model and consequently an abstract state for a live experience through traditional operating paradigms, such as mixing audio or operation of lighting, sound, and video systems. The hyperproduction system is able to interpret this universal state and automatically utilize additional production systems, allowing for a small number of producers to cohesively guide the attention and perspective of an audience using many or very complex production systems simultaneously. The toolkit is under active development and has been used for new pieces such as Fensadense, and to recreate older systems such as those for the original Hyperstring Triolgy as part of the Lucerne Festival in 2015. Work continues to enable new structures and abstraction within the framework.",,['opera-of-the-future'],,11095,--Choose Location,2016-12-05T00:17:14.278Z,"['tod@media.mit.edu', 'benb@media.mit.edu']",True,hyperproduction-advanced-production-systems,2013-01-01,[],Hyperproduction: Advanced Production Systems,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:09:55.969Z,Music software that lets anyone compose music. The first music software program designed to teach students and adults how to compose music simply by drawing lines on the screen.,,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/10/20/art_gallery-7851-802x600.jpg,11103,E15-443,2017-04-03T19:38:51.060Z,"['tod@media.mit.edu', 'tristan@media.mit.edu']",True,hyperscore,2001-01-01,"['artificial-intelligence', 'music', 'storytelling', 'performance', 'creativity', 'voice']",Hyperscore,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:56.162Z,"Hyperscore in the Hospital was an opportunity to use the Hyperscore composition system with a wide array of patients from the Tewksbury State Hospital.  The patients exhibited diseases such as schizophrenia, cerebral palsy, bipolar disorder, spina bifida, and various dementias. Through the use of Hyperscore, the researchers addressed major treatment goal areas for a number of patients.  On the mental health unit, staff reported that patients were exhibiting a marked decrease in self-damaging behaviors, while maintaining attention to a task, communicating sentiments of self-worth, and exhibiting developments in social behavior.  On the physical health unit, patients displayed fine-motor control over the interface, where for several patients, there was little prior evidence to indicate that such physical potential existed.  These measures, and many more, promise a wealth of benefits to be achieved in the domain of music and health-care services.  In the future we hope to design interfaces that are specifically tailored to investigate the exciting field of music, mind and health.",2004-09-01,['opera-of-the-future'],,11111,E15-443,2016-12-05T00:16:30.719Z,['tod@media.mit.edu'],True,hyperscore-in-the-hospital,2004-01-01,[],Hyperscore in the Hospital,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:56.328Z,"A new violin bow has been developed that can measure intricate aspects of violin technique: the subtle elements of physical gesture that immediately and directly impact the sound of the instrument while playing. A sensing system was implemented to measure changes in position, acceleration, and the downward and lateral strains in the bow stick; these sensors were fashioned using electromagnetic field sensing, MEMS accelerometers, and foil strain gauges. The resulting system is wireless and may be used to play an electric or acoustic violin. It can be used to exploit aspects of traditional violin bowing technique to allow sophisticated control of audio effects on the amplified sound of an electric violin, as well as synthesized sounds. New research focuses on calibrated measurement and analysis of bowing parameters in common bowing techniques and styles and the corresponding changes in the audio produced. ",2005-01-01,['opera-of-the-future'],,11119,E15-443D,2016-12-05T00:16:30.588Z,['tod@media.mit.edu'],True,hyperviolin-bow,2000-01-01,[],Hyperviolin Bow,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:56.492Z,"This project creates a synthesis engine that predicts the timbre of arbitrary acoustic instruments. Musically meaningful, it is controlled by the perceptual features extracted from the audio stream of an acoustic or electric violin in real-time. The timbre models are built from the analysis of pitch, loudness, brightness, and the instantaneous power spectrum of real recordings. Although highly compressed, the sound quality is fully preserved. Additionally, the playability remains smooth, expressive, predictable, and adequate for sound morphing.",2003-01-01,['opera-of-the-future'],,11127,E15-445,2016-12-05T00:17:14.299Z,"['tod@media.mit.edu', 'tristan@media.mit.edu']",True,hyperviolin-real-time-timbre-analysissynthesis,2000-01-01,[],Hyperviolin: Real-Time Timbre Analysis/Synthesis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:56.634Z,"Everyday objects are beginning to find expression in digital networks, creating a new family of objects that display personalized behavior and memory. But how can you weave a thousand objects in one room into the web? How can you interconnect them without a tangle of wires, the burden of battery packs, or a PhD in network administration? The goal of this work is to develop Hyphos, a wireless network for interconnecting thousands of everyday objects. By using very short-range transceivers, and relaying messages among the nodes, a new class of network emerges: a Hyphos network. This network is self-organizing with a low cost per node; transmissions are tightly localized, resulting in high bandwidth and low power consumption; fully distributed routing and control ensures robust connections even as nodes roam. 
",1999-12-30,['personal-information-architecture'],,11135,,2016-12-05T00:16:30.744Z,['mike@media.mit.edu'],True,hyphos,1996-12-31,[],Hyphos,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:56.775Z,"Increasing understanding of how to categorize patient symptoms for efficient diagnosis has led to structured patient interviews and diagnostic flowcharts that can provide diagnostic accuracy and save valuable physician time. But the rigidity of predefined questions and controlled vocabulary for answers can leave patients feeling over-constrained, as if the doctor (or computer system) is not really attending to them.  I'm Listening is a system for automatically conducting patient pre-visit interviews. It does not replace a human doctor, but can be used before an office visit to prepare the patient, deliver educational materials or triage care, and preorder appropriate tests, making better use of both doctor and patient time. It uses an on-screen avatar and natural language processing to (partially) understand the patient's response. Key is a common-sense reasoning system that lets patients express themselves in unconstrained natural language, even using metaphor, and that maps the language to medically relevant categories.",2013-01-01,['new-media-medicine'],https://dam-prod2.media.mit.edu/x/files/Display/im-listening.JPG,11143,E15-320,2020-07-29T16:20:15.111Z,"['jom@media.mit.edu', 'fmoss@media.mit.edu']",True,im-listening,2008-09-01,[],I'm Listening,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:57.070Z,"The �I/O Brush� is a drawing tool that not only releases ink, but also captures ink from the artist's immediate environment. Like an old pen or a paintbrush, it functions as both a tool to pick up ink and a tool to draw. But instead of picking up ink or paint, the I/O brush picks up color, texture, and movement from any surface or material. It looks and feels like a real paintbrush, but embedded inside is a tiny video camera with a special light, and a touch-sensitive Wacom pen. The artist may brush any surface outside of the drawing canvas (cloth, skin, her friend�s long hair waving with the wind), and the fiber optics on the brush light up to indicate that it's picked up that surface, and the artist can then draw on the canvas with the special ink.",2006-09-01,['tangible-media'],,11159,E15-344,2016-12-05T00:16:30.804Z,"['ishii@media.mit.edu', 'stefanm@media.mit.edu']",True,io-brush,2003-09-01,[],I/O Brush,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:57.190Z,"When it comes to designing wearable computers creators must play within the constraints of what is socially acceptable, or risk limiting the technology to a small audience.  With this in mind we endeavour to incorporate sensor networks into ordinary headphones, and use the information garnered from this network to reason about a user's state.  We are also developing software that will support the explicit and implicit interactions supported by I/O Earphones.",2007-01-01,['context-aware-computing'],,11167,--Choose Location,2016-12-05T00:17:14.318Z,[],True,io-earphones,2006-01-01,[],I/O Earphones,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:57.308Z,"I/O Stickers is an electronics construction kit made up of adhesive sensors and actuators. Users can place these special electronic stickers onto contact points in pre-wired and pre-programmed pages, and the pages will transmit the state of the input (sensor) sticker to a corresponding output (actuator) sticker. Building the electronics is a simple matter of matching the sticker to the correct footprint. Users can design the interaction by choosing the sensor and actuator stickers, and then personalize the interface by decorating over the flat, electronic stickers with their choice of craft materials. I/O Stickers is designed to empower users to create electronics while also leveraging existing skills in craft, resulting in works that are creatively expressive as well as technically sophisticated.",2011-09-01,"['personal-robots', 'high-low-tech']",,11175,,2016-12-05T00:16:30.839Z,"['cynthiab@media.mit.edu', 'leah@media.mit.edu', 'jieqi@media.mit.edu']",True,io-stickers,2011-01-01,[],I/O Stickers,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:57.605Z,"iCom is a communication portal intended to become part of the built environment of a set of collaborating research groups. It currently links our building with sponsor sites. It serves several purposes, including acting as a sort of ambient porthole, a live interpersonal communication system, and a community messaging center. Recent updates include voice control of the system.",2006-09-01,['object-based-media'],,11191,E15-368,2016-12-05T00:16:30.692Z,['vmb@media.mit.edu'],True,icom,2000-01-01,[],iCom,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:09:57.769Z,"In the Renaissance, kings rarely traveled to find a bride rather an artist was sent to paint her likeness. These artisans were often not the official court painter but lesser known ones who also illuminated manuscripts as their highly refined trade. One particular Netherlands family established a workshop serving the British monarch, Henry the VIII. They produced miniature portraits called Limnings, setting a trend that would continue in England and on the Continent for centuries. For today�s celebrated personages, Digital Limnings are also miniature memories  where the past is stored in the present.",2002-01-01,[],,11199,E15-468,2016-12-05T00:16:30.860Z,"['judith@media.mit.edu', 'monster@media.mit.edu']",True,identity,2001-01-01,[],ID/entity,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:57.942Z,"Alternative Autobiographies explores Richard Kostelanetz's long-time interest in describing a life (in this case, his own) in books and electronic media in ways other than chronological, continuous narrative. By bringing these elements together as never before, to be read, projected, heard, and seen, a truth and completeness is (perhaps) realized that was impossible in continuous prose.",2002-09-01,['sociable-media'],,11207,E15-468,2016-12-05T00:17:14.340Z,['judith@media.mit.edu'],True,identity-alternative-autobiographies,2001-01-01,[],ID/entity: Alternative Autobiographies,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:58.096Z,"In the Renaissance, kings rarely traveled to find a bride: instead, an artist was sent to paint her likeness. These artisans were often not the official court painter but lesser-known painters who also illuminated manuscripts as their highly refined trade. One particular Netherlandish family established a workshop serving British monarch Henry VIII, producing miniature portraits called ""Limnings"" and setting a trend that would continue in England and on the Continent for centuries. For today's celebrated personages, ""Digital Limnings"" are also miniature memories wherein the past is stored in the present.",2002-12-30,['sociable-media'],,11215,E15-468,2016-12-05T00:17:14.408Z,"['judith@media.mit.edu', 'monster@media.mit.edu']",True,identity-digital-limnings,2000-12-31,[],ID/entity: Digital Limnings,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:58.249Z,"The makings of identities: personal identity is guided by an invisible apparatus. This apparatus is ever-changing. Surveillance data is continuously being gathered about each of us as we move about in the world. From grocery tabs and credit card transactions to cameras in highway tollbooths and face recognition systems on telephone poles in major cities, we leave ever-more coherent sets of traces. Unknowingly. Perceptions and presentations of selves are assemblages continuously updated as one interacts with people and things. There is not such a thing as a being; there are only projections, interpretations, idealized memories, idolized constructions. A person exists through autorecollections, and knowingly surrounds herself with self-defining tokens. Autotopographies. A pulsed laser hologram takes hours to set up but only six nanoseconds to shoot�a portrait capturing self-conscious poses while revealing details more minute than could ever be planned. Bacteria on the skin and the location of every tiny hair. Digital voyeurism. How do we respond to these unsolicited incursions?",2002-12-30,['sociable-media'],,11223,E15-468,2016-12-05T00:17:14.438Z,"['judith@media.mit.edu', 'monster@media.mit.edu']",True,identity-strangers-to-ourselves,2000-12-31,[],ID/entity: Strangers to Ourselves,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:58.399Z,"Memory is elusive and the need to hold onto its origins is always fleeting. This installation is designed as a stage in which the viewer can become an actor. Triggered by the viewer's approach, the mirrored reflection of the painter transforms into the turning of his back. Enacting the metaphor of an uneasy marriage, Van Eyck's Mirror becomes a signifier of the divorce we, in the present, experience with the distant past.",2002-12-30,['sociable-media'],,11231,E15-468,2016-12-05T00:17:14.462Z,"['judith@media.mit.edu', 'stefan@media.mit.edu']",True,identity-van-eycks-mirror,2000-12-31,[],ID/entity: Van Eyck's Mirror,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:09:58.537Z,"Complex and expensive medical devices are mainly used in medical facilities by health professionals. IDA is an attempt to disrupt this paradigm and introduce a new type of device: easy to use, low cost, and open source. It is a digital stethoscope that can be connected to the Internet for streaming physiological data to remote clinicians. Designed to be fabricated anywhere in the world with minimal equipment, it can be operated by individuals without medical training.",2013-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/2011-04-07%2011.16.49.jpg,11239,,2017-07-27T04:58:24.482Z,"['picard@media.mit.edu', 'yadid@media.mit.edu']",True,ida-inexpensive-networked-digital-stethoscope,2011-01-01,[],IDA: Inexpensive Networked Digital Stethoscope,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:09:58.781Z,The IDEAS (Innovative Design Experiences After School) Institute is an innovative professional-development program for people working at after-school centers in low-income communities. Our goal is to create a network of people committed to deepening the learning experiences of youth through creative uses of new technologies.,2006-09-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/ideas-logo.jpg,11255,Cube,2016-12-05T00:17:14.525Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu']",True,ideas-institute,2005-01-01,[],IDEAS Institute,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:58.904Z,"Asthma is the most common chronic illness among children. The skills required to diagnose it make it an even greater concern. Our solution is a child-friendly wearable device that allows in-home diagnosis of asthma. The device acquires simultaneous measurements from multiple stethoscopes. The recordings are then sent to a specialist who uses assistive diagnosis algorithms that enable auscultation (listening to lung sounds with a stethoscope). Sound refocusing algorithms enable the specialist to listen to any location in the lungs. The specialist also has access to a sound ""heat map"" that shows the location of sound sources in the lungs.",,"['advancing-wellbeing', 'camera-culture']",https://dam-prod2.media.mit.edu/x/2016/10/19/stet__edHV6Vy.jpg,11263,--Choose Location,2019-04-19T18:25:32.160Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'guysatat@media.mit.edu']",True,identi-wheez-a-device-for-in-home-diagnosis-of-asthma,2016-01-01,"['health', 'wearable-computing']",Identi-Wheez: A device for in-home diagnosis of asthma,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:09:59.018Z,"Who are you now? This project aims to model the answer to that question, in all its relevant dimensions and constructs, within the context of an online and physical multi-user environment. Using federated identity management systems as a starting point, we are exploring interoperable expressions of names, rights, duties, preferences, self-expression, and pointers to other information. The prototypes are being designed against various concept car approaches so that driver and passengers can be authenticated to enter the car, enjoy personalized physical and data environments in it, and enable transactions and other interactions with people and systems in other cars or places. Modules and extensions will include support for self-motivational ticklers, health and wellness regimes, group dynamics intervention and religious or spiritual framing and thematic reminders. The expression dialect is currently XML, with LDAP compliant directory interoperability, and integration hooks for PIMs, phones, car systems, and desktop applications. ",2007-09-01,['smart-cities'],,11271,E15-001,2016-12-05T00:16:31.002Z,['dang@media.mit.edu'],True,identity-convergence-physical-digital-and-beyond,2004-01-01,[],"Identity Convergence: Physical, Digital, and Beyond",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:09:59.211Z,"We are using signaling theory to explore how identity is presented and perceived in online environments. The fundamental idea is that many qualities we are interested in knowing about others are not directly observable. Instead, we rely on signals that are more or less reliably correlated with the quality. Sometimes signals are very reliable indicators of a particular quality, but sometimes they are not. The power of signaling theory is that it provides a means of evaluating the reliability of signals. We are using this approach to analyze social networking sites, reputation systems, graphical representations, and humanoid agents. ",2017-05-01,['sociable-media'],,11287,E15-383,2022-06-10T14:41:29.899Z,['judith@media.mit.edu'],True,identity-signals,2004-01-01,[],Identity Signals,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:09:59.304Z,"What if a building could sense what was happening inside of it, and tell the world?  This project explores the concept of automated micro blogging�automatically generating and triggering short messages about what is going on in a particular context.  The current platform is built around a modular sensor network which combines proximity, temperature, light, and sound values to make guesses about an environment and put those guesses to words.",2011-01-01,['information-ecology'],,11295,,2016-12-05T00:16:31.046Z,['holtzman@media.mit.edu'],True,if-these-walls-could-tweet,2010-09-01,[],If These Walls Could Tweet,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:59.401Z,Illuminating Clay is system for the real-time computational analysis of physical landscape models. Users manipulate a malleable landscape model while the topography of the model is captured in real time by a laser-scanning device. The surface geometry of the model serves as an input to a library of landscape-analysis functions. The results of this analysis are projected back into the workspace and registered with the surfaces of the model. The system allows users to interact simultaneously with both physical and computational representations of the landscape.,2004-01-01,['tangible-media'],,11303,E15-344,2016-12-05T00:16:31.177Z,['ishii@media.mit.edu'],True,illuminating-clay,2000-01-01,[],Illuminating Clay,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:09:59.638Z,"This research aims to understand the types of technological and conceptual tools that communities need to develop rich understandings of local history and the process of investigating it. We are building tools that allow users to geo-reference images from their photo albums and Web pages. There are libraries of images in every home, and we want to provide people with tools to upload their historical records.",2005-01-01,['electronic-publishing'],,11327,E15-320G,2016-12-05T00:16:31.263Z,['walter@media.mit.edu'],True,image-maps,2002-01-01,[],Image Maps,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:09:59.721Z,"Locating and classifying florescent tags behind turbid layers using time-resovled inversion Using time resolved and sparse optimization framework to locate and classify fluorescent markers hidden behind turbid layer: The use of fluorescent probes and the recovery of their lifetimes allow for significant advances in many imaging systems, in particular medical imaging systems. Here, we propose and experimentally demonstrate reconstructing the locations and lifetimes of fluorescent markers hidden behind a turbid layer. This opens the door to various applications for non-invasive diagnosis, analysis, flowmetry, and inspection. The method is based on a time-resolved measurement which captures information about both fluorescence lifetime and spatial position of the probes. To reconstruct the scene, the method relies on a sparse optimization framework to invert time-resolved measurements. This wide-angle technique does not rely on coherence, and does not require the probes to be directly in line of sight of the camera, making it potentially suitable for long-range imaging.More details:http://web.media.mit.edu/~guysatat/project_scattering.html http://web.media.mit.edu/~guysatat/fl/",,"['advancing-wellbeing', 'terrestrial-sensing', 'camera-culture']",,11335,--Choose Location,2017-04-05T01:48:57.383Z,"['raskar@media.mit.edu', 'barmak@media.mit.edu']",True,imaging-behind-diffusive-layers,2015-01-01,[],Imaging Behind Diffusive Layers,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:09:59.805Z,"Imagine holding an animated pixel in the palm of your hand. What could you create with physical pixels that fit together like children's blocks? Envision a video display that is no longer rigid and flat, but any shape and size you wish to build. The Physical Pixel Project is developing a system of tangible, interactive pixels that enables a user to build dynamic color in a sculptural form. The goal is to move computer graphics off of the screen and into the physical world.",1999-12-30,"['toys-of-tomorrow', 'personal-information-architecture']",,11343,E15-468,2016-12-05T00:17:14.573Z,['mike@media.mit.edu'],True,imaging-in-3p-the-physical-pixel-project,1998-12-31,[],Imaging in 3P: The Physical Pixel Project,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:09:59.883Z,We use time-resolved information in an iterative optimization algorithm to recover reflectance of a three-dimensional scene hidden behind a diffuser. We demonstrate reconstruction of large images without relying on knowledge of diffuser properties.,,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/tr.bmp,11351,--Choose Location,2019-04-19T18:27:07.846Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,imaging-through-scattering-media-using-femtophotography,2012-09-01,['imaging'],Imaging through scattering media using femtophotography,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:09:59.961Z,"How to see through tissueWe demonstrate a new method to image through scattering materials like tissue and fog. The demonstration includes imaging an object hidden behind 1.5cm of tissue; it's like imaging through the palm of a hand. Our optical method is based on measuring and using all photons in the signal (as opposed to traditional methods, which use only part of the signal). Specifically, we use a time-resolved method that allows us to distinguish between photons that travel different paths in the tissue. Combining this unique measurement process with novel algorithms allows us to recover the hidden objects. This technique can be used in biomedical imaging, as well as imaging through fog and clouds.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/API.JPG,11359,--Choose Location,2017-04-05T01:49:57.109Z,"['raskar@media.mit.edu', 'guysatat@media.mit.edu', 'barmak@media.mit.edu']",True,imaging-with-all-photons,2015-09-01,['imaging'],Imaging with All Photons,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:00.042Z,"Immersion is a visual data experiment that delivers a fresh perspective of your email inbox. Focusing on a people-centric approach rather than the content of the emails, Immersion brings into view an important personal insight�the network of people you are connected to via email, and how it evolves over the course of many years. Given that this experiment deals with data that is extremely private, it is worthwhile to note that when given secure access to your Gmail inbox (which you can revoke any time), Immersion only uses data from email headers and not a single word of any email's subject or body content.",2016-10-22,['collective-learning'],https://dam-prod2.media.mit.edu/x/2016/08/23/immersion.png,11367,--Choose Location,2016-12-05T00:17:14.613Z,['hidalgo@media.mit.edu'],True,immersion,2012-09-01,[],Immersion,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:00.194Z,"We are identifying people's activities while browsing the Web solely by analyzing their mouse movement behavior. We use machine-learning algorithms to develop systems that predict user activities and user interest using algorithms that correlate mouse tracks and implicit metrics for activity and interest. Clustering low-level mouse data into a relatively small set of features is vital in tracking people�s behaviors as they occur. Our systems offer a trade-off between model, feature, and computational complexity to be suitable for online usage.",2007-01-01,['context-aware-computing'],,11383,E15-320,2016-12-05T00:17:14.649Z,[],True,implicit-metric-of-interest-and-attention,2007-09-01,[],Implicit Metric of Interest and Attention,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:00.346Z,"Humans need sleep, along with food, water, and oxygen, to survive. With about one-third of our lives spent sleeping, there has been increased attention and interest in understanding sleep and the overall state of our ""sleep health."" The rapid adoption of smartphones, along with a growing number of sleep tracking applications for these devices, presents an opportunity to use phones to encourage better sleep hygiene. Procrastinating going to bed and being unable to stick to a consistent bedtime can lead to inadequate sleep time, which in turn affects quality of life and overall wellbeing. To help address this problem, we developed two applications, Lights Out and Sleep Wallpaper, which provide a sensor-based bedtime alarm and a connected peripheral display on the wallpaper of the user's mobile phone to promote awareness with sleep data visualization.",2019-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/2016/10/23/appseq1.png,11399,--Choose Location,2020-07-20T18:15:07.806Z,"['picard@media.mit.edu', 'sra@media.mit.edu', 'cvx@media.mit.edu']",True,improving-sleep-wake-schedule-using-sleep-behavior-visualization-and-a-bedtime-alarm,2015-09-01,[],Improving Sleep-Wake Schedule Using Sleep Behavior Visualization and a Bedtime Alarm,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:00.428Z,"Most computational systems take for granted the servitude of computers, hoping to make them invisible entities that intuit our needs, and seamlessly respond to them. Linked with this idea is the one that we can formally analyze any situation, and program an overarching structure for the computer�s task. Much of this assumption stems from the military origins of computers, and the perceived need for total control; the experiences we have with computers reinforces this paradigm of direct manipulation. Most human activity, however, does not follow such formal structures. Our computer monitors routinely sport half a dozen post-it notes, evidence of the weakness of completely formal systems. Humans routinely improvise their activities, from casual social encounters to their first experience with a machine. For Improvitronic Islands, a small group at the Media Lab worked with two masters of Jazz improvisation, George Lewis and Toshinori Kondo, to develop computer music systems that help children learn principles of improvisation. The system enabled children to quickly use two of their most basic faculties for music making�body rhythm and voice�and to use them in collaborative, improvisational performances.",2002-12-30,['computing-culture'],,11407,,2016-12-05T00:16:31.354Z,['csik@media.mit.edu'],True,improvitronic-islands,2001-12-31,[],Improvitronic Islands,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:00.597Z,"The wonder that occurs while watching a good magic trick or admiring a gorgeous natural vista is a strong emotion that has not been well studied. Educators, media producers, entertainers, scientists and magicians could all benefit from a more robust understanding of wonder. A new model was developed, and an experiment was conducted to investigate how several variables affect how magic tricks are enjoyed. The experiment showed 70 subjects 10 videos of magic while recording their responses and reactions to the tricks. Some individuals were shown the explanations to the magic tricks to gauge their impact on enjoyment. The style of the presentation was varied between two groups to compare the effect of magic presented as a story to magic presented as a puzzle. Presentation style has an effect on magic enthusiasts' enjoyment and a story-oriented presentation is associated with individuals being more generous towards a charity.",2007-09-01,['affective-computing'],,11423,E15-448,2016-12-05T00:17:14.706Z,['picard@media.mit.edu'],True,in-search-of-wonder-measuring-our-response-to-the-miraculous,2006-09-01,[],In Search of Wonder: Measuring Our Response to the Miraculous,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:00.673Z,"This project addresses the question of how we can build a new style of telephone-like communications system that uses data shared among all of the users to make an architecture for new services. The system is made up of an infrastructure that facilitates the acquisition and exchange of user context information called Sensorama, and an IP-based telephony application based on this infrastructure.",2003-01-01,['media-and-networks'],,11431,E15-483,2016-12-05T00:17:14.737Z,['lip@media.mit.edu'],True,incall-exploring-the-role-of-context-in-communication,2002-01-01,[],inCall: Exploring the Role of Context in Communication,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:00.751Z,"Cooperation in a large society of self-interested individuals is notoriously difficult to achieve when the externality of one individual's action is spread thin and wide. This leads to the ""tragedy of the commons,"" with rational action ultimately leaving everyone worse off. Traditional policies to promote cooperation involve Pigouvian taxation or subsidies that make individuals internalize the externality they incur. We introduce a new approach to achieving global cooperation by localizing externalities to one's peers in a social network, thus leveraging the power of peer pressure to regulate behavior. The mechanism relies on a joint model of externalities and peer-pressure. Surprisingly, this mechanism can require a lower budget to operate than the Pigouvian mechanism, even when accounting for the social cost of peer pressure. Even when the available budget is very low, the social mechanisms achieve greater improvement in the outcome.",,"['connection-science', 'human-dynamics']",,11439,--Choose Location,2022-06-10T13:18:41.971Z,"['sandy@media.mit.edu', 'dhaval@media.mit.edu', 'shrier@media.mit.edu']",True,incentivizing-cooperation-using-social-pressure,2010-09-01,[],Incentivizing cooperation using social pressure,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:10:00.836Z,We present an indoor positioning system that measures location using disturbances of the Earth's magnetic field by structural steel elements in a building. The presence of these large steel members warps the geomagnetic field such that lines of magnetic force are locally not parallel. We measure the divergence of the lines of the magnetic force field using e-compass parts with slight physical offsets; these measurements are used to create local position signatures for later comparison with values in the same sensors at a location to be measured. We demonstrate accuracy within one meter 88 percent of the time in experiments in two buildings and across multiple floors within the buildings.,2014-01-01,['living-mobile'],,11447,--Choose Location,2016-12-05T00:16:31.462Z,"['nanwei@media.mit.edu', 'geek@media.mit.edu', 'joep@media.mit.edu']",True,indoor-location-sensing-using-geo-magnetism,2010-01-01,[],Indoor Location Sensing Using Geo-Magnetism,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:00.915Z,"Building on our Expressive Footwear project, we have constructed a low-cost, compact, wireless inertial measurement unit, featuring three axes of both gyroscopes and accelerometers. We have used this device in applications that determine rotational and translational gestural features and estimated orientation in 3-D with fairly good accuracy.  The cubical package measures 1.25"" on a side and the system will run continuously for two days on embedded batteries. More recently, we built a framework for gesture recognition and feedback through this device, enabling users to easily assemble a gesture-recognition front-end for a wide variety of applications. This system can recognize both the type and parameters of a number of atomic gestures, which can then by combined to create composite gestures whose execution can be tied to various outputs. A simplified hand-held version of this framework is now on display.",2001-12-30,"['personal-fabrication', 'toys-of-tomorrow', 'health', 'responsive-environments']",,11455,E15-357,2016-12-05T00:16:09.167Z,['joep@media.mit.edu'],True,inertial-gesture-recognition-and-compact-inertial-sensor-packages,1998-12-31,[],Inertial Gesture Recognition and Compact Inertial Sensor Packages,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:01.070Z,"We are developing a set of very low-cost, wireless, wearable sensors that enable a large group of people (e.g., hundreds or thousands) to participate in an interactive musical performance. The sensors themselves are simple piezoelectric accelerometers that detect extremes of limb motion, upon which they transmit a narrow RF pulse. Although one can distinguish between sensors on the upper and lower body by using a different carrier frequency, and zone the locations of activity roughly through carrier strength, we do not plan to independently ID each performer, but instead to measure and react to the characteristics of ensemble behavior. We have built this system and have developed algorithms that use this data to explore techniques of mapping large-group, real-time musical interaction.",2002-09-01,['responsive-environments'],,11463,E15-441,2016-12-05T00:17:14.811Z,"['geppetto@media.mit.edu', 'joep@media.mit.edu']",True,inexpensive-wearable-sensors-for-large-crowd-interaction,2001-01-01,[],Inexpensive Wearable Sensors for Large-Crowd Interaction,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:01.237Z,"We have been developing comfortable, safe, attractive physiological sensors that infants can wear around the clock to wirelessly communicate their internal physiological state changes. The sensors capture sympathetic nervous system arousal, temperature, physical activity, and other physiological indications that can be processed to signal changes in sleep, arousal, discomfort or distress, all of which are important for helping parents better understand the internal state of their child and what things stress or soothe their baby. The technology can also be used to collect physiological and circadian patterns of data in infants at risk for developmental disabilities.",2011-01-01,['affective-computing'],,11471,E15-443,2016-12-05T00:17:14.754Z,"['picard@media.mit.edu', 'fletcher@media.mit.edu', 'mgoodwin@media.mit.edu']",True,infant-monitoring-and-communication,2007-01-01,[],Infant Monitoring and Communication,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:01.407Z,"One of the defining characteristics of online auctions is the information bidders can get from other bidders during the process. In the current project, we are trying to understand how bidders make inferences, what the inferences are based upon, and how accurate their inferences are.",2007-01-01,['erationality'],,11479,--Choose Location,2016-12-05T00:17:14.830Z,[],True,inferring-values-from-others-in-online-auctions,2001-01-01,[],Inferring Values from Others in Online Auctions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:01.693Z,"The thrust of this research is the development of picture-coding techniques that are an order of magnitude more compact than existing codes for certain types of materials, and are also more easily manipulated. For example, we are attempting to store an entire year of a serial television program on a two-hour digital videodisc (DVD). In addition, since interactive video has suffered from a lack of ""re-usability"" (images seen once become boring on round two), this coding technique can make a video game where the same footage never reappears. The work is beginning with some long-term characterizations of television programs.",2003-01-01,['media-and-networks'],,11495,E15-344,2016-12-05T00:16:37.671Z,['lip@media.mit.edu'],True,infinite-digital-videodisc,2000-01-01,[],Infinite Digital Videodisc,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:01.832Z,"We are expanding the home-video viewing experience by generating imagery to extend the TV screen and give the impression that the scene wraps completely around the viewer. Optical flow, color analysis, and heuristics extrapolate beyond the screen edge, where projectors provide the viewer's perceptual vision with low-detail dynamic patterns that are perceptually consistent with the video imagery and increase the sense of immersive presence and participation. We perform this processing in real time using standard microprocessors and GPUs.",2014-01-13,['object-based-media'],,11503,--Choose Location,2020-01-13T19:31:45.928Z,"['vmb@media.mit.edu', 'novysan@media.mit.edu']",True,infinity-by-nine,2011-09-01,['imaging'],Infinity-by-Nine,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:01.969Z,"As part of human evolution and revolution, food is among the earliest forms of human interaction, but it has remained essentially unchanged from ancient to modern times. What if we introduced engineered and programmable food materials? With that change, food can change its role from passive to active. Food can ""communicate"" using its inherent behaviors combined with engineering accuracy. Food becomes media and interface. During an MIT winter course we initiated and taught, we encouraged students to design pneumatic food. Students successfully implemented inflatable sugar and cheese products. To inflate food, we use both an engineering approach and a biological approach; to solidify the inflated food, we introduce both heat via the oven, and coldness with liquid nitrogen.",,"['advancing-wellbeing', 'tangible-media']",,11511,--Choose Location,2016-12-24T22:05:38.254Z,"['ishii@media.mit.edu', 'liningy@media.mit.edu', 'jifei@media.mit.edu', 'wwen@media.mit.edu']",True,inflated-appetite,2016-01-01,[],Inflated Appetite,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:02.226Z,"InfoField aims to seamlessly connect physical space with the ubiquitous network, helping users select information in a dense RFID-tagged environment. The system uses a wearable RFID reader that monitors every tag within range (~1.5m), and sensor-embedded RFID tags which provide orientation, light level, and proximity data along with their identification codes. InfoField derives the user's interactions with the corresponding host object from the sensor data and enables browsing, logging, and selecting of the physical object's data. This system, fully compatible with EPC protocol, expands the RFID application area, including location tracking, inventory monitoring, activity logging, physical manipulation of data, and recommender systems.",2008-09-01,['information-ecology'],,11527,E15-301,2016-12-05T00:16:31.577Z,['holtzman@media.mit.edu'],True,infofield,2007-09-01,[],InfoField,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:02.342Z,"Shape displays can be used to render both 3D physical content and user interface elements. We propose to use shape displays in three different ways to mediate interaction: facilitate, providing dynamic physical affordances through shape change; restrict, guiding users through dynamic physical constraints; and manipulate, actuating passive physical objects on the interface surface. We demonstrate this on a new, high-resolution shape display.",,"['terrestrial-sensing', 'tangible-media']",https://dam-prod2.media.mit.edu/x/files/Display/inform.jpg,11535,--Choose Location,2023-08-11T16:52:06.090Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu', 'olwal@media.mit.edu']",True,inform,2013-01-01,"['design', 'human-computer-interaction', 'sensors', 'interfaces', 'community', 'gesture-interface']",inFORM,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:02.466Z,"This project uses gaze detection and eye tracking to create information displays that know when they are being read. By modeling reading behavior in real time, we can create information that continuously reacts to and is informed by the previous actions of the reader. In the future, we hope to expand this application to address multiple simultaneous readers gazing at multiple displays.",2008-09-01,['design-ecology'],,11543,E15-301,2016-12-05T00:17:01.394Z,['dsmall@media.mit.edu'],True,information-looks-back,2008-09-01,[],Information Looks Back,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:02.565Z,"The advent of PDAs, digital cameras, audio recorders, and wearables enables the collection of useful information in a variety of media. But how should it be organized? Information Pipette aims to use software agents to organize automatically the information to ""make sense"" for a particular task or user's context. For example, we are working on a scenario of assembling directions by taking pictures along a route or by recording audio advice at each step. Audio is run through a speech recognizer and pictures through an Optical Character Recognition (OCR) program (to read the street signs). While each of these methods alone is ineffectual, their information taken together and compiled with knowledge contextualized for the task can produce conherent descriptions.",2007-01-01,['software-agents'],,11551,E15-383,2016-12-05T00:16:31.647Z,['lieber@media.mit.edu'],True,information-pipette,2001-01-01,[],Information Pipette,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:02.762Z,"Much of information visualization is done, as the name would imply, visually. While research has looked into haptic feedback to help humans �feel� their way through information or interfaces, very little research has looked at the ways that smell can provide us with information or lead to user actions, outside of a replication of familiar smells. InfoSmell looks at how we can use our sense of smell to notify, indicate, or even persuade users, introducing a limited language of unique smells associated with specific information such as email, blogs, or news.",2010-01-01,['information-ecology'],,11567,--Choose Location,2016-12-05T00:17:14.854Z,['holtzman@media.mit.edu'],True,infosmell-smell-your-data,2009-09-01,[],InfoSmell: Smell Your Data,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:03.133Z,"The purpose of the INNER-active Journal system is to provide a way for users to reconstruct their emotions around events in their lives, and to see how recall of these events affects their physiology. Expressive writing, a task in which the participant is asked to write about extremely emotional events, is presented as a means towards story construction. Previous use of expressive writing has shown profound benefits for both psychological and physical health. In this system, measures of skin conductivity, instantaneous heart rate, and heart stress entropy are used as indicators of activities occurring in the body. Users have the ability to view these signals after taking part in an expressive writing task.",2005-01-01,['affective-computing'],,11599,E15-001,2016-12-05T00:17:14.956Z,['picard@media.mit.edu'],True,inner-active-journal,2004-01-01,[],INNER-active Journal,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:03.543Z,"Using the 300M IT Edition as a research platform, this project aims to develop ""aerospace systems"" that are applicable to cars. In cooperation with Professor R. John Hansman from the MIT Department of Aeronautics/Astronautics, there will be a comparative analysis of the impact of information technology on aircraft and automobile cockpits, including the identification of key human factors. After defining possible aerospace systems applicable to automobiles, this project will implement two new devices, based on preliminary studies, into the 300M. The Standby device includes an investigation of the driver's stress level. The Warning device assists the driver with warning and maintenance messages obtained from the car's network bus. Depending on the state of the Standby device, some non-essential warnings could be suppressed, so as not to distract the driver.",2004-01-01,[],,11639,E15-441,2016-12-05T00:17:15.004Z,[],True,insights-from-aerospace-transitions-into-automobile-cockpits,2001-01-01,[],Insights from Aerospace: Transitions into Automobile Cockpits,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:03.695Z,"Installation consists of a viewing window and a stylus with which users can create virtual forms and install them permanently into real space. By tracking the position and orientation of the stylus and the window itself, we are able to calibrate virtual coordinates with real viewing position. Virtual objects created in the system respond as though they were physically in the space of the room. Once objects are placed in the environment, they will stay there in perpetuity, changing and growing like plants in the air.",2003-01-01,['aesthetics-computation'],,11655,E15-305A,2016-12-05T00:16:09.212Z,[],True,installation,2001-01-01,[],Installation,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:03.773Z,"inStink explores what happens when we can manipulate smell by computer as easily as we currently manipulate sound and video. We have built prototype systems for smell production that use smell as an ambient medium, to communicate presence, activity, and abstract information. We are also examining the use of aromas to aid learning, in conjunction with other media, and as a communication device.",2001-12-30,"['counter-intelligence', 'personal-information-architecture']",,11663,E15-068,2016-12-05T00:16:31.950Z,['mike@media.mit.edu'],True,instink,1999-12-31,[],inStink,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:03.852Z,"Agent software can perform tasks automatically on behalf of a user, but how does the agent come to learn what the user wants? Sometimes the agent can learn just by observing user behavior, but there may also be instances where the user must instruct the agent more explicitly. This ""instructibility"" aspect is the focus of this project. The user may present examples of behavior that the agent should follow and give advice to the agent as to how the examples should be interpreted. The agent must give feedback to the user so that the user understands what the agent knows and is capable of doing. Multimodal interaction is important in both the instruction and feedback.",2007-09-01,['software-agents'],,11671,E15-383,2016-12-05T00:16:32.057Z,['lieber@media.mit.edu'],True,instructible-agents,2000-01-01,[],Instructible Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:03.923Z,"The Nanogate is a micro electromechanical systems (MEMS) device capable of accurately and repeatedly controlling a nanometer gap between two very flat surfaces. The nanometer gap is achieved using a circular lever-fulcrum structure made from patterned silicon and Pyrex wafers adhered to each other via anodic bonding. The distance in the center gap can be varied by pushing on the outside edge. The separation distance between the silicon and Pyrex wafers can range from a few nanometers to several microns, while the aspect ratio between the width of the gap can be as large as 10^6. The Nanogate has promising applications both as a tunable capacitor for frequency-agile wireless networks and as a nanoscale instrument for studying molecular-scale phenomena. We are developing a key technology for the Nanogate to measure the displacement of the nanometer gap using capacitive sensing.",2003-09-01,"['silicon-biology', 'responsive-environments']",,11679,E15-344,2016-12-05T00:17:01.421Z,['joep@media.mit.edu'],True,instrumentation-for-the-nanogate,2002-01-01,[],Instrumentation for the Nanogate,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:04.006Z,"Developing biologically inspired robotic prostheses necessitates precise understanding of the dynamic interaction between amputee, prosthesis, and the environment they act on. In our research, we are instrumenting a biomimetic ankle-foot powered prosthesis prototype with a series of sensory units to estimate the ground reaction forces (GRF) and zero moment point (ZMP) trajectory. The incorporation of this sensory information with a morphologically realistic human model and basic feedback methods will contribute to the development of balance-control strategies in the mentioned device. These strategies will enhance amputees� perception and control of their dynamic stability. With this new generation of robotic ankle-foot prostheses, we are addressing some of the main difficulties that amputees encounter with current passive devices, including non-symmetric gait, increased walking energy cost, and appropriate maintenance of balance during standing and walking.",2006-09-01,['biomechatronics'],,11687,E15-054,2016-12-05T00:17:15.039Z,['hherr@media.mit.edu'],True,instrumentation-of-a-biomimetic-ankle-foot-prosthesis,2005-01-01,[],Instrumentation of a Biomimetic Ankle-Foot Prosthesis,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:04.080Z,"The past several years have shown a proliferation of sensor and wireless communication technologies across the urban landscape, enabling large-scale, real-time data collection and information sharing. We are increasingly living in an �instrumented city,� in which all aspects of the environment can be quantified, analyzed, and monitored. Now, low-cost sensing technologies embedded in and connected to cell phones and other ubiquitous communications devices enable grassroots data collection and analysis. What does this mean for the future of urban experience?",2006-01-01,['smart-cities'],,11695,E15-001,2016-12-05T00:16:31.980Z,[],True,instrumented-cities,2004-09-01,[],Instrumented Cities,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:04.322Z,"Technical documentation for hardware and software is expensive to produce, and often inaccurate and inadequate. We are exploring a new approach to producing technical documentation in which an expert interacts with a simulation of a device, and the system automatically produces both written English descriptions and visual illustrations.",2010-09-01,['software-agents'],,11719,E15-383,2016-12-05T00:17:15.069Z,['lieber@media.mit.edu'],True,intelligent-technical-documentation,2000-01-01,[],Intelligent Technical Documentation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:04.665Z,"This project takes advantage of our group's Ubiquitous Media Portals platform, which enables a large suite of research around the broad theme of what we call Dynamic Ubiquitous Media. This will include relevant, personalized information delivered ambiently to Lab visitors, with intuitive non-contact gestural input for interacting with this information. This project will build a framework for implementing dynamic media, and demonstrate it running throughout our building through a variety of applications.",2010-01-01,['responsive-environments'],,11743,--Choose Location,2016-12-05T00:16:09.242Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,interaction-with-ubiquitous-dynamically-responsive-media,2009-01-01,[],Interaction with Ubiquitous Dynamically Responsive Media,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:04.838Z,"Interactive C was a programming environment used by robotic controllers such as the rev. 2.21 6.270 controller and the Handy Board.  It gave users the ability to control a robot by using C commands and additional functions tailored specifically for robotics (i.e.: actuator control, sensor inputs).  Interactive C allowed users to either enter commands interactively or load the program in as a file. The heart of Interactive C is a compiler that converts C commands into pseudocode for a custom stack machine implemented for the controller board.  The current version of Interactive C was written in C and has not been updated for several years.  This project focused on the re-implementation of this compiler to port it to Java and to enhance its current features�opening the door for future improvements in the hardware Interactive C can support.",2000-09-01,['lifelong-kindergarten'],,11751,--Choose Location,2016-12-05T00:16:32.091Z,['mres@media.mit.edu'],True,interactive-c,2000-01-01,[],Interactive C,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:05.007Z,"The Interactive Fountain is a volumetric water display in which laminar water colums are perturbed to create illuminated pixels. As the perturbations scan downward, illuminating LEDs are pulse-width modulated to scan out a 3-D image. The fountain will employ the use of new UV LEDs and phosphorescent dye to create as bright an image as possible.  A show-control software system is also being developed that would enable the images in the fountain to be linked with audio and other content.",2000-12-30,['opera-of-the-future'],,11759,E15-483,2016-12-05T00:16:32.141Z,['tod@media.mit.edu'],True,interactive-fountain,1999-12-31,[],Interactive Fountain,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:05.177Z,"The advent of Python-based networks creates new opportunities for mobile interaction. A compelling application would be interactive graphic control of audio on one device by deft gestures on a remote device. We have developed a stand-alone frequency modulation synthesizer using a Csound audio engine, Python, along with PyGTK and Cairo graphic control, and will extend this to other audio processing algorithms. This development, currently in a Linux environment, can be ported to networked platforms like mobile phones, handheld devices, and the $100 laptop.",2008-09-01,['music-mind-and-machine'],,11767,E15-484,2016-12-05T00:17:01.460Z,['bv@media.mit.edu'],True,interactive-graphic-control-of-networked-audio,2007-09-01,[],Interactive Graphic Control of Networked Audio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:05.369Z,"We developed a smartphone application that detects users’ affect and provides personalized positive psychology interventions in order to enhance users’ psychological wellbeing. Users’ emotional states were measured by analyzing facial expressions and the sentiment of SMS messages. A virtual character in the application prompted users to verbally journal about their day by providing three positive psychology interventions. The system used a Markov Decision Process (MDP) model and a State-Action-Reward-State-Action (SARSA) algorithm to learn users’ preferences about the positive psychology interventions. Nine participants were recruited for an experimental study to test the application. They used it daily for three weeks. The interactive journaling activity increased participants’ arousal and valence levels immediately following each interaction, and we saw a trend toward improved self-acceptance levels over the three week period. The interaction duration increased significantly throughout the study as well. The qualitative analysis on journal entries showed that the application users explored and reflected on various aspects of themselves by looking at daily events, and found novel appreciation for and meanings in their daily routine.",2016-12-31,"['advancing-wellbeing', 'personal-robots']",https://dam-prod2.media.mit.edu/x/2016/10/17/Screen Shot 2016-10-17 at 11.51.20 AM_bRa46Ev.png,11775,--Choose Location,2017-06-05T16:12:47.957Z,"['cynthiab@media.mit.edu', 'sooyeon6@media.mit.edu']",True,interactive-journaling,2015-01-01,[],Interactive Journaling,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:05.544Z,"Modern music education program for children emphasizes the importance of
creating a rich music environment in which children become involved as creative and active musicians. However, seldom do we see a music machine learn in such an interactive way. This project aims to take hints from the developmental process of children's music ability and build a machine which learns to listen, perform, and create music in an interactive and developmental fashion. Several machines will receive specific music curriculum and thus the difference between their behavior in response to music can be observed and analyzed. In addition, these music machines will be natural interactive performers in concert with human performers.",2007-01-01,['music-mind-and-machine'],,11783,E15-484,2016-12-05T00:17:15.149Z,['bv@media.mit.edu'],True,interactive-music-learning-machine,2006-09-01,[],Interactive Music-Learning Machine,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:05.697Z,"Radio news programs are most valuable when hands and eyes are busy, such as during the morning commute or while working in the kitchen. But radio news is presented in small segments and only at scheduled times. This project seeks to compile audio news from a number of sources, segment stories based on acoustic and possibly semantic cues, and present these on demand in an interactive environment, such as over the telephone or on a computer workstation.",1999-12-30,['living-mobile'],,11791,,2016-12-05T00:17:15.223Z,['geek@media.mit.edu'],True,interactive-radio,1999-12-31,[],Interactive Radio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:06.049Z,"We present the world's first interactive ""scratch and sniff.""  We don't consider this a serious application, but it starts to explore possible uses for computer-controlled scent, and to open up new potentials for alternate interaction modalities.",2001-12-30,['personal-information-architecture'],,11807,E15-068,2016-12-05T00:16:32.294Z,['mike@media.mit.edu'],True,interactive-scratch-sniff,2000-12-31,[],Interactive Scratch & Sniff,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:06.227Z,"Originating from the ""Wearable Seat"" concept, we are designing a seat that has the sensual qualities of clothing while allowing the driver to make natural movements, as, for example, in skiing. These movements are mapped and used to control a vehicle, resulting in a new, athletic, and sensual way of driving a vehicle.",2007-09-01,['smart-cities'],,11815,E15-001,2016-12-05T00:16:32.184Z,[],True,interactive-seat,2003-09-01,[],Interactive Seat,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:06.753Z,"The Interface Tailor is an agent that attempts to adapt a system in response to affective feedback. Frustration is being used as a fitness function to select between a wide variety of different system behaviors. Currently, the Microsoft Office Assistant (or Paperclip) is one example interface that is being made more adaptive. Ultimately the project seeks to provide a generalized framework for making all software more tailor-able.",2004-01-01,"['e-markets', 'affective-computing']",,11839,E15-001,2016-12-05T00:16:32.318Z,['picard@media.mit.edu'],True,interface-tailor,1999-01-01,[],Interface Tailor,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:06.917Z,"Plants are very common in our world and and contain a vast amount of information.  Although there are open debates about the intelligence of plants, it is undeniable that plants have a great ablity to do low-cost sensing and rudimentary communication. The electrophysiology of plants has sparked interest since the late 1800s, but this topic has not been explored recently in the context of modern information technology. We are building novel electronic sensors that ""take a peek"" inside the biological activity of living plants, and explore their use as sensors and thought-provoking educational tools for children and museum exhibits.",2007-01-01,['tangible-media'],,11847,E15-344,2016-12-05T00:17:15.172Z,"['ishii@media.mit.edu', 'fletcher@media.mit.edu']",True,interfacing-electronics-to-living-plants,2002-01-01,[],Interfacing Electronics to Living Plants,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:07.318Z,"The Internet constitutes an enormous electronic architecture that defines spaces without regard to physical structure. We navigate these spaces with browsers, moving from place to place with a click on a link. Internaut proposes that a physical architecture may be derived from the shape of the network and navigated with a first-person 3-D game engine. We are currently working simultaneously on processing Internet structures into maps and modifying an open-source version of the Quake2 engine from ID Software to be usable with this project. These spatial realms are inhabitable communally: as you wander the halls of the 'Net you may run into people you know and people you don't�you can talk to them or arrange a meeting with a friend at a Web site.",2003-01-01,['aesthetics-computation'],,11871,E15-301,2016-12-05T00:16:32.399Z,[],True,internaut,2002-01-01,[],Internaut,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:07.447Z,"Internet 0 is an experiment at networking at the ultra-lightweight scale. Instead of relying on the architectural notions of Internet 1 with its routing, servers, and layered network stacks, we are toying with very small, cheap, and simple ways to bring Internet Protocols all the way to the physical interface.",2007-01-01,['physics-and-media'],,11879,E15-023,2022-06-10T14:12:10.656Z,['neilg@media.mit.edu'],True,internet-0,2001-01-01,[],Internet 0,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:07.561Z,"How do high-level cognitive functions emerge from primitive neural computations to mediate complex human behavior?  We are developing precise, focal ways of investigating phenomena such as trust and risk-taking, in order to understand how they play roles in purchasing, decision-making, social interaction, and other real-world scenarios.",2010-09-01,"['social-machines', 'synthetic-neurobiology']",https://dam-prod2.media.mit.edu/x/files/Display/synthneuro.jpg,11887,E15-435,2016-12-05T00:17:15.278Z,"['dkroy@media.mit.edu', 'esb@media.mit.edu', 'aithpao@media.mit.edu', 'barbara@media.mit.edu']",True,internomics,2007-01-01,[],Internomics,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:07.691Z,"We design games for humans by observing naturally occurring behaviors and recreating them in a synthetic environment. We are now extending this procedure to animals based on our understanding of their behavior. Thus what we create are not only pet ""toys,"" but also tools for gaining insight into animal minds and motivation. We are in the early phase of this research. We have constructed a ""smart perch,"" complete with a controller, 170 LCD screen, speakers and a Webcam. This perch will serve as a hardware platform upon which we will add different forms of software. We are not trying to teach the bird how to ""surf the net.""  Rather, we are trying to give these birds a set of tools for interacting with their environment.",2001-12-30,['pet-projects'],,11895,E15-318,2016-12-05T00:16:32.427Z,['impepper@media.mit.edu'],True,interpet-explorer,1999-12-31,[],InterPet Explorer,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:07.926Z,The Interspace Theater project explores the concept of an online theater space for performing visual narratives. Elements such as stage and actors are visually represented using both abstract graphics and photo/video imagery. Performances are created in real-time by human controllers who interact with each other within the space through the manipulation of actor elements. The live performances can be viewed on the Web by the online audience.,2003-01-01,['aesthetics-computation'],,11911,E15-301,2016-12-05T00:16:32.372Z,[],True,interspace-theater,2002-01-01,[],Interspace Theater,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:08.021Z,"Bringing deliberative process and consensus decision-making to the 21st century! A practical set of tools for assisting in meeting structure, deliberative process, brainstorming, and negotiation. Helping groups to democratically engage with each other, across geographies and time zones.",2012-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/star-icon-128.png,11919,--Choose Location,2016-12-05T00:16:32.502Z,['geek@media.mit.edu'],True,intertwinkles,2011-01-01,[],InterTwinkles,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:08.290Z,"inTouch explores new forms of interpersonal communication through touch. Force-feedback technology is employed to create the illusion that people, separated by distance, are interacting with a shared physical object. The ""shared"" object provides a haptic link between geographically distributed users, opening up a channel for physical expression over distance.",2000-12-30,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/07/inlarge_21501584126_591ab0e224_o.jpg,11943,E15-441,2020-06-07T19:37:13.474Z,['ishii@media.mit.edu'],True,intouch,1995-12-31,['human-computer-interaction'],inTouch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:08.383Z,"While it is increasingly easy to send data to geosynchronous orbit, getting  the data the last few feet to where it is needed remains cumbersome at best. By perturbing the average voltage of a body by a tiny amount, it is possible to send data through a person, creating a Personal Area Network. This lets familiar gestures be associated with logical meaning, such as exchanging business cards through a handshake, authenticating an identity by touching a doorknob, or downloading messages by picking up a telephone.",2000-12-30,['physics-and-media'],,11951,,2016-12-05T00:17:15.343Z,['neilg@media.mit.edu'],True,intrabody-signaling,1995-12-31,[],Intrabody Signaling,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:08.474Z,"We are developing the next generation of invention kits for kids, expanding the range of what kids can design, create, and invent. This project builds on our previous work on programmable-brick technologies, which led to the LEGO MindStorms and PicoCricket products.",2009-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/spaghetti-low-res.jpg,11959,E15-001,2016-12-05T00:16:32.571Z,"['mres@media.mit.edu', 'millner@media.mit.edu', 'nrusk@media.mit.edu', 'ericr@media.mit.edu', 'bss@media.mit.edu']",True,invention-kits-for-kids,2006-01-01,[],Invention Kits for Kids,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:08.628Z,We are exploring mathematical modeling of time-of-flight imaging problems and solutions.,,['camera-culture'],,11975,--Choose Location,2019-04-19T18:28:33.852Z,"['raskar@media.mit.edu', 'achoo@media.mit.edu', 'ayush@media.mit.edu']",True,inverse-problems-in-time-of-flight-imaging,2013-09-01,['imaging'],Inverse problems in time-of-flight imaging,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:08.702Z,"Invisibilia seeks to explore the use of Augmented Reality (AR), head-mounted displays (HMD), and depth cameras to create a system that makes invisible data from our environment visible, combining widely accessible hardware to visualize layers of information on top of the physical world. Using our implemented prototype, the user can visualize, interact with, and modify properties of sound waves in real time by using intuitive hand gestures. Thus, the system supports experiential learning about certain physics phenomena through observation and hands-on experimentation.",2016-08-31,"['terrestrial-sensing', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/files/Display/Invis.png,11983,--Choose Location,2018-08-20T16:39:45.640Z,"['pattie@media.mit.edu', 'amores@media.mit.edu']",True,invisibilia-revealing-invisible-data-as-a-tool-for-experiential-learning,2015-01-01,[],Invisibilia: Revealing Invisible Data as a Tool for Experiential Learning,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:08.785Z,"Invisible Ink is a certified mail application that demonstrates the utility of the blockchain for maintaining a public ledger of transactions while keeping the content of those transactions private. In this case, the idea is a method for guaranteeing the delivery, receipt and existence of email messages. It archives the transaction in the Bitcoin blockchain and uses secure off-chain storage for the other details. Invisible Ink demonstrates the extensibility of this distributed technology for contracts, audits, and recovery of sensitive information. It is an evolution of work begun as the Ethos project. Since Bitcoin has shown that a distributed system of trust can be workable for irreversibly storing time-stamped information, these extensions and applications are potentially important for a wide variety of cases from finance to personal information.",2015-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/design-v3.png,11991,--Choose Location,2017-06-25T16:35:11.855Z,"['lip@media.mit.edu', 'amirl@media.mit.edu']",True,invisible-ink,2015-01-01,"['cryptocurrency', 'data', 'privacy', 'blockchain']",Invisible Ink,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:08.932Z,This research demonstrates how a person's visual pattern on a computer screen can identify his or her task or question. This is based on research and literature that correlates a person's vision pattern with interests.,2002-01-01,"['counter-intelligence', 'context-aware-computing']",,12007,E15-320,2016-12-05T00:16:31.847Z,[],True,invision,1999-09-01,[],InVision,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:09.088Z,"The physical world is increasingly coming online. We have things that measure, sense, and broadcast to the rest of the world. We call this the Internet of Things (IoT). But our cameras are blind to this new layer of metadata on reality. The IoT recorder is a camera that understands what IoT devices it sees and what data they are streaming, thus creating a rich information ""caption-track"" for the videos it records. Using this meta-data, we intend to explore how this enables new video applications, starting with cooking.",,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/iotrecorder.png,12023,--Choose Location,2016-12-05T00:16:32.661Z,"['thariq@media.mit.edu', 'lip@media.mit.edu']",True,iot-recorder,2015-01-01,[],IoT Recorder,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:09.386Z,"Jabberstamp is the first tool that allows children to synthesize drawings and voices. To use Jabberstamp, children create drawings, collages, or paintings on normal paper, and then press a special rubber stamp onto the page to record sounds into their drawings. When they touch the marks of the stamp with a small trumpet, the sounds play back, retelling the stories they have created. Jabberstamp can be used to embed names, narratives, characters' voices, and environmental sound effects into drawings, helping children to communicate their stories with peers and adults, and allowing them to record and situate stories in personally meaningful contexts to share with others�before they have mastered writing.",2008-01-01,['tangible-media'],,12055,E15-344,2016-12-05T00:16:32.720Z,['ishii@media.mit.edu'],True,jabberstamp,2005-09-01,[],Jabberstamp,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:09.468Z,"JaJan! is a telepresence system wherein remote users can learn a second language together while sharing the same virtual environment. JaJan! can support five aspects of language learning: learning in context; personalization of learning materials; learning with cultural information; enacting language-learning scenarios; and supporting creativity and collaboration. Although JaJan! is still in an early stage, we are confident that it will bring profound changes to the ways in which we experience language learning and can make a great contribution to the field of second language education.",2015-09-30,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/Jajan.png,12063,--Choose Location,2017-09-05T13:19:06.022Z,"['pattie@media.mit.edu', 'kevinw@media.mit.edu']",True,jajan-remote-language-learning-in-shared-virtual-space,2014-01-01,[],JaJan!: Remote Language Learning in Shared Virtual Space,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:09.551Z,"Malleable user interfaces have the potential to enable radically new forms of interactions and expressiveness through flexible, free-form, and computationally controlled shapes and displays. This work specifically focuses on particle jamming as a simple, effective method for flexible, shape-changing user interfaces where programmatic control of material stiffness enables haptic feedback, deformation, tunable affordances, and control gain. We explore the design space of malleable and organic user interfaces enabled by jamming through four motivational prototypes that highlight jamming�s potential in HCI, including applications for tabletops, tablets, and for portable shape-changing mobile devices.",2013-01-01,['tangible-media'],,12071,--Choose Location,2016-12-05T00:17:15.508Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu', 'olwal@media.mit.edu']",True,jamming-user-interfaces,2012-01-01,[],Jamming User Interfaces,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:09.633Z,"This project introduces layer jamming as an enabling technology for designing deformable, stiffness-tunable, thin sheet interfaces. Interfaces that exhibit tunable stiffness properties can yield dynamic haptic feedback and shape deformation capabilities. In contrast to particle jamming, layer jamming allows for constructing thin and lightweight form factors of an interface. We propose five-layer structure designs and an approach that composites multiple materials to control the deformability of the interfaces. We also present methods to embed different types of sensing and pneumatic actuation layers on the layer-jamming unit. Through three application prototypes we demonstrate the benefits of using layer jamming in interface design. Finally, we provide a survey of materials that have proven successful for layer jamming.",2016-12-31,['tangible-media'],,12079,--Choose Location,2020-06-09T17:31:57.229Z,"['ishii@media.mit.edu', 'liningy@media.mit.edu', 'jifei@media.mit.edu']",True,jamsheets-interacting-with-thin-stiffness-changing-material,2013-01-01,[],jamSheets: Interacting with thin stiffness-changing material,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:10.542Z,"There is a great opportunity to provide kids and adults with a collection of programmable, computationally enhanced, inexpensive, and user-friendly fabrication tools so that they can make a variety of things for meeting their own personal needs as well as the social challenges of the communities in which they live. But current computer-aided design tools are the main obstacle in increasing creativity and productivity. LaserLogo has been designed to address this issue. LaserLogo is a vector-based, graphic version of the programming language Logo, where the graphical turtle serves as a concrete metaphor for understanding the representations used in describing project files for common fabrication applications. In addition to a programming approach to designing objects, traditional direct manipulation tools are also available. The current designs from LaserLogo can be made with laser and waterjet cutters.",2004-01-01,['personal-fabrication'],,12191,E15-344,2016-12-05T00:16:32.740Z,[],True,jet,2001-01-01,[],JET,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:10.618Z,"Jeux Deux is an interactive concerto for Hyperpiano, Orchestra, and Live Computer Images.  Utilizing novel software, Jeux Deux allows a virtuosic pianist to interact with a real acoustic piano (Yamaha Disklavier) in real-time, creating sounds and textures that are unplayable by a human player alone.  Live computer graphics are generated in real-time from the piano's data and projected above the performer.",2007-09-01,['opera-of-the-future'],,12199,E15-443C,2016-12-05T00:16:32.771Z,['tod@media.mit.edu'],True,jeux-deux,2005-01-01,[],Jeux Deux,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:13.098Z,"How can we help people reflect on their own learning process? The goal of this project is to develop new technological tools and pedagogical strategies to cultivate reflection. Jots are brief updates that people write as they use our Scratch programming environment, to describe their thoughts, frustrations, and excitement.  Users' Jots are displayed on their Scratch user pages, so they can explore their own processes and share them with others.",2009-09-01,['lifelong-kindergarten'],,12391,LEGO Learning Lab,2016-12-05T00:16:32.836Z,"['mres@media.mit.edu', 'ericr@media.mit.edu']",True,jots,2008-09-01,[],Jots,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:13.355Z,"Over the last two decades, digital technologies have flattened old hierarchies in the news business and opened the conversation to a multitude of new voices. To help comprehend this promising but chaotic new public sphere, we're building a ""social news machine"" that will provide a structured view of the place where journalism meets social media. The basis of our project is a two-headed data ingest. On one side, all the news published online 24/7 by a sample group of influential US media outlets. On the other, all Twitter comments of the journalists who produced the stories. The two streams will be joined through network analysis and algorithmic inference. In future work we plan to expand the analysis to include all the journalism produced by major news outlets and the overall public response on Twitter, shedding new light on such issues as bias, originality, credibility, and impact.",2016-05-31,['social-machines'],,12407,--Choose Location,2016-12-05T00:17:15.736Z,"['dkroy@media.mit.edu', 'soph@media.mit.edu', 'mmv@media.mit.edu', 'pernghwa@media.mit.edu']",True,journalism-mapping-and-analytics-project-jmap,2014-09-01,[],Journalism Mapping and Analytics Project (JMAP),PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:13.750Z,"The Junkyard Jumbotron (JJ) lets one take a collection of random screens and instantly stitch them into one large virtual display simply by taking a picture of their arrangement. The software works with laptops, smartphones, tablets�anything that runs a web browser. It shows a new way of using mobile devices to create a feeling of community: ganging mobile devices together to create a shared experience. And the JJ is designed from the ground up to make the process of connecting heterogeneous user devices together ""in the wild"" easy and fun, with no anti-social wireless configuration, app installation, or device compatibility anxiety.",2012-01-01,['viral-communications'],,12439,,2016-12-05T00:16:32.886Z,"['borovoy@media.mit.edu', 'rahulb@media.mit.edu', 'csik@media.mit.edu', 'lip@media.mit.edu']",True,junkyard-jumbotron,2010-09-01,[],Junkyard Jumbotron,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:13.865Z,"Making optimal decisions can improve a wide array of situations. Humans often perform well on small, focused choices, but performance degrades as complexity increases. Justify leverages human fine-grained reasoning capabilities into a hierarchy that automatically aggregates and summarizes at each level. This flexible organization makes understanding complex arguments more manageable. A Justify discussion is comprised of points; each point has a type that conveys its domain-independent meaning and determines its ""summarization strategy."" There are points for questions, answers, arithmetic, pro and con rationale, voting, and grouping that help to crystalize an issue. These point types represent a language to facilitate reasoning both for humans and the Justify program itself.",2014-09-01,['software-agents'],,12447,--Choose Location,2022-06-10T14:18:57.510Z,"['lieber@media.mit.edu', 'cfry@media.mit.edu']",True,justify,2012-01-01,[],Justify,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:14.354Z,"If everyone says time is relative, why is it still so rigidly defined? There have been many attempts to address the issue of coordinating schedules, but each of these attempts runs into an issue of rigidity: in order to negotiate an event, a specific time must be designated in advance. This model is inherently poor at accommodating life's unpredictability. Kairoscope looks at time from a human perspective: allowing people to coordinate events socially and on the fly, without worrying about precision. This project evaluates the potential implications of a shared, malleable schedule, as well as the data inputs and user interactions necessary to create such a system.",2010-01-01,['information-ecology'],,12495,E15-301,2016-12-05T00:16:32.939Z,['holtzman@media.mit.edu'],True,kairoscope-social-time,2009-01-01,[],Kairoscope: Social Time,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:10:14.442Z,"One of the major barriers encountered by designers and artists when programming digital media is difficulty translating mental models of interactive creations into a format and language that can be interpreted by computers. This problem arises because current software-development environments demand a sequential format for code. In contrast, Kaleido proposes a new interface that enables a user-defined, conceptual, visiospatial representation of computation that complements the traditional text-based perspective. Kaleido is a tool designed to help visual thinkers program; users can use Kaleido to create personally meaningful visuals for their code. Kaleido allows individuals to plan, organize, and navigate code in the idiosyncratic way we each think.",2010-01-01,['design-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/logo_home.png,12503,E15-301,2022-06-10T13:42:34.391Z,['dsmall@media.mit.edu'],True,kaleido-idiosyncractic-graphical-interfaces-for-software-development,2009-01-01,[],Kaleido: Idiosyncractic Graphical Interfaces for Software Development,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:14.597Z,"The Kaoss network allows an interdependent group of musicians and novices to influence and control their own, as well as each other's, musical output using a mixture of digital and acoustic musical instruments. The project, which draws on theories in system dynamics, decentralized systems, expression, and aesthetics, investigates how coherent and meaningful music can emerge from an interconnected group of professionals and novices. Such a constellation bears the promise of providing new sorts of rich and immersive musical collaborations where the audience takes an active part in the musical experiance.",2000-12-30,['opera-of-the-future'],,12519,E15-491,2016-12-05T00:16:33.003Z,['tod@media.mit.edu'],True,kaoss,1999-12-31,[],Kaoss,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:15.315Z,"It takes quite a few people to keep the visuals flowing for live broadcast or display: cameramen filming, technicians cutting and fading clips, and still others controlling the segue animations and copy. Lacking access to that kind of manpower or equipment, we have built a digital video camera with onboard controls that allows for live, real-time editing and special effects for immediate redisplay or broadcast. A wireless version is currently in development.",2002-12-30,['aesthetics-computation'],,12607,E15-301,2016-12-05T00:16:33.085Z,[],True,key-grip,2000-12-31,[],Key Grip,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:15.392Z,KickSoul is a wearable device that maps natural foot movements into inputs for digital devices. It consists of an insole with embedded sensors that track movements and trigger actions in devices that surround us. We present a novel approach to use our feet as input devices in mobile situations when our hands are busy. We analyze the foot's natural movements and their meaning before activating an action.,2017-08-01,"['responsive-environments', 'fluid-interfaces']",,12615,--Choose Location,2017-08-07T14:36:03.410Z,"['pattie@media.mit.edu', 'changzj@media.mit.edu', 'joep@media.mit.edu']",True,kicksoul-a-wearable-system-for-foot-interactions-with-digital-devices,2015-01-01,[],KickSoul: A Wearable System for Foot Interactions with Digital Devices,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:15.475Z,"KidCAD is a digital clay interface for children to remix toys. KidCAD allows children to imprint 2.5D shapes from physical objects into their digital models by deforming a malleable gel input device, deForm. Users can mashup existing objects, edit and sculpt or draw new designs on a 2.5D canvas using physical objects, hands and tools as well as 2D touch gestures. Finally designs can be 3D printed so that children can play with the toys they designed using KidCAD.",2012-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/kidCAD.png,12623,--Choose Location,2016-12-05T00:16:33.106Z,['ishii@media.mit.edu'],True,kidcad,2011-01-01,[],KidCAD,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:15.551Z,"How could we enhance the experience of video-conferencing by utilizing an interactive display? With a Kinect camera and sound sensors, we explore how expanding a system's understanding of spatially calibrated depth and audio alongside a live video stream can generate semantically rich three-dimensional pixels, containing information regarding their material properties and location. Four features have been implemented: Talking to Focus, Freezing Former Frames, Privacy Zone, and Spacial Augmenting Reality.",2012-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/Screen%20shot%202011-04-06%20at%2011.07.16%20AM.png,12631,--Choose Location,2020-06-09T20:21:00.558Z,"['ishii@media.mit.edu', 'raskar@media.mit.edu', 'liningy@media.mit.edu']",True,kinected-conference,2011-01-01,[],Kinected Conference,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:15.629Z,"kiNET is an active wall that performs, changes, shifts, and adapts. The wall is no longer an opaque plane that encloses our bodies in a space; rather, it is an artistic intervention in a space, a window, a painting, a canvas. Distributed electromagnetic nodes actuate on a surface, creating propagating ripples. The canvas ceases to be a static surface on which the artist lays his or her work, and becomes a body in movement.",2005-09-01,['computing-culture'],,12639,E15-020C,2016-12-05T00:16:33.144Z,['csik@media.mit.edu'],True,kinet,2004-09-01,[],kiNET,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:15.709Z,"The ability of shape displays to move and manipulate objects enables the assembly, disassembly, and reassembly of different forms and structures using a set of fundamental building blocks (cubes). We present different assembly techniques such as stacking, scaffolding, and catapulting, which allow us to create 3D structures on a shape display.",2015-09-01,['tangible-media'],,12647,--Choose Location,2016-12-05T00:16:09.457Z,['ishii@media.mit.edu'],True,kinetic-blocks,2015-01-01,[],Kinetic Blocks,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:15.791Z,"Kinetic Sketch-Up is a system for prototyping motion and kinetic transformability into interfaces. A series of actuated modules allow users to embed programmable actuation into tangible interfaces, dynamically coupling digital information with self-reconfigurable structures. Modules in the systems combine varied materials and technologies for actuation  creating different forms of motion. This provides a method to prototype perceptual qualities which motion can elicit in the design of an interface: drawing our attention, providing physical feedback, and conveying information through physical change. As interaction design begins to incorporate many of the interaction principles of robotics, the system provides a new design tool and vocabulary for motion construction in the emerging areas of transformable products, environments, and architecture. ",2009-01-01,['tangible-media'],,12655,E15-344,2016-12-05T00:16:33.257Z,['ishii@media.mit.edu'],True,kinetic-sketch-up,2007-09-01,[],Kinetic Sketch-Up,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:16Z,"We use text-capable mobile devices, such as cellphones with SMS and two-way pagers, as mobile interfaces to our desktop computers, combining PDA functionality, communication, and Web access into a single device. Rather than put intelligence into the portable device, we rely on the wireless network to connect to services that enable access to multiple desktop databases, such as your calendar or Rolodex, and external sources, such as news, weather, and traffic.",2003-01-01,['living-mobile'],,12679,E15-344,2016-12-05T00:16:33.289Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,knothole,1997-01-01,[],Knothole,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:16.078Z,"""Know Your Exit"" is an interactive, web-based music composition. To create this piece, we crowdsourced over 1100 tracks from all over the world. People were asked to sing, clap, or tell a short story. The web-based version is dynamic and incorporates live tweets and geo-locations of the audio contributions.",,[],,12687,,2016-12-05T00:16:33.314Z,['rmorris@media.mit.edu'],True,know-your-exit,2011-09-01,[],Know Your Exit,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:16.511Z,"This is a suite of devices and protocols to support applications in wearable human/social sensing linked to a distributed camera and vision system. The current system includes a sensate wristwatch with biological and gestural sensors, and a lapel-pin device with motion and audio-affect sensing. These all communicate with wall-mounted devices (Portals), each of which has a high-resolution camera, environmental sensors, and a localization system for all devices in the network. All devices record data and audio in sync with the recorded video. A full-spec Zigbee network supports device synchronization and mesh networking. All devices have enough on-board power to extract features from the data.",2011-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/PortalPics.jpg,12735,E15-351,2016-12-05T00:16:33.391Z,['joep@media.mit.edu'],True,lab-wide-and-wearable-sensor-and-video-network,2007-01-01,[],Lab-Wide and Wearable Sensor and Video Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:16.727Z,"With growing dependence on digital voice and text messaging, it is crucial to be able to access messages in the most intuitive fashion. The LampMail project aims to construct a basic prototype of an interactive lamp, which is also used as an interface for voicemail retrieval and control. The lamp is built in the shape of a cat to enhance the idea of interactivitiy. The fiberoptic ""whiskers"" of the cat light up when a new voicemail message is received. When the lamp is turned on, the ""eyes"" (bulbs) light up and the lamp starts playing the new messages. The ""nose"" is a rotary switch-control used to skip to the next message. ",2004-01-01,['living-mobile'],,12759,E15-368,2016-12-05T00:16:33.422Z,['geek@media.mit.edu'],True,lampmail,2003-09-01,[],LampMail,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:16.884Z,"Landman Report Card is the first in a suite of applications designed to help communities affected by extractive industries to recognize, report, and act on their interests. LRC allows landowners to document, discuss, and rate their experiences with landmen, the professional negotiators who work for oil and gas companies. We are currently deploying the application in communities in several states in the US.",2011-01-01,['civic-media'],,12775,Cube,2016-12-05T00:16:33.531Z,['ethanz@media.mit.edu'],True,landman-report-card,2007-01-01,[],Landman Report Card,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:16.969Z,"Social media are becoming increasingly heterogeneous in nature and usage as online communication evolves. Understanding how communities use and repurpose media is important to our understanding of technology and society. We present Landscape of Words, a visualization that uses topic modeling to statistically summarize the semantics of arbitrarily large groups of individual contributors. Common topics, linguistic styles, and memes are projected onto an interactive 3-D landscape, allowing users to explore the usage patterns of millions of individuals. Our first prototype examines Twitter, illustrating how trends in topics evolve over time.",2008-09-01,['sociable-media'],,12783,E15-390,2016-12-05T00:16:33.447Z,['judith@media.mit.edu'],True,landscape-of-words,2008-01-01,[],Landscape of Words,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:17.047Z,"Language is inextricably linked to the activities and events that make up our daily lives. For a child learning language, everyday activities provide an important context for learning first words. This work builds on the corpus collected for the Human Speechome Project, the largest multimodal corpus of one child's early life, to explore how experience with language ties to space, time, and daily activity to support word learning. We use manual and fully automatic methods, ranging from direct annotation to computer vision and unsupervised latent variable approaches, to identify the abstract ""stuff of life"" that makes up early experience. We show how a word's contextual grounding predicts when it will be learned.",2014-01-01,['social-machines'],,12791,--Choose Location,2016-12-05T00:17:15.801Z,['dkroy@media.mit.edu'],True,language-word-learning-and-the-activity-substrate-of-everyday-life,2012-09-01,[],"Language, Word Learning, and the Activity Substrate of Everyday Life",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:17.205Z,"This study aims to bring objective measurement to the multiple ""pulse"" and ""pulse-like"" measures made by practitioners of traditional Chinese medicine (TCM). The measurements are traditionally made by manually palpitating the patient's inner wrist in multiple places, and relating the sensed responses to various medical conditions. Our project brings several new kinds of objective measurement to this practice, compares their efficacy, and examines the connection of the measured data to various other measures of health and stress. Our approach includes the possibility of building a smartwatch application that can analyze stress and health information from the point of view of TCM.",2019-08-31,"['advancing-wellbeing', 'affective-computing']",,12807,--Choose Location,2021-11-06T11:12:47.877Z,"['picard@media.mit.edu', 'javierhr@media.mit.edu', 'akanes@media.mit.edu', 'cvx@media.mit.edu']",True,large-scale-pulse-analysis,2015-09-01,['health'],Traditional Chinese medicine-inspired pulse analysis,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:17.290Z,"A very simple and inexpensive scanning laser rangefinder system has been developed for use as a precise gestural interface in front of a ""smart"" interactive surface. This device works by detecting the phase shift between the emitted laser and the detected reflection off of a bare hand. A simple microprocessor locates peaks in the intensity data corresponding to different objects, and outputs the angle and range information for each one over a serial connection. Unlike computer vision systems, our 2-D scanner measurement is unambiguous; it requires essentially no processing, and is unaffected by background light.  We have recently re-engineered our prototype and are making more systems to explore new applications in collaboration with sponsors and research partners.  We are also exploring integrating this system into small, portable barcode scanners.",2001-12-30,['responsive-environments'],,12815,,2016-12-05T00:16:33.553Z,['joep@media.mit.edu'],True,laserwall,1996-12-31,[],LaserWall,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:17.392Z,"LaserWho is large-scale gesture-based installation of the Visual Who project that uses the LaserWall for display and input. Users place anchors, each representing a different topic relevant to that community, on the screen. The names of people who are particularly drawn to a topic flow towards the associated anchor, and the resulting animation reveals the structure of the community in terms of shared interests and affiliations. Although LaserWho uses data visualization algorithms and techniques, the feel of this installation is very different than the typical analytic application. The image is shown on a large, rear-projection screen and its input is via the natural, intuitive gestures of picking up and placing objects. Music accompanies the visualization: each anchor has its own theme and the changing state of the system shapes and modulates the composition. LaserWho has been shown at Opera Totale in Venice and in New Orleans as part of the SIGGRAPH 2000 Emerging Technologies exhibition.",2000-12-30,['sociable-media'],,12823,BATCAVE,2016-12-05T00:17:15.848Z,"['judith@media.mit.edu', 'joep@media.mit.edu']",True,laserwho,1999-12-31,[],LaserWho,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:17.559Z,"We are developing tomographic techniques for image synthesis on displays composed of compact volumes of light-attenuating material. Such volumetric attenuators recreate a 4D light field or high-contrast 2D image when illuminated by a uniform backlight. Since arbitrary views may be inconsistent with any single attenuator, iterative tomographic reconstruction minimizes the difference between the emitted and target light fields, subject to physical constraints on attenuation. For 3D displays, spatial resolution, depth of field, and brightness are increased, compared to parallax barriers. We conclude by demonstrating the benefits and limitations of attenuation-based light field displays using an inexpensive fabrication method: separating multiple printed transparencies with acrylic sheets.",2015-09-15,['camera-culture'],,12839,--Choose Location,2016-12-05T00:16:45.881Z,"['raskar@media.mit.edu', 'gordonw@media.mit.edu', 'naik@media.mit.edu']",True,layered-3d-glasses-free-3d-printing,2010-09-01,[],Layered 3D: Glasses-Free 3D Printing,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:17.654Z,"Have you ever been forwarded an email that you just can't believe? Our inboxes are rife with misinformation. The truth is out there, just not when we actually need it. LazyTruth is a Gmail gadget that surfaces verified truths when you receive common chain emails. It all happens right in your inbox, without requiring you to search anywhere. The result is that it becomes much more convenient for citizens to combat misinformation, rather than acquiesce to its volume. Whether it's political rumors, gift card scams, or phishing attempts, fact is now as convenient as fiction. ",2013-09-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/chromestore-small.gif,12847,--Choose Location,2016-12-05T00:16:33.622Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'stempeck@media.mit.edu']",True,lazytruth,2012-01-01,[],LazyTruth,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:17.740Z,"L2T:T2L is a multi-year, three-phase program enabling urban youth to learn science, technology, engineering, and mathematics. First, the students study in a variety of technical areas, including programming, robotics, personal fabrication, digital video, alternative energy, and Web design and tools. Next, they develop their own projects over an extended period using computational tools. Finally, they will work with younger children in several community centers to provide the same type of learning experiences. The idea is that the youth will not only learn by teaching and thereby solidify their knowledge, they also will gain the satisfaction of contributing to the development of their community. The project also serves to test new content, tools, and learning materials for subsequent use in schools, and scales through consistent participation.",2003-01-01,[],,12855,E15-368,2016-12-05T00:17:15.866Z,"['papert@media.mit.edu', 'cavallo@media.mit.edu']",True,learn-to-teach-teach-to-learn,2003-01-01,[],Learn to Teach: Teach to Learn,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:17.827Z,"Designed specifically as a tool to help young elementary school children explore the fundamentals of mechanical motion, this project combined LEGO-based motion modules with a java-based search tool. Each motion module isolated and highlighted a specific type of mechanical motion (i.e. eccentric motion, up-and-down movement) which could then be built upon or integrated into a child's existing creations. The software component could then be used to lookup the mechanics underlying each module or as a reference to help the child choose and build the type of motion they were trying to replicate.     
",1999-09-01,['lifelong-kindergarten'],,12863,E15-001,2016-12-05T00:16:33.493Z,['mres@media.mit.edu'],True,learning-about-motion,1998-01-01,[],Learning About Motion,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:17.990Z,"This project explores how learning must be the basis of sustainable development and how constructionist approaches to learning can be applied to assist environmentally benign sustainable development. If solutions come from outside local contexts, the situation is inherently unsustainable. We are developing new computational tools for agriculture, environment, and collaborative problem-solving, including tools for those who are not textually literate. We are doing fieldwork in Senegal with the United Nations Food and Agriculture Organization (UN-FAO) and local partners to test and refine the tools and methodologies.",2003-01-01,['future-of-learning-2'],,12879,E15-489,2016-12-05T00:17:15.882Z,['cavallo@media.mit.edu'],True,learning-as-a-basis-for-sustainable-development,2001-09-01,[],Learning as a Basis for Sustainable Development,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:18.091Z,"It's well known that living in a foreign country dramatically improves the effectiveness of learning a second language over classroom study alone. We are designing language-teaching sequences for a sensor-equipped residence that can detect user interaction with household objects. We use our common-sense knowledge base and reasoning tools to construct teaching sequences, wholly in the target language, of sentences and question-answering interactions that gradually improve the learner's language competence. For example, the first time the user sits in a chair, the system responds with the foreign-language word for ""chair,"" and later with statements and questions that use complete sentences, tenses, questions, materials, goals, and plans.",2014-09-01,['software-agents'],,12887,E15-383,2022-06-10T14:18:57.521Z,['lieber@media.mit.edu'],True,learning-common-sense-in-a-second-language,2009-01-01,[],Learning Common Sense in a Second Language,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:18.192Z,"""I can't do this"" and ""I'm not good at this"" are common statements made by kids while trying to learn. Usually triggered by affective states of confusion, frustration, and hopelessness, these statements represent some of the greatest problems left unaddressed by educational reform. Education has emphasized conveying a great deal of information and facts, and has not modeled the learning process. When teachers present material to the class, it is usually in a polished form that omits the natural steps of making mistakes (feeling confused), recovering from them (overcoming frustration), deconstructing what went wrong (not becoming dispirited), and finally starting over again (with hope and maybe even enthusiasm). Learning naturally involves failure and a host of associated affective responses. This project aims to build a computerized learning companion that facilitates the child's own efforts at learning. The goal of the companion is to help keep the child's exploration going, by occasionally prompting with questions or feedback, and by watching and responding to the affective state of the child�watching especially for signs of frustration and boredom that may precede quitting, for signs of curiosity or interest that tend to indicate active exploration, and for signs of enjoyment and mastery, which might indicate a successful learning experience. The companion is not a tutor that knows all the answers but rather a player on the side of the student, there to help him or her learn, and in so doing, learn how to learn better.",2004-09-01,['affective-computing'],,12895,E15-001,2016-12-05T00:16:33.851Z,"['picard@media.mit.edu', 'atenea@media.mit.edu', 'reilly@media.mit.edu', 'bickmore@media.mit.edu']",True,learning-companion,2000-01-01,[],Learning Companion,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:18.286Z,"Learning Creative Learning is an online course and community of educators, designers, technologists, and tinkerers exploring creative learning. Participants create hands-on projects based on their interests, explore new technologies, and share ideas with peers from all over the world. The course is free, and open to everyone. Materials are translated in several languages and are always available on the course website. Participants can explore the materials at their own pace at any time, or participate with a cohort when a new round is offered. ",,"['ml-learning', 'lifelong-kindergarten', 'digital-learning-studio', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/53b7e530-7de5-48e6-a2f4-7aeafbd7592b,12903,--Choose Location,2023-09-07T23:24:31.411Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'ps1@media.mit.edu', 'tarmelop@media.mit.edu', 'eschill@media.mit.edu', 'gabaree@media.mit.edu', 'carolcr@media.mit.edu', 'rupal@media.mit.edu']",True,learning-creative-learning,2013-01-01,"['design', 'learning-teaching', 'makers']",Learning Creative Learning,PUBLIC,http://learn.media.mit.edu/lcl,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:18.490Z,"Many younger students find science and engineering uninteresting and difficult, while university students often learn numerical methods but don't get practical design experience. We have found robot building to be a highly engaging and pedagogically rich activity for students of all ages, facilitating explorations of sensing, control, and mechanism. This research included the development of technologies to facilitate robot-building workshops, and the evaluation of students' learning processes.",1999-09-01,['lifelong-kindergarten'],,12919,E15-001,2016-12-05T00:16:33.879Z,['mres@media.mit.edu'],True,learning-engineering-by-designing-robots,1992-01-01,[],Learning Engineering by Designing Robots,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:18.656Z,"Humans and animals display a flexible and rich array of social competencies, demonstrating the ability to interpret, predict, and react appropriately to the behavior of others, to engage others in a variety of complex social interactions, and to enrich their understanding of the environment by observing other's behavior. We believe that developing systems that have these same sorts of social abilities is a critical step in designing animated characters, robots, and other computer agents, who appear intelligent and capable in their interactions with humans (and each other), and who are intuitive and engaging for humans to interact with. Towards this end, we have begun adding a variety of social competencies to our character architecture, such as imitation, gaze following, and simple empathy (the ability to recognize other's emotions), as well taking steps towards creating characters who can form complex beliefs and expectations about other's behavior (such as Max T. Rat's ability to recall people with whom he has previously interacted, and to react appropriately to them). ",2006-01-01,[],,12927,E15-441,2016-12-05T00:16:09.645Z,[],True,learning-from-and-about-others-social-learning-for-synthetic-characters,2003-01-01,[],Learning From and About Others: Social Learning for Synthetic Characters,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:19.136Z,"Developing nations typically settle for technologies that were designed elsewhere, with other purposes in mind. The Learning Independence Networks project aims to change this, working with networks of organizations in developing nations (including universities, foundations, companies, and NGOs) to help them build the capacity to develop their own technologies, appropriate for local needs. We are working most closely with universities, helping them to develop new courses, research programs, technological infrastructures, and strategies for collaborating with other members of the network on the design of new technologies. The first Learning Independence Network, called Prometheus, is currently under development in Costa Rica, aiming to produce a new generation of technologies for use in community development and education.",2004-01-01,['personal-fabrication'],,12951,E15-344,2016-12-05T00:17:15.948Z,[],True,learning-independence-networks,2001-01-01,[],Learning Independence Networks,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:19.631Z,"We aim to computationally model the meaning of music by taking advantage of community usage and description�using the self-selected and natural similarity clusters, opinions ,and usage patterns as labels and ground truth to inform on-line and unsupervised ""music acquisition"" systems that learn about music by listening and reading. We present a framework for capturing community metadata from free-text sources, audio representations robust enough to handle event and meaning relationships yet general enough to work across domains of music, and a machine-learning framework for learning the relationship between meaning and music automatically and iteratively from a cold start. These unbiased and organic machine-learning approaches show superior accuracy in music and multimedia intelligence tasks such as similarity, artist classification, and recommendation.",2005-01-01,['music-mind-and-machine'],,12983,E15-484,2016-12-05T00:16:33.961Z,['bv@media.mit.edu'],True,learning-the-meaning-of-music,2002-01-01,[],Learning the Meaning of Music,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:19.852Z,"More and more computational activities revolve around collecting, accessing, and manipulating large sets of data, but introductory approaches for learning programming typically are centered around algorithmic concepts and flow of control, not around data. Computational exploration of data, especially data-sets, has been usually restricted to predefined operations in spreadsheet software like Microsoft Excel. This project builds on the Scratch programming language and environment to allow children to explore data and datasets. With the extensions provided by this project, children can build Scratch programs to not only manipulate and analyze data from online sources, but also to collect data through various means such as surveys and crowd-sourcing. This toolkit will support many different types of projects like online polls, turn-based multiplayer games, crowd-sourced stories, visualizations, information widgets, and quiz-type games.",2016-12-01,['lifelong-kindergarten'],,12999,--Choose Location,2016-12-11T15:29:24.776Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'sdg1@media.mit.edu']",True,learning-with-data,2011-01-01,[],Learning with Data,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:19.963Z,"We approach the problem of how machines, and humans, can learn words that describe actions. We put forth that such words are grounded not in the sensory-motor aspect of an action, but rather in the intentions of the person performing the action. We therefore pose the problem of action-word learning in two stages: intention recognition and linguistic mapping. The first of these stages is cast as a plan-recognition problem in which state-action sequences are parsed using a probabilistic online chart parser. The second stage casts mapping in a Bayesian framework, employing algorithms used in speech recognition and machine translation.",2005-01-01,['social-machines'],,13007,E15-487,2016-12-05T00:16:34.031Z,"['dkroy@media.mit.edu', 'mbf@media.mit.edu']",True,learning-words-for-actions,2004-01-01,[],Learning Words for Actions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:20.056Z,"The Brazilian Creative Learning Network is a collaboration between the MIT Media Lab and the Lemann Foundation to foster creative learning in Brazilian public education. Established in February 2015, the program designs new technologies, support materials, and innovative initiatives to engage Brazilian public schools, after-school centers, and families in learning practices that are more hands-on, creative, and centered on students' interests and ideas. ---Rede Brasileira de Aprendizagem Criativa é uma colaboração entre o MIT Media Lab e a Fundação Lemann visando incentivar a aprendizagem criativa na educação pública do Brasil.Criado em fevereiro de 2015, o programa cria novas tecnologias, materiais de apoio e iniciativas que ajudem escolas públicas, organizações de educação não formal, e famílias a implementar práticas de aprendizagem que sejam mais mão na massa, criativas e centradas nos interesses dos alunos.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/01/13/22923408766_cca66a24d6_o.jpg,13015,--Choose Location,2022-09-30T02:11:35.150Z,"['mres@media.mit.edu', 'leob@media.mit.edu', 'annbv@media.mit.edu', 'carolcr@media.mit.edu', 'thaisx@media.mit.edu']",True,creative-learning-in-brazil,2015-01-01,['learning-teaching'],Brazilian Creative Learning Network / Aprendizagem Criativa no Brasil,PUBLIC,http://aprendizagemcriativa.org/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:20.146Z,"With networked cameras in everyone's pockets, we are exploring the practical and creative possibilities of public imaging. LensChat allows cameras to communicate with each other using trusted optical communications, allowing users to share photos with a friend by taking pictures of each other, or borrow the perspective and abilities of many cameras.",,['camera-culture'],,13023,E15-320,2019-04-19T18:29:00.238Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,lenschat-sharing-photos-with-strangers,2009-09-01,[],LensChat: Sharing photos with strangers,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:20.242Z,"Conversations between two individuals—whether between doctor and patient, mental health therapist and client, or between two people romantically involved with each other—are complex. Each participant contributes to the conversation using her or his own ""lens."" This project involves advanced probabilistic graphical models to statistically extract and model these dual lenses across large datasets of real-world conversations, with applications that can improve crisis and psychotherapy counseling and patient-cardiologist consultations. We're working with top psychologists, cardiologists, and crisis counseling centers in the United States.",,"['advancing-wellbeing', 'ethics-and-governance']",,13031,--Choose Location,2019-09-18T16:57:18.574Z,"['picard@media.mit.edu', 'kdinakar@media.mit.edu']",True,lensing-cardiolinguistics-for-atypical-angina,2014-01-01,"['artificial-intelligence', 'health', 'machine-learning', 'healthcare', 'medicine', 'women']",Lensing: Cardiolinguistics for Atypical Angina,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:21.204Z,"We believe that the narrative of only listening to experts or trusting the wisdom of the crowd blindly is flawed. Instead we have developed a system that weighs experts and lay-people differently and dynamically and show that a good balance is required. We show that our methodology leads to a 15 percent improvement in mean performance, 15 percent decrease in variance, and almost 30 percent increase in Sharpe-type ratio in a real online market.",,"['connection-science', 'human-dynamics']",,13119,--Choose Location,2022-06-10T13:18:04.395Z,"['sandy@media.mit.edu', 'dhaval@media.mit.edu']",True,leveraging-leadership-expertise-more-effectively-in-organizations,2015-01-01,[],Leveraging leadership expertise more effectively in organizations,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:21.471Z,"Generating 3D Lichtenberg structures in sintered media (i.e. glass) using electricity offers a new approach to digital fabrication. By robotically controlling the electrodes, a digital form can be rapidly fabricated with the benefits of a fine fractal structure. There are numerous applications, ranging from chemical catalysts, to fractal antennas, to product design.",2015-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/small_glass_piece.jpg,13143,--Choose Location,2021-05-21T13:03:52.827Z,"['stevenk@media.mit.edu', 'neri@media.mit.edu']",True,lichtenberg-3d-printing,2013-09-01,[],Lichtenberg 3D Printing,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:21.685Z,"LifeNet is a probabilistic spatial and temporal model allowing common-sense physical simulation. Mixtures of gaussians are used to map from the physical world's real numbers to human language's symbolic conceptual world. Humans are very good at moving around and physically manipulating the world in which they live, quickly considering many collections of physical events and objects in order to choose a set of actions. The current state of the art in computer physics simulations does not take advantage of common-sense knowledge such as ""things usually fall if not supported."" Building a common-sense physics simulation allows us to take a paragraph of text and quickly reconstruct the physical situation in more or less detail as required for inference, planning, and further reflective algorithms. Immediately possible applications include interpreting sensor data, comparing physical descriptions of events in text, and efficiently planning in multiscale, robotic physical manipulation environments.",2008-01-01,['society-of-mind'],,13167,E15-309,2016-12-05T00:16:34.163Z,"['minsky@media.mit.edu', 'joep@media.mit.edu']",True,lifenet-common-sense-physics,2005-01-01,[],LifeNet: Common-Sense Physics,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:21.767Z,"Miniature sensors are so sophisticated that a wristwatch-mounted inertial measurement system can differentiate subtle actions such as shaking hands or turning a doorknob. Presently, sensors placed on objects in the environment can detect location, movement, sound, and temperature; however, interpretation of this data into human language remains challenging. Previous attempts at activity recognition force all descriptions into small, formal catagories specified in advance. For example, location could be at home or at the office�these models have not been adapted to the wider range of complex, dynamic, and idiosyncratic human activities. LifeNet constructs a mapping between sensor streams and partially ordered sequences of events in human language. We believe that mapping sensor data into LifeNet will act as a ""semantic mirror"" to meaningfully interpret sensory data into cohesive patterns in order to understand and predict not only human action, but give clear direction toward understanding human thought and motivation.",2005-09-01,['responsive-environments'],,13175,E15-309,2016-12-05T00:16:09.680Z,"['minsky@media.mit.edu', 'joep@media.mit.edu']",True,lifenet-learning-common-sense-from-sensors,2005-01-01,[],LifeNet: Learning Common Sense from Sensors,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:21.851Z,"LifeNet is a first-person model of a single person's knowledge. We are using multiple LifeNets to model multiple people and attempt to predict typical social knowledge-sharing patterns, given the different knowledge structures of different people. Possible applications include more efficient social communication protocols, project team suggestions, social user model comparisons, and story understanding.",2008-01-01,['society-of-mind'],,13183,E15-309,2016-12-05T00:17:16.246Z,['minsky@media.mit.edu'],True,lifenet-social-common-sense,2005-09-01,[],LifeNet: Social Common Sense,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:22.112Z,"Light Piping, an installation of solar technology on the roof of the new Media Lab extension, will bring natural light into the old building by focusing the light from an array of small lenses that focus the light into fiber optic cable. The light is transmitted as full-spectrum sunlight throughout the day, lowering energy costs, increasing psychological well being, and lighting an indoor garden symbolic of the connection between inside and outside in a more permeable building.",2009-01-01,['fluid-interfaces'],,13199,Lower Atrium,2016-12-05T00:17:16.410Z,['labrune@media.mit.edu'],True,light-piping-solar-panels,2009-01-01,[],Light Piping Solar Panels,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:22.869Z,"LightSwarm is a platform for interaction between humans and swarm robots. Swarm robots are implemented as augmented-reality agents that communicate and interact through movements. While each robot is simple, the aggregate shows complex behavior. In this way, LightSwarm invites one to think of the mind as a loose consensus of a society of agents, which allows for more nuanced interactions.",2015-09-01,['personal-robots'],,13231,--Choose Location,2016-12-05T00:17:16.350Z,"['cynthiab@media.mit.edu', 'palash@media.mit.edu']",True,lightswarm,2014-01-01,[],LightSwarm,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:23.064Z,"The LilyPad Arduino is a set of tools that empowers people to build soft, flexible, fabric-based computers. A set of sewable electronic modules enables users to blend textile craft, electrical engineering, and programming in surprising and beautiful ways. A series of workshops that employed the LilyPad have demonstrated that tools such as these, which introduce engineering from new perspectives, are capable of involving unusual and diverse groups in technology development. Ongoing research will explore how the LilyPad and similar devices can engage under-represented groups in engineering, change popular assumptions about the look and feel of technology, and spark hybrid communities that combine rich crafting traditions with high-tech materials and processes. ",2014-01-01,['high-low-tech'],https://dam-prod2.media.mit.edu/x/files/Display/IMG_0642_2.JPG,13239,E15-368,2022-06-10T13:57:55.515Z,['leah@media.mit.edu'],True,lilypad-arduino,2008-09-01,[],LilyPad Arduino,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:23.239Z,"LilyPond is a budding e-textile Web community that fosters creative collaboration through the sharing of personal projects. Home to a growing repository of skill- and project-based tutorials, LilyPond provides support for young adults who want to design and create soft, interactive circuits with the LilyPad Arduino toolkit.",2012-01-01,['high-low-tech'],,13247,,2016-12-05T00:16:34.329Z,['leah@media.mit.edu'],True,lilypond,2009-09-01,[],LilyPond,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:23.415Z,"The LilyTiny is a small sewable breakout board for ATtiny85 microcontrollers�devices which may be integrated into circuits to enable pre-determined interactions such as lights that flash or areas that can sense touch. The circuit board can be pre-loaded with a program, enabling students to incorporate dynamic behaviors into e-textile projects without having to know how to program microcontrollers.",2014-01-01,['high-low-tech'],https://dam-prod2.media.mit.edu/x/files/Display/LilyTiny.jpg,13255,,2022-06-10T13:57:55.522Z,['leah@media.mit.edu'],True,lilytiny,2012-01-01,[],LilyTiny,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:23.748Z,"Line of Sound shows how data can be used to deliver sound information only in the direction in which one looks. The demonstration is done using two 55-inch screens which transmit both human and machine relevant information. Each screen is used to show a video that flashes a single bit indicator, which transmits to a camera mounted on headphones. This is used to distinguish between the two screens, and to correlate an audio track to the video track.",2012-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/bio.jpg,13271,--Choose Location,2016-12-05T00:17:16.435Z,"['borovoy@media.mit.edu', 'lip@media.mit.edu']",True,line-of-sound,2010-09-01,[],Line of Sound,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:23.907Z,"In Linear Mandala, a single participant wears a custom headset outfitted with an electroencephalography (EEG) sensor. The participant walks alongside a row of monitors and speakers. Video, sound, and the physical headset are designed to support contemplation. In real time, a shadow-like avatar pushes a ball of objects representing each participant's brain activity. The participant must maintain a consistent brain state for the avatar and ball to move forward in tandem with his physical movement.",2012-01-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/LinearMandala_Thumbnail.jpg,13279,,2016-12-05T00:16:34.363Z,['holtzman@media.mit.edu'],True,linear-mandala,2012-01-01,[],Linear Mandala,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:24.079Z,"We propose a novel shape-changing interface that consists of a single line. Lines have several interesting characteristics from the perspective of interaction design: abstractness of data representation; a variety of inherent interactions/affordances; and constraints such as boundaries or borderlines. By using such aspects of lines together with added transformation capability, we present various applications in different scenarios: shape-changing cords, mobiles, body constraints, and data manipulation to investigate the design space of line-based shape-changing interfaces.",2016-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/e81efcaa-b199-4a6c-af2a-55f9ec17c2e5,13287,--Choose Location,2020-06-09T18:30:53.205Z,"['ishii@media.mit.edu', 'ken_n@media.mit.edu']",True,lineform,2015-01-01,"['human-computer-interaction', 'human-machine-interaction']",LineFORM,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:24.250Z,"The goal of this work is to create a communication link between disjoint public spaces for sociable casual conversation by combining the ease of conversation in public online forums with the physicality and the affordances of a public space.  In particular, we are interested in extending, modifying and reinventing the ""video-wall"" interface.",2003-09-01,['sociable-media'],,13295,E15-383,2016-12-05T00:16:34.422Z,['judith@media.mit.edu'],True,linked-spaces,2002-01-01,[],Linked Spaces,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:24.704Z,"LipSync is an interactive broadcast of famous monologues available in many different languages to multiple users at the same time. Users direct their smartphones to ""tune in"" to their desired language, either audio or closed captioning, by simply pointing them at the relevant part of the screen. This allows intuitive selection of the speaker and language preference. The precise synchronization between the video and the audio streams creates a seamless experience, where the user's natural motions give voice to moving lips. Multiple people can listen to different audio streams associated with the video toward which the mobile phone camera is pointed. This demonstrates with two technologies developed in the Viral Spaces group: CoSync and VRCodes.",2012-09-01,['viral-communications'],,13319,,2016-12-05T00:16:34.467Z,['lip@media.mit.edu'],True,lipsync,2012-09-01,[],LipSync,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:24.886Z,"ListenIn uses audio to link two environments; it is motivated by the desire by caregivers to monitor domestic environments with elderly or infant occupants. Wireless microphones in the home collect sound, classify it, and then transmits either iconic representations of the sound, a few seconds of the sound itself, or, if the sound contains speech, a ""garbled"" version of the sound. The remote client plays these snippets of sound as background audio, allowing one to listen in while away from home as an occasional, ambient auditory experience.",2004-09-01,['living-mobile'],,13327,E15-368,2016-12-05T00:16:34.443Z,['geek@media.mit.edu'],True,listenin,2003-09-01,[],ListenIn,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:25.055Z,"ListenTree is an audio-haptic display embedded in the natural environment. Visitors to our installation notice a faint sound emerging from a tree. By resting their heads against the tree, they are able to hear sound through bone conduction. To create this effect, an audio exciter transducer is weatherproofed and attached to the tree's roots, transforming it into a living speaker, channeling audio through its branches, and providing vibrotactile feedback. In one deployment, we used ListenTree to display live sound from an outdoor ecological monitoring sensor network, bringing a faraway wetland into the urban landscape. Our intervention is motivated by a need for forms of display that fade into the background, inviting attention rather than requiring it. We consume most digital information through devices that alienate us from our surroundings; ListenTree points to a future where digital information might become enmeshed in material.",2018-06-01,"['object-based-media', 'responsive-environments']",https://dam-prod2.media.mit.edu/x/2017/03/23/IMG_3569.JPG,13335,--Choose Location,2020-06-10T15:52:40.800Z,"['vmb@media.mit.edu', 'gershon@media.mit.edu', 'edwinapn@media.mit.edu', 'joep@media.mit.edu']",True,listentree-audio-haptic-display-in-the-natural-environment,2013-01-01,[],ListenTree: Audio-haptic display in the natural environment,PUBLIC,http://listentree.media.mit.edu/,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:25.236Z,"A Live Object is a small device that can stream media content wirelessly to nearby mobile devices without an Internet connection. Live Objects are associated with real objects in the environment, such as an art piece in a museum, a statue in a public space, or a product in a store. Users exploring a space can discover nearby Live Objects and view content associated with them, as well as leave comments for future visitors. The mobile device retains a record of the media viewed (and links to additional content), while the objects can retain a record of who viewed them. Future extensions will look into making the system more social, exploring game applications such as media ""scavenger hunts"" built on top of the platform, and incorporating other types of media such as live and historical data from sensors associated with the objects.",2017-06-01,"['ultimate-media', 'ce-20', 'future-storytelling', 'object-based-media']",https://dam-prod2.media.mit.edu/x/files/Display/ic_launcher.png,13343,--Choose Location,2020-06-10T16:19:49.082Z,"['vmb@media.mit.edu', 'arata@media.mit.edu', 'vpanzica@media.mit.edu']",True,live-objects,2015-01-01,[],Live Objects,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:25.437Z,"In this interactive experience we are interested in enabling quick input actions to Google Glass. The application allows users to trace an object or region of interest in their live view. We use the trace as the foundation for allowing the user to indicate interest in a visual region. Once selected, the user can choose to apply filters to the region, annotate the selection through speech input, or capture text through optical character recognition. These selection and processing tools could naturally integrate with quick note-taking applications where limited touchpad input precludes such input. The Live Trace app demonstrates the effectiveness of gestural control for head-mounted displays.",2014-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/livetrace2.png,13351,--Choose Location,2016-12-05T00:16:34.536Z,['geek@media.mit.edu'],True,live-trace,2014-01-01,[],Live Trace,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:25.780Z,"Currently, Web users have little knowledge about the activities of fellow users. They cannot see the flow of on-line crowds or identify centers of on-line activity. LiveWeb visualizes user activities at a Web site. It graphically lays out the structure of the site and then overlays real-time user accesses.  This system allows a user to perceive both the broad trends and the individual behaviors of other Web visitors.
",2000-12-30,['sociable-media'],,13367,E15-449,2016-12-05T00:16:34.552Z,['judith@media.mit.edu'],True,liveweb,1998-12-31,[],LiveWeb,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:25.951Z,"This work explores a dynamic future in which the accessories we wear are no longer static, but are instead mobile, living objects on the body. Engineered with the functionality of miniaturized robotics, this ""living"" jewelry roams on unmodified clothing, changing location and reconfiguring appearance according to social context and enabling multiple presentations of self. With the addition of sensor devices, they can actively respond to environmental conditions. They can also be paired with existing mobile devices to become personalized on-body assistants to help complete tasks. Attached to garments, they generate shape-changing clothing and kinetic pattern designs—creating a new, dynamic fashion. It is our vision that in the future, these robots will be miniaturized to the extent that they can be seamlessly integrated into existing practices of body ornamentation. With the addition of kinetic capabilities, traditionally static jewelry and accessories will start displaying life-like qualities, learning, shifting, and reconfiguring to the needs and preferences of the wearer, also assisting in fluid presentation of self. With wearables that possess hybrid qualities of the living and the crafted, we explore a new on-body ecology for human-wearable symbiosis. ",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2017/07/08/1_Kino_pattern-changing_img4.jpg,13375,--Choose Location,2022-06-10T14:01:01.946Z,"['artemd@media.mit.edu', 'cindykao@media.mit.edu', 'dmajilo@media.mit.edu', 'geek@media.mit.edu']",True,kino-kinetic-wearable,2015-01-01,"['robotics', 'design', 'fashion', 'human-machine-interaction', 'wearable-computing', 'interfaces']",Kino,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:26.232Z,How can we design relationships between the most primitive and the most  sophisticated life forms? Can we design wearables embedded with  synthetic microorganisms that can enhance and augment biological  functionality? Can we design wearables that generate consumable energy  when exposed to the sun?,2017-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2016/10/24/MM___SYSS___PR___06.jpg,13391,--Choose Location,2021-05-21T13:26:18.386Z,"['stevenk@media.mit.edu', 'ssunanda@media.mit.edu', 'moonshot@media.mit.edu', 'neri@media.mit.edu', 'bader_ch@media.mit.edu', 'kolb@media.mit.edu']",True,living-mushtari,2015-01-01,"['design', 'architecture', 'art', 'synthetic-biology', 'space', 'materials']",Living Mushtari,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:10:26.359Z,"In the Living Observatory installation at the Other Festival, we invite participants into a transductive encounter with a wetland environment in flux. Our installation brings sights, smells, sounds, and a bit of mud from a peat bog undergoing restoration near Plymouth, MA to the MIT Media Lab. As part of the Living Observatory initiative, we are developing sensor networks that document ecological processes and allow people to experience the data at different spatial and temporal scales. Small, distributed sensor devices capture climate and other environmental data, while others stream audio from high in the trees and underwater. Visit at any time from dawn till dusk and again after midnight, and check the weather report on our website (tidmarsh.media.mit.edu) for highlights; if youre lucky you might just catch an April storm.",2013-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/living_observatory_promo.jpg,13399,--Choose Location,2019-04-24T17:33:21.560Z,"['gershon@media.mit.edu', 'bmayton@media.mit.edu', 'joep@media.mit.edu']",True,living-observatory-installation-a-transductive-encounter-with-ecology,2013-01-01,"['virtual-reality', 'augmented-reality', 'ecology', 'wearable-computing', 'technology']",Living Observatory Installation: A Transductive Encounter with Ecology,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:26.490Z,"Living Observatory is an initiative for documenting and interpreting ecological change that will allow people, individually and collectively, to better understand relationships between ecological processes, human lifestyle choices, and climate change adaptation. As part of this initiative, we are developing sensor networks that document ecological processes and allow people to experience the data at different spatial and temporal scales. Low-power sensor nodes capture climate and other data at a high spatiotemporal resolution, while others stream audio. Sensors on trees measure transpiration and other cycles, while fiber-optic cables in streams capture high-resolution temperature data. At the same time, we are developing tools that allow people to explore this data, both remotely and onsite. The remote interface allows for immersive 3D exploration of the terrain, while visitors to the site will be able to access data from the network around them directly from wearable devices.",,"['terrestrial-sensing', 'responsive-environments']",https://dam-prod2.media.mit.edu/x/2016/10/17/cattails.jpg,13407,--Choose Location,2021-12-13T19:11:50.005Z,"['gid@media.mit.edu', 'gershon@media.mit.edu', 'bmayton@media.mit.edu', 'sfr@media.mit.edu', 'ddh@media.mit.edu', 'joep@media.mit.edu']",True,living-observatory-sensor-networks-for-documenting-and-experiencing-ecology,2012-01-01,"['virtual-reality', 'augmented-reality', 'data', 'ecology', 'environment', 'networks', 'wearable-computing', 'sensors', 'technology']",Living Observatory: Sensor networks for documenting and experiencing ecology,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:10:26.638Z,"Run your hand across this wallpaper to turn on a lamp, play music, or control your toaster. This project experiments with interactive wallpaper that can be programmed to monitor its environment, control lighting and sound, and generally serve as a beautiful and unobtrusive way to enrich environments with computational capabilities. The wallpaper itself is flat, constructed entirely from paper and paint. The paper is paired with magnetic electronic modules that serve as sensors, lamps, network interfaces, and interactive decorations.",2011-01-01,['high-low-tech'],https://dam-prod2.media.mit.edu/x/files/Display/IMG_1846-2.jpg,13415,E15-368,2016-12-05T00:16:34.586Z,"['leah@media.mit.edu', 'mellis@media.mit.edu']",True,living-wall,2009-01-01,[],Living Wall,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:26.923Z,"Plateau is a Java toolkit that aides developers in building scaleable collaborative applications. Collaborative applications rely on the transmission of a large amount of updates between hosts to operate correctly. As the number of client hosts grows, the amount of updates that each host needs to process increases asymptotically faster, choking slow hosts. Plateau will allow developers to use update prioritization at the client end to evenly distribute load over time, preventing slow hosts from choking. This will increase the number of users that can use a collaborative application before it breaks. 
",1999-12-30,['sociable-media'],,13431,,2016-12-05T00:16:34.612Z,['judith@media.mit.edu'],True,load-plateau,1997-12-31,[],Load Plateau,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:27.094Z,"Augmentation of human locomotion has proved an elusive goal. Natural human walking is extremely efficient, and the complex articulation of the human leg poses significant engineering difficulties. We present a wearable exoskeleton designed to reduce the metabolic cost of jogging. The exoskeleton places a stiff fiberglass spring in parallel with the complete leg during stance phase, then removes it so that the knee may bend during leg swing. The result is a bouncing gait with reduced reliance on the musculature of the knee and ankle.",,['biomechatronics'],,13439,E15-054,2021-09-17T19:29:05.460Z,['hherr@media.mit.edu'],True,load-bearing-exoskeleton-for-augmentation-of-human-running,2008-09-01,[],Load-bearing exoskeleton for augmentation of human running,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:27.303Z,"Pushpin Computing is a research platform for exploring sensing techniques and algorithms on a dense sensor network of one hundred nodes distributed across the area of two square meters. The goal of this project is to develop distributed estimation applications for the Pushpin network, principally an acoustic-phased array and an artificial retina, which capitalize on the high node-density in the Pushpin system. These algorithms have two challenging requirements which will form the foundation of this research: first, each sensor node must know its position in a globally agreed upon, ad hoc coordinate system; and second, time synchronization must be achieved and maintained throughout the entire network. At the moment, we are developing a novel localization approach based on creating points of correspondence which are shared by all nodes by using ultrasonic ranging techniques.",2005-09-01,['responsive-environments'],,13447,E15-344,2016-12-05T00:17:16.613Z,['joep@media.mit.edu'],True,localization-and-sensing-applications-in-the-pushpin-computing-network,2003-09-01,[],Localization and Sensing Applications in the Pushpin Computing Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:27.482Z,"Location Linked Information (LLI) introduces a scalable infrastructure to support access to and creation of virtual information nuggets that are tied to a physical space. Imagined as initially being embodied by an interactive, dynamic map viewed on a handheld computer, the system provides two functions for its urban users: 1) the retrieval of information about their surroundings, and 2) the optional annotation of location for communal benefit. LLI is implemented as an extension of the Jabber instant messaging protocol.",2003-01-01,['smart-cities'],,13455,NE18-4FL,2016-12-05T00:16:34.635Z,[],True,location-linked-information,2002-01-01,[],Location Linked Information,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:27.631Z,"Growth in leisure travel has become increasingly significant economically, socially, and environmentally. However, flexible but uncoordinated travel behaviors exacerbate traffic congestion. Mobile phone records not only reveal human mobility patterns, but also enable us to manage travel demand for system efficiency. We propose a location recommendation system that infers personal preferences while accounting for constraints imposed by road capacity in order to manage travel demand. We first infer unobserved preferences using a machine learning technique from phone records. We then formulate an optimization method to improve system efficiency. Coupling mobile phone data with traffic counts and road network infrastructures collected in Andorra, this study shows that uncoordinated travel behaviors lead to longer average travel delay, implying opportunities in managing travel demand by collective decisions. The interplay between congestion relief and overall satisfied location preferences observed in extensive simulations indicate that moderate sacrifices of individual utility lead to significant travel time savings. Specifically, the results show that under full compliance rate, travel delay fell by 52 percent at a cost of 31 percent less satisfaction. Under 60 percent compliance rate, 41 percent travel delay is saved with a 17 percent reduction in satisfaction.This research highlights the effectiveness of the synergy among collective behaviors in increasing system efficiency.",,['human-dynamics'],,13463,--Choose Location,2018-10-19T21:02:37.566Z,"['sandy@media.mit.edu', 'yleng@media.mit.edu']",True,location-recommendations-based-on-large-scale-call-detail-records,2015-12-01,"['data', 'transportation', 'urban-planning']",Managing Travel Demand: Location recommendation for system efficiency,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:27.799Z,"We leverage the ubiquity of Bluetooth-enabled devices and propose a decentralized, Web-based architecture that allows users to share their location by following each other in the style of Twitter. We demonstrate a prototype that operates in a large building and which generates a dataset of detected Bluetooth devices at a rate of ~30 new devices per day, including the respective location where they were last detected. Users then query the dataset using their unique Bluetooth ID and share their current locations with their followers by means of unique Universal Resource Identifiers (URIs) that they control. Our separation between producers (the building) and consumers (the users) of Bluetooth device location data allows us to create socially aware applications that respect user privacy while limiting the software necessary to run on mobile devices to just a Web browser.",2010-09-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/screenshot.png,13471,,2016-12-05T00:17:16.635Z,"['ypod@media.mit.edu', 'lip@media.mit.edu']",True,location-sharing-in-large-indoor-environments,2010-01-01,[],Location Sharing in Large Indoor Environments,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:28.057Z,"LocoRadio is a mobile, augmented-reality, audio browsing system that immerses you within a soundscape as you move. To enhance the browsing experience in high-density spatialized audio environments, we introduce a UI feature, ""auditory spatial scaling,"" which enables users to continuously adjust the spatial density of perceived sounds. The audio will come from a custom, geo-tagged audio database. The current demo uses iconic music to represent restaurants. As users move in the city, they encounter a series of pieces of music and the perception enhances their awareness of the numbers, styles, and locations of nearby restaurants. ",2013-01-01,['living-mobile'],,13487,--Choose Location,2016-12-05T00:16:34.707Z,['geek@media.mit.edu'],True,locoradio,2011-09-01,[],LocoRadio,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:28.170Z,"""Logo Blocks"" met the challenge of creating a  programming language for kids that is both powerful and easy to use. Simply put, Logo Blocks puts the Logo code into graphical blocks that may be dragged around the screen and ""clicked"" together. A palette of blocks shows the user what commands are available, and users may choose different options (e.g., which motor port to turn on) simply by clicking the ""ON"" block.",1999-09-01,['lifelong-kindergarten'],,13495,E15-001,2016-12-05T00:16:19.942Z,['mres@media.mit.edu'],True,logo-blocks,1995-01-01,[],Logo Blocks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:28.288Z,The LogoChip is a simple-to-use microcontroller that runs a special version of the Logo programming language. It serves as a versatile building block for constructing a wide range of electronic projects. Playing with LogoChips is a great way for people to get started learning about electronics; with LogoChips you can easily build your own customized version of the Cricket or new layers for the Tower system.,2004-01-01,['personal-fabrication'],,13503,E15-344,2016-12-05T00:16:20.003Z,['bss@media.mit.edu'],True,logochip,2001-01-01,[],LogoChip,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:28.513Z,"Can the voting machine tell when the stress on a voter might compromise their vote? A voter might be agitated because they aren�t sure of whom to vote for, because someone is manipulating them or because something else is wrong. It is our hypothesis that an agitated voter will have this agitation reflected in their mouse movements, and that by recording and reasoning about mouse movement, we will be able to determine if a voter is somehow agitated when casting their vote.",2007-01-01,['context-aware-computing'],,13519,E15-320,2016-12-05T00:16:34.674Z,[],True,long-voting,2006-09-01,[],Long Voting,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:10:28.605Z,"Using a femtosecond laser and a camera with a time resolution of about one trillion frames per second, we recover objects hidden out of sight. We measure speed-of-light timing information of light scattered by the hidden objects via diffuse surfaces in the scene. The object data are mixed up and are difficult to decode using traditional cameras. We combine this ""time-resolved"" information with novel reconstruction algorithms to untangle image information and demonstrate the ability to look around corners.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/LACLogo.PNG,13527,E15-320,2019-06-05T17:53:27.697Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'achoo@media.mit.edu', 'ayush@media.mit.edu', 'michaf@media.mit.edu', 'otkrist@media.mit.edu']",True,looking-around-corners,2008-01-01,['imaging'],Looking Around Corners,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:28.807Z,Loom creates visualizations of the participants and interactions in a  threaded newsgroup.  The renderings reveal patterns indicative of people's  role in the community and of the type of discussion prevalent in a  particular group.  The goal is to develop a visual vocabulary capable of  intuitively conveying this information.,1999-12-30,['sociable-media'],,13543,,2016-12-05T00:16:34.847Z,['judith@media.mit.edu'],True,loom-1,1997-12-31,[],Loom 1,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:28.899Z,"Based on Karrie Karahalios' past work on visualizing Usenet newsgroups, Loom2 is a more extensive version, trying to explore visually the identity of both the space and the people within. Loom2 attempts to develop a visual language that intuitively presents the socially salient features of online communication. Loom2's visualization is multiscale, showing a ""landscape"" view (an overview of many groups); a medium view (showing patterns within a single group�this is what the original Loom project attempted); a closeup view (portraying a single conversation); and a personal view (showing a person over time and group). Loom2 is intuitive and aims to develop a new visualization aesthetic, one that legibly depicts the meaning and feel of the content. Recent work includes developing database software for storing and analyzing Usenet messages, a series of design sketches, and software to test various ideas, in addition to constructing prototype software for a solid visualization derived from the previous sketches. We are developing this primary prototype and analyzing it as an effective tool for gaining an intuitive sense of social landscapes. We are also exploring different ways to extract more detailed social information from the text of various messages.",2004-09-01,['sociable-media'],,13551,E15-449,2016-12-05T00:16:34.873Z,['judith@media.mit.edu'],True,loom2,2000-01-01,[],Loom2,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:28.996Z,"""Loops"" combines the work of the Lab's Synthetic Characters group, Merce Cunningham, and digital artists Paul Kaiser and Shelley Eshkar. A visualization of Cunningham's dance for hands of the same name, this research was built upon, and extended from, the work of the Synthetic Characters group in a number of ways, motivating the creation of new motor system representations, new graphics-rendering techniques, and new creature-signaling systems.",2005-09-01,['opera-of-the-future'],,13559,E15-450,2019-08-16T18:32:48.888Z,"['tod@media.mit.edu', 'marcd@media.mit.edu']",True,loops,2002-01-01,"['virtual-reality', 'art', 'artificial-intelligence', 'music']",Loops,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:29.157Z,LostInBoston.org is about helping Bostonians work together to make neighborhoods more visitor-friendly. Community groups are partnering with local businesses and institutions to design signs that call out the key spots in their areas. Signs are placed on private land in public places.,2011-01-01,['civic-media'],,13575,,2016-12-05T00:16:34.928Z,"['ethanz@media.mit.edu', 'borovoy@media.mit.edu', 'rahulb@media.mit.edu', 'csik@media.mit.edu']",True,lostinbostonorg,2010-01-01,[],LostInBoston.org,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:29.488Z,"As information takes on new physical forms in the environment around us, there is a need for new types of instrumentation to detect and to interrogate the smart materials used to embody the information, and to sense the environment. We are currently working on low-cost, ultra-wideband methods of probing physical structures and decoding the information they contain. In addition, we are exploring further applications of such instrumentation, with particular interest in the frequency-dependent dielectric constant analysis of materials. Preliminary studies have been done in analyzing mail content, with applications in postal mail security, as well as in analyzing milk-fat content, with applications in the collection and distribution of milk in rural India. This project builds on work done by Richard Fletcher, Joe Paradiso, Olufemi Omojola and Matthew Reynolds.",2002-01-01,['physics-and-media'],,13607,E15-023,2016-12-05T00:16:09.844Z,"['neilg@media.mit.edu', 'fletcher@media.mit.edu']",True,low-cost-rf-analyzer,2001-01-01,[],Low-Cost RF Analyzer,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:29.645Z,"We demonstrate an always-available, on-body gestural interface. Using an array of pressure sensors worn around the wrist, it can distinguish subtle finger pinch gestures with high accuracy (>80?). We demonstrate that it is a complete system that works wirelessly in real time. The device is simple and light-weight in terms of power consumption and computational overhead. Prototype's sensor power consumption is 89uW, allowing the prototype to last more then a week on a small lithium polymer battery. Also, device is small and non-obtrusive, and can be integrated into a wristwatch or a bracelet. Custom pressure sensors can be printed with off-the-shelf conductive ink-jet technology. We demonstrate that number of gestures can be greatly extended by adding orientation data from an accelerometer. Also, we explore various usage scenarios with the device.",,['responsive-environments'],,13623,--Choose Location,2019-04-19T14:29:45.955Z,"['artemd@media.mit.edu', 'joep@media.mit.edu']",True,low-power-gesture-input-with-wrist-worn-pressure-sensors,2014-01-01,[],Low-power gesture input with wrist-worn pressure sensors,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:29.722Z,"This project is developing a very novel noncontact microwave sensor system that detects the presence and activity of participants in front of interactive surfaces or structures. We are taking two approaches. In one, we have embedded a real-time software feature detector onto a small, micropatch Doppler motion-detection radar�this device produces a continuous data stream that specifies the amount of motion detected, the dominant speed, and the dominant direction to and from the radar antenna. We are also adapting a low-power ranging radar originally designed by one of our sponsor collaborators to detect the level of fluid in large fuel tanks into a much smaller package able to determine the distance to users as they approach interactive surfaces or devices. Unlike IR, vision, or sonar systems, these techniques aren't affected by lighting, clothing, or visual clutter. Unlike capacitive sensing, we can design antennae to direct the sensing region into a forward-looking beam.These devices can also sense through opaque, nonconductive material, such as plastic, wood, or wallboard.",2003-09-01,['responsive-environments'],,13631,E15-441,2016-12-05T00:17:16.721Z,['joep@media.mit.edu'],True,low-power-radar-for-interactive-environments,2002-01-01,[],Low-Power Radar for Interactive Environments,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:29.804Z,"This project explores the application of new commercial RF chipsets and research devices being developed at the MIT Microelectronics Laboratory to short-range, moderate-to-high bit-rate, ultra-low-power, minimally complex, channel-shared wireless communication for ubiquitously embedded smart sensor modules and lightweight, reconfigurable communication networks. A software radio base station is now being built to coordinate these sensor modules. This base station will subsample and digitize the received signals at the first IF. Such an early digitization allows the base station to have a flexible, reconfigurable architecture. Since most of the signal processing will be done in the digital domain, the same base station will work for different modulation schemes. Another part of this project is to capture currently evolving commercial technology (e.g., Bluetooth, minimal 802.11, and miscellaneous chip-based or hybrid RF transcievers), introducing them into our sensor packages as appropriate.",2001-12-30,['responsive-environments'],,13639,,2016-12-05T00:17:16.746Z,['joep@media.mit.edu'],True,low-power-rf-chipsets-for-wireless-embedded-sensing,1999-12-31,[],"Low-Power RF Chipsets for Wireless, Embedded Sensing",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:30.086Z,"LuminAR reinvents the traditional incandescent bulb and desk lamp, evolving them into a new category of robotic, digital information devices. The LuminAR Bulb combines a Pico-projector, camera, and wireless computer in a compact form factor. This self-contained system enables users with just-in-time projected information and a gestural user interface, and it can be screwed into standard light fixtures everywhere. The LuminAR Lamp is an articulated robotic arm, designed to interface with the LuminAR Bulb. Both LuminAR form factors dynamically augment their environments with media and information, while seamlessly connecting with laptops, mobile phones, and other electronic devices. LuminAR transforms surfaces and objects into interactive spaces that blend digital media and information with the physical space. The project radically rethinks the design of traditional lighting objects, and explores how we can endow them with novel augmented-reality interfaces.",2013-09-30,['fluid-interfaces'],,13671,--Choose Location,2018-10-11T18:29:50.008Z,"['pattie@media.mit.edu', 'kubat@media.mit.edu', 'linder@media.mit.edu']",True,luminar,2010-01-01,"['robotics', 'human-computer-interaction', 'interfaces']",LuminAR,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:30.242Z,"How can we pervasively transform all of architectural space, so that every surface is rendered capable of displaying and collecting visual information? We start with the I/O Bulb, a conceptual evolution of the ordinary lightbulb: one which not only projects high-resolution information, but which also simultaneously collects live video of the region onto which it is projected. The Luminous Room results when we seed an enclosed space with a multiplicity of coordinated I/O Bulbs: enough, specifically, so that every location is treated by at least one I/O Bulb. ""Urp"" is a new I/O-Bulb-mediated urban planning workbench; here, simple architectural models cast accurate shadows, pedestrian-level wind patterns can be observed for different arrangements of buildings, reflections off the surfaces of glass buildings onto surrounding terrain are made visible.",2001-12-30,['tangible-media'],,13687,E15-441,2016-12-05T00:16:35.036Z,"['ishii@media.mit.edu', 'jh@media.mit.edu']",True,luminous-roomurp,1996-12-31,[],Luminous Room/Urp,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:30.323Z,"Based upon prior work on telepresence and tangible interfaces, LumiTouch explores emotional communication in tangible form. The LumiTouch system consists of a pair of interactive picture frames. When one user touches her picture frame, the other picture frame lights up. This touch is translated to light over an Internet connection.  We introduce a semi-ambient display that can transition seamlessly from periphery to foreground in addition to communicating emotional content. In addition to enhancing the communication between loved ones, people can use LumiTouch to develop a personal emotional language.",2001-12-30,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/09/lumitouch.jpg,13695,E15-441,2020-06-12T17:57:06.346Z,['ishii@media.mit.edu'],True,lumitouch,1999-12-31,[],LumiTouch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:30.405Z,"Light is an important factor in the regulation of all kinds of circadian rhythms in our body, such as heart rate, blood pressure, digestion, and mood. This project aims to measure the light environment around users, and provide real-time feedback about appropriate light intensity and color depending on the time of day.",2010-01-01,['affective-computing'],,13703,--Choose Location,2016-12-05T00:17:16.815Z,"['picard@media.mit.edu', 'akanes@media.mit.edu']",True,lux-meter-real-time-feedback-in-ambient-light-environment,2010-01-01,[],Lux Meter: Real-Time Feedback in Ambient Light Environment,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:30.519Z,"M-Views is a system for the development and deployment of context-aware mobile cinematic narratives. Designed around a generic messaging framework, the project hosts a map agent that allows for location discovery on an 802.11b network. An advanced story-scripting system and authoring tool allows authors to develop space-time story relationships with visual and simulation feedback. Cinematic narrative examples propose alternate methods of engaging with audience participants. A critical aspect of the work is the exploration of techniques to augment intercreativity, narrative play, and collective co-constructed production among participants who may or may not know each other.",2004-01-01,['gray-matters'],,13711,E15-351,2016-12-05T00:16:35.054Z,['gid@media.mit.edu'],True,m-views,2000-01-01,[],M-Views,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:30.841Z,"MACH, My Automated Conversation coacH, is a system for people to practice social interactions in face-to-face scenarios. MACH consists of a 3D character that can ""see,"" ""hear,"" and make its own ""decisions"" in real time. The system was validated in the context of job interviews with 90 MIT undergraduate students. Students who interacted with MACH demonstrated significant performance improvement compared to the students in the control group. We are currently expanding this technology to open up new possibilities in behavioral health (e.g., treating people with Asperger syndrome, social phobia, PTSD) as well as designing new interaction paradigms in human-computer interaction and robotics.",2014-09-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/Figure1.png,13727,--Choose Location,2023-10-18T16:19:51.501Z,"['picard@media.mit.edu', 'mehoque@media.mit.edu']",True,mach-my-automated-conversation-coach,2012-09-01,"['human-computer-interaction', 'artificial-intelligence', 'affective-computing']",MACH: My Automated Conversation coacH,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:31.022Z,"Jazz music is difficult to analyze harmonically because it purposefully uses ambiguous harmonies during improvisation. This project aims to understand its harmony from the vantage of three levels: time-frequency analysis beyond the estimation of individual notes, understanding of chord sequences, and knowledge stemming from the broader context of jazz music. One measure of this project's success will be when the harmonic analysis can automatically generate a walking bass accompaniment.",2009-09-01,[],,13735,E15-320,2016-12-05T00:16:35.114Z,[],True,machine-harmonic-transcription-of-jazz,2006-01-01,[],Machine Harmonic Transcription of Jazz,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:31.179Z,"This project develops new theory and algorithms to enable computers to make rapid and accurate inferences from multiple modes of data, such as determining a person's affective state from multiple sensors�video, mouse behavior, chair pressure patterns, typed selections, or physiology. Recent efforts focus on understanding the level of a person's attention, useful for things such as determining when to interrupt. Our approach is Bayesian: formulating probabilistic models on the basis of domain knowledge and training data, and then performing inference according to the rules of probability theory. This type of sensor fusion work is especially challenging due to problems of sensor channel drop-out, different kinds of noise in different channels, dependence between channels, scarce and sometimes inaccurate labels, and patterns to detect that are inherently time-varying. We have constructed a variety of new algorithms for solving these problems and demonstrated their performance gains over other state-of-the-art methods.",2010-01-01,['affective-computing'],,13743,E15-443,2016-12-05T00:17:16.849Z,['picard@media.mit.edu'],True,machine-learning-and-pattern-recognition-with-multiple-modalities,2003-09-01,[],Machine Learning and Pattern Recognition with Multiple Modalities,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:31.361Z,"Machine Therapy takes two forms: a series of wearable body organs (comforting armors) that are incorporated and used by participants, and the facilitation of live-action psychoanalytic sessions (therapy sessions) with motorized machines. By accessing and vitalizing the interplay of people and machines through psychotherapeutic techniques, a social awareness is presented and individuals are invited to question and reinvent their own existence and their relationships with the machines sharing their space. The Wearable Body Organs are a series of very visible, spectacular or carnival-like play-use objects, devices, or equipments. They offer context-sensitive functionality for their wearers, while simultaneously announcing their own need for existence through public use (without being hidden and small, as is the trend with consumer gadgets and self-helping devices). The Machine Sessions are sometimes private and sometimes public engagements/experiments/performances.",2006-09-01,['computing-culture'],,13751,E15-020C,2016-12-05T00:16:35.166Z,"['csik@media.mit.edu', 'monster@media.mit.edu']",True,machine-therapy,2003-09-01,[],Machine Therapy,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:10:31.662Z,"Inspired by the success of the fusible alloy clutch utilized in the digitally reconfigurable surface actuation system, we have been looking into the possibility of abstracting this concept into three dimensions, using fusible alloy to attach spheres or other particles together.  In a simple case this involves plating micro-milli spheres (metal, plastic, glass, etc.) in a solder wetting material (tin, silver, gold, copper, etc.) and then plating that coating with a low temperature solder alloy so that it can be reversibly �sintered� to adjacent particles.  In a more complex case, particles would have internal electronics that turn on or off (by heating) bond plates, resulting in a more �atom-like� particle that could self-assemble or self-disassemble.",2012-09-01,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/Ma'am.jpg,13767,,2016-12-05T00:16:09.919Z,['neri@media.mit.edu'],True,macro-atom-additive-manufacturing,2012-01-01,[],Macro Atom Additive Manufacturing,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:31.954Z,"Expert or fraud, the powerful person in front of an orchestra or choir attracts both hate and admiration. But what is the actual influence a conductor has on the musician and the sounding result? To throw light on the fundamental principles of this special gestural language, we try to prove a direct correlation between the conductor's gestures, muscle tension, and the physically measurable reactions of musicians in onset-precision, muscle tension, and sound quality. We also measure whether the mere form of these gestures causes different levels of stress or arousal. With this research we aim not only to contribute to the development of a theoretical framework on conducting, but also to enable a precise mapping of gestural parameters in order to develop and demonstrate a new system to the optional enhancement of musical learning, performance, and expression.",2016-06-30,['opera-of-the-future'],,13783,--Choose Location,2016-12-05T00:17:16.936Z,"['tod@media.mit.edu', 'platte@media.mit.edu']",True,maestro-myth-exploring-the-impact-of-conducting-gestures-on-musicians-body-and-sounding-result,2015-01-01,[],Maestro Myth: Exploring the Impact of Conducting Gestures on Musician's Body and Sounding Result,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:32.067Z,"Can a robot and magician collaborate on stage to create a believable, evocative performance? Close human-robot proximity and coordination on a performance stage is a recent development (rapid passing of objects between human hands and robot grippers). Our tools allow us to compose a human-robot performance that blends pre-rendered choreography with key moments of dynamic interactivity to enhance the realism of the character. For example, as the robot is playing back a series of poses, it might also track the face of the performer to maintain eye contact. We are studying how perceived agency and blended static/dynamic interactivity might affect an audience's perception of the performance and how changes in computational robot choreography might also influence a viewer's emotional state. We have built trajectory timeline composition software, a sympathetic interface to an industrial robot, and custom hardware to achieve magic effects.",2014-01-01,['personal-robots'],,13791,--Choose Location,2016-12-05T00:16:35.243Z,"['cynthiab@media.mit.edu', 'dnunez@media.mit.edu', 'lukulele@media.mit.edu']",True,magician-robot-interaction,2013-01-01,[],Magician-Robot Interaction,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:32.411Z,"MakeBelieve is a story-generation agent that uses common-sense knowledge to interactively compose short fictional texts with a user. A user starts a story, and MakeBelieve attempts to continue it by freely imagining possible sequences of events for the character the user has chosen. The agent uses ""common sense"" about causality and how the world works, mined from the Open Mind Common Sense corpus, and combines this with very simple lingustic techniques for story generation to produce pithy but interesting stories. MakeBelieve also uses common sense to evaluate a story it has written to catch logically inconsistent and incoherent events and actions.",2007-01-01,['software-agents'],,13815,E15-383,2016-12-05T00:16:37.693Z,['lieber@media.mit.edu'],True,makebelieve-interactive-computer-story-generation-using-common-sense,2001-01-01,[],MakeBelieve: Interactive Computer Story Generation Using Common Sense,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:32.521Z,"MaKey MaKey lets you transform everyday objects into computer interfaces. Make a game pad out of Play-Doh, a musical instrument out of bananas, or any other invention you can imagine. It's a little USB device you plug into your computer, and you use it to make your own switches that act like keys on the keyboard: Make + Key = MaKey MaKey!  It's plug and play. No need for any electronics or programming skills. Since MaKey MaKey looks to your computer like a regular mouse and keyboard, it's automatically compatible with any piece of software you can think of. It's great for beginners tinkering and exploring, for experts prototyping and inventing, and for everybody who wants to playfully transform their world.  ",2015-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/10/24/makey-makey.jpg,13823,--Choose Location,2016-12-05T00:17:00.828Z,"['mres@media.mit.edu', 'silver@media.mit.edu', 'ericr@media.mit.edu']",True,makey-makey,2012-01-01,"['gaming', 'kids']",MaKey MaKey,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:32.629Z,"Working with the New World Symphony, we measured participant skin conductance as they attended a classical concert for the first time. With the sensor technology, we noted times when the audience reacted or engaged with the music and other times when they became bored and drifted away. Our overall findings suggest that transitions, familiarity, and visual supplements can make concerts accessible and exciting for new concert goers. We hope this work can help entertainment industries better connect with their customers and refine the presentation of their work so that it can best be received by a more diverse audience.",2014-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/Slide2.JPG,13831,--Choose Location,2016-12-05T00:16:35.306Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",True,making-engaging-concerts,2013-01-01,[],Making Engaging Concerts,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:32.737Z,"Improving adult learning, especially for adults who are unemployed or unable to financially support their families, is a challenge that affects the future wellbeing of millions of individuals in the US. We are working with the Joyce Foundation, employers, learning researchers, and the Media Lab community to prototype three to five new models for adult learning that involve technology innovation and behavioral insights. ",2016-06-01,['ml-learning'],https://dam-prod2.media.mit.edu/x/files/Display/mlw_image.png,13839,--Choose Location,2016-12-05T00:16:23.585Z,"['ps1@media.mit.edu', 'jnazare@media.mit.edu', 'kamcco@media.mit.edu']",True,making-learning-work,2014-01-01,[],Making Learning Work,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:32.853Z,"We are developing a set of participatory ""maker"" activities to engage youth in creating tangible projects that depict stories about themselves and their worlds. These activities introduce electronics and computational tools as a medium to create, connect, express, and derive meaning from personal narratives. For example, we are offering workshops where participants design sewable circuits and bring them together to create a collaborative Story Quilt. Through the Making with Stories project we are exploring how story-based pedagogy can inspire youth participation in arts and engineering within formal and informal learning environments.",2015-09-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/StoryQuilt.jpg,13847,--Choose Location,2016-12-05T00:16:35.326Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu']",True,making-with-stories,2014-01-01,[],Making with Stories,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:32.962Z,"Map Scratch is an extension of Scratch that enables kids to program with maps within their Scratch projects. With Map Scratch, kids can create interactive tours, games, and data visualizations with real-world geographical data and maps.",2015-01-01,['lifelong-kindergarten'],,13855,--Choose Location,2016-12-05T00:16:35.343Z,"['mres@media.mit.edu', 'sdg1@media.mit.edu', 'bss@media.mit.edu']",True,map-scratch,2012-01-01,[],Map Scratch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:33.141Z,"Books are challenged and banned in public schools and libraries across the country. But which books, where, by whom, and for what reasons? The Mapping Banned Books project is a partnership between the Center for Civic Media, the American Library Association, and the National Coalition Against Censorship to a) visualize existing data on book challenges, b) detect what the existing data doesn't capture, and c) devise new methods to surface suppressed speech. ",2013-09-01,['civic-media'],,13871,--Choose Location,2016-12-05T00:16:35.289Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,mapping-banned-books,2012-09-01,[],Mapping Banned Books,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:10:33.326Z,Mapping the Globe is an interactive tool and map that helps us understand where The Boston Globe directs its attention. Media attention matters—in quantity and quality. It helps determine what we talk about as a public and how we talk about it. Mapping the Globe tracks where the paper's attention goes and what that attention looks like across different regional geographies in combination with diverse data sets like population and income. Produced in collaboration with The Boston Globe.,2015-09-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/20/globe-map.png,13887,--Choose Location,2019-11-07T19:23:20.516Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'dignazio@media.mit.edu', 'a_hashmi@media.mit.edu']",True,mapping-the-globe,2014-09-01,['data'],Mapping the Globe,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:33.417Z,"Receiving a shot or discussing health problems can be stressful, but does not always have to be. We measure participants' skin conductance as they use medical devices or visit hospitals and note times when stress occurs. We then prototype possible solutions and record how the emotional experience changes. We hope work like this will help bring the medical community closer to their customers.",2017-12-31,['affective-computing'],,13895,--Choose Location,2020-07-20T18:14:18.722Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",False,mapping-the-stress-of-medical-visits,2013-01-01,[],Mapping the Stress of Medical Visits,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:33.501Z,"Recognition-based technology has made substantial advances in the past few years because of enhanced algorithms and faster processing speeds. However, current recognition systems are still not reliable enough to be integrated into user interface designs. A possible solution to this problem is to combine results from existing recognition systems and mutually disambiguate the unreliable sections. Piecing together partial results obtained from each mode of recognition can derive more reliable results. In addition, the results of one recognition system can be used to prepare the other recognition system. We are experimenting with an approach that uses a software agent to integrate off-the-shelf recognition applications via scripting languages. We use a software agent called MARCO (Multimodal Agent for Route Construction) that utilizes multiple recognition systems to assist users in giving directions for urban navigation.",2010-09-01,['software-agents'],,13903,E15-383,2016-12-05T00:17:17.018Z,['lieber@media.mit.edu'],True,marco-mutual-disambiguation-of-recognition-errors-in-a-multimodal-navigational-agent,2002-01-01,[],MARCO: Mutual Disambiguation of Recognition Errors in a Multimodal Navigational Agent,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:33.576Z,"Even the most open content system is only as transparent as its interface. Taking cues from the timeless activity of annotating the margins of books, Marginalia provides a visual overlay for analyzing Wikipedia articles. Employing a number of different visualizations, users can browse Wikipedia with a critical eye for who authored each section, how contentious an article or topic is, or the geographic diversity of the authors. Marginalia pulls back the curtains on the collaborative authorship process, and has broader applications in the critical reading of online works.",2009-09-01,"['information-ecology', 'design-ecology']",,13911,E15-301,2016-12-05T00:16:09.941Z,"['holtzman@media.mit.edu', 'dsmall@media.mit.edu', 'jkestner@media.mit.edu']",True,marginalia-critical-lenses-for-reading-wikipedia,2009-01-01,[],Marginalia: Critical Lenses for Reading Wikipedia ,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:10:33.835Z,"Projected augmented reality in the manufacturing plant can increase worker productivity, reduce errors, gamify the workspace to increase worker satisfaction, and collect detailed metrics. We have built new LuminAR hardware customized for the needs of the manufacturing plant and software for a specific manufacturing use case.",2015-09-30,['fluid-interfaces'],,13935,--Choose Location,2017-09-05T13:24:16.601Z,"['pattie@media.mit.edu', 'kubat@media.mit.edu', 'linder@media.mit.edu', 'nfarve@media.mit.edu']",True,mars-manufacturing-augmented-reality-system,2012-01-01,[],MARS: Manufacturing Augmented Reality System,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:34.009Z,"MatchMaker is an automated collaborative filtering system that recommends friends to people on Facebook by analyzing and matching people's online profiles with the profiles of TV characters. The goal of MatchMaker is to produce friend recommendations with rich contextual information through collaborative filtering in the existing social network. Using relationships in TV programs as a parallel comparison matrix, MatchMaker projects these relationships into reality to help people find friends whose personality and characteristics have been voted to suit them well by their social network. MatchMaker also encourages more TV content viewing by using the social network context and connections to provoke people's curiosity of TV characters whom they have been matched with in their social network.",2012-01-01,['information-ecology'],,13951,,2016-12-05T00:16:35.439Z,['holtzman@media.mit.edu'],True,matchmaker,2011-01-01,[],MatchMaker,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:34.099Z,"Shape-changing interfaces give physical shape to digital data so that users can feel and manipulate data with their hands and body. Combining techniques from haptics with the field of shape-changing interfaces, we propose a technique to build a perceptive model of material properties by taking advantage of the shape display's ability to dynamically render flexibility, elasticity, and viscosity in response to the direct manipulation of any computationally rendered physical shape. Using a computer-generated relationship between the manipulated pins and nearby pins in the shape display, we can create human proprioception of various material properties. Our results show that users can identify varying material properties in our simulations through direct manipulation, and that this perception is gathered mainly from their physical relationship (touch) with the shape display and its dynamic movements.",,['tangible-media'],,13959,--Choose Location,2016-12-16T20:08:16.037Z,"['ishii@media.mit.edu', 'lajv@media.mit.edu', 'ken_n@media.mit.edu', 'daniell@media.mit.edu']",True,materiable-rendering-dynamic-material-properties-with-shape-changing-interfaces,2015-09-01,['human-machine-interaction'],Materiable,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:34.348Z,"MCam is an Internet-enabled camera disguised as a milk container. It allows users with cell phones or PDAs to remotely snap a picture of their refrigerator's interior. People don't upgrade their fridges frequently, making it difficult for new Internet-enabled refrigerators to gain popularity. This type of technology is a direct alternative to Internet-enabled refrigerators that are almost prohibitive to acquire.",2004-09-01,"['counter-intelligence', 'context-aware-computing']",,13983,E15-320,2016-12-05T00:16:35.464Z,[],True,mcam,2003-01-01,[],Mcam,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:34.511Z,"Many new applications for robots require them to work alongside people as capable members of human-robot teams.  We have developed Mars Escape, a two-player online game designed to study how humans engage in teamwork, coordination, and interaction. Data gathered from hundreds of online games is being used to develop computational models of human collaborative behavior in order to create an autonomous robot capable of acting as a reliable human teammate.  In the summer of 2010, we will recreate the Mars Escape game in real life at the Boston Museum of Science and invite museum visitors to perform collaborative tasks together with the autonomous MDS robot Nexi.",2011-09-01,['personal-robots'],,13999,,2016-12-05T00:17:17.153Z,['cynthiab@media.mit.edu'],True,mds-crowdsourcing-human-robot-interaction-online-game-to-study-collaborative-human-behavior,2009-09-01,"['robotics', 'social-robotics']",MDS: Crowdsourcing Human-Robot Interaction: Online Game to Study  Collaborative Human Behavior,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:34.587Z,"As robots become more and more capable, we will begin to invite them into our daily lives. There have been few examples of mobile robots able to carry out everyday tasks alongside humans. Though research on this topic is becoming more and more prevalent, we are just now beginning to understand what it means to collaborate. This project aims to unravel the dynamics involved in taking on leadership roles in collaborative tasks as well as balancing and maintaining the expectations of each member of the group (whether it be robot or human). This challenge involves aspects of inferring internal human state, role support and planning, as well as optimizing and keeping synchrony amongst team members ""tight"" in their collaboration.",2012-01-01,['personal-robots'],,14007,,2016-12-05T00:17:17.171Z,"['cynthiab@media.mit.edu', 'jinjoo@media.mit.edu', 'ndepalma@media.mit.edu', 'robbel@media.mit.edu']",True,mds-exploring-the-dynamics-of-human-robot-collaboration,2011-09-01,"['robotics', 'social-robotics']",MDS: Exploring the Dynamics of Human-Robot Collaboration,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:34.911Z,"We are exploring the impact of mobile phones on the future of credit card usage and payments. We are attempting to reinvent the payment process through mobile devices by providing users with tools to help reflect on their finances and eating habits.  We have deployed the Meal Time system on the MIT campus, interlinking it with MIT's TechCASH payment system.  Students have been actively using it to share their experiences and choose locations to eat.",2011-09-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/MealTime%20Logo.png,14039,,2016-12-05T00:16:35.191Z,['lip@media.mit.edu'],True,meal-time,2009-09-01,[],Meal Time,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:34.993Z,"There have been few studies of bowing data from real players, in part due to the difficulty in capturing this information. We have designed an  interface to measure bowing parameters produced by real players, while maintaining the portability and playability of a traditional violin bow. This interface, the Hyperbow, consists of a carbon-fiber violin bow with a custom wireless sensing system (with accelerometers, gyroscopes, electric field position and force sensors). The Hyperbow is partnered with a Yamaha Silent Violin SV-200 also augmented with gesture sensors. This playable measurement system is the core component of an experimental setup used to investigate the bowing parameters produced by real violinists. In these investigations, the parameters are recorded with the sound produced during performances of different bowing techniques (d�tach�, martel�, spiccato). These data can then be analyzed to help understand the various strategies employed by violinists to achieve similar goals in sound production.",2009-01-01,['opera-of-the-future'],,14047,E15-443,2016-12-05T00:16:35.576Z,['tod@media.mit.edu'],True,measurement-of-violin-bowing-technique,2005-01-01,[],Measurement of Violin Bowing Technique,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:35.071Z,"Physiological arousal is an important part of occupational therapy for children with autism and ADHD, but therapists do not have a way to objectively measure how therapy affects arousal. We hypothesize that when children participate in guided activities within an occupational therapy setting, informative changes in electrodermal activity (EDA) can be detected using iCalm. iCalm is a small, wireless sensor that measures EDA and motion, worn on the wrist or above the ankle. Statistical analysis describing how equipment affects EDA was inconclusive, suggesting that many factors play a role in how a child's EDA changes. Case studies provided examples of how occupational therapy affected children's EDA. This is the first study of the effects of occupational therapy's in situ activities using continuous physiologic measures. The results suggest that careful case study analyses of the relation between therapeutic activities and physiological arousal may inform clinical practice.",2016-05-17,['affective-computing'],,14055,--Choose Location,2019-04-19T17:17:14.754Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",True,measuring-arousal-during-therapy-for-children-with-autism-and-adhd,2010-09-01,[],Measuring arousal during therapy for children with autism and ADHD,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:35.145Z,"How can we better understand people�s emotional experiences with a product or service? Traditional interview methods require people to remember their emotional state, which is difficult. We use psychophysiological measurements such as heart rate and skin conductance to map people�s emotional changes across time. We then interview people about times when their emotions changed, in order to gain insight into the experiences that corresponded with the emotional changes. This method has been used to generate hundreds of insights with a variety of products including games, interfaces, therapeutic activities, and self-driving cars.",2012-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/Slide1.jpg,14063,--Choose Location,2016-12-05T00:16:35.545Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",True,measuring-customer-experiences-with-arousal,2009-01-01,[],Measuring Customer Experiences with Arousal,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:35.226Z,"That people have emotional responses to music is a truism. However, we have little understanding of the ways in which music brings about these emotions.  Indeed, we lack decent ways to measure these responses in a quantitative way. As an early step in this area, we devised a listening experiment with a novel response paradigm. Listeners chose from a set of around twenty emotional descriptors, selecting a strength value for each chosen word. Importantly, we did not prevent the listener from selecting conflicting words, or limit her to only one choice. We then used unsupervised machine learning techniques to explore the space of responses. Early results show good agreement with prior studies, but with the potential for more nuanced understanding. We plan to extend this work into considering a broader space of influencing factors on emotional response.",2007-01-01,['opera-of-the-future'],,14071,E15-445,2016-12-05T00:17:17.295Z,['tod@media.mit.edu'],True,measuring-emotional-responses-to-music,2005-09-01,[],Measuring Emotional Responses to Music,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:36.048Z,"Media Cloud is a platform for studying media ecosystems. By tracking millions of stories published online, the system allows researchers to track the spread of memes, media framings, and the tone of coverage of different stories. We aggregate data from over 50,000 news sources from around the world and in over 20 languages including Spanish, French, Hindi, Chinese, and Japanese. Our tools help analyze, deliver, and visualize  information about media conversations on three primary levels: attention and coverage peaks of issues, network analysis, and clustered language use. The platform is open source and open data, designed to be a substrate for a wide range of communications research efforts. Media Cloud is a collaboration between Civic Media and the Berkman Klein Center for Internet and Society at Harvard Law School.To learn more or register for a free account, check out www.mediacloud.org.",2020-08-01,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2018/03/20/Screen Shot 2018-03-20 at 4.17.45 PM.png,14119,--Choose Location,2022-06-10T13:29:45.071Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'ngyenes@media.mit.edu', 'anushkas@media.mit.edu', 'sands@media.mit.edu', 'ahope@media.mit.edu']",True,media-cloud,2011-09-01,"['social-networks', 'civic-media', 'data', 'politics', 'social-media', 'social-science', 'ethics', 'women']",Media Cloud,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:36.233Z,"Media Cloud is a system that facilitates massive content analysis of news on the Web. Developed by the Berkman Center for Internet and Society at Harvard University, Media Cloud already analyzes content in English and Russian. During the last months, we have been working on support for Portuguese content. We intend to analyze the online debate on the most controversial and politically hot topics of the Brazilian Civil Rights Framework for the Internet, namely network neutrality and copyright reform. At the same time, we are writing a step-by-step guide to Media Cloud localization. In the near future, we will be able to compare different media ecosystems around the world.",2015-01-01,['civic-media'],,14127,--Choose Location,2016-12-05T00:16:45.553Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,media-cloud-brazil,2013-01-01,[],Media Cloud Brazil,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:36.603Z,"In this project we examine the social media and traditional media's response to the Boston Marathon bombings from the moment of the explosion to two weeks after the events, including the search, hunt, and capture of the suspects. We use big data analytics, natural language processing, and complex system and network analysis techniques. We focus specifically on information flow, engagement and attention of the audience, emergence of broadcasters, source and spread of rumors, and interplay of various media. We hope to develop a better understanding of the nature of information generation and flow from broadcasters and audiences across different media. Using this event as a case study, we can find out what went wrong or right, and come up with recommendations for different actors (news sources, social media participants, police departments) to better facilitate information flow and minimize misunderstanding and the spread of false information.",2014-09-01,['social-machines'],,14143,--Choose Location,2016-12-05T00:17:17.386Z,"['dkroy@media.mit.edu', 'soroush@media.mit.edu']",True,media-ecosystem-analysis-lessons-from-the-boston-marathon-bombings,2013-01-01,[],Media Ecosystem Analysis: Lessons from the Boston Marathon Bombings,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:10:36.768Z,"Digital media is increasingly becoming part of our everyday world, inviting us to think, communicate, and express ourselves more heterogeneously than ever. With the realization of an ubiquitous mobile channel for exchange, our interface to media must become more of a partner or an accomplice. The concept of media fabrics suggests that media can become synergistic with our selves, intention-aware of its potential for meaning-making. Such fabrics can invite us to spontaneously co-construct and reflect on the media content and meaning in collaborative and individual improvisational frameworks.",2007-09-01,['media-fabrics'],,14151,E15-368,2016-12-05T00:16:35.694Z,"['gid@media.mit.edu', 'barbara@media.mit.edu']",True,media-fabrics,2007-01-01,[],Media Fabrics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:36.947Z,"Building on earlier work done at the Media Lab, MIT has formed the Digital Credentials Consortium an international network of universities to develop a shared system for digital academic credentials. See https://digitalcredentials.mit.edu for more information. Public ledger technology enables us to develop new tools, standards, and strategies to store and manage digital academic credentials. Certificates can be cryptographically signed and tamper-proof. They can represent or recognize many different types of achievements. After developing a number of prototypes (we issued digital credentials to Media Lab Director's Fellows and Media Lab alumni) our code base grew into the largest open-source project for digital academic credentials. We have since shifted efforts to contributing to the Digital Credentials Consortium. ",2023-05-25,"['ml-learning', 'digital-learning-studio']",https://dam-prod2.media.mit.edu/x/2020/10/20/1_wdgOncLZ4VE2lKqOK4Ogiw.jpg,14159,--Choose Location,2023-05-25T22:05:12.264Z,"['ps1@media.mit.edu', 'guyzys@media.mit.edu', 'srishti@media.mit.edu', 'jnazare@media.mit.edu', 'kamcco@media.mit.edu']",True,media-lab-digital-certificates,2015-01-01,"['cryptocurrency', 'open-source']",Digital Academic Credentials,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:37.375Z,"Media Lab Virtual Visit is intended to open up the doors of the Media Lab to people from all around the world. The visit is hosted on the Unhangout platform, a new way of running large-scale unconferences on the web that was developed at the Media Lab. It is an opportunity for students or potential collaborators to talk with current researchers at the Lab, learn about their work, and share ideas. ",2016-08-01,['ml-learning'],https://dam-prod2.media.mit.edu/x/2017/02/10/1-4K0was4KSR1wa1hjj89qGw.png,14183,--Choose Location,2018-06-20T19:58:01.122Z,"['ps1@media.mit.edu', 'srishti@media.mit.edu', 'kamcco@media.mit.edu']",True,media-lab-virtual-visit,2014-01-01,['learning-teaching'],Media Lab Virtual Visit,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:10:37.511Z,"The Media Matrix project draws on ideas from distributed, embedded, and parallel computing in order to address the creation and management of databases composed of large collections of physical objects (e.g. mini DVs in a rack or books in a library). The project's primary goal, however, is to serve as both embodiment and test bed for ideas in distributed, embedded, and ubiquitous computing. The Media Matrix concept drew inspiration from concepts in the Tangible Media Group (TMG) and the Object-Based Media Group's Paintable Computing project. Collaborators Josh Lifton and TMG's Jay Lee initiated the Media Matrix project as a final project for Ted Selker's Industrial Design Intelligence class offered Fall 2000. It has since been carried on by Josh in the Responsive Environments Group (ResEnv). Related work includes Pushpin Computing in ResEnv and TouchCounters in TMG. The Media Matrix was presented as an interactive poster at CHI 2001 in Seattle, WA. The next stage in this project involves converting over to battery power, eliminating the need of the powered shelf system. Each unit will consume under a microamp in sleep mode. A special coded ""flashlight"" can be beamed at any unit, causing it to wake up and read the ID sent in the optical beam. This ID can be compared to the local ID on the object and/or transmitted to neighboring objects in the matrix, allowing the requested object to be rapidly located.",2001-09-01,"['counter-intelligence', 'responsive-environments']",,14191,E15-344,2016-12-05T00:16:35.765Z,['joep@media.mit.edu'],True,media-matrix-2,2000-01-01,[],Media Matrix,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:37.638Z,"We present two scalable ways to explore and distribute media in all forms: video, text, and graphics; published and conversational. The first presentation has been demonstrated as an interactive, dynamic time/source array where one can see the pulse of publication and suggest media for friends. A revision organizes content as 3D �stacks� that correspond to people and topics. The Matrix dissolves media silos and types and assembles it in a data- and socially driven way. �Glue� is the engine that drives assembly.",2014-09-01,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/Screenshot%202014-03-27%2017.38.38.png,14199,--Choose Location,2016-12-05T00:17:17.472Z,"['vdiep@media.mit.edu', 'lip@media.mit.edu']",True,media-matrix,2014-01-01,[],Media Matrix,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:37.886Z,Media Meter Focus shows focus mapping of global media attention. What was covered in the news this week? Did the issues you care about get the attention you think they deserved? Did the media talk about these topics in the way you want them to? The tool-set also shows news topics mapped against country locations.,2015-01-01,"['future-of-news', 'civic-media']",https://dam-prod2.media.mit.edu/x/files/Display/MediaMeterLogo.png,14215,--Choose Location,2022-06-10T12:56:41.407Z,"['ethanz@media.mit.edu', 'a_hashmi@media.mit.edu']",False,media-meter-focus,2014-01-01,[],Media Meter Focus,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:10:38.044Z,"Media Perspective brings a data visualization into 3D space. This data sculpture represents mainstream media coverage of Net Neutrality over 15 months, during the debate over the FCC's classification of broadband services. Each transparent pane shows a slice in time, allowing users to physically move and look through the timeline. The topics cutting through the panes show how attention shifted between aspects of the debate over time.",2016-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/Media%20Perspective.png,14223,--Choose Location,2016-12-05T00:16:23.229Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'elplatt@media.mit.edu']",True,media-perspective,2015-01-01,[],Media Perspective,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:38.378Z,"Media scores provide a means to orchestrate multiple modalities in the creation of expressive works of art and performance. New technologies afford numerous opportunities to tell stories and create expressive artworks through a variety of media. Media scores extend the concept of a musical score to other modalities in order to facilitate the process of authoring and performing multimedia compositions, providing a medium through which to realize a modern-day Gesamtkunstwerk. Through research into the representation and the encoding of expressive intent, systems for composing with media scores are being developed. Using such a tool, the composer will be able to shape an artistic work that may be performed through human and technological means in a variety of media and utilizing various modalities of expression. Media scores offer the potential for authoring content considering live performance data and the potential for audience participation and interaction. This paradigm bridges the extremes of the continuum from composition to performance, allowing for improvisatory compositional acts at performance-time. The media score also provides a common point of reference in collaborative productions as well as the infrastructure for the real-time control of any technologies used during a live performance.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/10/20/ms_bg.jpg,14239,--Choose Location,2018-10-23T15:21:21.829Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu']",True,media-scores,2011-01-01,[],Media Scores,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:38.685Z,"The Media Windshield is a demonstration that changes the use of an automobile windshield to serve multiple purposes. When no one is in the car, it can present information to the outside world: advertising, augmentation of the road signs, even personalized information for a party that is walking by. When the driver gets into the car, it adjusts the audio to be appropriate for their seating arrangement and puts up an Internet interface. When the driver lies back, the windshield can be used as a movie screen for DVD videos. Finally, when the driver is using the car, the windshield can shade the sun and lights, put up virtual signs, and annotate difficult-to-see objects.",2003-01-01,['context-aware-computing'],,14255,E15-320,2016-12-05T00:16:35.913Z,[],True,media-windshield,2001-01-01,[],Media Windshield,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:38.833Z,"A pair of glasses that darken whenever a television is in view. The user should wear the glasses whenever she feels that she is likely to encounter unpleasant media. The glasses are intended to be used as a therapy system, relieving the stress and despondency associated with heavy television viewing.",2005-01-01,['computing-culture'],,14263,E15-020D,2016-12-05T00:16:10.009Z,['csik@media.mit.edu'],True,media-sensitive-sunglasses,2004-09-01,[],Media-Sensitive Sunglasses,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:38.971Z,"The mediaBlocks prototype is a tangible user interface based upon small,  digitally tagged wooden blocks. The blocks serve as physical icons (""phicons"") for the containment, transport, and manipulation of online  media. MediaBlocks interface with media input and output devices such as  video cameras and projectors, allowing digital media to be rapidly ""copied""  from a media source and ""pasted"" into a media display.  MediaBlocks are  also compatible with traditional GUIs, providing seamless gateways between  tangible and graphical interfaces. Finally, mediaBlocks act as physical  ""controls"" in tangible interfaces for tasks such as sequencing collections of media elements.",1998-12-30,"['personal-fabrication', 'tangible-media']",,14271,E15-441,2016-12-05T00:16:36.027Z,['ishii@media.mit.edu'],True,mediablocks,1996-12-31,[],mediaBlocks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:39.368Z,"Medina is a social-networking site based around the idea of exchanging knowledge. The project explores new interfaces for visualizing connections between people and ideas. Knowledge and interests are valuable in and of themselves, but also provide useful structures for traversing the network. The site constantly measures the interactions between people and their interests in order to provide a more accurate picture of what relationships and information are important. The goal is to build an interface that more accurately represents the state of the network. ",2017-05-01,['sociable-media'],,14295,E15-383,2022-06-10T14:41:29.929Z,['judith@media.mit.edu'],True,medina,2007-09-01,[],Medina,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:10:39.509Z,"Medley deals with personal, physically proximate communications. We look at different aspects of data sharing between members of a community where there is better connectivity among group members than to a core network at large such as may be found with the $100 laptop. We diffuse the information among the population, sharing memory and localizing traffic. We face research questions of how information is distributed, what conflicts there are between memory, energy, and communication costs, how one integrates local storage, and how we can use social parameters (e.g., friendships) to determine routing. Additional research interests relate to the human interface aspect, such as what parts of the platform should be exposed to the user and/or give him control over and what parts should be made transparent.",2007-01-01,['viral-communications'],,14303,E15-495,2016-12-05T00:16:36.156Z,"['ypod@media.mit.edu', 'nadav@media.mit.edu', 'lip@media.mit.edu']",True,medley,2006-09-01,[],Medley,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:40.048Z,"One dimension of our group�s unifying goals is research in learning itself, but postulating new ideas for learning is not enough. What can we do to help bring about real impact and tap the latent global learning potential to provide opportunities for every child (and adult) to have the learning environments they deserve? We make a conceptual distinction along three integrated lines: content, tools and environments, and real-world initiatives. Each of our projects emphasizes at least one of these. We work on extremely low-cost technologies to achieve a high ratio of computational material to learner and provide an immersive environment that opens new possibilities for learning otherwise not achievable. We develop new content and support materials that take advantage of access. We create demonstration projects to explicate new ideas, show possibilities, and provide a means for reflection for learning and to facilitate the change process.",2005-09-01,[],,14327,E15-368,2016-12-05T00:16:36.219Z,"['papert@media.mit.edu', 'cavallo@media.mit.edu']",True,mega-change-in-learning,2005-09-01,[],Mega-Change in Learning,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:10:40.622Z,"The emergence of digital music on the Internet requires new information-retrieval methods adapted to specific characteristics and needs. A query-by-humming system, which can find a piece of music in the digital music repository based on a few hummed notes, can help people search for music on the Internet, even if they cannot memorize the title or any other text information about the music. New melody representation and matching methods are employed in this project. Combining Internet, audio signal processing, and database techniques, we are attempting to provide a friendlier interface for Internet music searches.",2005-01-01,['music-mind-and-machine'],,14351,E15-484,2016-12-05T00:16:36.244Z,['bv@media.mit.edu'],True,melody-retrieval-on-the-web,2000-01-01,[],Melody Retrieval on the Web,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:40.784Z,"MelodyMorph is an interface for constructing melodies and making improvised music. It removes a constraint of traditional musical instruments: a fixed mapping between space and pitch. What if you blew up the piano so you could put the keys anywhere you want? With MelodyMorph you can create a customized musical instrument, unique to the piece of music, the player, or the moment.",2015-01-01,['lifelong-kindergarten'],,14359,--Choose Location,2016-12-05T00:16:36.329Z,"['mres@media.mit.edu', 'ericr@media.mit.edu']",True,melodymorph,2010-09-01,[],MelodyMorph,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:40.938Z,Memento is a home-based reminder system that allows users to associate customized voice reminders with specific activities in their daily lives. Memento consists of three major components: (1) a handheld computing interface for initiating and receiving reminders; (2) a sensor subsystem that detects activity-related changes in the home environment; and (3) a central server that acquires sensor readings and attempts to infer user activity from the data in real time. These three components function together to provide a new and viable approach to augmenting human memory.,2005-01-01,['city-science'],https://dam-prod2.media.mit.edu/uuid/1ed4da27-1e97-40e3-986b-8d1a011bc69b,14367,--Choose Location,2018-05-04T18:21:03.600Z,"['kll@media.mit.edu', 'intille@media.mit.edu']",True,memento,2004-09-01,"['architecture', 'interfaces', 'behavioral-science']",Memento,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:41.307Z,"Merry Miser is a mobile application which persuades people to spend less money, and think more about their spending. By combining users' real financial transaction information, their location, and personal assessments of spending, the application presents deeply personalized and compelling interventions at the time and place when they are near an opportunity to shop. The interventions help to reinforce choices that are in the users' better long-term self interest, against short-term impulses.",2010-01-01,['living-mobile'],,14383,E15-368,2016-12-05T00:17:17.546Z,['geek@media.mit.edu'],True,merry-miser,2008-09-01,[],Merry Miser,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:41.615Z,"MessageMe is a location-based messaging infrastructure. It consists of a messaging server that delivers messages to recipients as they enter a designated physical space in the Lab. MessageMe builds on the Glass Infrastructure system, utilizing the RFID readers at each screen to determine users' locations.",2016-08-31,['digital-intuition'],,14399,,2021-07-14T15:33:37.953Z,['havasi@media.mit.edu'],True,messageme,2013-01-01,[],MessageMe,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:41.783Z,"The Meta Meta Project is an open-source movement aimed at accomplishing a simple, powerful goal: take in media, spit back metadata. The system can power any project that needs to be smart with its content. For instance, identifying important themes, building smart associations between chunks, and augmenting content with additional information layers. Using a single standardized API, programmers can submit multiple forms of media (text, image, video, audio), and extract information from that media (keywords, OCR, entities). For more information about the project and the contributors visit http://www.metametaproject.org.",2011-01-01,['information-ecology'],,14407,,2016-12-05T00:16:36.529Z,['holtzman@media.mit.edu'],True,meta-meta-project,2011-01-01,[],Meta Meta Project,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:41.948Z,"A collaboration between Professor Christine Ortiz (project lead), Professor Mary C. Boyce, Katia Zolotovsky, and Swati Varshaney (MIT). Operating at the intersection of biomimetic design and additive manufacturing, this research proposes a computational approach for designing multifunctional scaled-armors that offer structural protection and flexibility in movement. Inspired by the segmented exoskeleton of Polypterus senegalus, an ancient fish, we have developed a hierarchical computational model that emulates structure-function relationships found in the biological exoskeleton. Our research provides a methodology for the generation of biomimetic protective surfaces using segmented, articulated components that maintain user mobility alongside full-body coverage of doubly curved surfaces typical of the human body. The research is supported by the MIT Institute for Soldier Nanotechnologies, the Institute for Collaborative Biotechnologies, and the National Security Science and Engineering Faculty Fellowship Program.",2016-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/Fig-0 copy.png,14415,--Choose Location,2021-05-21T13:19:30.175Z,"['dumo@media.mit.edu', 'j_duro@media.mit.edu', 'neri@media.mit.edu']",True,meta-mesh-computational-model-for-design-and-fabrication-of-biomimetic-scaled-body-armors,2013-09-01,[],Meta-Mesh: Computational model for design and fabrication of biomimetic scaled body armors,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:42.083Z,"Most current virtual reality interactions are mediated by hand-held input devices or hand gestures, and usually display only a partial representation of the user in the synthetic environment. We believe that representing the user as a full avatar controlled by the natural movements of the person in the real world can lead to a greater sense of presence in VR. MetaSpace I is a virtual reality system that allows co-located users to explore a VR world together by walking around in physical space. Each user’s body is represented by an avatar that is dynamically controlled by their body movements. Users can see their own avatar and the other person’s avatar allowing them to perceive and act intuitively in the virtual environment.",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/11/22/twopeople.png,14423,--Choose Location,2022-06-10T14:01:02.011Z,"['sra@media.mit.edu', 'geek@media.mit.edu']",True,meta-physical-space-vr,2014-11-01,[],MetaSpace I,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:42.214Z,"Gait research on trans-femoral prosthesis users has shown that the metabolic costs for these individuals are significantly higher than those of able-bodied individuals for level-ground walking.  Additionally, trans-femoral amputees exhibit a much higher degree of gait asymmetry between the affected and non-affected sides, leading to reduced walking speeds and increased hip and back pain compared to non-amputees.  This project consists of a clinical study of five to ten unilateral trans-femoral amputees using either a conventional or a powered knee prosthesis and height-weight matched able-bodied individuals. This work will compare the metabolic cost of transport and biomechanics of conventional standard of care prosthetic knees with a novel powered knee.  It is hypothesized by using a powered prosthetic knee both the metabolic and biomechanical aspects of amputee gait can be improved.",2012-09-01,['biomechatronics'],,14431,,2016-12-05T00:16:10.044Z,['hherr@media.mit.edu'],True,metabolic-and-biomechanical-effects-of-using-a-powered-prosthetic-knee,2010-01-01,[],Metabolic and Biomechanical Effects of Using a Powered Prosthetic Knee,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:42.488Z,"Programming is the art of constructing a story about objects and what they do in various situations, expressed in programming languages which are easy for the computer to accurately convert into executable code, but difficult for people to write and understand. This project explores using descriptions in a natural language like English as a representation for programs. We cannot convert arbitrary English descriptions to fully specified code, but we can use an expressive subset of English as a visualization tool. Simple descriptions of program objects and their behavior are converted to scaffolding (underspecified) code fragments that can be used as feedback for the designer and then elaborated. Our parser can infer a surprising amount of information about program structure from relations implicit in the linguistic structure; we call this ""programmatic semantics."" Our program editor, Metafor, dynamically converts a user's stories into program code. Users found it useful as a brainstorming tool.",2006-09-01,['fluid-interfaces'],,14447,E15-320,2016-12-05T00:16:36.559Z,"['pattie@media.mit.edu', 'lieber@media.mit.edu']",True,metafor-programming-by-storytelling,2003-01-01,[],Metafor: Programming by Storytelling,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:42.600Z,"We are synthesizing artificial materials for manipulating electromagnetic waves. These materials have a negative index of refraction, allowing novel imaging and fabrication geometries which will lead to improved lithography and measurement techniques.",2005-01-01,['molecular-machines'],,14455,E15-427,2016-12-05T00:17:17.651Z,"['jacobson@media.mit.edu', 'ike@media.mit.edu']",True,metamaterials,2001-01-01,[],MetaMaterials,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:42.715Z,"A successfully engineered physical system will reach a state of complexity such that it needs its own vocabulary and semantics for proper comprehension. Among many other things, I have designed a three-tier control system for an autonomous submarine, a language for a tabletop quantum computer, and two proposed experiments in neuroengineering: a neural prosthesis that would program a fly's natural avionics, and a 'microbrain' comprising 1,000 cultured neurons that could perform a sophisticated behavioral task.",2000-12-30,"['personal-fabrication', 'physics-and-media']",,14463,E15-023,2016-12-05T00:16:36.671Z,"['neilg@media.mit.edu', 'esb@media.mit.edu']",True,metaoperating-systems,1997-12-31,[],Metaoperating Systems,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:10:42.830Z,Metarchivist is a virtual environment for organizing and browsing documents and other content. It seeks to provide a seamless space for both browsing content and visualizing high-level relationships among resources.,2008-09-01,['object-based-media'],,14471,Garden Conference Room,2016-12-05T00:16:36.628Z,['vmb@media.mit.edu'],True,metarchivist,2007-01-01,[],Metarchivist,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:42.939Z,"Mental health therapies are complex and, to be computer-deliverable, must be customizable and adaptive.  We are applying software engineering principles to automate, and make customizable and adapatable, such therapies via a Web-based application.  The technology also provides a new platform for studying the cognitive process and neural circuitry of therapy to further non-pharmacological methods of health interventions and management.  ",2009-09-01,['synthetic-neurobiology'],,14479,E15-435,2016-12-05T00:16:36.692Z,"['esb@media.mit.edu', 'barbara@media.mit.edu']",True,metatherapy-customized-adaptive-therapy-systems,2006-09-01,[],"Metatherapy: Customized, Adaptive Therapy Systems",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:43.043Z,The goal of the Metazine is to provide a jumping-off point for research into the magazine of the future. We have developed an application for Internet tablets to supplement and enhance the content of paper magazines with digital content in a way that preserves the flexibility and robustness of the tried-and-true paper format. The Metazine follows coded links placed near articles and advertisements in a magazine to seamlessly deliver digital video or images on demand.,2007-01-01,['information-ecology'],,14487,E15-301,2016-12-05T00:16:36.582Z,['holtzman@media.mit.edu'],True,metazine,2007-09-01,[],Metazine,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:43.137Z,"Meteorite is a permanent, underground, interactive museum that opened in Essen, Germany in June 1998.  Our group designed all of the interaction for the museum, and Tod Machover composed a special  ""walk-through"" opera that varies as visitors explore the building.  A TransFlow Room was designed to allow up to fifty visitors to simultaneously control a coherent environment of interactive sound and image, playing the one hundred sensors lining the walls to create, shape, modify, synchronize, and save-or-eliminate interlocking fragments.",1998-12-30,['opera-of-the-future'],,14495,,2016-12-05T00:16:36.487Z,['tod@media.mit.edu'],True,meteorite-museum,1997-12-31,[],Meteorite Museum,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:43.234Z,"Metropath(ologies) is an installation that immersively explores the experience of living in our increasingly information filled�and information extracting�urban world, a world that is both vibrantly connected and sinisterly surveillant.",2009-09-01,[],,14503,E15-348,2016-12-05T00:16:36.714Z,['judith@media.mit.edu'],True,metropathologies,2008-01-01,[],Metropath(ologies),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:10:43.674Z,"This project focuses on fabricating precision micro-machine systems in one, two, and three dimensions. The typical systems are smaller than 1mm x 1mm in size, and incorporate sensing and actuation in a single, micro-fabricated device.  We are also interested in novel mechanical mechanisms and how they translate to micro-scale devices.",2005-01-01,['molecular-machines'],,14551,E15-015,2016-12-05T00:17:17.689Z,['jacobson@media.mit.edu'],True,micro-electro-mechanical-devices-mems,2000-01-01,[],Micro-Electro Mechanical Devices (MEMS),PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:43.751Z,"Micro-Macro Fluidic Fabrication (MMFF) enables the control of mechanical properties through the design of non-linear lattices embedded within multi-material matrices. At its core it is a hybrid technique that integrates molding, casting, and macro-fluidics. Its workflow allows for the fabrication of complex matrices with geometrical channels injected with polymers of different pre-set mechanical combinations. This novel fabrication technique is implemented in the design and fabrication of a midsole running shoe. The goal is to passively tune material stiffness across surface area in order to absorb the impact force of the user's body weight relative to the ground, and enhance the direction of the foot-strike impulse force relative to the center of body mass.",2014-12-12,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/Logo_midsole.jpg,14559,--Choose Location,2016-12-05T00:16:48.864Z,['neri@media.mit.edu'],True,micro-macro-fluidic-fabrication-of-a-mid-sole-running-shoe,2014-01-01,[],Micro-Macro Fluidic Fabrication of a Mid-Sole Running Shoe,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:43.930Z,"We�ve developed a set of tools and techniques that make it easy to use microcontrollers as an art or craft material, embedding them directly into drawings or other artifacts. We use the ATtiny45 from Atmel, a small and cheap (~$1) microcontroller that can be glued directly to paper or other objects. We then construct circuits using conductive silver ink, dispensed from squeeze bottles with needle tips. This makes it possible to draw a circuit, adding lights, speakers, and other electronic components.",2013-01-01,['high-low-tech'],https://dam-prod2.media.mit.edu/x/files/Display/Microntrollers%20as%20Material.jpg,14575,--Choose Location,2016-12-05T00:16:37.081Z,"['leah@media.mit.edu', 'mellis@media.mit.edu', 'jieqi@media.mit.edu']",True,microcontrollers-as-material,2011-09-01,[],Microcontrollers As Material,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:44.089Z,"Microculture gardens are a network of small-scale permaculture gardens that are aimed at reimagining our urban food systems, remediating our air supply, and making our streets more amenable to human-scale mobility. Microculture combines micro-gardening with the principles of permaculture, creatively occupying viable space throughout our communities for small-scale self-sustaining food forests. Micro-gardens have proven to be successful for the production of a broad range of species, including leafy vegetables, fruit, root vegetables, herbs, and more. Traditionally, container-based micro-gardens occupy approximately one meter of space or less and are made from found, up-cycled materials. Our innovations involve the combining of permaculture and micro-gardening principles, developing materials and designs that allow for modularity, mobility, easy replicability, placement in parking spots, and software that supports the placement, creation, and maintenance of these gardens.",2017-05-01,['social-computing'],,14583,--Choose Location,2022-06-10T14:40:02.031Z,"['sdkamvar@media.mit.edu', 'yonatanc@media.mit.edu']",True,microculture,2014-01-01,[],Microculture,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:44.280Z,"This project aims to demonstrate rapid-prototyping of logic gates in the field. We are working on a  candidate mechanism of microfluidic logic at low Reynolds numbers. Cascades of appropriate, simple, nonlinear elements allow synthesis of any logic arbitrary nonlinear function. We are using the nonlinear medium of air because it is so readily available. We are studying Newtonian fluid flow at low Reynolds numbers in specially designed micro-geometries ranging from 100 microns to a few microns. Fluidic Logic was an active field of research in the 60s and 70s, but died prematurely because it could not keep up with the shrinking sizes and improved performance of electronic circuits. Current manufacturing processes can easily create extremely small feature sizes, made possible by increasing the resolution of precision machines and improved metrology techniques.",2005-01-01,['physics-and-media'],,14591,E15-023,2016-12-05T00:16:36.784Z,['neilg@media.mit.edu'],True,microfluidic-logic,2003-09-01,[],Microfluidic Logic,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:44.450Z,"A major challenge for the development of novel biosensors is packaging the sensor for a specific application or experiment. The use of microfluidic systems in conjunction with microfabricated sensors promises advantages over traditional benchtop biology, including small-volume analyte consumption and higher throughput. Using rapid prototyping processes in this lab and in the microfabrication facility on campus, we develop microfluidic systems in silicon, glass, cast polymers, and laser-cut plastics. These include parallel channel arrays that allow rapid typing of many analytes, small volume fluidic cells that incorporate electrical contacts, and microcapillaries for functionalizing microfabricated biosensors.",2005-09-01,[],,14599,E15-420,2016-12-05T00:17:17.576Z,['scottm@media.mit.edu'],True,microfluidics,2000-01-01,[],Microfluidics,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:44.614Z,"The MicroPsi project explores broad models of cognition, built on a motivational system that gives rise to autonomous social and cognitive behaviors. MicroPsi agents are grounded AI agents, with neuro-symbolic representations, affect, top-down/bottom-up perception, and autonomous decision making. We are interested in finding out how motivation informs social interaction (cooperation and competition, communication and deception), learning, and playing; shapes personality; and influences perception and creative problem-solving.",2017-06-01,['playful-systems'],,14607,--Choose Location,2022-06-10T14:14:48.935Z,"['slavin@media.mit.edu', 'joscha@media.mit.edu']",True,micropsi-an-architecture-for-motivated-cognition,2014-01-01,[],MicroPsi: An Architecture for Motivated Cognition,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:44.779Z,"Midas is a touch-based personalization system that bridges the gap between personal area networks (PANs) and explicit authentication. It is designed to interact with ubiquitous interfaces in the objects and environment surrounding us in order to achieve seamless access to information via public and pseudo-public surfaces and environments that modify themselves to the needs to the users on the fly and on contact. By confining the communication to touched object while avoiding the need to use an explicit authentication token such as a RFID, we enable new means of customizing our surroundings to our needs.",2010-09-01,['fluid-interfaces'],,14615,E15-320,2016-12-05T00:16:36.807Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu']",True,midas,2007-01-01,[],Midas,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:45.085Z,"In the spirit of Midnight Basketball, which was created to give inner-city youths positive nighttime activities in the summer, we have created Midnight Computing, to give the option of non-athletic, positive activities. To our surprise, rather than only attracting teenagers, we have attracted whole families, ranging in age from 6 to 78. This has given rise to a whole set of community-based learning activities, taking place in technology centers in public housing projects. Beginning in the fall of 1988, graduates from the vocational training are working with this group at several of the tech centers.",2000-12-30,['future-of-learning-2'],,14631,,2016-12-05T00:16:10.078Z,['cavallo@media.mit.edu'],True,midnight-computingcommunity-computing,1999-12-31,[],Midnight Computing/Community Computing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:45.333Z,"Mime is a compact, low-power 3D sensor for short-range gestural control of small display devices. The sensor's performance is based on a novel signal processing pipeline that combines low-power time-of-flight (TOF) sensing for 3D hand-motion tracking with RGB image-based computer vision algorithms for finer gestural control. Mime is an addition to a growing number of input devices developed around the engineering design philosophy of sacrificing generality for battery-friendly and accurate performance to retain the portability advantages of our smart devices. We demonstrate the utility of Mime for head-mounted display control and smartphones with a variety of application scenarios, including 3D spatial input using close range gestures, gaming, on-the-move interaction, and operation in cluttered environments and in broad daylight conditions.",2014-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/uist406-colaco.jpg,14647,--Choose Location,2016-12-05T00:16:37.270Z,['geek@media.mit.edu'],True,mime,2013-09-01,[],Mime,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:45.468Z,"Mind-Theoretic Planning (MTP) is a technique for robots to plan in social domains. This system takes into account probability distributions over the initial beliefs and goals of people in the environment that are relevant to the task, and creates a prediction of how they will rationally act on their beliefs to achieve their goals. The MTP system then proceeds to create an action plan for the robot that simultaneously takes advantage of the effects of anticipated actions of others and also avoids interfering with them.",2014-05-31,['personal-robots'],,14655,,2017-05-31T18:46:51.951Z,['cynthiab@media.mit.edu'],True,mind-theoretic-planning-for-robots,2012-01-01,[],Mind-Theoretic Planning for Robots,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:45.700Z,"Documentary videography is a practice of observing the real world, recording video of desired events, and organizing the collection for presentation to an audience. Observations and decisions made during capture highly impact the process of documentary creation. The Mindful Camera is designed to support the story-construction goals of the documentary videographer during capture and organization of content collections. The video camera is imbued with rudimentary story-understanding capabilities incited by annotation by the videographer in the moment of capture. The camera uses common-sense knowledge and reasoning to expand metadata of captured video clips, suggest shots to the videographer, and track story patterns in the information track of a video collection. The capture process leverages the videographer�s observations during capture to increase the narrative possibility of his or her video collections.",2005-01-01,['media-fabrics'],,14671,E15-368,2016-12-05T00:16:37.924Z,"['gid@media.mit.edu', 'barbara@media.mit.edu']",True,mindful-camera,2003-09-01,[],Mindful Camera,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:45.815Z,"Light enables our visual perception. It is the most common medium for displaying digital information. Light regulates our circadian rhythms, affects productivity and social interaction, and makes people feel safe. Yet despite the significance of light in structuring human relationships with their environments on all these levels, we communicate very little with our artificial lighting systems. Occupancy, ambient illuminance, intensity, and color preferences are the only input signals currently provided to these systems. With advanced sensing technology, we can establish better communication with our devices. This effort is often described as context-awareness. Context has typically been divided into properties such as location, identity, affective state, and activity. Using wearable and infrastructure sensors, we are interested in detecting these properties and using them to control lighting. The Mindful Photons Project aims to close the loop and allow our light sources to ""see"" us.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2016/10/20/mindful logo.jpg,14679,--Choose Location,2017-06-14T21:42:10.971Z,"['joep@media.mit.edu', 'nanzhao@media.mit.edu', 'maldrich@media.mit.edu']",True,mindful-photons-context-aware-lighting,2013-01-01,"['architecture', 'cognition', 'environment', 'human-machine-interaction', 'wearable-computing', 'sensors', 'nonverbal-behavior', 'physiology']",Mindful Photons: Context-Aware Lighting,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:45.928Z,"Motivated by the question ""Why do people behave strangely while driving cars with GPS navigation systems?"", we reviewed previous work in the field and paid attention to the psychological (short- and long-term) effects of ""mindlessness"" in human minds while interacting with these machines. This psychological phenomenon is analyzed in the framework of mindlessness/mindfulness theory (Ellen Langer, 1989), and we suggest a new approach to the user-interface (UI) design based on the analysis. In this project, we explore problems found in GPS-based car-navigation systems, and suggest new design ideas—such as illusion of control or perceived control—in UI, via the preliminary experiment of ""Mindful Walking."" In addition, we present some other design possibilities based on Langer's work. Lastly, we suggest a new role for an intelligent agent.",2008-01-01,['living-mobile'],,14687,E15-384C,2019-08-13T19:50:50.229Z,['geek@media.mit.edu'],True,mindless-machine,2008-09-01,[],Mindless Machine,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:10:46.024Z,"MindRider is a helmet that translates electroencephalogram (EEG) feedback into an embedded LED display. For the wearer, green lights indicate a focused, active mental state, while red lights indicate drowsiness, anxiety, and other states not conducive to operating a bike or vehicle. Flashing red lights indicate extreme anxiety (panic). As many people return to cycling as a primary means of transportation, MindRider can support safety by adding visibility and increased awareness to the cyclist/motorist interaction process. In future versions, MindRider may be outfitted with an expanded set of EEG contacts, GPS radio, non-helmet wearable visualization, and other features to increase the cyclist's awareness of self and environment. These features may also allow for hands-free control of cycle function. A networked set of MindRiders may be useful for urban planning and emergency response situations.",2013-01-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202013-04-09%20at%2012.00.19%20PM.png,14695,--Choose Location,2016-12-05T00:16:37.473Z,['holtzman@media.mit.edu'],True,mindrider,2011-09-01,[],MindRider,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:46.214Z,"Minecraft is a video game focused on creativity and building. Players build constructions out of textured cubes in a 3D world�everything from a hut, to a train station, to a fully functional computer. Why can't we take those virtual creations, and bring them into the real world? Minecraft.Print() is our attempt to do so by creating a bridge between Minecraft and the real world, via 3D printers. A Minecraft player defines a 3D space to be printed, after which the software extracts the object, structure, or other creation from the virtual space and creates 3D-printable version. Minecraft.Print() takes advantage of the basic CAD functions of the game, thus allowing 14,000,000 (and counting) players to experience 3D modeling and printing�an area previously limited to those with more specific technical backgrounds.",2011-09-01,['human-dynamics'],https://dam-prod2.media.mit.edu/x/files/Display/logo.png,14711,,2016-12-05T00:16:37.296Z,['sandy@media.mit.edu'],True,minecraftprint,2011-01-01,[],Minecraft.Print(),PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:46.409Z,"Linear arrays of surface-emitting semiconductor lasers provide the basis for very small, low-power video projectors requiring minimal optics. Such projectors are suitable for handheld devices like phones.",2005-09-01,['object-based-media'],,14727,Garden Conference Room,2016-12-05T00:17:17.795Z,['vmb@media.mit.edu'],True,miniature-video-projector-personal-projection,2001-01-01,[],Miniature Video Projector (Personal Projection),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:46.495Z,"MirrorFugue is an installation for a player piano that evokes the impression that the ""reflection"" of a disembodied pianist is playing the physically moving keys. Live music emanates from a grand piano, whose keys move under the supple touch of a pianist's hands reflected on the lacquered surface of the instrument. The pianist's face is displayed on the music stand, with subtle expressions projecting the emotions of the music. MirrorFugue recreates the feeling of a live performance, but no one is actually there. The pianist is an illusion of light and mirrors, a ghost both present and absent. Viewing MirrorFugue evokes the sense of walking into a memory, where the pianist plays without awareness of the viewer's presence; or, it is as if viewers were ghosts in another's dream, able to sit down in place of the performing pianist and play along.",2012-12-31,"['advancing-wellbeing', 'tangible-media']",https://dam-prod2.media.mit.edu/x/2020/06/07/mf_KAKiG9V.png,14735,--Choose Location,2020-06-09T17:25:51.177Z,"['ishii@media.mit.edu', 'x_x@media.mit.edu']",True,mirrorfugue,2012-01-01,[],MirrorFugue,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:46.576Z,"MirrorFugue is an interface for the piano that bridges the gap of location in music playing by connecting pianists in a virtual shared space reflected on the piano. Built on a previous design that only showed the hands, our new prototype displays both the hands and upper body of the pianist. MirrorFugue II may be used for watching a remote or recorded performance, taking a remote lesson, and remote duet playing.",2011-01-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/mirrorfugue_square.jpg,14743,E15-344,2016-12-05T00:16:37.955Z,"['ishii@media.mit.edu', 'x_x@media.mit.edu']",True,mirrorfugue-ii,2009-09-01,[],MirrorFugue II,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:46.899Z,"MITes (MIT environmental sensors) are low-cost, wireless devices for collecting data about human behavior and the state of the environment. Nine versions of MITes have now been developed, including MITes for people movement (3-axis accelerometers), object movement (2-axis accelerometers), temperature, light levels, indoor location, ultra-violet light exposure, heart rate, haptic output, and electrical current flow.  MITes are being deployed to study human behavior in natural settings. We are also developing activity recognition algorithms using MITes data for health and energy applications. (A House_n Research Consortium Initiative funded by the National Science Foundation.)",2016-12-01,['city-science'],,14775,1CC-4th Floor,2017-10-13T16:20:40.650Z,"['kll@media.mit.edu', 'intille@media.mit.edu']",True,mites-portable-wireless-sensors-for-studying-behavior-in-natural-settings,2005-09-01,[],MITes: Portable Wireless Sensors for Studying Behavior in Natural Settings,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:47.230Z,"Pure networks and pure hierarchies both have distinct strengths and weaknesses. These become glaringly apparent during disaster response. By combining these modes, their strengths (predictability, accountability, appropriateness, adaptability) can be optimized, and their weaknesses (fragility, inadequate resources) can be compensated for. Bridging these two worlds is not merely a technical challenge, but also a social issue.",2016-01-01,['civic-media'],,14807,--Choose Location,2016-12-05T00:17:03.349Z,"['ethanz@media.mit.edu', 'bl00@media.mit.edu']",True,mixed-mode-systems-in-disaster-response,2015-01-01,[],Mixed-Mode Systems in Disaster Response,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:10:47.310Z,"The Mixer-Subverter is an online system that allows children to integrate the activites of play (from giving to stealing; from sharing to being forced to receive) and the activities of video editing (creating, juxtaposing, controlling) into a never-ending process of mix and subversion. It invites the storyteller within each of us to compose and visualize movies, images, and sound environments while writing a story. In addition, the Mixer-Subverter encourages playful collaboration in an exchange network of unique media artifacts.",2004-01-01,['media-fabrics'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/D%3A/mix-subvert-logo1.jpg,14815,E15-368,2016-12-05T00:16:37.838Z,['gid@media.mit.edu'],True,mixer-subverter,2003-01-01,[],Mixer-Subverter,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:47.452Z,"ML Open creates online learning experiences that are engaging, social, and project based, in areas where the Media Lab has unique expertise, such as learning, innovation, or design. Rather than distribute content, we design activities, and let learners participate in the Media Lab's way of thinking and doing. Our first experiment was Learning Creative Learning, a course taught at the Media Lab, which attracted 24,000 participants. The tools and strategies developed by ML Open are widely applicable in educational and corporate settings.",,['ml-learning'],,14831,--Choose Location,2018-06-20T19:54:56.815Z,"['ps1@media.mit.edu', 'srishti@media.mit.edu']",True,ml-open,2013-01-01,[],ML Open,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:47.537Z,"MMODM is an online drum machine based on the Twitter streaming API, using tweets from around the world to create and perform musical sequences together in real time. Users anywhere can express 16-beat note sequences across 26 different instruments, using plain-text tweets from any device. Meanwhile, users on the site itself can use the graphical interface to locally DJ the rhythm, filters, and sequence blending. By harnessing this duo of website and Twitter network, MMODM enables a whole new scale of synchronous musical collaboration between users locally, remotely, across a wide variety of computing devices, and across a variety of cultures.",2017-06-01,"['tangible-media', 'responsive-environments']",https://dam-prod2.media.mit.edu/x/2017/05/17/4_zaDb3Dv.jpg,14839,--Choose Location,2023-04-04T15:18:34.152Z,"['tod@media.mit.edu', 'basheer@media.mit.edu', 'ddh@media.mit.edu', 'joep@media.mit.edu']",True,mmodm,2015-09-01,['music'],MMODM: Massively Multiplayer Online Drum Machine,PUBLIC,http://mmodm.co/,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:47.619Z,"MobiCollab explores the non-intrusive use of mobile phones in meetings. Currently, mobile phones are only non-intrusive during meetings if used for text messaging, and do not provide a specific set of tools for meetings. MobiCollab allows users to record key moments in a meeting and then collaboratively generate a transcript of these events. We have shown that MobiCollab can be used in a meeting without disturbing the meeting while providing a record of events better than hand-written notes.",2007-09-01,['context-aware-computing'],,14847,E15-320,2016-12-05T00:16:37.866Z,[],True,mobicollab,2005-09-01,[],MobiCollab,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:47.708Z,"The Digital Construction Platform (DCP) is an in-progress research project consisting of a compound robotic arm system. The system comprises a 6-axis KUKA robotic arm attached to the endpoint of a 3-axis Altec hydraulic boom arm, which is mounted on a mobile platform. Akin to the biological model of the human shoulder and hand, this compound system utilizes the large boom arm for gross positioning and the small robotic arm for fine positioning and oscillation correction, respectively. Potential applications include fabrication of non-standard architectural forms, integration of real-time on-site sensing data, improvements in construction efficiency, enhanced resolution, lower error rates, and increased safety.",2017-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/DCP_2.jpg,14855,--Choose Location,2021-05-21T13:25:36.210Z,"['stevenk@media.mit.edu', 'j_klein@media.mit.edu', 'jleland@media.mit.edu', 'neri@media.mit.edu', 'cail@media.mit.edu']",True,mobile-digital-construction-platform-mdcp,2013-01-01,"['robotics', 'architecture', 'construction']",Digital Construction Platform v.1,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:47.796Z,"We are developing a mobile phone-based platform to assist people with chronic diseases, panic-anxiety disorders, or addictions. Making use of wearable, wireless biosensors, the mobile phone uses pattern analysis and machine learning algorithms to detect specific physiological states and perform automatic interventions in the form of text/images plus sound files and social networking elements.  We are currently working with the Veterans Administration drug rehabilitation program involving veterans with PTSD.",2014-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/iheal.png,14863,--Choose Location,2022-05-12T17:11:28.944Z,"['picard@media.mit.edu', 'fletcher@media.mit.edu']",True,mobile-health-interventions-for-drug-addiction-and-ptsd,2009-09-01,[],Mobile health interventions for drug addiction and PTSD,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:47.874Z,"The scientific community is making marked progress in the area of Alzheimer's disease (AD) treatment: memory-related pharmaceuticals are available, the neurobiology of AD is fairly well understood, and the genetic underpinnings of the disease continue to be unraveled. However, despite these advances, it has been shown that individuals often present the symptoms of AD years before they seek a diagnosis. The barrier to treatment is the lack of structure with which to obtain a diagnosis or even predict the onset of disease in a stigmatized environment. With technology, we can build clinically valid assessment into the tools we use every day�the tools we care about. We are developing music tools to detect cognitive performance in the memory domains at risk of decline in the earliest stages of AD. These tools are mobile, longitudinal, and the patient is the first point of feedback.",2010-01-01,['opera-of-the-future'],,14871,E15-445,2016-12-05T00:16:10.106Z,['tod@media.mit.edu'],True,mobile-music-diagnostics-targeting-alzheimers-disease,2007-09-01,[],Mobile Music Diagnostics: Targeting Alzheimer's Disease,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:47.963Z,"A fast moving workplace, calls for... a fast moving workstation!  The mobile office is a prototype robotic office fitted with a remote controlled, motorized base, onboard AC power storage for 6-8 hours, and 4 axis robotic arm.  The mobile office is great for taking your work down into the machine shop or to lengthy collaboration meetings.",2012-09-01,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/mobile%20office.png,14879,,2016-12-05T00:17:17.848Z,['neri@media.mit.edu'],True,mobile-office,2012-01-01,[],Mobile Office,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:48.142Z,"We are developing a special version of our Scratch programming language to enable people to create, play, and share interactive media on mobile devices. Mobile Scratch is designed especially for interacting with the outside world, taking inputs from microphone, camera, and external sensors, and communicating with other mobile devices. We are running an initial pilot project at an innovative school serving children in low-income communities in India.",2010-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/bdaysquare-sm.jpg,14887,LEGO Learning Lab,2016-12-05T00:16:38.085Z,"['mres@media.mit.edu', 'jmaloney@media.mit.edu', 'silver@media.mit.edu', 'kbrennan@media.mit.edu']",True,mobile-scratch,2006-01-01,[],Mobile Scratch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:48.325Z,"The Mobile Territorial Lab (MTL) aims at creating a ""living"" laboratory integrated in the real life of the Trento territory in Italy, open to manifold kinds of experimentations. In particular, the MTL is focused on exploiting the sensing capabilities of mobile phones to track and understand human behaviors (e.g., families' spending behaviors, lifestyles, mood, and stress patterns); on designing and testing social strategies aimed at empowering individual and collective lifestyles through attitude and behavior change; and on investigating new paradigms in personal data management and sharing. This project is a collaboration with Telecom Italia SKIL Lab, Foundation Bruno Kessler, and Telefonica I+D.",,"['connection-science', 'human-dynamics']",,14895,--Choose Location,2022-06-10T13:17:15.682Z,"['sandy@media.mit.edu', 'shrier@media.mit.edu']",True,mobile-territorial-lab,2012-09-01,[],Mobile Territorial Lab,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:48.509Z,"Using the built-in camera of a mobile phone and inexpensive printed tags, users can leave and retrieve video messages in specific locations or associated to specific objects. At the heart of the project is d-touch: an open-source system for computer vision recognition of markers. D-touch's peculiarity, compared to other marker systems, is that its recognition mechanism is based on topology rather than geometry. As a consequence, the system allows considerable freedom in the visual design of the markers, so that they can be ""hidden"" or designed to fit the specific aesthetic requirements of different applications.",2007-01-01,['fluid-interfaces'],,14903,E15-320,2016-12-05T00:16:38.328Z,['pattie@media.mit.edu'],True,mobile-video-d-touch,2006-09-01,[],Mobile Video D-Touch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:48.650Z,"As part of the Living Observatory ecological sensing initiative, we've been developing new approaches to mobile, wearable sensor data visualization. The Tidmarsh app for Google Glass visualizes real-time sensor network data based on the wearer's location and gaze. A user can approach a sensor node to see 2D plots of its real-time data stream, and look across an expanse to see 3D plots encompassing multiple devices. On the back-end, the app showcases our Chain API, crawling linked data resources to build a dynamic picture of the sensor network. Besides development of new visualizations, we are building in support for voice queries, and exploring ways to encourage distributed data collection by users.",2015-01-01,['responsive-environments'],,14911,--Choose Location,2016-12-05T00:17:17.896Z,"['gershon@media.mit.edu', 'bmayton@media.mit.edu', 'sfr@media.mit.edu', 'ddh@media.mit.edu', 'joep@media.mit.edu']",True,mobile-wearable-sensor-data-visualization,2014-01-01,[],"Mobile, Wearable Sensor Data Visualization",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:48.793Z,"MobileEssence explores the non-intrusive use of mobile phones in meetings. Currently mobile phones are only non-intrusive during meetings if used for text messaging, they do not currently provide a specific set of tools for meetings. MobileEssence allows users to record key moments in a meeting and then collaboratively generate a transcript of these events.",2007-09-01,['context-aware-computing'],,14919,--Choose Location,2016-12-05T00:16:38.350Z,[],True,mobileessence,2006-09-01,[],MobileEssence,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:48.926Z,"Mobility on Demand (MoD) systems are fleets of lightweight electric vehicles at strategically distributed electrical charging stations throughout a city. MoD systems solve the ""first and last mile"" problem of public transit, providing mobility between transit station and home/workplace. Users swipe a membership card at the MoD station and drive a vehicle to any other station (one-way rental). The Velib' system of 20,000+ shared bicycles in Paris is the largest and most popular one-way rental system in the world. MoD systems incorporate intelligent fleet management through sensor networks, pattern recognition, and dynamic pricing, and the benefits of Smart Grid technologies include intelligent electrical charging (including rapid charging), vehicle-to-grid (V2G), and surplus energy storage for renewable power generation and peak sharing for the local utility. We have designed three MoD vehicles: CityCar, RoboScooter, and GreenWheel bicycle. (Continuing the vision of William J. Mitchell.)",2016-12-01,['changing-places'],,14927,E15-001,2016-12-08T20:04:57.913Z,"['kll@media.mit.edu', 'rchin@media.mit.edu', 'kdinakar@media.mit.edu', 'bmander@media.mit.edu']",True,OLD_mobility-on-demand-systems,2008-01-01,[],_Mobility on Demand Systems,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:49.273Z,"Can we recognize stress, mood, and health conditions from wearable sensors and mobile-phone usage data? We analyze long-term, multi-modal physiological, behavioral, and social data (electrodermal activity, skin temperature, accelerometer, phone usage, social network patterns) in daily lives with wearable sensors and mobile phones to extract bio-markers related to health conditions, interpret inter-individual differences, and develop systems to keep people healthy.",2013-09-01,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/files/Display/sleep_calendar.jpg,14943,--Choose Location,2016-12-05T00:17:17.965Z,"['picard@media.mit.edu', 'akanes@media.mit.edu']",True,mobisensus-predicting-your-stressmood-from-mobile-sensor-data,2012-01-01,[],Mobisensus: Predicting Your Stress/Mood from Mobile Sensor Data,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:49.455Z,"OpenScope is an open source project that combines three components for anyone to explore the micro world anytime, anywhere. The 3D-printable open hardware turns your smartphone into a 200x microscope, the image processing application helps you recognize specific objects, and the online community allows you to share and contribute your findings from the microscope. OpenScope is expanding microscopy technologies beyond research laboratories and transforming the way we interact with the micro world.",2017-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/12/openscope_big.jpg,14951,--Choose Location,2018-05-04T10:52:12.271Z,"['ethanz@media.mit.edu', 'nsavidis@media.mit.edu', 'emreiser@media.mit.edu', 'oi7@media.mit.edu']",True,openscope,2016-01-01,[],OpenScope,PUBLIC,http://oi7.me,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:49.589Z,"Five billion of the world's six some billion people live in developing countries, where many have no access to basic medical treatments and preventive care. Moca, or ""Mobile Care,"" is cell-phone-based telemedicine system specifically for use in developing countries. The system allows semi-skilled community health workers to go into the field carrying a smartphone, collect diagnostic information, transmit it to a central medical records system for expert review, and receive real-time decision support. The project is build on open platforms: the Android smartphone platform and the OpenMRS medical record system. OpenMRS is designed for use in developing countries and is deployed in a dozen countries. Members of the Moca team traveled to the Philippines to explore a pilot deployment in Capiz; they are also developing partnerships with other organizations around the world.",2010-01-01,['new-media-medicine'],https://dam-prod2.media.mit.edu/x/files/Display/moca.jpg,14959,E15-320,2016-12-05T00:16:38.164Z,['fmoss@media.mit.edu'],True,moca,2008-09-01,[],Moca,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:10:49.896Z,"Synthesizing music from non-Western cultures is a problem for current music technology, which is based on constant pitch and note-oriented concepts such as MIDI. This is an issue for Indian music because time-varying pitch inflections, called gamakas, are an essential part of its construct. We are analyzing songs and instrumental pieces from the South Indian tradition, and developing software that enables musicians to synthesize Indian music with the required ornamentations. Such innovations will allow the music industry to cross cultural boundaries and provide appropriate representations for the expressive artifacts of non-Western music, in particular from Asia and the Middle-East.",2011-01-01,['music-mind-and-machine'],,14975,E15-484,2016-12-05T00:17:18.013Z,['bv@media.mit.edu'],True,modeling-indian-music,2006-01-01,[],Modeling Indian Music,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:50.015Z,"This project investigates the nature of mental representations of musical structure through cognitive study and machine modeling. These models create a core for systems that can organize sounds in a manner consistent with human aesthetic reasoning, without relying on preset rules of any given ""artistic style."" The emphasis is on identifying musical structures through perceptual processes and recognition, in contrast to modeling reasoning based on musical training.",2005-01-01,['music-mind-and-machine'],,14983,--Choose Location,2016-12-05T00:16:38.468Z,['bv@media.mit.edu'],True,modeling-musical-structure,2002-01-01,[],Modeling Musical Structure,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:50.125Z,"How do people perceive the way music can convey a story or a feeling, tension and resolution in music, and musical structures? In order to answer these questions, we are collecting data from all around the world to help illuminate these issues and reveal universal aspects of music perception. An online study has been designed with examples that take into account a number of musical parameters: pitch contour, rhythmic regularity, tempo, loudness, and harmony. These examples both isolate and combine these parameters, allowing us to gather empirical data. The model is trained on this data in order to mirror human perception as closely as possible. As of September 2005, more than 2,000 people have taken part in our study.",2006-01-01,['opera-of-the-future'],,14991,E15-445,2016-12-05T00:17:18.037Z,['tod@media.mit.edu'],True,modeling-tension-and-resolution-in-music,2004-01-01,[],Modeling Tension and Resolution in Music,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:10:50.348Z,This project aims to build a modular lighting system where users can customize the design and lighting patterns.,2012-09-01,['affective-computing'],,15007,--Choose Location,2016-12-05T00:16:38.487Z,"['picard@media.mit.edu', 'akanes@media.mit.edu']",True,modular-light-for-better-sleep,2012-09-01,"['design', 'art', 'music', 'performance']",Modular Light for Better Sleep,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:50.457Z,"We have developed a simple modular platform for wireless sensing. This system allows for easy prototyping and testing of embedded sensor applications, and encapsulates much of the wireless system design and makes it reusable. All boards in the system are 1.4 inch square; the master board contains a 22 MIPS microcontroller with 12-bit ADC and a 115.2 kBps wireless link with TDMA channel sharing. A flash memory board is also available for local storage. Sensor boards can be attached to the master via a 26-pin fixed link, which provides for both direct and multiplexed connections, as well as a variety of other data protocols and power distribution. So far, constructed sensor boards include a six degree of freedom inertial measurement unit, a tactile (e.g., bend and pressure) sensor signal processing board, and a promixity sensing board based on both sonar and capacitve sensors. An ambient sensor (camera, IR, heat, sound) board is currently in testing. This platform has been used in a number of lab projects, including a wearable gait laboratory, a novel musical controller and an instrument plush bear.",2007-01-01,['responsive-environments'],,15015,E15-344,2016-12-05T00:16:10.138Z,['joep@media.mit.edu'],True,modular-platform-for-high-density-wireless-sensing,2001-01-01,[],Modular Platform for High-Density Wireless Sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:50.560Z,"Modular Sound Blocks offer a way to create, design, and explore sound and music. Essentially a programming environment for music with a tangible interface, Modular Sound Blocks allow children or adults to experiment with and construct original creations. Using simple mathematics, exploring relationships, or simply �playing,� people can create complex structures of sound by networking the blocks together. Young children can create and explore simple relationships with only a few blocks, while older children and adults will construct enormously intricate, complicated networks.",2005-01-01,['music-mind-and-machine'],,15023,E15-484,2016-12-05T00:17:18.085Z,['bv@media.mit.edu'],True,modular-sound-blocks,2004-01-01,[],Modular Sound Blocks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:50.657Z,"We are conducting EEG studies to identify the musical features and musical interaction patterns that universally impact measures of arousal. We hypothesize that we can induce states of high and low arousal using electrodermal activity (EDA) biofeedback, and that these states will produce correlated differences in concurrently recorded skin conductance and EEG data, establishing a connection between peripherally recorded physiological arousal and cortical arousal as revealed in EEG. We also hypothesize that manipulation of musical features of a computer-generated musical stimulus track will produce changes in peripheral and cortical arousal. These musical stimuli and programmed interactions may be incorporated into music technology therapy, designed to reduce arousal or increase learning capability by increasing attention. We aim to provide a framework for the neural basis of emotion-cognition integration of learning that may shed light on education and possible applications to improve learning by emotion regulation.",2018-12-31,['affective-computing'],,15031,--Choose Location,2022-05-12T17:27:32.727Z,"['picard@media.mit.edu', 'gleslie@media.mit.edu']",True,modulating-peripheral-and-cortical-arousal-using-a-musical-motor-response-task,2015-09-01,"['human-machine-interaction', 'music', 'affective-computing']",Modulating peripheral and cortical arousal using a musical motor response task,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:50.855Z,"Nuclear magnetic resonance can be used to manipulate and to control simple molecules and compex biomolecules. We have demonstrated the use of one molecule as a seven quantum bit quantum computer. In an additional joint project with Joseph Jacobson's Molecular Machines group, we demonstrate control over biomolecules to cause them to selectively heat up and undergo chemical changes by building antennas into them.",2004-09-01,['quanta'],,15047,--Choose Location,2016-12-05T00:17:18.238Z,['ike@media.mit.edu'],True,molecular-control-with-nmr,2001-01-01,[],Molecular Control with NMR,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:51.138Z,"VLSI will hit fundamental physical limits within two decades. Quantum mechanics is the one resource in our universe that is exponentially large, but has been untapped for computation. We've found that, paradoxically, it is possible to preserve coherence in a quantum computer while still providing external access by using the orientation of the nuclei of atoms in molecules in a liquid to store quantum information, and programming them with radio-frequency pulses.",2003-09-01,"['personal-fabrication', 'silicon-biology', 'physics-and-media']",,15063,E15-023,2016-12-05T00:17:18.136Z,['neilg@media.mit.edu'],True,molecular-quantum-computation,2000-01-01,[],Molecular Quantum Computation,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:10:51.320Z,"This project involves the development of nanofabrication techniques that achieve a spatial resolution of 10 nm and below. In one system, single-wall carbon nanotubes are combined with a scanning-probe microscope to oxidize anodically the surface of atomically flat titanium. In another, micromechanical devices with interferometric sensors are being designed and fabricated to control the separation of an electrode gap with angstrom precision. The goal is to use such techniques to study biomolecular activity in-situ.",2005-09-01,[],,15071,E15-420,2016-12-05T00:16:38.752Z,['scottm@media.mit.edu'],True,molecular-scale-device-fabrication,2000-01-01,[],Molecular-Scale Device Fabrication,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:10:51.470Z,"The Museum of Modern Art in New York City presents a compelling environment in which to explore technnologies for
smart environments. We are working with a team led by Terry Riley, the Chief Curator for Architecture and Design, initially on an exhibit exploring changing notions of public and private spaces. The goal of this installation is to replace conventional kiosks with embedded sensing and ambient displays so that the supporting electronic information is accessible in its natural, rich context.
",2000-12-30,['physics-and-media'],,15079,E15-023,2016-12-05T00:16:38.575Z,"['neilg@media.mit.edu', 'rehmi@media.mit.edu']",True,moma,1997-12-31,[],MOMA,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:51.762Z,"Monkey Business is a system to keep distributed members of a group aware of each other�s presence and activities in a light-hearted manner, while striving to remain non-intrusive. The system also aims to facilitate unplanned and informal communication among distributed colleagues. It consists of a network of animatronic agents, specifically monkeys, which are situated in the offices or rooms of each member of a group. Through subtle movements, gestures, and sounds, the monkeys indicate the current activities of the other members of the group. The monkeys are meant to be ambient, at the periphery of one�s attention; but they can also be used more proactively as communication mechanisms, and promote informal exchanges among members of a distributed team.",2008-01-01,['living-mobile'],,15095,E15-384C,2016-12-05T00:16:38.974Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,monkey-business,2005-09-01,[],Monkey Business,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:51.912Z,"MonkeyToGo is a mobile extension of Monkey Business, our animatronic, remote- awareness network. Cell phones running our MonkeyToGo application can easily be added to the network in order to interact with and receive information from the agents in real time. If active, the connected cell phone vibrates and plays short animations and sound snippets in order to inform its user of ongoing activities at the remote locations. The MonkeyToGo user can also use the cell phone to ""tickle"" (call for attention) or to set up a voice connection with other monkey agents. Several cell phones can be added at the same time.",2008-01-01,['living-mobile'],,15103,E15-368,2016-12-05T00:16:39.007Z,['geek@media.mit.edu'],True,monkeytogo,2006-09-01,[],MonkeyToGo,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:52.040Z,"French for ""single shell,"" Monocoque stands for a construction technique that supports structural load using an object's external skin. Contrary to the traditional design of building skins that distinguish between internal structural frameworks and non-bearing skin elements, this approach promotes heterogeneity and differentiation of material properties. The project demonstrates the notion of a structural skin using a Voronoi pattern, the density of which corresponds to multi-scalar loading conditions. The distribution of shear-stress lines and surface pressure is embodied in the allocation and relative thickness of the vein-like elements built into the skin. Its innovative 3D printing technology provides for the ability to print parts and assemblies made of multiple materials within a single build, as well as to create composite materials that present preset combinations of mechanical properties.",2015-05-31,['mediated-matter'],,15111,--Choose Location,2021-05-21T13:13:41.452Z,['neri@media.mit.edu'],True,monocoque,2007-09-01,"['design', 'art', 'biology', 'materials', 'covid19']",Monocoque,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:10:52.152Z,"Moodtrack grew from a desire to create soundtracks based only on natural language input. By breaking down music into essential forms, music can be reconstructed based on affective cues. Applications include sample management, film music supervision, Web publishing, and personal sound devices.",2006-01-01,['electronic-publishing'],,15119,Pond,2016-12-05T00:16:38.796Z,['walter@media.mit.edu'],True,moodtrack,2005-09-01,[],Moodtrack,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:52.367Z,"The Moral Machine is a platform for gathering a human perspective on moral decisions made by machine intelligence, such as self-driving cars. We generate moral dilemmas, where a driverless car must choose the lesser of two evils, such as killing two passengers or five pedestrians. As an outside observer, people judge which outcome they think is more acceptable. They can then see how their responses compare with other people. If they are feeling creative, people can also design their own scenarios, for others to view, share, and discuss.Visit the Moral Machine.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/07/24/MMJudge.png.1400x1400.jpg,15135,--Choose Location,2020-07-23T18:44:11.551Z,"['irahwan@media.mit.edu', 'awad@media.mit.edu', 'dsouza@media.mit.edu']",True,moral-machine,2016-01-01,"['design', 'architecture', 'artificial-intelligence', 'civic-technology', 'crowdsourcing', 'data', 'social-media', 'transportation', 'urban-planning', 'social-science', 'collective-intelligence', 'behavioral-science', 'ethics', 'technology', 'autonomous-vehicles', 'data-science']",Moral Machine,PUBLIC,http://moralmachine.mit.edu,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:10:52.501Z,"The computer's emerging capacity to communicate an individual's affect raises critical ethical concerns. Additionally, designers of perceptual computer systems face moral decisions about how the information gathered by computers with sensors can be used. As humans, we have ethical considerations that come into play when we observe and report each other's behavior. Computers, as they are currently designed, do not employ such ethical considerations. This project assess the ethical acceptability of affect sensing in three different adversarial contexts, where within each context there are also different kinds of motivations (self-oriented and charity-oriented) for the individuals to perform as best as they can.    ",2005-09-01,['affective-computing'],,15143,E15-001,2016-12-05T00:16:39.096Z,['picard@media.mit.edu'],True,moral-sensors,2004-01-01,[],Moral Sensors,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:10:52.664Z,"Granular materials can be put into a jammed state through the application of pressure to achieve a pseudo-solid material with controllable rigidity and geometry. While jamming principles have been long known, large-scale applications of jammed structures have not been significantly explored. The possibilities for shape-changing machines and structures are vast and jamming provides a plausible mechanism to achieve this effect. In this work, jamming prototypes are constructed to gain a better understanding of this effect. As well, potential specific applications are highlighted and demoed. Such applications range from a morphable chair, to a floor which dynamically changes its softness in response to a user falling down to reduce injury, to artistic free-form sculpting.",2012-01-01,['mediated-matter'],,15151,--Choose Location,2016-12-14T01:46:49.791Z,"['stevenk@media.mit.edu', 'neri@media.mit.edu']",True,morphable-structures,2011-01-01,[],Morphable Structures,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:53.139Z,Mountain is a visualization of email that focuses on people instead of the messages they have exchanged. The project explores the ever-growing lists of contacts that people keep over the years via email.,2007-09-01,['sociable-media'],,15175,E15-383,2016-12-05T00:16:39.244Z,['judith@media.mit.edu'],True,mountain,2004-09-01,[],Mountain,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:53.272Z,"The goal of this project is to develop tools to sense and adapt to a user's affective state based on his or her mouse behavior. We are developing algorithms to detect frustration level for use in usability studies. We are also exploring how more permanent personality characteristics and changes in mood are reflected in the user�s mouse behavior. Ultimately, we seek to build adaptive relational agents that tailor their interactions with the user based on these sensed affective states.",2005-01-01,['affective-computing'],,15183,E15-001,2016-12-05T00:16:10.161Z,['picard@media.mit.edu'],True,mouse-behavior-analysis-and-adaptive-relational-agents,2004-01-01,[],Mouse-Behavior Analysis and Adaptive Relational Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:53.409Z,"Mouseless is an invisible computer mouse that provides the familiarity of interaction of a physical mouse without actually needing a real hardware mouse. Despite the advances in computing hardware technologies, the two-button computer mouse has remained the predominant means to interact with a computer. Mouseless removes the requirement of having a physical mouse altogether, but still provides the intuitive interaction of a physical mouse with which users are familiar. Mouseless consists of an Infrared (IR) laser beam and an Infrared camera. Both IR laser and IR camera are embedded in the computer. The laser beam module is modified with a line cap and placed such that it creates a plane of IR laser just above the surface the computer sits on. The user cups their hand, as if a physical mouse was present underneath, and the laser beam lights up the hand which is in contact with the surface. The IR camera detects those bright IR blobs using computer vision. The change in the position and arrangements of these blobs are interpreted as mouse cursor movement and mouse clicks.",2012-09-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/mouseless02.jpg,15191,--Choose Location,2018-10-12T16:32:37.681Z,"['pattie@media.mit.edu', 'pranav@media.mit.edu']",True,mouseless,2010-01-01,[],Mouseless,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:53.547Z,"MouseTrack is a web logging system that tracks mouse movements on websites. The system includes a visualization tool that displays the mouse cursor path followed by website visitors. It helps web site administrators run usability tests and analyze the collected data. Practitioners can track any existing webpage by simply entering its URL. 

The visualization tool allowed inexperienced observers to concretely learn important user interface principles by observation. Future work includes fully validating the approach by correlating eye gaze with mouse tracks in multiple web environments. Our goal is to introduce this approach to web usability practitioners and obtain universally identifiable mouse patterns that could be automatically labeled to reveal useful information about site design and speed up the iterative design process.",2007-01-01,['context-aware-computing'],,15199,E15-320,2016-12-05T00:17:18.339Z,[],True,mousetrack-a-web-based-usability-tool,2004-01-01,[],MouseTrack: A Web-Based Usability Tool,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:53.688Z,"Move Your Glass is an app built using the JavaScript-based Wearscript library to communicate with the native sensors and camera on the device. The app is an activity and behavior tracker that also tries to increase wellness by nudging the wearer to engage in positive behaviors. In particular, data from the tri-axial accelerometer and camera are collected and analyzed offline using a designated server. These results are then fed into a k-nearest neighbors machine learning routine and compared to training data to provide continuous differentiation between sitting/standing, walking, and running while the user is on the go. The ultimate goal is to log this information (and additional activity parameters) and convey activity summaries and, if necessary, prompts for increasing activity, back to users.",,"['advancing-wellbeing', 'fluid-interfaces']",,15207,--Choose Location,2018-10-18T01:17:21.506Z,"['pattie@media.mit.edu', 'nfarve@media.mit.edu']",True,move-your-glass,2013-01-01,[],Move Your Glass,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:54.210Z,"MOVITS aim to make the exchanging, sharing, and publishing of phone-captured media more integrated, evolving, and fun. Modifiable presentation templates allow the user to put together multi-media messages containing images, movies, audio, and text. We imagine these messages as a form of movie ""Post-It note"" that can be quickly created, exchanged, modified, or destroyed. These MOVITS can also become part of an evolving chain of MOVITS, as media is shared between phones and also published and downloaded from the Web. This project is being developed in conjunction with Nokia.",2004-09-01,['media-fabrics'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/C%3A/Documents%20and%20Settings/aisling.MEDIA-LAB/Desktop/movits.jpg,15239,E15-320B,2016-12-05T00:16:39.304Z,['gid@media.mit.edu'],True,movits,2003-09-01,[],MOVITS,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:54.325Z,"Mr. Java is an intelligent coffee machine: it recognizes you, learns your preferences, and feeds you both the bits and the atoms you desire.  A cornerstone of the Kitchen Sync project, Mr. Java demonstrates a new concept of appliances that work with and not against you.  Mr. Java also logs cumulative coffee consumption for the entire Lab over time, a concept easily extensible to a wide domain of applications.  We are currently looking at other applications for the existing Mr. Java technology.",1999-12-30,"['counter-intelligence', 'personal-information-architecture']",,15247,,2016-12-05T00:17:18.313Z,['mike@media.mit.edu'],True,mr-java,1997-12-31,[],Mr. Java,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:54.446Z,"Mr. Java is the Media Lab's wired coffee machine, which keeps track of usage patterns and user preferences. The focus of this project is to give Mr. Java a tangible customer-feedback system that collects data on user complaints or compliments.  ""Thumbs-up"" and ""thumbs-down""  pressure sensors were built and their signals integrated with the state of the machine to gather data from customers regarding their ongoing experiences with the machine.  Potentially, the data gathered can be used to learn how to improve the system. The system also portrays an affective, social interface to the user: helpful, polite, and attempting to be responsive to any problems reported.
",1999-12-30,"['counter-intelligence', 'affective-computing']",,15255,3rd Floor Kitchen,2016-12-05T00:16:39.336Z,['picard@media.mit.edu'],True,mr-java-customer-support,1997-12-31,[],Mr. Java:  Customer Support,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:54.707Z,"This project explores how a group Web space can become more dynamic and useful, rather than a static display of finished projects. We have given our Web server its own identity, MrWeb. MrWeb is designed to act as another member of the group, whose job is to display, coordinate, organize, and facilitate the way the group interacts. Our hypothesis is that by making interactions with the Web server more transparent, group members will be more likely both to keep information up-to-date and to document experiences for the group.",2003-01-01,['context-aware-computing'],,15279,E15-320,2016-12-05T00:16:39.483Z,[],True,mrweb,2002-01-01,[],MrWeb,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:10:54.882Z,"MTM ""Little John"" is a multi-purpose, mid-size, rapid prototyping machine with the goal of being a personal fabricator capable of performing a variety of tasks (3D printing, milling, scanning, vinyl cutting) at a price point in the hundreds rather than thousands of dollars. The machine was designed and built in collaboration with the MTM (Machines that Make) Project at MIT Center for Bits and Atoms.",2012-01-01,['fluid-interfaces'],,15295,--Choose Location,2016-12-05T00:16:39.413Z,"['pattie@media.mit.edu', 'linder@media.mit.edu']",True,mtm-little-john,2010-09-01,[],"MTM ""Little John""",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:10:55.052Z,"MugShots enables visual communication though everyday objects. We embed a small display into a coffee mug, an object with frequent daily use. Targeted for the workplace, the mug transitions between different communication modes in public and private spaces. In the private office space, the mug is an object for intimate communication between remote friends; users receive emoticon stickers via the display. When brought to a public area, the mug switches to a pre-selected image of the user's choice, serving as a social catalyst to trigger conversations in public spaces.",2014-01-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/DSC_0620_.JPG,15311,--Choose Location,2016-12-05T00:16:39.445Z,"['cindykao@media.mit.edu', 'geek@media.mit.edu']",True,mugshots,2013-09-01,[],MugShots,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:55.125Z,"A ConceptNet in English is already established and working well. We are now attempting to expand it to other languages and cultures. This project is an extended ConceptNet with Korean common-sense language, which is fundamentally different from English. Through this project, we can learn how to expand the ConceptNet into other languages and how to connect them. By connecting English and Korean ConceptNets, we are hoping not only to see cultural or linguistic differences, but also to solve problems such as the ambiguity of multivocal words, which were difficult to solve with only one ConceptNet.",2014-09-01,['software-agents'],,15319,E15-384,2022-06-10T14:18:57.536Z,['lieber@media.mit.edu'],True,multi-lingual-conceptnet,2005-01-01,[],Multi-Lingual ConceptNet,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:55.204Z,"The entire hyperinstruments project was founded on the idea of giving individuals powerful control over many layers and levels of music, in a way that has previously only been possible in complex multitrack digital recording studios�and then only in non-real-time, as sound engineers, producers, and musicians manipulate and arrange every last sound of the CDs we hear, often achieving ""perfection"" but sacrificing the intuition, gestuality, and communication of real-time music performance. Using the many conceptual and technical advances of over ten years of hyperinstrument development, we are returning to our original inspiration, to develop a multi-interface environment designed for the precise, subtle, and expressive manipulation of multiple layers of sound. We are construcing an environment�and a special hand-manipulated sensor interface�that can be tuned either as a high-end system for the most discerning musicians, or as a ""sonic playground"" for the youngest children. The first application of the MMM will be as a complement to the Sensor Chair in the Future Music Blender.",2000-12-30,['opera-of-the-future'],,15327,E15-483,2016-12-05T00:16:39.600Z,['tod@media.mit.edu'],True,multi-modal-mixer-mmm,1999-12-31,[],Multi-Modal-Mixer (MMM),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:55.278Z,"This project aims to collect and reason over common-sense knowledge in languages other than English. We have collected large bodies of common-sense knowledge in Portuguese and Korean, and we are expanding to other languages such as Spanish, Dutch, and Italian. We can use techniques based on AnalogySpace to discover correlations between languages, enabling our knowledge bases in different languages to learn from each other.",2014-09-01,['software-agents'],,15335,E15-383,2022-06-10T14:18:57.555Z,['lieber@media.mit.edu'],True,multilingual-common-sense,2005-09-01,[],Multilingual Common Sense,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:10:55.367Z,"We are developing a new interface for text chat, to assist conversational partners who don't necessarily speak each others' languages perfectly. It will be especially useful to connect students trying to learn foreign languages with native speakers of that language who are trying to learn the first student's language. The idea is to provide ""bad translations"" together with tools that let the user quickly analyze the original and translated versions to correct translation mistakes, understand other possibilities for the meaning of a word or phase, provide subject context that helps disambiguate words, provide additional vocabularly in the other language for continuing the conversation, enlist the conversation partner in helping a user learn how to say something, and understand where there may be cultural differences between the speakers. It relies on natural language processing technology together with common-sense knowledge bases in both languages and cultures.",2007-01-01,['software-agents'],,15343,E15-385,2016-12-05T00:16:39.639Z,['lieber@media.mit.edu'],True,multilingual-educational-chat-assistant,2007-01-01,[],Multilingual Educational Chat Assistant,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:55.526Z,"This project involves providing a traditional bed with multimedia computing capabilities, eye-tracking, and coordination of the electronics with a projection screen mounted above the bed. The system provides the user with an alarm clock that projects a sunrise on the ceiling at the time the user wishes to wake. It allows the user to go to sleep with a star-lit sky projection, or with a constellation game. And if you play the game well past your bedtime, the system may ask you about resetting your alarm to a later time the following morning. It makes reading in bed easier by projecting your book onto the screen, eliminating the need to prop yourself up using your elbows or a pillow.",2007-09-01,['context-aware-computing'],,15359,E15-320,2016-12-05T00:16:39.686Z,['win@media.mit.edu'],True,multimedia-bed,1999-01-01,[],Multimedia Bed,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:55.798Z,"This project explores how we can visually compare multiple newsgroups. We represent some of the implicit information about group communication that takes place within a newsgroup by tracking how much people respond to each others' comments, the frequency of supportive or angry words used, or the frequency with which people return to the group and post follow-up messages. Mapping this sort of information to visual characteristics such as size, shape, color, and nesting of objects will give each representation of a newsgroup a distinct visual appearance.",2004-01-01,['sociable-media'],,15383,E15-450,2016-12-05T00:16:10.187Z,['judith@media.mit.edu'],True,multiples-of-news,2003-01-01,[],Multiples of News,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:56.343Z,"In the online world, many of the social cues that are fundamental to coherent communication and the establishment of a vibrant community are missing. The goal of social visualization research is to make legible the patterns and interactions that occur in online social environments. This project extends that research into the audio domain. We are developing techniques for audibly depicting online crowds. Key to this work is both the development of a 3-D audio environment and the creation of a cognitively grounded and aesthetically interesting sound representation.",2002-12-30,['sociable-media'],,15423,,2016-12-05T00:16:39.790Z,['judith@media.mit.edu'],True,murmurized,2001-12-31,[],Murmurized,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:56.700Z,"This work is an application of a synthetic character metaphor to the production of music. Presented here is a small colony of creatures whose virtual bodies produce sound. Played out in a spatial representation, passages of music are exchanged between the creatures and consequently distorted; retrograde variations and repeated subsegments are swapped back and forth in an accidental, recombinant evolution. It draws parallels between musical problems and the problems of motor control and learning in synthetic characters. This work also points towards a system in which characters' movements are shaped (by training) in real time; consequently, by creating music with the resulting system, we are able to perform similar training and shaping within a musical domain.",2005-01-01,['opera-of-the-future'],,15439,E15-450,2016-12-05T00:17:18.400Z,['tod@media.mit.edu'],True,music-creatures,2000-01-01,[],Music Creatures,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:56.876Z,"For the 42nd annual Bachman's/Marshall Field's flower show, we worked with landscape designer Julie Messervy and the production team from Marshall Field's to develop a set of interactive music and sound experiences that could integrate into their large indoor garden. This included four Yamaha Disklavier pianos playing different pieces that visitors could transform by using a touchpad station in front of each piano, a forest with actuated illuminated windchimes that would respond to visitors blowing on pinwheels, a pretend underwater garden with ""Music Shapers"" that made environmental and musical sounds when squeezed, and music that tied those sounds together, along with new material that was presented at the end of the experience.",2005-09-01,['opera-of-the-future'],,15447,E15-443,2016-12-05T00:16:39.923Z,['tod@media.mit.edu'],True,music-in-the-garden,2005-01-01,[],Music in the Garden,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:57.066Z,"The Music Shapers are soft, squeezable instruments that allow players to shape and explore musical compositions. Using capacitive sensing and conductive embroidery to measure the squeezing gesture, Shapers allow children access to high-level musical concepts such as contour, timbre, and orchestration, rarely accessible by traditional musical instruments.",2005-09-01,['opera-of-the-future'],,15455,E15-443,2016-12-05T00:16:39.947Z,['tod@media.mit.edu'],True,music-shapers,2000-01-01,[],Music Shapers,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:57.249Z,"Almost all kids love to sing, even if they sadly forget this later in life. Music is never work: you can only ""play"" it. Today's incredible music synthesizers are a stepping stone to tomorrow's possibilities, and kids will lead the way. Imagine a toy piano that sounds like a Steinway, a stuffed toy that sings lullabies, or an opera that is performed with a cast of toys while children play with them. We are devoting considerable effort to designing a new world of musical play for children, starting with the very young. A coordinated collection of Music Toys is being designed, covering three basic categories: Simple Things, Music Shapers, and Big Thing.",2000-12-30,"['toys-of-tomorrow', 'opera-of-the-future']",,15463,E15-495,2016-12-05T00:16:39.892Z,['tod@media.mit.edu'],True,music-toys,1999-12-31,[],Music Toys,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:57.434Z,"The Hyperinstruments group is exploring new ways to visualize music both at the song-level and the collection-level. At song-level: SoundSieve is a music visualizer that takes the intrinsic qualities of a musical piece�pitch, time, and timbre�and makes their patterns readily apparent in a visual manner. For example, you can quickly pick out repeating themes, chords, and complexity from the pictures and video. At collection-level: MusicBox focuses on the problem of navigating a large body of music. It aims to help you find music you like, both inside your own collection (to match a particular mood, for example), or from a body of entirely new music. MusicBox visualizes your music collection in space, giving each track a location based on how similar it sounds to other tracks. This new manner of navigation stands in stark contrast to traditional, text-dependent media players like iTunes and Windows Media Player.",2008-01-01,['opera-of-the-future'],,15471,E15-443,2016-12-05T00:16:39.838Z,['tod@media.mit.edu'],True,music-visualization,2007-01-01,[],Music Visualization,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:57.600Z,"In a study of human perception of music in relation to different representations of video graphics, this project explores the automatic synchronization in real time between audio and image. This aims to make the relationship seem smaller and more consistent. The connection is made using techniques that rely on audio signal processing to automatically extract data from the music, which subsequently are mapped to the visual objects. The visual elements are influenced by data obtained from various Musical Information Retrieval (MIR) techniques. By visualizing music, one can stimulate the nervous system to recognize different musical patterns and extract new features.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/files/wp-content/uploads/2015/10/11057390_10153449799872247_3663251234376200016_o.jpg,15479,--Choose Location,2016-12-05T00:17:18.424Z,"['tod@media.mit.edu', 'thomassl@media.mit.edu']",True,music-visualization-via-musical-information-retrieval,2015-01-01,"['data', 'music', 'perception']",Music Visualization via Musical Information Retrieval,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:57.786Z,"Our work in Music, Mind, and Health has culminated in a recent PhD thesis, showing the technologies and perspectives required to build on the transformative nature of music to drive specific neurological, physical, and psychological change. A radically new ""Personal Instrument"" is currently being used by Dan Ellsey, a quadraplegic individual, who controls this interface to sculpt an expressive performance of music in real time. A three-month study of Ellsey's expressive behavior�its potential as well as its limits�resulted in an interface tailored just for him, enabling him to access expressive performance despite his physical disability. This new line of work highlights principles for future instruments and applications, where the impact is in the marriage of the interface and uniqueness of the person. In this way, we are pursuing new design philosophies, technologies, and collaborations within the scientific community, public performance, and clinical research.",2010-09-01,['opera-of-the-future'],,15487,E15-445,2016-12-05T00:17:18.455Z,['tod@media.mit.edu'],True,music-mind-and-health,2005-09-01,[],"Music, Mind, and Health",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:57.950Z,"Using creative software for piano learning, we train non-musicians to play short melodies on a piano. We are conducting an fMRI study to look at the neurobehavioral changes associated with listening to, learning, and playing music. The software has been designed to automate all aspects of subject training and behavioral testing.",2005-01-01,['opera-of-the-future'],,15495,E15-443,2016-12-05T00:16:10.221Z,['tod@media.mit.edu'],True,music-training-software-for-fmri,2004-01-01,[],Music-Training Software for fMRI,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:58.125Z,"Stage performances present many challenges and opportunities in the field of robotics. Here we create a family of furniture robots that look and act like organic entities for the production of Death and the Powers. Mei-Mei is a six-legged walking robot that is being developed in the lab as a moving workbench. It can move forward, backward, and even turn around with differential steering control. Di-Di is a modular robot that can transform itself into a sofa onstage and walk with a rolling, lurching, and gliding motion. These two robots will also be endowed with interactive behavior structured around scenes, beats, and actions. The design of these robots not only incorporates technological, conceptual, and aesthetic innovations, but also coordinates with narrative and musical materials in the opera.",2010-09-01,['opera-of-the-future'],,15503,E15-443,2016-12-05T00:16:40.067Z,[],True,musical-furniture,2009-01-01,[],Musical Furniture,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:58.291Z,"The Musical Jacket, first developed in fall 1997, has recently undergone hardware and software improvements, as well as functional updating. A special version was prepared for permanent exhibition at the reconfigured Innoventions (Walt Disney World, Orlando), and a new application is being designed for Internet MP3 soundfile selection and play.",1998-12-30,"['toys-of-tomorrow', 'opera-of-the-future']",,15511,E15-483,2016-12-05T00:16:40.093Z,['tod@media.mit.edu'],True,musical-jacket,1996-12-31,[],Musical Jacket,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:58.623Z,"Different people respond to and query for music in vastly different ways. Someone's favorite song is only barely recognizable by another: this leads us to question why and how we attach preference and memory to musical information. We are working on an architecture to model the individual and singular representation of a piece of music by a human listener. This model can then be applied to common music retrieval tasks such as recommendation and search, which in turn could leverage the power of the immediate delivery of music over networks while allowing users to discover new and varied music.",2002-12-30,['music-mind-and-machine'],,15527,,2016-12-05T00:16:40.153Z,['bv@media.mit.edu'],True,musical-preference-understanding,2000-12-31,[],Musical Preference Understanding,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:10:58.781Z,"Robots and performers make beautiful music together. The opera ""Death and the Powers"" features a chorus of seven-foot tall, autonomous, polymorphic Operarobots and three large fifteen-foot tall robotic walls. At various times, these function as characters, set pieces, and lighting elements. Using state-of-the-art control electronics, and a novel real-time performance control system, a total of 9 individually addressable Operabots reflect on, participate in, and illuminate the action onstage.",2010-09-01,['opera-of-the-future'],,15535,E15-443,2016-12-05T00:16:39.998Z,[],True,musical-robotics,2008-09-01,[],Musical Robotics,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:10:58.940Z,"musicBottles introduces a tangible interface that deploys bottles as containers and controls for digital information. The system consists of a specially designed table and three corked bottles that ""contain"" the sounds of the violin, the cello, and the piano in Edouard Lalo's ""Piano Trio in C Minor, Op. 7."" Custom-designed electromagnetic tags embedded in the bottles enable each one to be identified wirelessly. The opening and closing of a bottle is also detected. When a bottle is placed onto the stage area of the table and the cork is removed, the corresponding instrument becomes audible. A pattern of colored light is rear-projected onto the table's translucent surface to reflect changes in pitch and volume. The interface allows users to structure the experience of the musical composition by physically manipulating the different sound tracks.",2000-12-30,['tangible-media'],,15543,E15-441,2016-12-05T00:17:18.480Z,"['ishii@media.mit.edu', 'fletcher@media.mit.edu', 'joep@media.mit.edu']",True,musicbottles,1997-12-31,"['design', 'human-computer-interaction', 'interfaces']",musicBottles,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:59.183Z,"The musicbox is a tangible interface supporting remote awareness through the medium of music and light. The musicbox is linked over the Internet to the music and light levels surrounding a remote piano. The system communicates live music and a sense of physical movement through a lighted wooden ""music box,"" while also providing browsable access to music previously played by the device.",1998-12-30,['tangible-media'],,15559,E15-441,2016-12-05T00:16:40.024Z,['ishii@media.mit.edu'],True,musicbox,1996-12-31,[],musicbox,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:59.300Z,"MusicBox focuses on the problem of navigating a large body of music.  It aims to help you find music you like, both inside your own collection (to match a particular mood, for example), or from a body of entirely new music.  MusicBox visualizes your music collection in space, giving each track a location based on how similar it sounds to other tracks.  This new manner of navigation stands in stark contrast to traditional, text-dependent media players like iTunes and Windows Media Player.",2008-01-01,['opera-of-the-future'],,15567,E15-443,2016-12-05T00:16:33.826Z,['tod@media.mit.edu'],True,musicbox-navigating-the-space-of-your-music,2008-01-01,[],MusicBox: Navigating the Space of Your Music,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:59.418Z,"Musicpainter is a networked, graphical composing environment that encourages sharing and collaboration within the composing process. It provides a social environment where users can gather and learn from each other. The approach is based on sharing and managing music creation in small and large scales. At the small scale, users are encouraged to begin composing by conceiving small musical ideas, such as melodic or rhythmic fragments, all of which are collected and made available to all users as a shared composing resource. The collection provides a dynamic source of composing material that is inspiring and reusable. At the large scale, users can access full compositions that are shared as open projects. Users can listen to and change any piece. The system generates an attribution list on the edited piece, allowing users to trace how it evolves in the environment. ",2012-09-01,['living-mobile'],,15575,E15-310,2016-12-05T00:16:40.246Z,['geek@media.mit.edu'],True,musicpainter,2007-01-01,[],Musicpainter,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:10:59.573Z,"Musicscape is a two-dimensional, spatial music navigation interface designed for browsing large-scale sound archives. It simulates a 2D sound field by applying Head-Related Transfer Function (HRTF), and enables users to virtually walk around the sound space with a computer mouse.",2011-01-01,['living-mobile'],,15583,E15-310,2016-12-05T00:16:40.184Z,['geek@media.mit.edu'],True,musicscape,2007-09-01,[],Musicscape,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:59.742Z,"muStick is a tribal instrument turned digital. It is a simple stick to which sensors are attached. They detect the thumping of the stick against the ground and tapping on the stick. The angle at which the stick is held is also measured. These signals are tranformed into MIDI signals, and turn ?Stick into a simple yet powerful music controller.",2011-01-01,['opera-of-the-future'],,15591,,2016-12-05T00:16:49.188Z,['tod@media.mit.edu'],True,mustick,2010-01-01,[],muStick,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:10:59.908Z,"Audio recordings are an archive of a performance.  As an archive, they cannot be changed; they embody certain choices on the part of the performers and the producers. Thus, a recording only allows certain interpretations to be heard at a later date, even if the source material itself is open to a plethora of expositions.  We are developing tools that allow a single recording to encapsulate multiple interpretations: a new phrase here, and entirely new performance there.  These new mutable recordings will give performers the chance to present a variety of interpretations to the listener, while allowing the listener to have new experiences each time she listens to the piece.",2007-01-01,['opera-of-the-future'],,15599,E15-445,2016-12-05T00:16:48.888Z,['tod@media.mit.edu'],True,mutable-recordings,2006-01-01,[],Mutable Recordings,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:00.073Z,"We provide a software and hardware toolkit for creating an on-body network of tactile and ambient information accessories, connecting people physically with information accessed via the Internet. My Ears Are Burning uses the toolkit to make a user aware of attention being paid to her online presence. Heating elements placed on the user's ears are activated when, for example, her Web page is accessed or she is tagged in a photograph on Facebook. The toolkit hardware consists of a Bluetooth module outfitted with simple-to-use I/O pins for connecting input sensors and output actuators. The software component resides on a cell phone that acts as a router between the Bluetooth modules and the Internet. This platform is also used for the Proverbial Wallets project.",2009-01-01,['information-ecology'],,15607,E15-301,2016-12-05T00:16:48.727Z,"['holtzman@media.mit.edu', 'jkestner@media.mit.edu']",True,my-ears-are-burning,2009-01-01,[],My Ears Are Burning,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:00.259Z,"This project is a novel concept for a social TV application, targeting the demographic of viewers enjoying live sports events, such as road bicycle racing. We intend to enhance the viewing experiences of spectators with sensor-fitted bikes tied to an interactive biking environment on television. The system enables a new form of personalized, physical, and virtual-reality interaction between viewers and a TV program, as well as interactions within or among communities of friends. We have created a prototype, My Second Bike, which uses a 3-D mirrored world environment (Google Earth) to visually represent participating spectators, competing athletes, and outdoor bikers. We contend that the system has the potential to attract and support a large user base on account of its scalability, ease of deployment, and ability to promote audience participation in live sports events on TV.",2010-09-01,['living-mobile'],,15615,E15-368,2016-12-05T00:16:48.753Z,['geek@media.mit.edu'],True,my-second-bike,2009-01-01,[],My Second-Bike,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:00.453Z,"The rise in popularity of the Weblog, and the development of its many variants such as photoblogs, vlogs, moblogs, and tumblelogs, demonstrate that people are increasingly willing to share what they are doing, seeing, and thinking. Micro-blogging has opened this space up even further to those who would not at all consider themselves authors; services like Twitter and the status updates common to social networking sites open up a form of publication that is well suited to this wide and fundamentally amateur audience. Mycrocosm is a Web service that uses the visual language of statistics to share even smaller chunks of personal information�individual numbers and words that are full of meaning in our lives�and allows users to track a wide variety of the minutiae of their daily lives to build up a rich online picture of the tiny things they find meaningful.",2017-05-01,['sociable-media'],,15623,E15-390,2022-06-10T14:41:29.951Z,['judith@media.mit.edu'],True,mycrocosm,2008-01-01,[],Mycrocosm,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:00.841Z,"NailO is a wearable input device in the form of a commercialized nail art sticker. It works as a miniaturized trackpad the size and thickness of a fingernail that can connect to your mobile devices; it also enables wearers to customize the device to fit the wearer’s personal style. NailO allows wearers to perform different functions on a phone or PC with different gestures, and the wearer can easily alter its appearance with a nail art design layer, creating a combination of functionality and aesthetics. From the fashion-conscious, to techies, and anyone in between, NailO can make a style, art, or a design statement; but in its more neutral, natural-looking example it can be worn and used only for its functionality. As a nail art sticker, NailO is small, discreet, and removable. Interactions through NailO can be private and subtle, for example attracting minimal attention when you are in a meeting but need to reply to an urgent text message. Mimicking the form of a cosmetic extension, NailO blends into and decorates one’s body when attached, yet remains removable at the wearer’s discretion, giving the wearer power and control over the level of intimacy of the device to one’s body.",,"['living-mobile', 'responsive-environments']",https://dam-prod2.media.mit.edu/x/2016/10/21/1_NailO_FlexiblePCB_.jpg,15639,--Choose Location,2017-03-27T21:12:03.637Z,"['artemd@media.mit.edu', 'cindykao@media.mit.edu', 'geek@media.mit.edu', 'joep@media.mit.edu']",True,nailo,2014-09-01,"['fashion', 'wearable-computing']",NailO,PUBLIC,http://nailo.media.mit.edu,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:01.339Z,We are developing techniques to use a focused ion beam to program the fabrication of nanowires-based nanostructures and logic devices.,2011-01-01,['molecular-machines'],,15663,E15-015,2016-12-05T00:16:59.650Z,['jacobson@media.mit.edu'],True,nanofab,2001-01-01,[],NanoFab,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:02.448Z,"Narratarium augments printed and oral stories and creative play by projecting immersive images and sounds. We are using natural language processing to listen to and understand stories being told, and analysis tools to recognize activity among sensor-equipped objects such as toys, then thematically augmenting the environment using video and sound. New work addresses the creation and representation of audiovisual content for immersive story experiences and the association of such content with viewer context. ",2014-12-14,"['ultimate-media', 'future-storytelling', 'object-based-media']",https://dam-prod2.media.mit.edu/x/2018/05/10/IMG_0053.JPG,15719,--Choose Location,2020-01-13T19:53:53.127Z,"['vmb@media.mit.edu', 'havasi@media.mit.edu', 'jieqi@media.mit.edu', 'kmh@media.mit.edu', 'novysan@media.mit.edu']",True,narratarium,2012-01-01,[],Narratarium,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:02.814Z,"We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: What billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, Big Data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.",,"['emerging-worlds', 'camera-culture']",,15735,--Choose Location,2016-12-05T00:17:18.564Z,"['raskar@media.mit.edu', 'ajdas@media.mit.edu']",True,nashik-smart-citizen-collaboration-with-tcs,2016-01-01,[],Nashik Smart Citizen Collaboration with TCS,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:02.993Z,"About a decade ago, University of Edinburgh and its partners put together a collection of transcribed human/human dialogues in which both participants were given similar maps. The person giving instructions had to navigate the other particpant along a path only on the giver's map. Since then, this corpus has been used for various tasks. To our knowledge, there have been no attempts to model actual understanding on the instruction receiver end; this study aims to fill this gap. We introduce Navigational Information Units (NIUs) describing individual intervals of the paths, drawn from several categories (such as ""moves"" and ""positions""). Our first aim is to decompose these NIUs into a number of possibly independent constituents and ground the latter in terms of the objects on the maps. Having such models at hand, we can further employ the notion of context and try and replicate the entire paths as a sequence of extracted NIUs.",2005-01-01,['social-machines'],,15743,E15-469,2016-12-05T00:16:10.242Z,['dkroy@media.mit.edu'],True,natural-language-understanding-for-maptask,2005-01-01,[],Natural Language Understanding for MapTask,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:03.160Z,"How would you browse a VERY large display space, such as a street map of the entire world? The traditional solution is zoom and pan, but these operations have drawbacks that have gone unchallenged for decades. Shifting attention loses the wider context, leading to that ""lost in hyperspace"" feeling. We are exploring alternative solutions, such as a new technique that allows zooming and panning in multiple translucent layers.",2014-09-01,['software-agents'],,15751,E15-383,2022-06-10T14:18:57.566Z,['lieber@media.mit.edu'],True,navigating-in-very-large-display-spaces,2000-01-01,[],Navigating in Very Large Display Spaces,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:03.601Z,"In this project we are attempting to create a large set of simple, mobile, wirelessly connected computing units and to treat them as a physical test-bed to explore and demonstrate ad hoc routing and self-organization, active networking, distributed multicasting, and distributed storage of information. These networks are characterized by ad hoc organizations and mobility, and they feature cooperative computing and transmission techniques operating in real-time, and performing distributed processing functions of media delivery and sensing.",2003-01-01,['media-and-networks'],,15775,E15-483,2016-12-05T00:16:40.653Z,['lip@media.mit.edu'],True,net-hockey,2001-01-01,[],Net-Hockey,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:11:03.740Z,"Can a person look at a portable display, click on a few buttons, and recover his or her refractive condition? Our optometry solution combines inexpensive optical elements and interactive software components to create a new optometry device suitable for developing countries. The technology allows for early, extremely low-cost, mobile, fast, and automated diagnosis of the most common refractive eye disorders: myopia (nearsightedness), hypermetropia (farsightedness), astigmatism, and presbyopia (age-related visual impairment). The patient overlaps lines in up to eight meridians, and the Android app computes the prescription. The average accuracy is comparable to the traditional method—and in some cases, even better. We propose the use of our technology as a self-evaluation tool for use in homes, schools, and at health centers in developing countries, and in places where an optometrist is not available or is too expensive.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/300x300-representative_image-v1.jpg,15783,--Choose Location,2019-04-19T18:30:08.590Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,netra-smartphone-add-on-for-eye-tests,2009-01-01,"['bioengineering', 'consumer-electronics', 'health', 'imaging']",NETRA: Smartphone add-on for eye tests,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:04.006Z,"We are extending our Scratch programming language to interact across networks. This extension will enable kids to bring information and artifacts from the Web and from other users' projects into their own Scratch projects. In designing interactive networked projects, kids are able to think about how their individual ideas and creations can connect to the Internet, their friends, their communities, and the world.",2008-01-01,['lifelong-kindergarten'],,15799,Cube,2016-12-05T00:16:39.862Z,"['mres@media.mit.edu', 'jmaloney@media.mit.edu']",True,netscratch,2006-09-01,[],NetScratch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:04.120Z,"Recent years have witnessed a surge in online digital storytelling tools, enabling users to more easily create engaging multimedia narratives. Increasing Internet access and powerful in-browser functionality have laid the foundation for the proliferation of new online storytelling technologies, ranging from tools for creating interactive online videos to tools for data visualization. While these tools may contribute to diversification of online storytelling capacity, sifting through tools and understanding their respective limitations and affordances poses a challenge to storytellers. The NetStories research initiative explores emergent online storytelling tools and strategies through a combination of analyzing tools, facilitating story-hack days, and creating an online database of storytelling tools.",2020-08-01,['civic-media'],,15807,--Choose Location,2022-06-10T13:35:29.956Z,['ethanz@media.mit.edu'],True,netstories,2014-01-01,[],NetStories,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:04.240Z,"What is the point of interaction for bio-data collected on a daily basis  from black boxes on your person? NetWeight is a scale which weighs the  person, ID's the person through a toe-scan, and then becomes a network  device for interacting with data collected previously from activities such  as running a marathon or climbing Mount Everest if you happened to do  that...
",1996-12-30,['personal-information-architecture'],,15815,,2016-12-05T00:16:40.678Z,['mike@media.mit.edu'],True,netweight,1998-12-31,[],NetWeight,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:04.559Z,"A live music event where musicians are located remotely from each other is possible through the Internet but highly constrained by network latency. This is especially true with rhythmic music that requires tight synchrony, or in situations where musicians are separated by long distances. To overcome time delays, we propose an intelligent system that listens to the audio input at one end and synthesizes a predicted audio output at the other. In this context, we study how our musical exposure, or enculturation, gives rise to musical anticipation. Moreover, as we admit that such prediction cannot be error-free, we aim to model the musical intentions of the performers and the expectations of the listeners.",2011-01-01,['music-mind-and-machine'],,15839,E15-310,2016-12-05T00:16:40.748Z,['bv@media.mit.edu'],True,network-music-performance,2006-09-01,[],Network Music Performance,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:11:04.664Z,"You can Telnet directly to my ear�but no, I won't tell you the IP address! The mobile user wears a wirelessly networked computer with a headset. A daemon listens to incoming connections on the telnet port and reads messages out loud, employing a text-to-speech engine.",2003-09-01,['smart-cities'],,15847,--Choose Location,2016-12-05T00:16:40.696Z,[],True,networked-ear,2003-01-01,[],Networked Ear,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:04.757Z,"Networked Playscapes re-imagines outdoor play by merging the flexibility of the digital world with the tangible, sensorial properties of physical play to create hybrid interactions for the urban environment. Dig Deep takes the classic sandbox found in children's playgrounds and merges it with the common fantasy of ""digging your way to the other side of the world"" to create a networked interaction in tune with child cosmogony. ",2018-06-01,['object-based-media'],,15855,--Choose Location,2020-06-10T15:52:20.981Z,"['vmb@media.mit.edu', 'edwinapn@media.mit.edu']",True,networked-playscapes-dig-deep,2014-01-01,"['kids', 'learning-teaching']",Networked Playscapes: Dig Deep,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:04.849Z,"Recent advances in artificial limbs have resulted in the provision of powered ankle and knee function for lower extremity amputees and powered elbow, wrist, and finger joints for upper extremity prostheses. Researchers still struggle, however, with how to provide prosthesis users with full volitional and simultaneous control of the powered joints.  This project seeks to develop means to allow amputees to control their powered prostheses by activating the peripheral nerves present in their residual limb. Such neural control can be more natural than currently used myoelectric control, since the same functions previously served by particular motor fascicles can be directed to the corresponding prosthesis actuators for simultaneous joint control, as in normal limbs. Future plans include the capability to electrically activate the sensory components of residual limb nerves to provide amputees with tactile feedback and an awareness of joint position from their prostheses.",,['biomechatronics'],https://dam-prod2.media.mit.edu/uuid/459d9c1c-6546-41cb-b7e6-5f6cf020540c,15863,--Choose Location,2021-09-17T19:28:05.210Z,"['hherr@media.mit.edu', 'esb@media.mit.edu', 'rriso@media.mit.edu', 'clites@media.mit.edu', 'crtaylor@media.mit.edu', 'shriyas@media.mit.edu', 'syeon@media.mit.edu', 'lfreed@media.mit.edu', 'bmaimon@media.mit.edu']",True,neural-interface-technology-for-advanced-prosthetic-limbs,2011-09-01,"['robotics', 'bioengineering', 'health', 'human-machine-interaction', 'sensors', 'interfaces', 'biomechanics', 'mechatronics', 'prosthetics', 'medicine', 'bionics', 'neural-interfacing-and-control']",Neural interface technology for advanced prosthetic limbs,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:04.961Z,"This project addresses human cognitive models of reflective problem solving in terms of psychology, neuroscience, and artificial intelligence. A programming language describing reflective human thought processes is being developed for the purpose of understanding the biological process of thought. This description language allows distributed reflective monitoring and control of parallel threads.  In addition to being a novel method for the robust control of distributed computer programs, this technology is directed toward consumer HCI and medical cures for neuropsychological problems, and has applications for neural-interface computer gaming peripherals, aging population cognitive evaluation, and training.",2008-09-01,"['society-of-mind', 'responsive-environments']",,15871,E15-351,2016-12-05T00:17:18.669Z,"['minsky@media.mit.edu', 'joep@media.mit.edu']",True,neural-models-of-mind,2006-09-01,[],Neural Models of Mind,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:05.130Z,"Inspired by the Surrealists' Exquisite Corpse art game, the NeverEnding Drawing project is one of several applications developed on a scalable architecture and platform for collaborative creativity. Users co-create and edit each other's augmented sketchbooks in real time. By tracking individual pages of each live sketchbook, the system loads the appropriate background audiovisual content and enables users to add to it using a variety of real materials and means of mark-making. Users take pictures and record sounds to be sent back and forth between collaborators on the network. Additionally, the live sketchbooks facilitate non-linear, asynchronous  access to the evolving, co-created content through their physical editing interface. By using crayons, colored pens, and various tactile and light-diffusing materials, the analog/digital hybrid model of content creation requires no expertise and creates a safe environment for sharing unfinished work with others.",2011-01-01,"['object-based-media', 'tangible-media', 'personal-robots']",,15887,,2017-03-24T14:50:40.999Z,"['vmb@media.mit.edu', 'cynthiab@media.mit.edu', 'gid@media.mit.edu']",True,neverending-drawing-machine,2010-01-01,[],NeverEnding Drawing Machine,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:05.223Z,"New Century Cities is a joint research initiative between MIT�s Center for Real Estate, City Design and Development (Urban Studies and Planning), and the Media Lab�s Smart Cities group. It focuses on a new generation of development projects. These ambitious initiatives are emerging at the intersection of social policy, technology, urban design, and real estate development, and are located in what we call New Century Cities (NCCs). You will find them today in New York City; Cambridge, Massachusetts; Belfast; Helsinki; Copenhagen; Seoul; and Singapore. In addition, NCC projects are on the drawing boards in places such as Florianopolis, Brazil and Zaragoza, Spain. These projects vary in size and in how their development is organized and led. The research initiative focuses on developing a profile of these projects, creating development guidelines, and collecting case studies of technologies that fulfill these visions for future urban spaces.",2005-01-01,['smart-cities'],,15895,E15-309,2016-12-05T00:17:18.715Z,['susanne@media.mit.edu'],True,new-century-cities-real-estate-value-in-a-digital-world,2004-09-01,[],New Century Cities: Real Estate Value in a Digital World,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:05.312Z,"This project explores the new learning possibilities created through interaction between physical and virtual objects. We hope to show how the tangibility of physical objects and the programmability of computers can be combined to encourage and enable deep understanding of mechanical, structural, and interactive design. Two case studies are being developed: the first involves interactive sculpture design through the use of wire and found objects as construction materials, combined with active sensors/actuators to provide interaction; the other study involves self-balancing robots through which learners develop strategies of balance based on the analysis of their own bodies.",2005-09-01,[],,15903,E15-385,2016-12-05T00:17:18.816Z,['cavallo@media.mit.edu'],True,new-content-materials-and-environments-for-the-digital-age,2005-09-01,[],"New Content, Materials, and Environments for the Digital Age",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:05.385Z,"New Day New Standard is an interactive hotline that informs nannies, housekeepers, eldercare-givers, and their employers about the landmark Domestic Workers' Bill of Rights, passed in New York State in November 2010. Operating in English and Spanish, it's a hybrid application that combines regular touchtone phones and Internet-based telephony within an open source framework. The Center for Civic Media and REV- (http://www.rev-it.org) are currently developing Call to Action, a generalized version of the platform and associated GUI to allow other groups to create interactive hotlines for a wide range of use cases. NDNS was presented to the White House's Open Government Initative.",2013-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/NewDay_web_horiz_plain.png,15911,--Choose Location,2016-12-05T00:16:40.766Z,"['leob@media.mit.edu', 'rahulb@media.mit.edu']",True,new-day-new-standard-646-699-3989,2012-09-01,[],New Day New Standard: (646) 699-3989,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:05.473Z,"Building on the Media Lab hyperstring project of 1990-1993 (Machover, Chung, Gershenfeld, Paradiso, et al.), we are improving and redefining the technology and the functionality of this enhanced expert performance instrument. Sensor improvements are being made, and new techniques for pitch, timbre, and ""expressivity"" measurement are being developed. A new model of hyperstring performance is also being explored, which uses the virtuosic instrument as an intermediary for training young performers, as well as for shaping and manipulating complicated musical events (such as the sonic output from a roomful of Simple Thing players). In addition, the new hyperstring instrument will be interfaced with a communication and modulation environment defined by Big Thing construction, allowing children to define the meaning of a hyperstring gesture and performance.",2000-12-30,['opera-of-the-future'],,15919,E15-483,2016-12-05T00:17:18.837Z,"['tod@media.mit.edu', 'tristan@media.mit.edu']",True,new-generation-hyperstrings,1999-12-31,[],New Generation Hyperstrings,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:05.551Z,We have designed an online algorithm to approximate maximum welfare in selling adwords when click-through rates are unknown. This algorithm will aid advertisers as well as ad-placement agencies (such as search-engine companies) in achieving increased revenue through advertising placement.,2007-01-01,['erationality'],,15927,E15-313,2016-12-05T00:17:18.746Z,[],True,new-methods-for-selling-adwords,2006-09-01,[],New Methods for Selling Adwords,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:05.627Z,"Time-of-flight (ToF) cameras are commercialized consumer cameras that provide a depth map of a scene, with many applications in computer vision and quality assurance.  Currently, we are exploring novel ways of integrating the camera illumination and detection circuits with computational methods to handle challenging environments, including multiple scattering and fluorescence emission. ",,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/tof.bmp,15935,--Choose Location,2019-04-19T18:31:33.509Z,"['raskar@media.mit.edu', 'achoo@media.mit.edu', 'ayush@media.mit.edu', 'michaf@media.mit.edu', 'ajdas@media.mit.edu']",True,new-methods-in-time-of-flight-imaging,2013-01-01,[],New methods in time-of-flight imaging,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:05.802Z,"It has become increasingly common for new dance theater works to combine the movement of live dancers with the movement of computer-generated graphics projected onto the stage. Such works typically either manipulate carefully pre-made motion-capture material or, alternatively, a variety of indirect sensing technologies on the dancers themselves. What happens if the computers can see a high-resolution, real-time motion capture of the dancers in the theatre as the dance unfolds? How should such hybrid, collaborative pieces be made? Two new pieces are being created to help answer these questions. ""How long will the subject linger on the edge of the volume?"" is a new piece with the Trisha Brown Dance Company, and ""22"" is being created with Bill T. Jones.",2005-09-01,['opera-of-the-future'],,15951,E15-450,2016-12-05T00:16:49.251Z,['tod@media.mit.edu'],True,new-works-for-dance-theater,2003-01-01,[],New Works for Dance Theater,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:05.887Z,"NPS is a way to archive and share your news geographically. Through NPS you can create a shared archive of news, as well as represent that archive on a searchable map. You can choose to share your news only with your group, or with the public in general. Location matters when sharing news, and NPS can be used to find out about news in your area.",2011-01-01,['civic-media'],,15959,E15-001,2016-12-05T00:16:41.142Z,"['ethanz@media.mit.edu', 'csik@media.mit.edu']",True,news-positioning-system,2008-09-01,[],News Positioning System,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:05.980Z,"NewsClouds presents a visual exploration of how the news reporting of an event evolves over time. Each ""cloud"" represents a publication and each competing news organization usually emphasizes different aspects of that same story. Using the time sliders, that evolution becomes evident. In addition, each word or phrase can be expanded to show its links and context. We are building an archive of events associated with ongoing US election developments.",2018-06-06,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/logo.png,15967,--Choose Location,2018-10-09T01:51:14.404Z,"['thariq@media.mit.edu', 'lip@media.mit.edu']",True,newsclouds,2014-09-01,[],NewsClouds,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:11:06.170Z,"Newsflow is a dynamic, real-time map of news reporting that displays both the latest top stories as well as the news organizations which covered them. All articles are from the last few minutes. Viewing news in this way lets us see how the choice of 'top stories' by news bureaus is geographically unequal, or rather, what areas of the world are neglected by various national news sources. Built with HTML5 on the dynamic mapping framework Cartagen, Newsflow draws on real-time data from over 200 news organizations as well as Google, Yahoo, and other sources. The ability to view such data in real time offers viewers a chance to see how news editors shape national attention as stories unfold.",2010-01-01,['design-ecology'],,15983,E15-301,2016-12-05T00:16:41.176Z,['dsmall@media.mit.edu'],True,newsflow-where-news-happens,2009-01-01,[],Newsflow: Where News Happens,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:11:06.261Z,"NGDB is designed to be a fast, flexible database of Usenet newsgroup messages that supports a variety of applications, including visualization of the conversations, designing novel interfaces for news reading, and conducting basic sociological inquiry into the activity in the groups. It combines a relational database with INN, the open-source Usenet server, to work transparently within the existing news infrastructure.",2004-09-01,['sociable-media'],,15991,E15-468,2016-12-05T00:16:41.210Z,['judith@media.mit.edu'],True,newsgroup-database-ngdb,2003-01-01,[],Newsgroup Database (NGDB),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:06.354Z,"NewsJack is a media remixing tool built from Mozilla's Hackasaurus.  It allows users to modify the front pages of news sites, changing language and headlines to change the news into what they wish it could be.",2013-09-01,"['information-ecology', 'civic-media']",https://dam-prod2.media.mit.edu/x/files/Display/squre_logo.png,15999,--Choose Location,2016-12-05T00:16:41.442Z,"['holtzman@media.mit.edu', 'ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,newsjack,2012-09-01,[],NewsJack,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:06.528Z,"NewsPad is a collaborative article editor that empowers small communities to write articles collaboratively through community sourcing, structured stories, and distributed syndication.",2014-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/06/files/Display/NewsPad.png,16015,--Choose Location,2016-12-05T00:16:41.269Z,"['ethanz@media.mit.edu', 'jnmatias@media.mit.edu']",True,newspad,2013-01-01,[],NewsPad,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:06.605Z,"NewsPix is a simple news-engagement application that helps users encounter breaking news in the form of high-impact photos. It is currently a Chrome browser extension (mobile app to come) that is customizable for small and large news organizations. Currently, when users open a new, blank page in Chrome, they get a new tab with tiles that show recently visited pages. NewsPix replaces that view with a high-quality picture from a news site. Users interested in more information about the photo can click through to the news site. News organizations can upload photos ranging from breaking news to historic sporting events, with photos changing every time a new tab is clicked.",2020-08-01,"['future-of-news', 'civic-media']",https://dam-prod2.media.mit.edu/x/2016/10/20/Politically_engaged.png,16023,--Choose Location,2022-06-10T13:37:37.041Z,"['ethanz@media.mit.edu', 'matt54@media.mit.edu', 'dignazio@media.mit.edu']",False,newspix,2014-01-01,[],NewsPix,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:06.690Z,"Within the next three years, another billion people will make regular use of cell phones, continuing the fastest adoption of a new technology in history. This will unleash a wave of entrepreneurship, collaboration, and wealth creation, thereby transforming this Next Billion into a powerful force in the global economy. By innovating applications and business models, and by testing them locally in the field, this initiative explores how mobile technologies can reduce friction in bottom of the pyramid markets across the developing world.",2007-09-01,[],,16031,E15-384,2016-12-05T00:16:41.108Z,[],True,next-billion-network,2006-09-01,[],Next Billion Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:06.757Z,"Functionally, television content delivery has remained largely unchanged since the introduction of television networks.  NeXtream explores an experience where the role of the corporate network is replaced by a social network. User interests, communities, and peers are leveraged to determine television content, combining sequences of short videos to create a set of channels customized to each user. This project creates an interface to explore television socially, connecting a user with a community through content, with varying levels of interactivity: from passively consuming a series, to actively crafting one's own television and social experience.",2013-09-01,['information-ecology'],,16039,E15-301,2016-12-05T00:16:41.348Z,['holtzman@media.mit.edu'],True,nextream-social-television,2009-01-01,[],NeXtream: Social Television,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:06.838Z,"NGO2.0 is a project grown out of the work of MIT's New Media Action Lab. The goal of NGO2.0 is to strengthen the digital and social media literacy of Chinese grassroots NGOs.  Learn more at www.ngo20.org.Project Staff: Jing Wang, Yu Wang, Su Han, and Huan SunSince 2009, the project has established collaborative relationships with IT corporations, universities, and city-based software developers' communities to advocate the development of a new brand of public interest sector that utilizes new media and nonprofit technology to build a better society. NGO2.0 addresses three major need categories of grassroots NGOs: communication, resources, and technology. Within each category, NGO2.0 developed and implemented online and offline projects. These include: Web 2.0 training workshops, Web 2.0 toolbox, a crowdsourced philanthropy map, news stories and videos for NGOs, crowd funding project design, NGO-CSR Partnership Forum, database of Chinese NGOs, and online survey of Chinese NGOs' Internet usage.See the crowdsourced philanthropy map.",2015-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2018/08/27/2012_07成都全景 050.jpg,16047,--Choose Location,2018-08-27T20:27:58.872Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,ngo20,2011-09-01,[],NGO2.0,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:07.198Z,"No Park is a Web site for people interested in the hidden politics of the urban environment; the way public space is appropriated for art making; and how people re-interpret the urban landscape for recreation and pleasure. Here you will find guides for parkour, buildering, weird architecture, public art, hidden green space, and the politics of our cities.",2009-01-01,['computing-culture'],,16087,E15-001,2016-12-05T00:16:41.462Z,['csik@media.mit.edu'],True,no-park,2009-01-01,[],No Park,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:07.346Z,"We are building a nonlinear dynamic system that constitutes a new kind of phase-locked loop. This Noise Locked Loop (NLL) will entrain with an incoming pseudo-random bit stream generated by a matched LFSR, instead of simple sinusoidal signals. The hope is that this will be a cheap and very effective front-end for spread-spectrum receivers, allowing spread-spectrum to be deployed for new applications. For example, one could potentially use spread-spectrum clock distribution on large chips. In the process, we have begun to understand how to compute using nonlinear dynamic systems.",2003-09-01,"['silicon-biology', 'physics-and-media']",,16103,E15-023,2016-12-05T00:16:41.489Z,['neilg@media.mit.edu'],True,noise-locked-loop,2000-01-01,[],Noise Locked Loop,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:07.436Z,"Nomadic Radio explores speech interfaces for communication purposes in a wearable computer. Incoming messages and phone calls are filtered, with graceful alerting based on spatially presented audio.  Nomadic Radio is adaptive to user situation and previous reactions, to avoid, for example, annoying message announcements when busy.",2000-12-30,"['e-markets', 'living-mobile']",,16111,E15-344,2016-12-05T00:16:41.520Z,"['nitin@media.mit.edu', 'geek@media.mit.edu']",True,nomadic-radio,1995-12-31,[],Nomadic Radio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:07.517Z,"Despite use in treating depression, and promise in treating stroke, Parkinson's, tinnitus, and other disorders, non-invasive brain stimulation technology is bulky, power-hungry, non-focal, and requires precision alignment with neural structures. We are applying modern engineering techniques to create a portable, focal, non-invasive brain stimulator that will enable a new platform for therapeutic neuromodulation.",2011-01-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/files/Display/synthneuro.jpg,16119,E15-435,2016-12-05T00:16:10.295Z,"['esb@media.mit.edu', 'henninge@media.mit.edu']",True,non-invasive-focal-and-portable-brain-stimulators,2006-09-01,[],"Non-Invasive, Focal, and Portable Brain Stimulators",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:07.596Z,"Can your information on social networking sites be used to create emotional experiences? The Nostalgia Room ranks your Facebook photos based on tags, likes, comments, and albums in order to create a 3D visual memoir. Swinging on a motion-tracked swing with emotive music creates a multi-sensory immersive experience that evokes your memories.",2013-01-01,"['information-ecology', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/files/Display/pldb.jpg,16127,--Choose Location,2016-12-05T00:16:38.196Z,"['holtzman@media.mit.edu', 'pattie@media.mit.edu']",True,nostalgia-room,2013-01-01,[],Nostalgia Room,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:07.925Z,"The most common functions of mobile devices involve alerting the user; whether notifying of incoming calls and messages or reminding of calendar events, the system has to catch the user's attention. This causes undesired interruptions to those surrounding the mobile device user, and in some circumstances the notification can be disruptive even for the user himself. A wearable peripheral display, embedded in a pair of eyeglasses, was built to deliver notification cues in a private, subtle, and non-obtrusive way. The display is composed of arrays of small LEDs of different colors, each placed at the end of the arms near the lens. The LEDs are lit (at very dim intensity) to display moving patterns in the wearers' peripheral vision. The display has a Bluetooth interface, so it can be controlled by standard mobile devices.",2006-01-01,['fluid-interfaces'],,16159,E15-320,2016-12-05T00:16:41.595Z,['pattie@media.mit.edu'],True,notifying-glasses,2004-01-01,[],Notifying Glasses,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:08.008Z,"This project is an experiment in material and scale: a life-sized pop-up book that you can open up and step into, made using only cardboard, an X-acto knife, tape, and glue. Inside the book is a kinetic mural of breathing pleated flowers.  As you tug on a string of beads leading from one flower, the rest come to life, moving like puppets using a series of strings attached to motors. The mural itself is drawn using conductive fabric and copper tape, which serve as both expressive and functioning traces within the circuit. Electronic components are also openly displayed and emphasized to explain the electronic workings behind the mural.",2011-09-01,['high-low-tech'],https://dam-prod2.media.mit.edu/x/files/Display/arch_square.jpg,16167,,2016-12-05T00:16:41.823Z,"['leah@media.mit.edu', 'jieqi@media.mit.edu']",True,novel-architecture,2010-09-01,[],Novel Architecture,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:08.160Z,"Despite recent widespread interest in hobbyist electronics and the maker movement, the design of printed circuit boards (PCBs) remains an obscure and often intimidating activity. This project attempts to introduce PCB design and production to new audiences by creating examples, activities, and other resources that provide context and motivation for those practices. We've developed a series of interactive lights that demonstrate the creation of useable products with simple circuits. These examples introduce novices to the space of possibilities and provide them with a starting point for creating their own designs. In workshops, novices design, produce, assemble, and program their own electronic circuits. These workshops provide an entry point to understanding the way that electronic products are made and an opportunity for discussion and reflection about how more people might get involved in their production.",2015-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/IMG_0153.JPG,16183,--Choose Location,2016-12-05T00:16:49.276Z,"['mres@media.mit.edu', 'mellis@media.mit.edu']",True,novice-design-of-interactive-products,2014-01-01,[],Novice Design of Interactive Products,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:08.316Z,"O-Link is a communication tool developed by NTT Comware in collaboration with the Tangible Media group. O-Link allows users to link video clips to an object intuitively, as if sealing wonderful memories of a trip in a souvenir. It also has an object-recognition feature that loads the video clips tagged with the object and plays them back automatically, just by putting the object in front of the monitor. NTT Comware aims to provide novel services based on O-Link that anyone can use�regardless of his or her computer literacy�such as intergenerational communication tools, video manual services directly equipped with products, or educational infrastructures for the exchange of student work and opinion.",2010-09-01,['tangible-media'],,16199,E15-344,2016-12-05T00:16:41.665Z,['ishii@media.mit.edu'],True,o-link,2009-01-01,[],O-Link,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:08.605Z,"The goal of this project is to allow a user with a Bluetooth-enabled cell phone or PDA to gather personally relevant information about, and interact with, inanimate objects such as books, tabletops, or works of art in the user's vicinity. The system allows users to passively gather information about their environment and get just-in-time reminders or suggestions from useful objects in the environment. For example, walking into a room full of augmented books, the books of interest to the user ""blink"" their LEDs, thereby making it easier for the user to find the books that may be of interest.",2006-01-01,['fluid-interfaces'],,16223,E15-320,2016-12-05T00:16:41.900Z,['pattie@media.mit.edu'],True,object-awareness,2004-01-01,[],Object Awareness,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:08.788Z,"In this project we explore forms of representing that emphasize notions such  as discovery, embodiment, and fluidity. We are currently working on new techniques for shape matching and object recognition, with applications in generic object detection in natural scenes, pose discrimination, and active vision. This work provides an important link between the physical world and the situation-aware communication systems designed by other members of the Cognitive Machines group.",2005-01-01,['social-machines'],,16231,E15-483,2016-12-05T00:16:10.452Z,['dkroy@media.mit.edu'],True,object-vision-for-language,2003-01-01,[],Object Vision for Language,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:08.987Z,"Analogous to our work in representing video as a set of objects and a scripting language, we are exploring methods for recording audio such that the result is a set of localized sound sources and an acoustical model for the space in which they are placed. Techniques involved include source separation and blind deconvolution.",1999-12-30,['object-based-media'],,16239,,2016-12-05T00:16:41.930Z,['vmb@media.mit.edu'],True,object-based-audio-capture,1996-12-31,[],Object-Based Audio Capture,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:09.158Z,"Current methods to assess depression and then ultimately select appropriate treatment have many limitations. They are usually based on having a clinician rate scales, which were developed in the 1960s. Their main drawbacks are lack of objectivity, being symptom-based and not preventative, and requiring accurate communication. This work explores new technology to assess depression, including its increase or decrease, in an automatic, more objective, pre-symptomatic, and cost-effective way using wearable sensors and smart phones for 24/7 monitoring of different personal parameters such as physiological data, voice characteristics, sleep, and social interaction. We aim to enable early diagnosis of depression, prevention of depression, assessment of depression for people who cannot communicate, better assignment of a treatment, early detection of treatment remission and response, and anticipation of post-treatment relapse or recovery.",2019-01-01,"['advancing-wellbeing', 'affective-computing']",,16247,--Choose Location,2022-09-29T01:13:01.902Z,"['picard@media.mit.edu', 'sfedor@media.mit.edu']",True,objective-asessment-of-depression-and-its-improvement,2014-01-01,[],Objective Asessment of Depression and its Improvement,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:09.355Z,"How can technology help us understand ourselves better? In order to measure the physiological arousal of children with sensory challenges such as ASD and ADHD, tools were developed to help children understand and control what makes them overexcited. Using iCalm hardware, children in therapy sessions measured their arousal while eating, throwing tantrums, playing in ball pits, and making challenging choices. Beyond progressive findings in the field of occupational therapy, this research is a basis for bio-information technology: tools to help children, their parents, and their teachers better understand what is going on in their bodies in a comfortable, affordable, and adaptable way. With future work, technology will be developed to help children understand and control their own internal states. In addition, this project will go beyond children�s therapy�helping adults in various settings including business and home life.",2009-01-01,['affective-computing'],,16255,E15-450,2016-12-05T00:16:41.968Z,"['picard@media.mit.edu', 'hedman@media.mit.edu', 'mgoodwin@media.mit.edu']",True,objective-self-understanding-internal-responses,2008-01-01,[],Objective Self: Understanding Internal Responses,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:10.226Z,"Omo is an alternative relational object. While similar to ""carebots"" and companion robots, Omo draws on ongoing Machine Therapy work revealing the psychological, social, and political dynamics between people and machines. As a result, Omo's role is empathic and sometimes unexpected rather than normative. Omo breathes and senses the breathing of anyone interacting closely with it, matching--or seeking to lead--patterns of breathing. Omo does not always privilege soothing.test",2008-01-01,['computing-culture'],,16303,E15-020D,2016-12-05T00:16:42.031Z,"['csik@media.mit.edu', 'monster@media.mit.edu']",True,omo,2005-09-01,[],Omo,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:10.396Z,"Even when real names and other personal information are stripped from metadata datasets, it is often possible to use just a few pieces of information to identify a specific person. Here, we study three months of credit card records for 1.1 million people and show that four spatiotemporal points are enough to uniquely reidentify 90 percent of individuals. We show that knowing the price of a transaction increases the risk of reidentification by 22 percent, on average. Finally, we show that even data sets that provide coarse information at any or all of the dimensions provide little anonymity, and that women are more reidentifiable than men in credit card metadata.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/files/Display/uniquesm_small.png,16311,--Choose Location,2019-04-19T14:48:17.075Z,"['sandy@media.mit.edu', 'yva@media.mit.edu']",True,on-the-reidentifiability-of-credit-card-metadata,2015-01-01,[],On the re-identifiability of credit card metadata,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:11:10.679Z,"While research has given us high-fidelity technology, such as 24-bit digital audio, the lush jungle of one-bit sound beckons to be explored. This research leverages the last 25 years of DSP/microcontroller technology and music culture to design elegant electronic instruments, making sounds both nostalgic and cutting-edge to reach people emotionally.",2005-09-01,['computing-culture'],,16327,E15-020C,2016-12-05T00:16:42.082Z,['csik@media.mit.edu'],True,one-bit-synthesis,2005-01-01,[],One-Bit Synthesis,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:10.813Z,"This project is aimed at building a system to recognize emotional expression given four physiological signals.  Data was gathered from a graduate student with  acting experience as she intentionally tried to experience eight different emotional states daily over a period of several weeks. Several features are extracted from each of her physiological signals. The first classifiers gave a classification result of 88% success when discriminating among 3 emotions (pure chance would be 33.3%), and of 51% when discriminating among 8 emotions (pure chance 12.5%). New, improved classifiers reach an 81% success rate when discriminating among all 8 emotions. Furthermore, an online classifier has now been built using the old method, which gives a success rate only 8% less than its old offline counterpart (i.e. 43%). We expect this percentage to sharply increase when the new methods are adapted to run online.",1999-12-30,['affective-computing'],,16335,,2016-12-05T00:17:19.028Z,['picard@media.mit.edu'],True,online-emotion-recognition,1996-12-31,[],Online Emotion Recognition,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:11.076Z,"OnObject transforms any object or surface into a gesture-triggered interactive toy. By applying an RFID tag to an object, grabbing it wearing a sensing device, and making hand gestures like shake, swing and tilt, you can trigger interactive audio output in stuffed animals, a mug, your desk, or your body.",2010-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/pldb_logo.jpg,16351,,2016-12-05T00:16:42.210Z,['ishii@media.mit.edu'],True,onobject,2010-01-01,[],OnObject,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:11.209Z,"As mobile device screens continue to get smaller (smartwatches, head-mounted devices like Google Glass), touch-based interactions with them become harder. With OnTheGo, our goal is to complement touch- and voice-based input on these devices by adding interactions through in-air gestures around the devices. Gestural interactions are not only intuitive for certain situations where touch may be cumbersome like running, skiing, or cooking, but are also convenient for things like quick application and task management, certain types of navigation and interaction, and simple inputs to applications.",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/11/22/onthego.png,16359,--Choose Location,2022-06-10T14:01:02.088Z,"['sra@media.mit.edu', 'geek@media.mit.edu']",True,onthego,2014-09-01,[],OnTheGo,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:11.321Z,"OnTheRun is a location-based exercise game designed for the iPhone. The player assumes the role of a fugitive trying to gather clues to clear his name. The game is played outdoors while running, creating missions that are tailored to the player's neighborhood and running ability. The game is primarily an audio experience, and gameplay involves following turn-by-turn directions, outrunning virtual enemies, and reaching destinations.",2011-09-01,['living-mobile'],,16367,--Choose Location,2016-12-05T00:16:42.233Z,['geek@media.mit.edu'],True,ontherun,2011-01-01,[],OnTheRun,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:11.432Z,"Patient adherence to physical therapy regimens is poor, and there is a lack of quantitative data about patient performance, particularly at home. This project is an end-to-end virtual rehabilitation system for supporting patient adherence to home exercise that addresses the multi-factorial nature of the problem. The physical therapist and patient make shared decisions about appropriate exercises and goals and patients use a sensor-enabled gaming interface at home to perform exercises. Quantitative data is then fed back to the therapist, who can properly adjust the regimen and give reinforcing feedback and support.",2013-01-01,['new-media-medicine'],https://dam-prod2.media.mit.edu/x/files/Display/oovit-pt.jpg,16375,,2020-07-29T16:19:38.912Z,"['jom@media.mit.edu', 'fmoss@media.mit.edu']",True,oovit-pt,2010-01-01,[],Oovit PT,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:11.533Z,"We present Open Badges, an open-source framework and toolkit for measuring and shaping face-to-face social interactions using either custom hardware devices or smart phones, and real-time web-based visualizations. Open Badges is a modular system that allows researchers to monitor and collect interaction data from people engaged in real-life social settings.",,"['connection-science', 'human-dynamics']",,16383,--Choose Location,2022-06-10T13:16:27.105Z,"['sandy@media.mit.edu', 'orenled@media.mit.edu', 'amohan@media.mit.edu']",True,open-badges,2014-09-01,[],Open Badges,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:11.734Z,"Open Gender Tracker is a suite of open-source tools and APIs that make it easy for newsrooms and media monitors to collect metrics and gain a better understanding of gender diversity in their publications and audiences. This project has been created in partnership with Irene Ros of Bocoup, with funding from the Knight Foundation.",2016-10-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/Google%20ChromeScreenSnapz372.png,16399,--Choose Location,2016-12-05T00:17:19.067Z,"['ethanz@media.mit.edu', 'jnmatias@media.mit.edu']",True,open-gender-tracker,2013-01-01,"['civic-media', 'open-source']",Open Gender Tracker,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:11.835Z,"The Open Government Information Awareness program is an effort to increase transparency in the United States government. As the US government increases its supervision of civilian lives, it is crucial to ensure accountability. Since democracy requires an informed public, every effort must be made to give citizens access to government information. The Open Government Information Awareness program builds a framework for US citizens to construct and analyze the world's most comprehensive database on our government. Citizens will explore data, track events, find patterns, and build risk profiles, all in an effort to encourage action.",2004-01-01,['computing-culture'],,16407,E15-020D,2016-12-05T00:16:38.508Z,['csik@media.mit.edu'],True,open-government-information-awareness,2003-01-01,[],Open Government Information Awareness,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:11.929Z,Open Hybrid is an open source augmented reality platform for physical computing and Internet of Things. It is based on the web and Arduino.This platform allows you to:Create augmented reality content with HTML toolsCreate augmented reality without any knowledge of 3D programmingConnect the functionality of objects with a simple drag and drop paradigmProgram your physical hybrid objects and connect them to the AR-UI using ArduinoLearn more about this project at: http://www.openhybrid.org,2017-06-30,['fluid-interfaces'],,16415,--Choose Location,2018-10-20T23:06:14.400Z,"['pattie@media.mit.edu', 'heun@media.mit.edu']",True,open-hybrid,2015-01-01,[],Open Hybrid,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:12.012Z,"OPEN I/O provides a hardware base and a suite of network services for the development of I/O devices. With OPEN I/O, we aspire to make the creation of Ethernet-enabled devices accessible to artists and software engineers.  OPEN I/O devices are automatically discovered by the OPEN I/O service and assigned to users who control the devices. Using the OPEN I/O router, devices can communicate with each other, or with network-based applications and computation, such as provided by OPENSTUDIO and OPENCODE. Users can configure and program their hardware remotely, and interact with similar hardware devices around the world. OPEN I/O provides common device libraries to help bridge the gap between software and hardware development.",2008-09-01,['information-ecology'],,16423,E15-301,2016-12-05T00:17:18.916Z,['holtzman@media.mit.edu'],True,open-io,2005-01-01,[],OPEN I/O,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:12.090Z,"The biggest problem facing artificial intelligence today is how to teach computers enough about the everyday world so that they can reason about it like we do—so that they can develop ""common sense."" We think this problem may be solved by harnessing the knowledge of people on the Internet, and we have built a website to make it easy and fun for people to work together to give computers the millions of little pieces of ordinary knowledge that constitute ""common sense."" Teaching computers how to describe and reason about the world will give us exactly the technology we need to take the Internet to the next level, from a giant repository of web pages to a new state where it can think about all the knowledge it contains; in essence, to make it a living entity.",2016-08-31,['digital-intuition'],,16431,E15-383,2021-07-14T15:33:24.710Z,"['lieber@media.mit.edu', 'eslick@media.mit.edu', 'havasi@media.mit.edu']",True,open-mind-common-sense,1999-01-01,[],Open Mind Common Sense,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:12.175Z,"The Open Mind Common Sense project has collected hundreds of thousands of statements of common-sense knowledge from volunteers on the Internet, using a variety of online activities in several different languages. Open Mind Commons aims to use analogical reasoning to make connections between similar ideas while highlighting the relevant differences as well. These analogies can give a computer a better understanding of the relationships between objects, situations, and cultures. It is often difficult to search through and coordinate lexical information across data sources, each of which has its own separate interface and viewing software. We have approached this problem by creating a unified, flexible interface for various natural-language processing resources.",2010-09-01,['software-agents'],,16439,E15-383,2016-12-05T00:16:42.348Z,"['lieber@media.mit.edu', 'minsky@media.mit.edu']",True,open-mind-commons,2010-01-01,[],Open Mind Commons,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:12.351Z,"The aim of this project is to create an open source digital library with open data sets that cross link phenotypic response in plants (taste, nutrition, etc) to environmental variables, biologic variables, genetic variables, and resources required in cultivation (inputs). While plants can be altered genetically to produce different or more desirable traits, plants with the same genetics may naturally vary in color, size, texture growth rate, yield, flavor, and nutrient density depending on the environmental conditions in which they are grown. Each specific set of conditions can be thought of as a ""Climate Recipe"" that produces unique phenotypic results. As users experiment with new Climate Recipes, their input data and phenotypic results will be recorded and filed in an open source digital platform so that it can be shared, borrowed, scaled up, and improved upon around the world, instantly. ",2020-04-30,['open-agriculture-openag'],https://dam-prod2.media.mit.edu/x/2017/10/06/openag_phenome.png,16455,--Choose Location,2020-06-17T15:25:55.033Z,"['hildreth@media.mit.edu', 'calebh@media.mit.edu']",True,open-phenome-project,2015-01-01,"['artificial-intelligence', 'environment']",Open Phenome Project,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:12.536Z,"We offer case studies in the ways that digital fabrication allows us to treat the designs of products as a kind of source code: files that can be freely shared, modified, and produced. In particular, the case studies combine traditional electronic circuit boards and components (a mature digital fabrication process) with laser-cut or 3D printed materials. They demonstrate numerous possibilities for individual customizations both pre- and post-fabrication, as well as a variety of potential production and distribution processes and scales.",2011-09-01,['lifelong-kindergarten'],,16471,--Choose Location,2016-12-05T00:16:38.528Z,"['mres@media.mit.edu', 'leah@media.mit.edu', 'mellis@media.mit.edu']",True,open-source-consumer-electronics,2010-09-01,[],Open Source Consumer Electronics,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:12.612Z,"Biomolecules to biopolitics: hormones with institutional biopower! Open Source Estrogen combines do-it-yourself science, body and gender politics, and ethics of hormonal manipulation. The goal of the project is to create an open source protocol for estrogen biosynthesis. The kitchen is a politically charged space, prescribed to women as their proper dwelling, making it the appropriate place to prepare an estrogen synthesis recipe. With recent developments in the field of synthetic biology, the customized kitchen laboratory may be a ubiquitous possibility in the near future. Open-access estrogen would allow women and transgender females to exercise greater control over their bodies by circumventing governments and institutions. We want to ask: What are the biopolitics governing our bodies? More importantly, is it ethical to self-administer self-synthesized hormones?",2017-10-31,['design-fiction'],https://dam-prod2.media.mit.edu/x/2016/10/24/OSEHMD_ZIcjQqG.jpg,16479,--Choose Location,2017-10-11T20:29:44.233Z,"['sputniko@media.mit.edu', 'maggic@media.mit.edu']",True,open-source-estrogen,2015-09-01,"['art', 'civic-technology', 'open-source', 'synthetic-biology', 'ethics']",Open Source Estrogen,PUBLIC,http://maggic.ooo/Open-Source-Estrogen-2015,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:11:12.686Z,"Open Sources visualizes the roles and relationships among developers in free software communities. It is designed to help members of an open-source community quickly and easily answer questions such as: Is ""A"" prolific and reliable, frequently contributing code that remains part of the project? Is ""B"" a pruner, someone who comes in and mostly deletes other users' code?  Do some of those deleted coders then retaliate by undoing ""B""'s work? Does ""C"" discuss plans before committing code? Is ""D""'s work frequently discussed in messages?  Do you always talk about your code before you commit it, or do particular individuals always discuss your code months later?",2005-01-01,['sociable-media'],,16487,E15-390,2016-12-05T00:16:49.298Z,['judith@media.mit.edu'],True,open-sources,2004-09-01,[],Open Sources,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:12.762Z,Open Spaces combines the use of public displays with private mobile devices.  This allows group collaboration and shared use of public screens.  We demonstrate this via a public/private Scrabble game and a shared calendar.  The underlying architecture extends to use social predicates to determine who sees what.,2010-09-01,['viral-communications'],,16495,E15-483,2016-12-05T00:16:42.396Z,['lip@media.mit.edu'],True,open-spaces,2009-01-01,[],Open Spaces,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:12.914Z,"The Open Water Project aims to develop and curate a set of low-cost, open source tools enabling communities everywhere to collect, interpret, and share their water quality data. Traditional water monitoring uses expensive, proprietary technology, severely limiting the scope and accessibility of water quality data. Homeowners interested in testing well water, watershed managers concerned about fish migration and health, and other groups could benefit from an open source, inexpensive, accessible approach to water quality monitoring. We're developing low-cost, open source hardware devices that will measure some of the most common water quality parameters, using designs that makes it possible for anyone to build, modify, and deploy water quality sensors in their own neighborhood.",2020-08-01,['civic-media'],,16511,--Choose Location,2022-06-10T13:35:29.979Z,"['ethanz@media.mit.edu', 'dignazio@media.mit.edu']",True,open-water-project,2014-01-01,[],Open Water Project,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:13.327Z,"OPENCODE is a programming tool, perhaps the most elegantly simple programming environment ever. It offers ""two click"" programming for the inexperienced, along with sharing and tagging features for more advanced users. In essence, OPENCODE is a programming tool designed to leverage programming communities to enhance creative potential.",2008-01-01,['information-ecology'],,16551,E15-301,2016-12-05T00:16:42.429Z,['holtzman@media.mit.edu'],True,opencode,2006-09-01,[],OPENCODE,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:13.405Z,"When crowd maps track an eco-disaster, social information isn't enough to determine the severity of the disaster. This Ushahidi plugin introduces infrared environmental maps so that social and satellite data can be analyzed together. It's been deployed in Jakarta and NYC.",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202013-04-09%20at%2011.32.10%20AM.png,16559,--Choose Location,2016-12-05T00:16:42.465Z,['holtzman@media.mit.edu'],True,openir-crowd-map,2012-09-01,[],OpenIR: Crowd Map,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:13.490Z,"When an environmental crisis strikes, the most important element to saving lives is information. Information regarding water depths, spread of oil, fault lines, burn scars, and elevation are all crucial in the face of disaster. Much of this information is publicly available as infrared satellite data. However, with today�s technology, this data is difficult to obtain, and even more difficult to interpret. Open Infrared, or OpenIR, is an ICT (information communication technology) offering geo-located infrared satellite data as on-demand map layers and translating the data so that anyone can understand it easily. OpenIR will be pilot tested in Indonesia, where ecological and economic vulnerability is apparent from frequent seismic activity and limited supporting infrastructure. The OpenIR team will explore how increased accessibility to environmental information can help infrastructure-challenged regions to deal with environmental crises of many kinds.",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202013-04-09%20at%2011.26.47%20AM.png,16567,--Choose Location,2016-12-05T00:17:19.270Z,['holtzman@media.mit.edu'],True,openir-data-viewer,2012-01-01,[],OpenIR: Data Viewer,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:13.574Z,"OpenLocker is a Simple Registration compatible OpenID authentication server. Users have lockers that act as their identity pages as well as an RSS aggregator. The identity URI corresponds to unique locker locations, and passwords are replaced with combination locks, creating anonymity in the URI and preventing the use of the same password the users may use throughout multiple sites.",2007-09-01,['information-ecology'],,16575,E15-301,2016-12-05T00:16:42.485Z,['holtzman@media.mit.edu'],True,openlocker,2006-09-01,[],OPENLOCKER,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:13.726Z,"In a world where sensors, data storage, and processing power are too cheap to meter, how do you ensure that users can realize the full value of their data while protecting their privacy? openPDS is a field-tested, personal metadata management framework that allows individuals to collect, store, and give fine-grained access to their metadata to third parties. SafeAnswers is a new and practical way of protecting the privacy of metadata at an individual level. SafeAnswers turns a hard anonymization problem into a more tractable security one. It allows services to ask questions whose answers are calculated against the metadata, instead of trying to anonymize individuals' metadata. Together, openPDS and SafeAnswers provide a new way of dynamically protecting personal metadata.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/files/images/overview.png,16591,--Choose Location,2019-04-19T14:49:52.126Z,"['sandy@media.mit.edu', 'yva@media.mit.edu', 'brian717@media.mit.edu', 'shmueli@media.mit.edu']",True,openpds-saferanswers-protecting-the-privacy-of-metadata,2011-09-01,[],openPDS/ SaferAnswers: Protecting the privacy of metadata,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:13.884Z,"For new members of a work community, it can be difficult to learn the social landscape of the organization. Where are the casual meeting areas? When are people likely to be socially active? Are there spontaneous gatherings happening in parts of the workspace? OPENSPACE enables awareness of presence and activity by combining a grid of motion sensors with data logging, pattern analysis, and a variety of visualization techniques. With OPENSPACE, we aim to increase the social awareness of the larger spaces in which we work.",2007-09-01,['information-ecology'],,16607,E15-301,2016-12-05T00:16:49.324Z,['holtzman@media.mit.edu'],True,openspace,2006-09-01,[],OPENSPACE,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:13.964Z,"The Physical Language Workshop designs tools for creating digital content in a networked environment, and the means by which the content can be leveraged as creative capital within an experimental, online micro-economy that we call OPENSTUDIO. Our primary impact targets are in the areas of general digital media service architectures, global e-commerce, distance education, and visual information display systems.",2008-01-01,['information-ecology'],,16615,E15-301,2016-12-05T00:17:06.609Z,['holtzman@media.mit.edu'],True,openstudio,2003-01-01,[],OPENSTUDIO,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:14.061Z,"OPENTAG is an RFID development platform for advancing ubiquitous electronic tagging of items and people. With OPENTAG, we break through the barrier of application-specific and proprietary RFID product engineering by developing our own tag and associated firmware. An OPENTAG can adapt to varying demands as a tagged item moves through its life cycle from manufacturing to supply-chain to retail to consumer to disposal. OPENTAG is security conscious and privacy friendly.",2008-09-01,['information-ecology'],,16623,E15-301,2016-12-05T00:16:42.514Z,['holtzman@media.mit.edu'],True,opentag,2005-09-01,[],OPENTAG,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:14.214Z,"We are studying and implementing a swarm of 36 holonomic-drive robots for use in an upcoming robotic opera. Each robot will eventually comprise roughly eight degrees of freedom, and will follow a centralized control, allowing swarm behaviors as well as pre-scripted paths. In May, a test platform of three 3-DOF robots will be dancing, controlled by an animation with music composed by Tod Machover.",2008-09-01,"['opera-of-the-future', 'personal-robots']",,16639,E15-468,2016-12-05T00:17:19.356Z,"['cynthiab@media.mit.edu', 'tod@media.mit.edu']",True,operobots-a-robotic-swarm-for-artistic-expression,2006-09-01,[],Operobots: A Robotic Swarm for Artistic Expression,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:14.297Z,"Our deformable camera exploits new, flexible form factors for imaging in turbid media. In this study we enable a brush-like form factor with a time-of-flight camera. This has enabled us to reconstruct images through a set of 1100 optical fibers that are randomly distributed and permuted in a medium.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2016/10/21/optical_brush_bigger.jpg,16647,--Choose Location,2019-04-19T18:32:51.175Z,"['raskar@media.mit.edu', 'barmak@media.mit.edu']",True,optical-brush-enabling-deformable-imaging-interfaces,2013-01-01,[],Optical Brush: Enabling deformable imaging interfaces,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:14.372Z,Information processing in ultrafast laser dynamics by constrained cavity optimization,2005-01-01,['physics-and-media'],,16655,E15-023,2016-12-05T00:16:42.579Z,['neilg@media.mit.edu'],True,optical-programming,2005-01-01,[],Optical Programming,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:14.530Z,NMR spectrometers are in almost every lab and hospital for their unique ability to do chemical spectroscopy and imaging. They could be incredibly interesting in homes and offices as new types of diagnostic instruments. Part of what holds back this development is that a human user is required to supervise the magnetic field of the spectrometer to ensure it is as homogeneous as possible. An algorithm has been developed to make this process dramatically easier and can be done solely with a computer.,2002-09-01,['physics-and-media'],,16671,E15-489,2016-12-05T00:16:42.600Z,['neilg@media.mit.edu'],True,optimal-shimming,2001-01-01,[],Optimal Shimming,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:14.681Z,"We have pioneered the development of fully genetically encoded reagents that, when targeted to specific cells, enable their physiology to be controlled via light. These reagents, known as optogenetic tools, enable temporally precise control of neural electrical activity, cellular signaling, and other high-speed physiological processes using light. Such tools are in widespread use in neuroscience and bioengineering, for the study of how specific neurons contribute to cognition, emotion, and movement, and to brain disorder states, or to the remedy thereof. These tools are also being evaluated as components of prototype optical neural control prosthetics for ultraprecise treatment of intractable brain disorders. Derived from the natural world, these tools highlight the power of ecological diversity, in yielding technologies for analyzing biological complexity and addressing human health. We distribute these tools as freely as possible, and routinely host visitors to learn optogenetics.",2020-07-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/2016/07/16/optogeneticsmolecules.large.jpg,16687,E15-427,2020-09-10T17:28:21.764Z,['esb@media.mit.edu'],True,optogenetics-and-synthetic-biology-tools,2006-09-01,"['bioengineering', 'health', 'neurobiology', 'nanoscience']",Optogenetics: Molecules enabling neural control by light,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:14.758Z,"Opus is an online tool exploring the work and trajectory of scholars. Through a suite of interactive visualizations, Opus help users explore the academic impact of a scholar's publications, discover her network of collaborators, and identify her peers.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/files/Display/opus.png,16695,--Choose Location,2020-08-24T17:57:26.736Z,['hidalgo@media.mit.edu'],True,opus,2013-09-01,[],Opus,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:14.908Z,"If one considers true costs, bicycles are by far the most efficient form of transportation. We explore how individual actors in a city can collaborate asynchronously to create a city-wide bike-sharing network. At the core of this system is a bike that unlocks with your phone or RFID. The lock is designed as a kickstand, so that when users in the network shakes a bicycle in this network, the bicycle recognizes them, unlocks the bike for them, and allows them to ride, then charging for the time they spend before putting the kickstand down, thus re-engaging the lock. Such a system allows micro-entrepreneurs to keep adding bikes in the network in a peer-to-peer fashion, benefiting both the users and the providers.",,[],,16711,--Choose Location,2016-12-05T00:16:42.700Z,['sdkamvar@media.mit.edu'],True,organic-bike-networks,2015-01-01,[],Organic Bike Networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:15.858Z,"Origami Desk is an interactive installation where users learn to fold paper into beautiful shapes. Origami Desk improves on the inscrutable origami diagrams we all know and love by showing videos that demonstrate what the hands should do, projecting lines onto the paper indicating where the folds should be, and monitoring the paper folding to give the budding origami artist feedback should his folding go awry.",2001-12-30,['physics-and-media'],,16759,E15-468,2016-12-05T00:16:42.891Z,"['mike@media.mit.edu', 'rehmi@media.mit.edu', 'fletcher@media.mit.edu']",True,origami-desk,2000-12-31,[],Origami Desk,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:16.009Z,"The digital revolution has fundamentally changed our lives. Multimedia content-creation tools allow us to instantiate and share ideas easily, but most outcomes only exist on-screen and online--the physical world and everyday objects are largely excluded from a parallel explosion of mechatronic object creation. Services like Ponoko and Shapeways allow professionals and non-professionals to access computer-aided manufacturing (CAM) tools like 3D-printing and laser-cutting, but there are few (if any) design tools for creating complex mechanical assemblies that take full advantage of CAM systems. Creating unique mechatronic artifacts�Original Machines�thus requires more specific and sophisticated design tools than exist today. Object-oriented mechatronics is a parametric design approach that connects knowledge about mechanical assemblies and electronics with the requirements of digital manufacturing processes. The approach addresses the missing link between accessible rapid-manufacturing services and currently available design tools, creating new opportunities for self-expression through mechatronic objects and machines.",2011-09-01,['personal-robots'],,16767,,2016-12-05T00:16:42.920Z,['cynthiab@media.mit.edu'],True,originalmachines,2009-09-01,[],originalMachines,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:16.498Z,"The Out for Change Transformative Media Organizing Project (OCTOP) links LGBTQ, Two-Spirit, and allied media makers, online organizers, and tech-activists across the United States. In 2013-2014, we are conducting a strengths/needs assessment of the media and organizing capacity of the movement, as well as offering a series of workshops and skillshares around transmedia organizing. The project is guided by a core group of project partners and advisers who work with LGBTQ and Two-Spirit folks. The project is supported by faculty and staff at the MIT Center for Civic Media, Research Action Design and by the Ford Foundation's Advancing LGBT Rights Initiative.",2015-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/cropped-NEWOCTOPlogoR1.jpg,16799,--Choose Location,2016-12-05T00:16:44.784Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'elplatt@media.mit.edu']",True,out-for-change-transformative-media-organizing-project,2014-01-01,[],Out for Change: Transformative Media Organizing Project,PUBLIC,,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:16.835Z,"Newspaper front pages are a key source of data about our media ecology. Newsrooms spend massive time and effort deciding what stories make it to the front page. PageOneX makes coding and visualizing newspaper front page content much easier, democratizing access to newspaper attention data. Communication researchers have analyzed newspaper front pages for decades, using slow, laborious methods. PageOneX simplifies, digitizes, and distributes the process across the net and makes it available for researchers, citizens, and activists.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/u0n6u7aqnv5qdtg08qac.png,16823,,2022-06-10T13:35:29.995Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'elplatt@media.mit.edu']",True,pageonex,2012-01-01,[],PageOneX,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:17.041Z,"PalimPost is a converged system for storing, searching, and sharing digital and physical world information using sticky notes and mobile devices.  PalimPost extracts contextual cues from a user's physical environment and activities, connects them to the user's digital world research, and subsequently presents to the user systematically categorized, relevant, and JIT information.  Whether a user is writing down a shopping list on a sticky note after surfing the internet at home, or checking out hundreds of products at hand in a physical store, whether a user is preparing a list of dinner ingredients in the kitchen or buying food outside in the market, PalimPost integrates information from different time and location to form a seamlessly connected experiences for the user. Wayback Archive Description:PalimPost is taking sticky notes to the cloud era. So far sticky notes could only be used for saving a very short message, as a pointer to a bigger piece of information in our memory. However nothing in our new computing environment behaves this way. Every object now has an internal memory of Giga-Bytes, and our human memory is used for more emotional, social or inspirational information. But the affordances of plain sticky-paper notes are compelling: they are writable and also sticky! So how can we take them to our new ditized world? Enter: The Cloud, allowing digital information to be stored and retrieved swiftly. However, the experience of using the sticky note is about recording a moment, a fleeting thought. This is the goal of PalimPost, to help us capture moments in time, digitize them, and store them on the cloud.",2012-01-01,"['information-ecology', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/files/Display/PalimPost_logo.jpg,16839,,2018-10-12T17:59:51.275Z,"['holtzman@media.mit.edu', 'pattie@media.mit.edu', 'roys@media.mit.edu']",True,palimpost,2011-01-01,[],PalimPost,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:17.134Z,"A palimpsest is a manuscript consisting of a later writing superimposed upon an original writing. This name has been borrowed for the title of this project that aims to superimpose layers of recorded social interaction and present them as a single image. In contrast to conferencing tools and portals that enable chance encounters between distant locations, the palimpsest facilitates chance encounters between different points in time.",2002-12-30,['tangible-media'],,16847,,2016-12-05T00:16:43.126Z,['stefan@media.mit.edu'],True,palimpsest,2001-12-31,[],Palimpsest,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:17.224Z,"Drivers spend a significant amount of time multi-tasking while they are behind the wheel, especially parent drivers that attend to child passengers. These dangerous behaviors, particularly texting while driving, can lead to distractions and ultimately to accidents. Many in-car interfaces do not assist the driver with the task of entertaining the passengers. In a collaboration with Volkswagen/Audi and the SENSEable City Lab, we are developing PANDA (parental affective natural driver assistant), a robotic driver-vehicle interface that acts as a sociable partner and assists parent drivers. PANDA elicits facial expressions for engaging social interaction with the driver and passengers. PANDA uses car entertainment to entertain and engage the children in educational games while in the car and frees the parent to focus on the task of driving.",2015-05-01,['personal-robots'],,16855,--Choose Location,2017-05-31T18:41:51.451Z,"['cynthiab@media.mit.edu', 'michalg@media.mit.edu']",True,panda-parental-affective-natural-driver-assistant,2014-01-01,[],PANDA: Parental Affective Natural Driver Assistant,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:17.315Z,"Panoply is a crowdsourcing application for mental health and emotional wellbeing. The platform offers a novel approach to computer-based psychotherapy, targeting accessibility without stigma, engagement, and therapeutic efficacy. A three-week randomized-controlled trial with 166 participants showed Panoply conferred greater or equal benefits for nearly every therapeutic outcome measure compared to an active control task (online expressive writing). Panoply significantly outperformed the control task also on all measures of engagement, and is now being commercialized at itskoko.com.",2018-12-31,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/files/Display/panoply_logo_small.jpg,16863,--Choose Location,2020-07-18T21:10:06.096Z,"['picard@media.mit.edu', 'rmorris@media.mit.edu']",True,panoply,2011-01-01,['health'],Panoply,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:17.500Z,"Papert's World invites colleagues, collaborators, friends, and family of Seymour Papert to contribute rich-media stories about his life, his research, and research that he inspired. Participants use the rich-media story authoring and publishing tool, Confectionary, to collaborate on a ""media scrapbook"" collection. The site is continually updated with useful features such as a shared media library. ",2008-09-01,[],,16879,E15-368,2016-12-05T00:16:43.255Z,['gid@media.mit.edu'],True,paperts-world,2007-01-01,[],Papert's World,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:17.570Z,"Procedural representations, enabled through programming, are a powerful tool for digital illustration, but writing code conflicts with the intuitiveness and immediacy of direct manipulation. Para is a digital illustration tool that uses direct manipulation to define and edit procedural artwork. Through creating and altering vector paths, artists can define iterative distributions and parametric constraints. Para makes it easier for people to create generative artwork, and creates an intuitive workflow between manual and procedural drawing methods.",2016-09-01,['lifelong-kindergarten'],,16887,--Choose Location,2016-12-11T15:31:18.256Z,"['mres@media.mit.edu', 'jacobsj@media.mit.edu']",True,para,2014-09-01,[],Para,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:17.650Z,"Distributed sensor networks offer many new capabilities for monitoring environments with applicability to medical, industrial, military, environmental, and experiential fields. By making the system mobile, we increase the application-space for the distributed sensor network mainly by providing context-aware deployment, continual relocatabililty, automatic node recovery, and a larger area of coverage. In existing models, the addition of actuation to sensor network nodes has exacerbated three of the main problems with these types of systems: power usage, node size, and node complexity. This work proposes a solution to these problems in the form of parasitically actuated nodes that gain their mobility and local navigational intelligence by selectively engaging and disengaging from mobile hosts in their environment. This work aims to design, implement, evaluate, and demonstrate a parasitically actuated wireless sensor network as a solution to these problems and to explore new applications and features of a system with this type of mobility.",2004-09-01,['responsive-environments'],,16895,E15-344,2016-12-05T00:17:19.694Z,['joep@media.mit.edu'],True,parasitic-mobility-for-sensor-networks,2003-09-01,[],Parasitic Mobility for Sensor Networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:17.723Z,"As the power requirements for microelectronics continue decreasing, environmental energy sources can begin to replace batteries in certain wearable subsystems. In this spirit, this effort has examined devices that can be built into a shoe (where excess energy is readily harvested), and used for generating electrical power ""parasitically"" while walking. Two of these are piezoelectric in nature: a unimorph strip made from piezoceramic composite material and a stave made from a multilayer laminate of PVDF foil. The third is a shoe-mounted rotary magnetic generator. As a self-powered application example, a system had been built around the piezoelectric shoes that periodically broadcasts a 12-bit digital RFID as the wearer walks.  Nate Shenk's research explored better matching to the piezo source with a switching regulator. Ongoing work investigates exciting the piezo sources at higher frequency, and more optimal integration of the magnetic generators.",2001-12-30,['responsive-environments'],,16903,E15-357,2016-12-05T00:17:19.725Z,"['geppetto@media.mit.edu', 'joep@media.mit.edu']",True,parasitic-power-harvesting,1997-12-31,[],Parasitic Power Harvesting,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:17.801Z,"Part.Preview adds three-dimensional input and output devices to traditional 3D printers and fabrication machines. These enhanced capabilities enable users to ""print preview"" 3D objects before actually creating them.",2012-01-01,['information-ecology'],,16911,,2016-12-05T00:16:43.205Z,['holtzman@media.mit.edu'],True,partpreview,2011-01-01,[],Part.Preview,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:17.876Z,"Air and water pollution are well-known concerns in cities throughout the world. However, communities often lack practical tools to measure and record pollution levels, and thus are often powerless to motivate policy change or government action. Current government-funded pollution monitors are sparsely located, and many large national and local governments fail to disclose this environmental data in areas where pollution is most prevalent. We have been developing very low-cost, ultra low-power environmental sensors for air, soil, and water, that enable communities to easily sample their environment and upload data to their mobile phone and an online map. This not only empowers communities to enact new policies, but also serves as a public resource for city health services, traffic control, and general urban design.",2012-01-01,['changing-places'],,16919,--Choose Location,2016-12-05T00:17:19.758Z,"['kll@media.mit.edu', 'fletcher@media.mit.edu']",True,participatory-environmental-sensing-for-communities,2012-01-01,[],Participatory Environmental Sensing for Communities ,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:17.951Z,"The Participatory Networked Camera (PNC) project supports ad hoc communication and image-sharing between independent camerapeople. The ongoing development of the PNC project includes several prototypes of still and motion cameras that are augmented with temp-tie software, allowing camerapeople to network in decentralized ways without a central director or producer. We have structured experimental productions with a micro-goal approach to enable a rotating set of makers to participate in a production.",2004-01-01,[],,16927,E15-368,2016-12-05T00:16:43.307Z,['gid@media.mit.edu'],True,participatory-networked-camera,2001-01-01,[],Participatory Networked Camera,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:18.020Z,"We used tiny, wearable computers to create ""participatory simulations"" for a new generation of educational activities in which students could learn about dynamical systems by actually participating in simulations of the systems. For example, students could simulate the spread of an epidemic in which a ""virus"" jumps from one wearable computer to another. Our preliminary analysis indicated that the combination of physical and computer interaction lead to a richer learning experience than was possible with traditional computer-simulation activities, or with traditional group activities without computer support.",2000-09-01,['lifelong-kindergarten'],,16935,E15-001,2016-12-05T00:16:43.283Z,['mres@media.mit.edu'],True,participatory-simulations,1995-01-01,[],Participatory Simulations,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:18.095Z,"Protocols for Alternative Sexuality and Sensibility (PASS) is a wireless, networked system designed to function with a multidisciplinary description of a network in mind, incorporating conceptual implications and technical implementation of networking. PASS visibly tracks connections in public space based on the embodiment of protocols associated with sexual identity. These user-configured devices exchange information with other devices in order to uncover the often hidden interconnections created by the internalization of sexual identity. Sexual identity is represented by several alternative paradigms in addition to the culturally predicated homosexual/heterosexual binary. ",2006-01-01,['computing-culture'],,16943,E15-020A,2016-12-05T00:16:10.518Z,['csik@media.mit.edu'],True,pass-protocols-for-alternative-sexuality-and-sensibility,2006-01-01,[],PASS: Protocols for Alternative Sexuality and Sensibility,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:11:18.171Z,"We have developed a low-cost device that can wirelessly detect a beating heart over a short distance (1m) and does not require any sensor placed on the person's body. This device can be used for wireless medical/health applications as well as security and safety applications, such as automobile/truck drivers as well as ATM machines. We have also created a small battery-powered version of this sensor that can be worn on a person's clothing but does not require touching the person's skin.",2010-09-01,['affective-computing'],,16951,E15-448,2016-12-05T00:17:19.783Z,"['picard@media.mit.edu', 'fletcher@media.mit.edu']",True,passive-wireless-heart-rate-sensor,2006-09-01,[],Passive Wireless Heart-Rate Sensor,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:18.252Z,"Pasts and Presents is a visualization of activity in a space, both current and in the past. The visualization is an abstract, animated image in which the movements of the elements are shaped by the actions of the people passing by; it is an abstract visualization in much the same way that ripples on a pond are a visualization of activity on and near the surface of the water.",2017-05-01,['sociable-media'],,16959,E15-391,2022-06-10T14:41:29.967Z,['judith@media.mit.edu'],True,pasts-and-presents,2006-01-01,[],Pasts and Presents,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:18.407Z,"The automated processing of natural language by computer has become a paramount concern in nearly every field of human endeavor. The amount of information available in current literature and even the mass of information being generated on a daily basis is well beyond the scope of human understanding in any detail. Because of the sheer breadth of information, human analysis and curation of this information is rarely comprehensive and often error-prone. Machine-aided curation of this information could increase its utility by allowing more comprehensive analyses, more complete summarization, and extensive comparisons to current knowledge. Such curation requires a method for arriving at a high-level semantic description of the text at hand. Patent Semantics is an attempt to attack this problem in the domain of biochemical synthesis, by building structured representations of the procedures involved in different synthesis descriptions, grounding the components of those models, and developing algorithms by which detailed, human-readable comparisons of the descriptions may be produced. ",2004-01-01,['social-machines'],,16975,E15-483,2016-12-05T00:16:43.352Z,['dkroy@media.mit.edu'],True,patent-semantics,2003-09-01,[],Patent Semantics,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:18.547Z,"Pavlov is a virtual pet that encourages you to be physically active. He has ambient presence in the screens with which you interact. Pavlov is happy and healthy when you have walked a certain number of steps every day. When you are sedentary for a while, Pavlov nags you to take him out for a walk. He also craves to be the leader of all Pavlovs in your area. He can only be so when you, as his owner, become the most physically active person amongst your friends. Pavlov pings you every day at a certain time, telling you that he is going to have a dog-fight with other Pavlovs. You have the option to watch the dog-fight. Otherwise Pavlov simply tells you if he has won the fight, which may indicate that today you were more physically active than your friends.",2013-01-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/Pavlov%20unhappy%20-%20with%20dialog.png,16991,--Choose Location,2016-12-05T00:16:43.374Z,"['sujoy@media.mit.edu', 'geek@media.mit.edu']",True,pavlov,2012-09-01,[],Pavlov,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:18.621Z,"The PCB Origami project is an innovative concept for printing digital materials and creating 3D objects with Rigid-flex PCBs and pick-and-place machines. These machines allow printing of digital electronic materials, while controlling the location and property of each of the components printed. By combining this technology with Rigid-flex PCB and computational origami, it is possible to create from a single sheet of PCB almost any 3D shape that is already embedded with electronics, to produce a finished product with that will be both structural and functional.",2017-05-31,['mediated-matter'],,16999,--Choose Location,2021-05-21T13:15:04.420Z,['neri@media.mit.edu'],True,pcb-origami,2012-09-01,[],PCB Origami,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:18.696Z,"Pearls of Wisdom (PoW) is a suite of computational tools and activities designed to engage learners in intentional reflection on their learning experiences. PoW supports the construction and sharing of Pearls, intentional-reflection artifacts containing project how-to information and refection on design and learning experiences. Pearls give learners an ""objects-to-reflect-with"" to make deeper connections between those meaningful and important ideas the learner has chosen to share. The collection of Pearls embodies the ideas and activities culturally resonant within the community. This diversity of perspectives may then be appropriated by other learners. The project goal is to evolve a community of practice around reflection and other deep-learning activities. That included reflection practices that emerged at both the learner and mentor levels.  This work has culminated in the Cooperative Constructionism Framework, a guide for leveraging the learning potential of constructionist environments and expanding the constructionist-learning space. The study took place at the Flagship Computer Clubhouse, part of an international network of over one hundred after-school technology centers for youth from underserved communities.",2006-09-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/pearl-cluster.jpg,17007,E15-001,2016-12-05T00:16:43.424Z,['mres@media.mit.edu'],True,pearls-of-wisdom,2003-09-01,[],Pearls of Wisdom,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:18.772Z,"Peddl creates a localized, perfect market. All offers are broadcasts, allowing users to spot trends, bargains, and opportunities. With GPS- and Internet-enabled mobile devices in almost every pocket, we see an opportunity for a new type of marketplace which takes into account your physical location, availability, and open negotiation. Like other real-time activities, we are exploring transactions as an organizing principle among people that, like Barter, may be strong, rich, and long-lived.",2012-01-01,"['tangible-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/peddl_512.png,17015,,2016-12-05T00:16:43.452Z,"['ishii@media.mit.edu', 'dlakatos@media.mit.edu', 'lip@media.mit.edu']",True,peddl,2012-01-01,[],Peddl,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:18.857Z,"Peer 2 Peer University (P2PU) has developed ""learning circles,"" a model for facilitating in-person study groups at community libraries. Aimed at adult learners, learning circles take advantage of libraries as public community spaces for learning. We curate open, online courses and pair learners up with their peers to foster deeper, more meaningful adult basic educational experiences.",2020-08-01,['ml-learning'],https://dam-prod2.media.mit.edu/x/2017/02/10/Screen Shot 2017-02-10 at 9.59.19 AM.png,17023,--Choose Location,2022-06-10T14:06:49.069Z,"['ps1@media.mit.edu', 'kamcco@media.mit.edu']",True,peer-2-peer-university,2015-01-01,['learning-teaching'],Peer 2 Peer University,PUBLIC,https://www.p2pu.org,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:18.937Z,"Organizations are deploying gratitude-tracking systems to encourage appreciation, promote pro-sociality, and monitor employee wellbeing. We present the case study of one such system, called Gratia, adopted by a Fortune 500 company for over four years. We analyzed 422,209 messages of thanks and examined temporal patterns of appreciation, reciprocity, and repeated interactions. We also compared the formal organizational chart to the informal network expressed through the system. We found that gratitude is strongly reciprocated, that time between thanks is relatively long, and that it is predominantly given to peers outside one's immediate team.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/NetworksOfGratitude.png,17031,--Choose Location,2022-06-10T13:35:30.010Z,"['ethanz@media.mit.edu', 'jnmatias@media.mit.edu']",True,peer-appreciation-in-the-workplace,2014-01-01,[],Peer Appreciation in the Workplace,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:19.100Z,"PegBlocks are networked tactile transducers. As users manipulate the array of pegs, sliding them back and forth, motion is converted to electricity and converted back into motion throughout the rest of the network. The resulting movements of the pegs are determined not by any one individual input, but instead by the networked group as a whole.",2002-12-30,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/07/MakingDigTan_27_zDJnuyL.jpg,17047,E15-441,2020-06-11T18:12:53.047Z,['ishii@media.mit.edu'],True,pegblocks,2000-12-31,[],PegBlocks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:19.179Z,"The Pengachu Pocket Linux Server is our effort to bring affordable open-source and open-hardware computing to individuals and communities around the world. We've designed a first generation prototype in a PDA form factor which includes the Linux core, a small LCD display, a button/knob interface, an MPEG audio DSP, a wireless LAN, and a docking station which includes keyboard, wired LAN, and battery charger functionality.",2001-12-30,"['personal-fabrication', 'health', 'physics-and-media']",,17055,E15-023,2016-12-05T00:16:43.476Z,"['neilg@media.mit.edu', 'rehmi@media.mit.edu']",True,pengachu-pocket-linux-server,1999-12-31,[],Pengachu Pocket Linux Server,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:19.264Z,"The need for low-cost sensors and ID tags is ongoing. Our work explores how ID and sensor information can be encoded in the electromagnetic spectrum of inexpensive material structures without the need for electronic chips. Through the use of novel electromagnetic structures and active materials, it is possible to manipulate and to control the electromagnetic response of structures in order to create a ""representation language"" for conveying ID and sensor information. Applications include smart packaging, automatic inventory systems, and low-cost sensors for home, industrial, and environmental uses.",2001-12-30,"['counter-intelligence', 'personal-fabrication', 'toys-of-tomorrow', 'cc', 'health', 'silicon-biology', 'physics-and-media']",,17063,E15-023,2016-12-05T00:16:43.495Z,"['neilg@media.mit.edu', 'fletcher@media.mit.edu']",True,penny-tags-general,1993-12-31,[],Penny Tags: General,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:19.434Z,"Over the last decade, tracking people has attracted considerable research interest. Cameras enable efficient tracking of people but it is a nontrivial task, especially when the sensors have a non-overlapping range. The goal is to track�that is, both locate and identify�people around a multi-story building with a sparse network formed by non-overlapping cameras and RFID sensors.",2013-09-01,['responsive-environments'],,17079,--Choose Location,2016-12-05T00:16:43.620Z,['joep@media.mit.edu'],True,people-tracking-system,2013-01-01,[],People Tracking System,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:19.577Z,"During the nineties, software and computer graphics developers came head-to-head with artists and graphic designers, facilitating a need for their respective fields to understand and merge new technologies and academic concepts. However, a silence currently remains about the aesthetic qualities of garment design, as new �wearable� computing mechanisms develop. This project explores how the two disciplines redefine each other as they merge. Built with an emphasis on qualitative elements such as style, comfort, and functionality, Peppermint is a handbag adorned with a dynamic display surface. Each morning, the bag can be presented with the important events of the current day (such as class, a demo, or crew practice) via a tactile interface. Over the course of the day, the dynamic surface gradually modulates in behavior. This bag is an exploration of kinetic design in relation to the aesthetics of fashion.",2002-12-30,['aesthetics-computation'],,17087,E15-305A,2016-12-05T00:17:19.916Z,[],True,peppermint,2000-12-31,[],Peppermint,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:19.740Z,"The ability to display and perceive emotion is a key component in intelligent behavior. Emotion helps to guide both human and animal behavior, and is a useful method of conveying information between species. This project is focused on building tools that help synthetic characters perceive emotion in vocalizations from their human interactors. The project also focuses on creating creatures that use these tools to react appropriately to the emotion they perceive based on their own personalities and experiences.",2004-01-01,['synthetic-characters'],,17095,E15-441,2016-12-05T00:17:19.945Z,[],True,perceiving-emotional-affect-in-vocalizations,2003-09-01,[],Perceiving Emotional Affect in Vocalizations,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:20.325Z,"Most modern electronic musical instruments are constructed of artifacts from the military-industrial complex. As their components become obsolete, musicians must invent new technologies for synthesizing electronic music.  Using naturally occurring materials, this design will survive on Earth for at least 3 million years�because it does not require expensive, intricate manufacturing processes.",2005-09-01,['computing-culture'],,17127,E15-001,2016-12-05T00:16:43.696Z,['csik@media.mit.edu'],True,permanent-synthesis,2004-09-01,[],Permanent Synthesis,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:11:20.626Z,"The OpenAg™ Personal Food Computer is a tabletop-sized, controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber. Climate variables such as carbon dioxide, air temperature, humidity, dissolved oxygen, potential hydrogen, electrical conductivity, and root-zone temperature are among the many conditions that can be controlled and monitored within the growing chamber to yield various phenotypic expressions in the plants. Our latest version—the PFC v3.0, or ""PFC_EDU""—has been scaled down from previous PFC's in terms of cost, size, and complexity, and designed specifically with educators and children aged 8-14 in mind. It offers a spectrum of control so that users can make their growing experience as manual or as automated as they would like.Click here to Build a Food Computer.Like all OpenAg's Food Computers, the PFC_EDU is open source and can be made from easily accessible components so that #nerdfarmers with a broad spectrum of skills, resources, and interests can build, modify, share, and upgrade over time. Build instructions, design files, and helpful resources for all our OpenAg™ Personal Food Computers are on our OpenAg Wiki, OpenAg Github so nerd farmers can band together (using the OpenAg Forum) to conduct scientifically rigorous citizen-science experimentation, all over the world.",2020-04-30,['open-agriculture-openag'],https://dam-prod2.media.mit.edu/uuid/38946ceb-71f1-4520-b69f-703f93d0e616,17143,--Choose Location,2020-05-06T18:57:25.482Z,"['calebh@media.mit.edu', 'hildreth@media.mit.edu', 'rbaynes@media.mit.edu', 'poitrast@media.mit.edu', 'rebekahj@media.mit.edu']",True,personal-food-computer,2015-09-01,"['robotics', 'design', 'agriculture', 'art', 'artificial-intelligence', 'data', 'environment', 'food', 'hacking', 'human-machine-interaction', 'learning-teaching', 'open-source', 'machine-learning', 'technology', 'wellbeing', 'racial-justice']",Personal Food Computer,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:20.807Z,"The saying, ""if you can't measure it, you can't manage it"" may be appropriate for stress. Many people are unaware of their stress level, and of what is good or bad for it. The issue is complicated by the fact that while too much stress is unhealthy, a certain amount of stress can be healthy as it motivates and energizes. The ""right"" level varies with temperment, task, and other factors, many of which are unknown. There seems to be no data analyzing how stress levels vary for the average healthy individual, over day-to-day activities. We would like to build a device that helps to gather and present data for improving an individual's understanding of both healthy and unhealthy stress in his or her life. The device itself should be comfortable and should not increase the user's stress. (It is noteworthy that stress monitoring is also important in human-computer interaction for testing new designs.) Currently, we are building a new, wireless, stress-mornitoring system by integrating Fitsense's heart-rate sensors and Motorola's iDen cell phone with our heart-rate-variability estimation algorithm.",2004-01-01,"['gray-matters', 'affective-computing']",,17151,E15-001,2016-12-05T00:17:19.969Z,['picard@media.mit.edu'],True,personal-heart-stress-monitor,2001-01-01,[],Personal Heart-Stress Monitor,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:20.983Z,"Personal Opera is a radically innovative creative environment that enables anyone to create musical masterpieces sharing personal thoughts, feelings, and memories. Based on our design of, and experience with, such projects as Hyperscore and the Brain Opera, we are developing a totally new environment to allow the incorporation of personal stories, images, and both original and well-loved music and sounds. Personal Opera builds on our guiding principle that active music creation yields far more powerful benefits than passive listening. Using music as the through-line for assembling and conveying our own individual legacies, Personal Opera represents a new form of expressive archiving: easy to use and powerful to experience. In partnership with the Royal Opera House in London, we have begun conducting Personal Opera workshops specifically targeting seniors to help them tell their own meaningful stories through music, text, visuals, and acting.",2010-09-01,['opera-of-the-future'],,17159,Swatch Lab,2016-12-05T00:16:43.803Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu']",True,personal-opera,2007-09-01,[],Personal Opera,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:21.115Z,"We are developing an opt-in camera network, in which users carrying wearable tags are visible to the network and everyone else is invisible. Existing systems for configurable dynamic privacy in this context are opt-out and catch-all; users desiring privacy carry pre-registered tags that disable sensing and networked media services for everyone in the room. To address these issues, we separate video into layers of flexible sprites representing each person in the field of view, and transmit video of only those who opt-in. Our system can also define groups of users who can be dialed in and out of the video stream dynamically. For cross-reality applications, these dynamic layers achieve a new level of video granularity, allowing users and groups to uncover correspondences between their activities across spaces.",2013-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/logo_square.jpg,17167,,2016-12-05T00:17:19.988Z,"['gershon@media.mit.edu', 'joep@media.mit.edu']",True,personal-video-layers-for-privacy,2010-01-01,[],Personal Video Layers for Privacy,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:21.260Z,"Personas is a component of the Metropath(ologies) exhibit, recently on display at the MIT Museum by the Sociable Media Group. It uses sophisticated natural language processing and the Internet to create a data portrait of one's aggregated online identity. In short, Personas shows you how the Internet sees you.",2009-01-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/personasLogo.png,17175,E15-318,2016-12-05T00:16:43.825Z,['judith@media.mit.edu'],True,personas,2009-01-01,[],Personas,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:21.398Z,"Perspectives was an interactive-art construction kit, designed specifically to help high-school students explore and document the different individual, social, and cultural perspectives that make up human experience. Students  created a new kind of interactive documentary, programming sensor input to control multiple video streams�and, in the process, explored multiple points-of-view of a single entity.",2004-01-01,['lifelong-kindergarten'],,17183,E15-120B,2016-12-05T00:16:21.389Z,['mres@media.mit.edu'],True,perspectives-llk,2003-01-01,[],Perspectives LLK,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:21.743Z,"Persuasive Cities research is aimed at advancing urban spaces to facilitate societal changes. According to social science research, any well-designed environment can become a strong influencer of what people think and do. There is an endlessly dynamic interaction between a person, a particular behavior, and a specific environment. Persuasive Cities research leverages this knowledge to engineer persuasive environments and interventions for altering human behavior on a societal level. This research is focused on socially engaging environments for supporting entrepreneurship and innovation, reshaping routines and behavioral patterns in urban spaces, deploying intelligent outdoor sensing for shifting mobility modes, enhancing environmentally friendly behaviors through social norms, introducing interactive public feedback channels to alter attitudes at scale, engaging residents through socially influencing systems, exploring methods for designing persuasive neighborhoods, and fostering adoption of novel urban systems. More: http://bit.ly/TEDxp",,"['advancing-wellbeing', 'changing-places']",,17207,--Choose Location,2017-03-31T20:52:58.972Z,"['kll@media.mit.edu', 'agnis@media.mit.edu', 'rchin@media.mit.edu']",True,persuasive-cities,2015-09-01,[],Persuasive Cities,PUBLIC,http://cp.media.mit.edu/agnis-stibe,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:11:21.862Z,"An Alternative Autonomous Revolution System design for emerging urban contexts and societal aspirationsThe Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.The PEV uses standard bicycle components and is lightweight (<50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.  The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/05/triplet.png,17215,--Choose Location,2023-02-02T20:37:47.075Z,"['kll@media.mit.edu', 'mcllin@media.mit.edu', 'ptinn@media.mit.edu', 'lukeji@media.mit.edu', 'cq_zhang@media.mit.edu', 'inigo@media.mit.edu', 'taiyu@media.mit.edu', 'abhia@media.mit.edu']",True,pev,2014-09-01,"['design', 'architecture', 'civic-technology', 'data', 'energy', 'environment', 'kids', 'transportation', 'urban-planning', 'data-science']",Persuasive Electric Vehicle (PEV),PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:22.170Z,"We share the vision of cities as places where people live and work, connected by better mobility systems that are sustainable and enable a high quality of living. As a result of rapid urbanization, many existing transportation systems and particularly car depended ones can neither provide sufficient capacity nor help reaching that vision of the future city. Besides a rethinking in urban planning and public investments in urban transport systems, changes in the citizens’ mobility choices are necessary to realize a vision of a high quality of life and sustainable city.The project goal of Persuasive Urban Mobility is to examine how persuasive technologies can be utilized to shift the mobility behavior of citizens. We are particularly interested in studying the key persuasive strategies to enable, motivate and trigger users to shift from high energy to low energy modes of transportation, with the current research focusing on promoting cycling as an alternative to single passenger car use.This project is a research collaboration of AIT Austrian Institute of Technology and Changing Places at MIT Media Labby Agnis Stibe, Matthias Wunsch, Alexandra Millonig, Chengzhen Dai, Stefan Seer, Katja Schechtner, Ryan C.C. Chin, and Kent Larson.Former affiliates: Felipe Lozano-Landinez, Francesco Pilla",,['changing-places'],https://dam-prod2.media.mit.edu/x/2017/08/11/static1.squarespace_mKf5gqr.jpg,17239,--Choose Location,2017-08-11T17:26:57.370Z,"['kll@media.mit.edu', 'rchin@media.mit.edu', 'agnis@media.mit.edu', 'katjas@media.mit.edu']",True,persuasive-urban-mobility,2013-09-01,[],Persuasive Urban Mobility,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:11:22.363Z,"With the tremendous explosion of community photos, how can we present a coherent visual model for a large-scale community photo collection? In this project, we aim to build a photo space explorer for effective image browsing, retrieval, and annotation on top of our existing, distributed, community storage mechanism. The key is to identify object and feature points relations among community photos in an effective and scalable way, and to distribute the storage through an ensemble of mobile devices. The photo space explorer also enables users to navigate community photo collections using a 3-D browser.",2009-01-01,['viral-communications'],,17255,E15-483,2016-12-05T00:16:43.897Z,['lip@media.mit.edu'],True,photo-space,2008-09-01,[],Photo Space,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:22.454Z,"We present a near-real-time system for interactively exploring a collectively captured moment without explicit 3D reconstruction. Our system favors immediacy and local coherency to global consistency. It is common to represent photos as vertices of a weighted graph. The weighted angled graphs of photos used in this work can be regarded as the result of discretizing the Riemannian geometry of the high dimensional manifold of all possible photos. Ultimately, our system enables everyday people to take advantage of each others' perspectives in order to create on-the-spot spatiotemporal visual experiences similar to the popular bullet-time sequence. We believe that this type of application will greatly enhance shared human experiences, spanning from events as personal as parents watching their children's football game to highly publicized red-carpet galas.",,['camera-culture'],,17263,--Choose Location,2019-04-19T18:33:30.110Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'otkrist@media.mit.edu']",True,photocloud-personal-to-shared-moments-with-angled-graphs-of-pictures,2010-01-01,[],PhotoCloud: Personal to shared moments with angled graphs of pictures,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:22.609Z,"Imagine taking a digital picture with your new camera-phone and the picture is able to annotate itself. For example, if you are at Fenway Park and take a picture, the picture will automatically be annotated with keywords such as ""Fenway Park,"" ""Baseball,"" or ""Red Sox."" This project takes advantage of a GPS receiver and a geographical search engine called MetaCarta to annotate pictures in real time with relevant keywords. These keywords help users to make sense of their surroundings when they take the picture, as well as to organize and retrieve pictures afterwards.",2008-09-01,['fluid-interfaces'],,17279,E15-320,2016-12-05T00:16:44.110Z,['pattie@media.mit.edu'],True,photowhere,2003-09-01,[],Photowhere,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:22.686Z,"This project is about embodying the user interface for a call-handling agent in a small animatronics device. The embodied agent�s primary function is to interact socially, with both the user and other co-located people. In order for an agent to be understandable by humans, it must have a naturalistic embodiment and must interact with its environment as living creatures do by sending out readable social cues that convey its internal state. Our conversational agent has physical presence through interactive �stuffed animals� of different shapes and sizes, remotely controlled by a computer. These creatures interact with a combination of pet-like and human-like behaviors, such as waking up, waving for attention, or eye contact. These subtle non-verbal cues are socially intuitive to us, and therefore could be an ideal platform for unobtrusive interruption by mobile communication devices.",2005-01-01,['living-mobile'],,17287,E15-368,2016-12-05T00:16:10.605Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,physical-embodiments-for-mobile-communication-agents,2002-01-01,[],Physical Embodiments for Mobile Communication Agents,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:22.760Z,"This project introduces the concept of physical one-way functions. Modern public key cryptography relies extensively on one-way functions and trapdoor one-way functions. Our use of physical mechanisms for one-way functions is motivated by emerging cryptographic applications which require a means for authenticating digital information that itself cannot be copied, and adds little cost to the system. The asymmetry that we exploit is in the ease of constructing and characterizing large physical structures with fine feature sizes, and the difficulty in reproducing them.",2001-12-30,['physics-and-media'],,17295,E15-023,2016-12-05T00:16:43.939Z,['neilg@media.mit.edu'],True,physical-one-way-functions,1998-12-31,[],Physical One-Way Functions,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:22.837Z,"We propose a new approach to physical telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In the research paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of users' body parts can be altered to amplify their capabilities for teleoperation. A preliminary evaluation found that users were able to manipulate simple objects remotely, and found many different techniques for manipulation that highlight the expressive nature of our system.",2015-01-01,"['terrestrial-sensing', 'tangible-media']",https://dam-prod2.media.mit.edu/x/files/Display/embodied_teleoperation01.jpg,17303,--Choose Location,2016-12-05T00:16:44.043Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu', 'olwal@media.mit.edu']",True,physical-telepresence,2014-09-01,[],Physical Telepresence,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:23.109Z,"PICO is a new type of human-computer interface combining the usability advantages of mechanical systems with the abstract computational power of computers. Before modern computers, mechanical devices used for computation were limited to relatively simple tasks, but easy to use and understand because operation details are observable. Computers can handle more abstract and complex tasks, but the processes are hidden and difficult to understand. We are merging software-based computation with dynamic physical processes that are exposed to and modified by the user. By designing interfaces employing the dynamic behavior of objects in the physical world as an interface vocabulary, we aim to create interfaces that allow people and computers to collaborate in novel ways. Projects include an interface for planning cellular telephone networks on an actuated tabletop sensing surface; objects on this surface are moved under software control using electromagnets, but also by users standing at the table, who can physically intervene in the computational optimization. We will investigate whether the communication bandwidth afforded by an interface that heavily engages the sense of touch can change the approach that experts take to solving some types of optimization problems.",2006-09-01,['tangible-media'],,17335,E15-344,2016-12-05T00:16:43.663Z,"['ishii@media.mit.edu', 'jpatten@media.mit.edu']",True,pico,2004-01-01,[],PICO,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:23.191Z,"Picture This! is a video editing tool progressing toward children's natural expression of play while telling stories with their toys. We use technology to offer visual feedback regarding how the scene looks from the point of view of an imaginary audience. If the toy has an immediately accessible visual perspective, a new world is opened to the child. The toy helps her explore visual and narrative perspectives of character props, expanding the discovery of her environment. The child storyteller enters the world of the movie maker. Cameras become part of a toy system showing how things look from a toy's point of view. They can be integrated in LEGO people, car drivers, and even coffee mugs! The video process, supported by gesture-induced editing, benefits children in practicing social interrelationships and visual perspective taking.",2010-01-01,['tangible-media'],,17343,E15-350,2016-12-05T00:16:44.135Z,['ishii@media.mit.edu'],True,picture-this,2007-09-01,[],Picture This!,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:23.338Z,"PictureXS is a browser-constrained data repository that builds a semantic network of images and keywords. It explores issues related to user interfaces, organization and visualization of large amounts of digital content, and alternative ways of regulating social systems on the World Wide Web by avoiding the user account convention, and implementing a simple open censorship model.",2008-01-01,['information-ecology'],,17359,E15-301,2016-12-05T00:17:20.117Z,['holtzman@media.mit.edu'],True,picturexs,2007-01-01,[],PictureXS,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:11:23.412Z,"Most uses of digital technologies pull young people away from hands-on experimentation with the physical world. The Playful Invention and Exploration (PIE) Network has bridged that divide, integrating digital technologies and hands-on experimentation in a new generation of science and engineering education activities that have proven very engaging for young people and their families. PIE activities have been developed by a network of six museums around the country, using Cricket technology developed at the Media Lab (with research support from NSF). Science museums around the country are now incorporating Crickets and PIE activities into their public programs.",2006-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/pie-logo.gif,17367,E15-001,2016-12-05T00:16:44.166Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'sylvan@media.mit.edu', 'bss@media.mit.edu']",True,pie-network,1999-01-01,[],PIE Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:23.494Z,"Piezing is an outfit which generates power using the natural gestures of the human body in motion. Around the joints of the elbows and hips, the garment is embedded with piezoelectric material elements which generate an electric potential in response to applied mechanical stress. The electric potential is then stored as voltage in a centralized small battery and later can be discharged into a device.",2009-09-01,['tangible-media'],,17375,E15-344,2016-12-05T00:16:44.263Z,['ishii@media.mit.edu'],True,piezing,2008-01-01,[],Piezing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:23.568Z,"An electric tambourine that is completely powered by the playing of the instrument.  The jingles of the tambourine are lined with piezoelectric elements, which generate voltage when impacted. This voltage is then harvested to turn on LED lights on the tambourine. The harder the tambourine rattles, the greater the voltage generated by the piezoelectric elements and thus the brighter the light. Yellow LED lights on the jingles light up when the corresponding piezo is rattled. If the tambourine is played with enough force, blue and red LED lights on the band also light up. Thus, the player can both hear and see the music generated by this instrument.",2011-09-01,['high-low-tech'],,17383,,2016-12-05T00:16:44.288Z,"['leah@media.mit.edu', 'jieqi@media.mit.edu']",True,piezo-powered-tambourine,2011-01-01,[],Piezo Powered Tambourine,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:23.643Z,"Pillow-Talk is a set of connected objects intended to assist in the capture and recall digitally stored dreams and memories via natural and tangible interfaces. It consists of two devices, one of which is a pillow embedded with a voice recorder that is activated upon squeezing together several conductive patches at the corner of the pillow. This interaction minimizes the steps necessary to record a fresh memory of a dream immediately upon awakening. After the dream is recorded into the pillow, the audio file is transmitted wirelessly to a jar containing shimmering LEDs to display the ""capture"" of a new memory, and electronics in the jar can play back the recordings through a small speaker under its lid when it is opened.",2018-06-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2017/03/24/jar_NcX0NUY.jpg,17391,--Choose Location,2020-06-10T15:51:42.226Z,"['vmb@media.mit.edu', 'edwinapn@media.mit.edu']",True,pillow-talk,2010-09-01,"['consumer-electronics', 'storytelling', 'interfaces']",Pillow-Talk,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:23.718Z,"PingPongPlus is a digitally enhanced version of the classic ping-pong game. It is played with ordinary, un-tethered paddles and balls, and features a ""reactive table"" that incorporates sensing, sound, and projection technologies. Projectors display patterns of light and shadow on the table; bouncing balls leave images of rippling water; and the rhythm of play drives accompanying sound and visuals. In the process, this project  explores new ways to couple athletic recreation and social interaction with engaging digital enhancements.",2000-12-30,['tangible-media'],,17399,E15-441,2016-12-05T00:16:44.315Z,['ishii@media.mit.edu'],True,pingpongplus,1996-12-31,[],PingPongPlus,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:23.798Z,"PingPong++ (PingPongPlusPlus) builds on PingPongPlus (1998), a ping pong table that could sense ball hits, and reuse that data to control visualizations projected on the table. We have redesigned the system using open-source hardware and software platforms so that anyone in the world can build their own reactive table. We are exploring ways that people can customize their ping pong game experience. This kiosk allows players to create their own visualizations based on a set of templates. For more control of custom visualizations, we have released a software API based on the popular Processing language to enable users to write their own visualizations. We are always looking for collaborators! Visit pppp.media.mit.edu to learn more.",2012-01-01,['tangible-media'],,17407,--Choose Location,2016-12-05T00:16:44.338Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu', 'x_x@media.mit.edu']",True,pingpongplusplus,2010-01-01,[],PingPongPlusPlus,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:23.954Z,Pintail is a travel companion app for guided storytelling. It will start by capturing your travel plan so that it can nudge you with personalized story-creation triggers at the right context. Pintail would act as a work-in-progress scrapbook from the moment a trip is planned. It will provide users the structure and tools for storytelling while taking into account the short attention span of today's audience. Pintail would use priming as a technique by showing the user what others feel or have drawn about the places he or she is visiting. Some of the content of Pintail prompts would be automatically collected from travel review sites. Users can then use the Pintail story-creation tools to reflect and create their own stories. Pintail would also attempt to balance between the story creation activity and the actual trip experience.,2015-01-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/10/06/files/Display/pintail%20logo%20v2.png,17423,--Choose Location,2016-12-05T00:16:44.365Z,"['sujoy@media.mit.edu', 'geek@media.mit.edu']",True,pintail,2013-09-01,['storytelling'],Pintail,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:24.034Z,"The pinwheels project continues the study of Ambient Fixtures to communicate digital information at the periphery of human perception through ambient media. This project takes fields of pinwheels and explores what arrangements create interfaces that are intuitive and informative for the user while being an ambient source of information. Its other focus is on finding interesting application domains and ways to map computationally the information to the pinwheels. Its current application domains include stock market activity monitoring, Web site hits tracking, natural wind movement, and network activity monitoring.",2000-12-30,['tangible-media'],,17431,E15-441,2016-12-05T00:16:44.390Z,['ishii@media.mit.edu'],True,pinwheels,1997-12-31,[],pinwheels,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:24.113Z,"Pitcher is an efficient and flexible real-time software-based pitch shifter. It goes well beyond an octave shift with no strong timbre artifact. It applies particularly well to the singing voice, or to instruments such as the Hyperviolin. It is now being networked for collaborative new music experiences.",2002-12-30,"['toys-of-tomorrow', 'opera-of-the-future']",,17439,E15-445,2016-12-05T00:16:44.428Z,"['tod@media.mit.edu', 'tristan@media.mit.edu']",True,pitcher,2001-12-31,[],Pitcher,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:24.442Z,"Place Pulse is a website that allows anybody to quickly run a crowdsourced study and interactively visualize the results. It works by taking a complex question, such as ""Which place in Boston looks the safest?"" and breaking it down into easier-to-answer binary pairs. Internet participants are given two images and asked ""Which place looks safer?"" From the responses, directed graphs are generated and can be mined, allowing the experimenter to identify interesting patterns in the data and form new hypotheses based on their observations. It works with any city or question and is highly scalable. With an increased understanding of human perception, it should be possible for calculated policy decisions to have a disproportionate impact on public opinion.",2016-10-22,['collective-learning'],,17479,--Choose Location,2016-12-05T00:17:20.139Z,['hidalgo@media.mit.edu'],True,place-pulse,2011-09-01,[],Place Pulse,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:24.524Z,"The placebo effect is not limited to sugar pills in clinical trials of medications. In fact, placebos are a specific example of the affect of meaning on health. All medicines work in two parallel ways, as a direct result of the active medication, and indirectly as a placebo response. Expectations and beliefs lead to measurable and real health outcomes, and one important way that expectations and beliefs are manipulated is by pricing, branding, and marketing. These are powerful factors that can affect how people respond to medicines. This project investigates how differences in pricing, branding, and other marketing factors affect people�s response to medication. By understanding this response, we can understand how people respond to drugs in the real world outside of the clinical trial.",2008-01-01,['erationality'],,17487,E15-320A,2016-12-05T00:16:44.446Z,[],True,placebos-marketing,2006-09-01,[],Placebos & Marketing,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:11:24.800Z,"This top-secret project is aimed at improving human cognition and happiness, by empowering people to control their lives.",2009-09-01,['synthetic-neurobiology'],,17519,E15-435,2016-12-05T00:16:44.611Z,['esb@media.mit.edu'],True,plasma-planning,2009-01-01,[],Plasma Planning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:25.025Z,"While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.To learn more about this project, please check out: http://playfulwords.org/",,['social-machines'],https://dam-prod2.media.mit.edu/x/2017/08/04/playfulwords_header.jpeg,17543,--Choose Location,2018-04-30T20:28:15.298Z,"['dkroy@media.mit.edu', 'isysoev@media.mit.edu', 'anneli@media.mit.edu', 'minasg@media.mit.edu', 'pbansal@media.mit.edu', 'jnazare@media.mit.edu', 'snehapm@media.mit.edu', 'echu@media.mit.edu', 'saquib@media.mit.edu']",True,playful-words,2014-09-01,['learning-teaching'],Playful Words,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:25.340Z,"Plazza brings group formation and dynamic content distribution capabilities to broadband wireless devices, together with the illusion of infinite bandwidth and  storage. Formerly restricted to PDA functionality, these devices dynamically discover content available in the wireless local network and other user's access patterns to it. Users can discover like-minded individuals or leverage implicit collaborative filtering to select the content they may be interested in, share the content with other stationary or mobile nodes, and view it in their devices or arrange for it to be transferred to their home node, where they can enjoy without battery or screen size constraints. ",2003-01-01,['viral-communications'],,17575,E15-344,2016-12-13T19:42:10.892Z,['lip@media.mit.edu'],True,plazza-group-formation-and-dynamic-content-distribution-for-wireless-and-mobile-systems,2001-01-01,[],Plazza: Group Formation and Dynamic Content Distribution for Wireless and Mobile Systems,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:25.418Z,"Plethora creates simultaneous, localized, personal broadcasting networks that allow audiences to form on-the-fly and build their own media streams. The display ranges from personal devices such as phones, to embedded screens in kitchens. It uses Bluetooth LE beacons to emulate local broadcast stations that signal the proximity. Plethora inverts the relationship between mobility and the multiplicity of screens with which we interact on a moment-to-moment basis. It also considers media as migrating away from larger, high definition screens to a universe of personal, anytime, anywhere representations.",2016-01-01,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/2016/10/06/files/Display/plethora.jpg,17583,--Choose Location,2016-12-05T00:16:44.818Z,"['weller@media.mit.edu', 'lip@media.mit.edu']",True,plethora,2014-09-01,[],Plethora,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:25.497Z,"The massive sensor network assumed by most advanced scenarios of ubiquitous computing must first be innocously deployed in devices used for other purposes. Accordingly, this project has developed a sensor node embedded into a common power strip. This device has access to power (and potentially networking) through its cord, can control and measure the detailed current profile consumed by devices plugged into its outlets, supports an ensemble of sensors (microphone, light, temperature, and vibration sensors are intrinsic, and other sensors such as thermal motion and cameras can be added easily), and hosts an RF network that can connect to other PlugPoint sensors and small nearby wireless sensors. We are now exploring a host of ubiquitous computing applications that can be developed on these devices when they are deployed across the Media Lab.",2007-01-01,['responsive-environments'],,17591,E15-344,2017-10-13T18:26:49.483Z,"['geppetto@media.mit.edu', 'joep@media.mit.edu']",True,plugpointa-platform-for-ubiquitous-sensor-networks,2005-09-01,[],PlugPoint—A Platform for Ubiquitous Sensor Networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:25.575Z,"As RFID technology matures, and PDAs such as smart phones become ubiquitous, the time is right to look at the future of RFID tags beyond the inventory list. This project examines using smart phones as a mediator between RFID tags and smart appliances, making the use of RFID tags seamless and intuitive, and supporting applications using everyday objects in everyday scenarios.",2005-09-01,['fluid-interfaces'],,17599,Pond,2016-12-05T00:16:44.725Z,['pattie@media.mit.edu'],True,plunge-n-play,2005-09-01,[],Plunge-n-Play,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:25.650Z,"PlusShorts is a Java applet designed to encourage and facilitate online collaboration between videomakers. The system uses punctuation as an iconic system for describing and augmenting edited video, with the intention of providing users with a level of interactivity that is both playful and meaningful.",2001-12-30,['interactive-cinema'],,17607,E15-351,2016-12-05T00:16:44.844Z,['gid@media.mit.edu'],True,plusshorts,1999-12-31,[],PlusShorts,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:25.809Z,"Pneuduino is a hardware platform for kids, students, artists, designers, and researchers who are interested in controlling air flow and pressure for their projects. The Pneuduino toolkit is currently used in workshops with high school or college students. While each workshop has a different focus, they all introduce concepts of air as actuator and sensor as well as different fabrication methods to create transforming artifacts. Air is one the most abundant resources on earth. By adding computation ability to air, we can create new types of materials that enable us to design robots that are soft, furniture that is adaptive, clothing that is intelligent, and art pieces that are breathing.",2015-12-31,['tangible-media'],,17623,--Choose Location,2020-06-09T17:13:35.865Z,"['ishii@media.mit.edu', 'liningy@media.mit.edu', 'jifei@media.mit.edu', 'heibeck@media.mit.edu']",True,pneuduino,2015-01-01,[],Pneuduino,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:25.967Z,"An enabling technology to build shape-changing interfaces through pneumatically driven, soft-composite materials. The composite materials integrate the capabilities of both input sensing and active shape output. We explore four applications: a multi-shape mobile device, table-top shape-changing tangibles, dynamically programmable texture for gaming, and a shape-shifting lighting apparatus.",2015-12-31,['tangible-media'],,17639,--Choose Location,2020-06-09T17:32:37.937Z,"['ishii@media.mit.edu', 'liningy@media.mit.edu', 'jifei@media.mit.edu']",True,pneumatic-shape-changing-interfaces,2012-09-01,[],Pneumatic Shape-Changing Interfaces,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:26.046Z,"PoCoMo is an implementation of a vision in future-projected social interfaces. In this project we try to capture the playfulness of collaborative gaming and apply it to projected interfaces. The maturing of handheld micro-projector technology, in conjunction with advanced mobile environments, enable this novel type of interaction. Our system is made of a micro-projector mobile device with a specially designed case that turns it into a first-of-a-kind handheld mini-projector-camera system. Computer Vision algorithms support collaborative interaction between multiple users of the system. Through PoCoMo, we wish to explore the social nature of projected interfaces. To accommodate this we designed the projection to be of human cartoon-like characters that play out a personal interaction. Following their human controllers, they recognize each other, wave hello, shake hands, and exchange presents. ",2012-09-01,['fluid-interfaces'],,17647,,2018-10-12T16:40:54.851Z,"['pattie@media.mit.edu', 'roys@media.mit.edu']",True,pocomo,2011-01-01,[],PoCoMo,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:26.369Z,"Point & Shoot Data explores the use of visible light as a wireless communication medium for mobile devices. A snap-on case allows users to send messages to other mobile devices based on directionality and proximity. No email address, phone number, or account login is needed, just point and shoot your messages! The project enables infrastructure-free, scalable, proximity-based communication between two mobile devices.",2012-01-01,['viral-communications'],,17679,--Choose Location,2016-12-05T00:16:45.032Z,"['trich@media.mit.edu', 'lip@media.mit.edu']",True,point-shoot-data,2011-09-01,[],Point & Shoot Data,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:26.449Z,"Imagine a room filled with electronic devices�appliances, thermostats, computers, portable music players, PDAs�all of which can communicate wirelessly. This may not even require an act of imagination. Now imagine yourself with a handheld device in the middle of all this wanting to use it to communicate with just one of these devices. How will you do it? Will you select its name from a menu on a screen? Will you tap on an icon? Either way your attention will be focused on the device in your hand, not the true target of your intention. Your selection will be a selection by metaphor. Why not simply point at the thing you want to communicate with? Pointing is a natural extension of the human capacity to focus attention. It establishes a spatial axis relative to an agent, unambiguously identifying anything in line-of-sight without a need to name it. This brings our interactions with electronic devices closer to our interactions with physical objects, which we name only when we have to. Pointable Computing proposes that a visible laser be coupled to a handheld device and cheap sensors attached to potential objects of communication. Information may be conveyed along a laser beam, establishing a channel of communication along a perceivable axis of pointing. Ultimately, the goal of this project is�through a subtle change in the focus of interaction�to foster decentralized computation in which individual computational objects scattered in the environment may be controlled directly rather than through a single hub. This kind of electronic autonomy curtails the ability of faceless authorities or proprietary standards to shape the future of computation.",2003-01-01,['aesthetics-computation'],,17687,E15-301,2016-12-05T00:16:45.069Z,[],True,pointable-computing,2002-01-01,[],Pointable Computing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:26.524Z,We are using an online poker game as a platform for studying how people share and interpret non-verbal social information online.,2017-05-01,['sociable-media'],,17695,E15-391,2022-06-10T14:41:29.980Z,['judith@media.mit.edu'],True,pokerspaces-hiding-and-revealing-social-information-online,2004-01-01,[],PokerSpaces: Hiding and Revealing Social Information Online,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:26.600Z,"We introduce polarization field displays as an optically efficient design for dynamic light field display using multi-layered LCDs. Such displays consist of a stacked set of liquid crystal panels with a single pair of crossed linear polarizers. Each layer is modeled as a spatially controllable polarization rotator, as opposed to a conventional spatial light modulator that directly attenuates light. We demonstrate that such displays can be controlled, at interactive refresh rates, by adopting the SART algorithm to tomographically solve for the optimal spatially varying polarization state rotations applied by each layer. We validate our design by constructing a prototype using modified off-the-shelf panels. We demonstrate interactive display using a GPU-based SART implementation supporting both polarization-based and attenuation-based architectures.",2015-09-15,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/polarization-fields-thumbnail.jpeg,17703,--Choose Location,2016-12-05T00:16:45.164Z,"['raskar@media.mit.edu', 'gordonw@media.mit.edu', 'naik@media.mit.edu']",True,polarization-fields-glasses-free-3dtv,2011-01-01,[],Polarization Fields: Glasses-Free 3DTV,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:26.676Z,"Liquid crystals modulate the polarization state of light, and thus a significant amount of light (and money) is lost converting light into a polarized state.  We explore as an alternative liquid crystal modes that do not require the use of a polarizer to allow liquid crystal displays to be readable in bright sunlight.",2008-01-01,[],,17711,--Choose Location,2016-12-05T00:17:20.299Z,['mlj@media.mit.edu'],True,polarization-independent-liquid-crystals-display,2005-09-01,[],Polarization-Independent Liquid Crystals Display,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:26.974Z,"The term ""polynym"" represents a theory that involves categorical division of a topic. A polynym knowledge base can enhance cultural disambiguation for automatic machine translation.",2006-01-01,['electronic-publishing'],,17743,E15-320,2016-12-05T00:16:45.134Z,['walter@media.mit.edu'],True,polynyms,2005-01-01,[],Polynyms,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:27.051Z,"Mobility in urban environments is often coupled with a quest for efficiencies such as a nearby parking spot, a ride to some destination, a date for dinner, a ticket to the theater, or any other kind of service that one might need while on the move. The idea is an ad hoc, mobile ""Craig's List."" We demonstrate a system that provides an easy way to advertise services or goods offered or sought by means of mobile phones and ultra-mobile PCs, and we are concerned with issues of ensuring trust, security, and anonymity for all participants.",2009-01-01,['viral-communications'],,17751,E15-483,2016-12-05T00:16:45.192Z,"['ypod@media.mit.edu', 'lip@media.mit.edu']",True,polyquest,2008-01-01,[],PolyQuest,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:27.207Z,"Facing issues of food crisis by overpopulation, this project explores a possible future where a small community of activists arises to design an edible cockroach that can survive in harsh environments. These genetically modified roaches are designed to pass their genes to the next generations; thus the awful black and brown roaches will be pushed to extinction by the newly designed, cute, colorful, tasty, and highly nutritional ""pop roach."" The color of these ""pop roaches"" corresponds to a different flavor, nutrition, and function, while the original ones remain black or brown, and not recommended to be eaten. How will genetic engineering shift our perception of food and eating habits? Pop Roach explores how we can expand our perception of cuisine to solve some of the world's most pressing problems.",2017-08-31,['design-fiction'],https://dam-prod2.media.mit.edu/x/files/Display/poproach1920-1080%20copy.jpg,17767,--Choose Location,2017-10-11T20:31:47.421Z,"['sputniko@media.mit.edu', 'aih@media.mit.edu']",True,pop-roach,2015-01-01,[],Pop Roach,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:11:27.450Z,"The major challenge in preventing blindness is identifying patients and bringing them to specialty care. Diseases that affect the retina, the image sensor in the human eye, are particularly challenging to address, because they require highly trained eye specialists (ophthalmologists) who use expensive equipment to visualize the inner parts of the eye. Diabetic retinopathy, HIV/AIDS-related retinitis, and age-related macular degeneration are three conditions that can be screened and diagnosed to prevent blindness caused by damage to retina. We exploit a combination of two novel ideas to simplify the constraints of traditional devices, with simplified optics and cleaver illumination in order to capture and visualize images of the retina in a standalone device easily operated by the user. Prototypes are conveniently embedded in either a mobile hand-held retinal camera, or wearable eyeglasses.",,['camera-culture'],,17791,--Choose Location,2020-06-16T22:29:59.712Z,"['raskar@media.mit.edu', 'gordonw@media.mit.edu', 'naik@media.mit.edu', 'elawson@media.mit.edu', 'olwal@media.mit.edu']",True,portable-retinal-imaging,2012-09-01,[],Portable Retinal Imaging,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:27.606Z,"Portrait of Cati 2 is an homage to the project ""Portrait of Cati"" done by Stefan Agamanolis. This portrait reacts differently to a man or a woman. While reacting to a woman in a more natural way, Cati's image is more of woman's icon as a reaction to a man.",2007-01-01,['fluid-interfaces'],,17807,E15-320,2016-12-05T00:16:45.332Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu', 'stefan@media.mit.edu']",True,portrait-of-cati-2,2006-01-01,[],Portrait of Cati 2,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:27.684Z,"PostHistory visualizes users' email mailboxes and the activity patterns within them. The idea is not to create yet another email reader, but instead is an attempt at making activity patterns within one's email account visible and visually interesting. Some of the questions in the project are: how much email do you get on any given day?; how active are you in replying, deleting, or moving messages around?; and how different does your mailbox activity look from someone else's?",2017-05-01,['sociable-media'],,17815,E15-468,2022-06-10T14:41:30.013Z,['judith@media.mit.edu'],True,posthistory,2002-01-01,[],PostHistory,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:27.760Z,"Each day we walk by silent buildings and lots. Absent landlords, hibernating developers, abandoned lots�their impact as part of the patchwork is subtle but real. Be they eyesore or brownfield, spaces are reactive. Unused spaces trap the vitality of the neighborhoods they surround. For the last 15 years, Vail Court has fallen into disrepair. The 24-unit housing complex is uninhabitable and unsafe as a structure. Conversation continues within the community, but how does the conversation flow back out�beyond city councilors to the decision-making owners? Postmarked asked neighbors and frequent passersby to fill postcards that were mailed directly to the property owners. These postcards captured the lived experience and hopes of those who share Vail Court in common. The postcards and process were documented online before being mailed�two cards per day�with an invitation for owners to respond to the community online.",2013-01-01,['civic-media'],,17823,--Choose Location,2016-12-05T00:16:45.379Z,['ethanz@media.mit.edu'],True,postmarked,2013-01-01,[],Postmarked,PUBLIC,,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:27.835Z,"We have developed a system to recognize posture patterns and associated affective states in real time, in an unobtrusive way, from a set of pressure sensors on a chair. This system discriminates states of children in learning situations, such as when the child is interested, or is starting to take frequent breaks and looking bored. The system uses pattern recognition techniques, while watching natural behaviors, to ""learn"" what behaviors tend to accompany which states. The system thus detects the surface-level behaviors (postures) and their mappings during a learning situation in an unobtrusive manner so that they don't interfere with the natural learning process. Through the chair, we can reliably detect nine static postures, and four temporal patterns associated with affective states.",2004-01-01,['affective-computing'],,17831,E15-001,2016-12-05T00:16:45.454Z,"['picard@media.mit.edu', 'win@media.mit.edu']",True,posture-recognition-chair,2001-01-01,[],Posture Recognition Chair,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:27.997Z,"The human ankle provides a significant amount of net positive work during the stance period of walking, especially at moderate to fast walking speeds. Conversely, conventional ankle-foot prostheses are completely passive during stance, and consequently, cannot provide net positive work. Clinical studies indicate that transtibial amputees using conventional prostheses experience many problems during locomotion, including a high gait metabolism, a low gait speed, and gait asymmetry. Researchers believe the main cause for the observed locomotion is due to the inability of conventional prostheses to provide net positive work during stance. The objective of this project is to develop a powered ankle-foot prosthesis that is capable of providing net positive work during the stance period of walking. To this end, we are investigating the mechanical design and control system architectures for the prosthesis. We are also conducting a clinical evaluation of the proposed prosthesis on different amputee participants.",,['biomechatronics'],https://dam-prod2.media.mit.edu/uuid/64563eb8-7d15-42b1-b466-b1ba5aa5ec28,17847,E15-054,2021-09-17T19:28:17.051Z,"['hherr@media.mit.edu', 'mcarney@media.mit.edu']",True,powered-ankle-foot-prosthesis,2004-01-01,"['robotics', 'design', 'human-computer-interaction', 'bioengineering', 'consumer-electronics', 'health', 'human-machine-interaction', 'wearable-computing', 'biomechanics', 'mechanical-engineering', 'electrical-engineering', 'engineering', 'mechatronics', 'human-augmentation', 'bionics']",Powered Ankle-Foot Prosthesis,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:28.147Z,"The Powers Sensor Chair gives visitors a special glimpse into Tod Machover�s robotic opera ""Death and the Powers,"" by providing a new way to explore the sonic world of the opera. A solo participant sitting in a chair discovers that when she moves her hands and arms, the air in front of her becomes an instrument. With a small, delicate gesture, a sharp and energetic thrust of her hand, or a smooth caress of the space around her, she can use her expressive movement and gesture to play with and sculpt a rich sound environment drawn from the opera, including vocal outbursts and murmurs and the sounds of the show�s special Hyperinstruments. This installation explores the body as a subtle and powerful instrument, providing continuous control of continuous expression, and incorporates alum Elena Jessop�s high-level analysis frameworks for recognition and extension of expressive movement.",2014-09-01,['opera-of-the-future'],,17863,--Choose Location,2016-12-05T00:16:45.403Z,"['tod@media.mit.edu', 'ejessop@media.mit.edu']",True,powers-sensor-chair,2014-01-01,[],Powers Sensor Chair,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:28.560Z,"This project focuses on the development of a low-cost system to track the precise 3-D position of large numbers of objects tagged with passive microwave RF transponders at short-range (3-100m) and in real time. The proposed system will allow selection, identification, tracking, and encoding of data from connected sensors to be relayed by the passive tags. These tags can address a multitude of applications, including new human-computer interfaces, robot control, animation and virtual reality, gaming, low-cost tracking of machinery and animals, search and rescue, and warehouse monitoring.",2008-09-01,['responsive-environments'],,17911,E15-344,2016-12-05T00:17:20.687Z,['joep@media.mit.edu'],True,precision-wide-area-tracking-of-small-passive-rf-tags,2005-09-01,[],"Precision, Wide-Area Tracking of Small, Passive RF Tags",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:28.636Z,"'PreCursor' is an invisible layer that hovers in front of the screen and enables novel interaction that reaches beyond the current touchscreens. Using a computer mouse provides two levels of depth when interacting with content on a screen: one can either just hover or click. Hovering allows receiving short descriptions, while clicking selects or performs an action. PreCursor provides this missing sense of interaction to touchscreens. PreCursor technology has the potential to expand beyond a basic computer screen. It can also be applied to mobile touchscreens, to objects in the real world, or can be the launching pad for creating a 3D space for interaction.",2012-01-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/precursor.jpg,17919,--Choose Location,2016-12-05T00:16:45.535Z,"['pattie@media.mit.edu', 'pranav@media.mit.edu']",True,precursor,2011-01-01,[],PreCursor,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:28.884Z,"The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration. We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.",2018-12-31,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/2016/12/13/AffectiveComputingHero.png,17943,--Choose Location,2022-05-12T17:19:40.703Z,"['picard@media.mit.edu', 'akanes@media.mit.edu', 'sataylor@media.mit.edu', 'jaquesn@media.mit.edu', 'ehinosa@media.mit.edu', 'asma_gh@media.mit.edu', 'azaria@media.mit.edu']",True,predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data,2015-01-01,"['data', 'health', 'machine-learning', 'affective-computing', 'wellbeing']","Predicting students' wellbeing from physiology, phone, mobility, and behavioral data",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:28.970Z,"People use cues for determining appropriate moments for interruption. If somebody just finished talking on the telephone, that action might be a good indication to knock at their door. What cue detail level is necessary for people to make appropriate interruption decisions? We explored if humans are capable of inferring interruption times with an experiment that presented a series of screenshots from people navigating Websites. Subjects were asked to select three possible interruption times based on mouse activity. Mouse movement trajectories can indicate how information is interpreted while browsing a Webpage. Slow and arched trajectories as users move their mouse would indicate an ambiguous state of mind.",2007-01-01,['context-aware-computing'],,17951,E15-320,2016-12-05T00:16:45.640Z,[],True,predicting-web-interruptibility,2004-01-01,[],Predicting Web Interruptibility,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:29.046Z,"We have developed a novel market game, Prediction Game and Experience Sharing (PreGES, pronounced PreGuess), that harnesses people's collective prediction and experience sharing to forecast success or failure of new items (e.g., products, services, UI designs). Companies can register their new items on this market (as a testbed) to ask for collective opinions. In each PreGES trial session, participants makes their own best predictions on other people's overall opinions about the new items to get incentives (e.g., real opportunities to experience the items) and have fun in gambling-like games. As a participant�s guess (or portfolio) approaches the collective guess of all participants, he or she has a greater chance of winning an incentive. Participants improve the accuracy of their next prediction by sharing experiences. As participants have more trial sessions, their collective prediction converges into one common opinion (forecasting the success or failure of new items).",2010-01-01,['affective-computing'],,17959,E15-443,2016-12-05T00:16:10.716Z,"['picard@media.mit.edu', 'rmorris@media.mit.edu']",True,prediction-game-and-experience-sharing-market-for-forecasting-marketplace-success,2008-09-01,[],Prediction Game and Experience Sharing Market for Forecasting Marketplace Success,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:11:29.123Z,"A critical question relevant to the increasing importance of crowd-sourced-based finance is how to optimize collective information processing and decision-making. Here, we investigate an often under-studied aspect of the performance of online traders: beyond focusing on just accuracy, what gives rise to the trade-off between risk and accuracy at the collective level? Answers to this question will lead to designing and deploying more effective crowd-sourced financial platforms and to minimizing issues stemming from risk such as implied volatility. To investigate this trade-off, we conducted a large online Wisdom of the Crowd study where 2037 participants predicted the prices of real financial assets (S&P 500, WTI Oil and Gold prices). Using the data collected, we modeled the belief update process of participants using models inspired by Bayesian models of cognition. We show that subsets of predictions chosen based on their belief update strategies lie on a Pareto frontier between accuracy and risk, mediated by social learning. We also observe that social learning led to superior accuracy during one of our rounds that occurred during the high market uncertainty of the Brexit vote.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2021/07/13/entropy-23-00801-g001_bLgHAHF.webp,17967,--Choose Location,2021-07-13T14:04:06.845Z,"['sandy@media.mit.edu', 'dhaval@media.mit.edu', 'pkrafft@media.mit.edu', 'emoro@media.mit.edu', 'cshikai@media.mit.edu', 'yleng@media.mit.edu']",True,prediction-markets-leveraging-internal-knowledge-to-beat-industry-prediction-experts,2015-01-01,[],Accuracy-Risk Trade-Off Due to Social Learning in Crowd-Sourced Financial Predictions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:29.202Z,"Presentations in virtual environments tend to suffer from a number of common problems: presenters are easily overwhelmed by unfamiliar audience activity, communication among audience members is often difficult and distracting to the presenter, and managing what you're looking at is difficult for both presenters and audience members.  In this project, we designed a set of standalone components for Sun Microsystem's Project Wonderland virtual environment that can be combined into a system that addresses these core issues with presentations by taking advantage of the spatial properties of virtual worlds and creating presentation experiences that are more than recreations of face to face experiences. This work was done at Sun Labs in collaboration with the Project Wonderland research group.",2009-09-01,['living-mobile'],,17975,E15-368,2016-12-05T00:16:45.666Z,['geek@media.mit.edu'],True,presentation-spaces,2009-01-01,[],Presentation Spaces,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:29.282Z,"Lyme disease is the most common vector-borne infection in North America. People are infected when bitten by ticks; ticks are typically infected when they bite white-footed mice, the primary ""reservoir"" of the disease. We are exploring the possibility of permanently immunizing mouse populations to block transmission by making and releasing mice that produce protective mouse antibodies from birth and pass immunity on to their pups. The project has been guided by representatives in offshore island communities from inception. Communities will choose which type of antibodies, pick uninhabited islands to serve as field trial sites, select independent monitors, and ultimately decide whether to volunteer their own islands for the next stage. If successful, prevention could be expanded to the mainland using local or global gene drive systems. Whether or not communities decide to proceed, we hope the process will become a model for responsive science worldwide.",,"['sculpting-evolution', 'media-lab-research-theme-future-worlds']",,17983,--Choose Location,2023-09-07T22:36:50.855Z,"['esvelt@media.mit.edu', 'buchthal@media.mit.edu']",True,preventing-tick-borne-disease-by-permanently-immunizing-mice,2016-01-01,"['agriculture', 'bioengineering', 'crowdsourcing', 'environment', 'genetics', 'health', 'synthetic-biology', 'ethics', 'community', 'biology', 'technology', 'wellbeing']",Mice Against Ticks - Preventing tick-borne disease by permanently immunizing mice,PUBLIC,https://www.responsivescience.org/mice-against-ticks,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:29.358Z,"Neurological and psychiatric disorders afflict over one billion people worldwide, presenting annual costs exceeding $1 trillion. What are the principles of controlling neural circuits, in order to improve their functions and overcome intractable neurological and psychiatric disorders? We have invented cell-type-specific optical neural control technologies, and with them we are seeking to parse out the methods with which to fix activity in aberrant neural circuits, correcting the computational dynamics within, in order to discover new principles of treating neural disease.",2011-01-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/files/Display/synthneuro.jpg,17991,E15-435,2016-12-05T00:16:10.749Z,"['esb@media.mit.edu', 'henninge@media.mit.edu', 'jberns@media.mit.edu', 'geggio@media.mit.edu', 'bdallen@media.mit.edu']",True,principles-of-controlling-neural-circuits,2009-01-01,[],Principles of Controlling Neural Circuits,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:29.547Z,"Microencapsulated electrophoretic displays offer low power, flexibility, high contrast, bistability, and flexibility for a wide range of applications including electronic books, signs, and portable handheld devices.",2005-01-01,['molecular-machines'],,18007,E15-015,2016-12-05T00:16:45.730Z,"['jacobson@media.mit.edu', 'hamad@media.mit.edu']",True,printed-displays,2000-01-01,[],Printed Displays,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:29.633Z,"We are developing tabletop and desktop printers capable of directly printing both structure and function in two and three dimensions. To date, we have printed actuators, logic, and display media. Such printers may be useful as personal fabricators, giving great design freedom and fabrication capability to the individual, as well as a means of ultra-cheap manufacture of oligo-layered electromechanical and electronic structures.",2005-01-01,['molecular-machines'],,18015,--Choose Location,2016-12-05T00:16:45.608Z,"['jacobson@media.mit.edu', 'neilg@media.mit.edu']",True,printed-electro-mechanical-systems-pems,2000-01-01,[],Printed Electro-Mechanical Systems (PEMS),PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:29.714Z,"We are developing materials and methods for the printing of high-quality, inorganic semiconductor devices. By exploiting the reduced melting point and high solubility of nanoparticles, we have demonstrated that inorganic materials can be processed at plastic-compatible temperatures. Coupled with our efforts in sub-micron printing, microelectronic devices produced from nanoparticle-based inks promise to enjoy the processing advantages usually associated with organic materials while retaining the performance advantages typically associated with inorganic materials.",2005-01-01,['molecular-machines'],,18023,E15-015,2016-12-05T00:16:45.693Z,"['jacobson@media.mit.edu', 'hamad@media.mit.edu']",True,printed-electronics,2000-01-01,[],Printed Electronics,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:29.884Z,The printing of nanoparticle-based inks has been extended to produce three-dimensional structures consisting of hundreds of layers. Linear drive motors and thermal actuators have been demonstrated.,2005-01-01,['molecular-machines'],,18039,E15-015,2016-12-05T00:17:20.840Z,"['jacobson@media.mit.edu', 'hamad@media.mit.edu']",True,printed-machines,2000-01-01,[],Printed Machines,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:29.972Z,"Holographic displays offer many advantages, including comfort and maximum realism. In this project we adapt our guided-wave light-modulator technology to see-through lenses to create a wearable 3D display suitable for augmented or virtual reality applications. As part of this work we also are developing a femtosecond-laser-based process that can fabricate the entire device by ""printing.""",2018-09-01,"['ultimate-media', 'object-based-media']",,18047,--Choose Location,2020-06-10T16:23:51.977Z,"['vmb@media.mit.edu', 'sjolly@media.mit.edu', 'nsavidis@media.mit.edu', 'bdatta@media.mit.edu']",True,printed-wearable-holographic-display,2015-09-01,"['consumer-electronics', 'holography']",Printed Wearable Holographic Display,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:30.058Z,"How can biological organisms be incorporated into product, fashion, and architectural design to enable the generation of multi-functional, responsive, and highly adaptable objects? This research pursues the intersection of synthetic biology, digital fabrication, and design. Our goal is to incorporate engineered biological organisms into inorganic and organic materials to vary material properties in space and time. We aim to use synthetic biology to engineer organisms with varied output functionalities and digital fabrication tools to pattern these organisms and induce their specific capabilities with spatiotemporal precision.",2020-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/TEM%20image%20ecoli.png,18055,--Choose Location,2021-05-21T13:17:22.123Z,"['stevenk@media.mit.edu', 'ssunanda@media.mit.edu', 'moonshot@media.mit.edu', 'neri@media.mit.edu']",True,printing-living-materials,2014-01-01,[],Printing Living Materials,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:11:30.143Z,"Computation and fabrication in biology occur in aqueous environments. Through on-chip mixing, analysis, and fabrication, microfluidic chips have introduced new possibilities in biology for over two decades. Existing construction processes for microfluidics use complex, cumbersome, and expensive lithography methods that produce single-material, multi-layered 2D chips. Multi-material 3D printing presents a promising alternative to existing methods that would allow microfluidics to be fabricated in a single step with functionally graded material properties. We aim to create multi-material microfluidic devices using additive manufacturing to replicate current devices, such as valves and ring mixers, and to explore new possibilities enabled by 3D geometries and functionally graded materials. Applications range from medicine to genetic engineering to product design.",2016-05-31,['mediated-matter'],,18063,--Choose Location,2021-05-21T13:27:52.280Z,"['stevenk@media.mit.edu', 'moonshot@media.mit.edu', 'neri@media.mit.edu']",True,printing-multi-material-3d-microfluidics,2014-01-01,[],Printing Multi-Material 3D Microfluidics,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:30.303Z,"Touch sensing has become established for a range of devices and systems both commercially and in academia. In particular, multi-touch scenarios based on flexible sensing substrates are popular for products and research projects. We leverage recent developments in single-layer, off-the-shelf, inkjet-printed conductors on flexible substrates as a practical way to prototype the necessary electrode patterns, and combine this with our custom-designed PrintSense hardware module which uses the full range of sensing techniques. Not only do we support touch detection, but in many scenarios also pressure, flexing, and close proximity gestures.",2014-01-01,['responsive-environments'],,18079,--Choose Location,2016-12-05T00:16:10.790Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,printsense-a-versatile-sensing-technique-to-support-flexible-surface-interaction,2013-01-01,[],PrintSense: A Versatile Sensing Technique to Support Flexible Surface Interaction,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:30.459Z,"Computers usually require us to be precise about what we want them to do and how we want them to do it, but humans find it hard to be so formal. If we were to give computers formal examples of our informal instructions, maybe they could learn to relate the natural instructions of ordinary users with the specifications, code, and tests with which they are comfortable. Zones and ProcedureSpace are examples of this. Zones is a code search interface that connects code with comments about its purpose. Completed searches become annotations, so the system learns by example. The backend, ProcedureSpace, finds code for a purpose comment (or vice versa) by relating words and phrases to code characteristics and natural language background knowledge. Users of the system are able to describe what they want in their own words, and often find that the system gives them helpful code.",2014-09-01,['software-agents'],,18095,E15-385,2022-06-10T14:18:57.581Z,['lieber@media.mit.edu'],True,procedurespace-managing-informality-by-example,2009-01-01,[],ProcedureSpace: Managing Informality by Example,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:30.536Z,"This work presents an architecture for developing Web-based design tools that  automatically generate searchable tutorials for every document created. The goal of these tools is to integrate Web-based tutorial annotation, search, and viewing software within the creative tools themselves. By making the process of creating  tutorial material implicit, a large, searchable, and application-specific knowledge bank would be generated on the Web. This knowledge bank, when searched from within the creative tool itself, allows quick access to a wide array of commands,  gestures, and annotations. The work includes an analysis and evaluation of current tutorial techniques, and a synthesis of design principles for integrating an evolving, collaborative knowledge base of tutorials into digital art  and design tools.   ",2005-01-01,['information-ecology'],,18103,E15-301,2016-12-05T00:16:45.817Z,[],True,process-makes-perfect,2005-01-01,[],Process Makes Perfect,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:30.614Z,"As part of the Toys to Think With project, we built several strings of Programmable Beads: computational jewelry that children (of all ages) can play with and learn from. Each bead is a tiny computer, able to talk to its neighbors and to change color. The Programmable Beads were an attempt to extend classic bead play activity to include exploration of dynamic patterns  and we hoped that Programmable Beads, like the non-programmable kind, would be particularly attractive to girls, as girls as a group have usually been under-served by developers of technology.   
",1999-09-01,['lifelong-kindergarten'],,18111,--Choose Location,2016-12-05T00:16:46.092Z,"['mres@media.mit.edu', 'borovoy@media.mit.edu']",True,programmable-beads,1995-01-01,[],Programmable Beads,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:30.773Z,"We are extending the child's construction kit, building computational power directly into LEGO bricks. Children are using these Programmable Bricks to build everything from robotic creatures to interactive kinetic sculptures, and, in the process, learning about engineering and design. We are currently developing a family of Programmable Bricks, known as Crickets, that designed especially for integrating art and engineering. The LEGO company now sells a product (called MindStorms) based on our Programmable Brick research.",2005-09-01,['lifelong-kindergarten'],,18127,E15-001,2016-12-05T00:17:20.932Z,"['mres@media.mit.edu', 'borovoy@media.mit.edu', 'bss@media.mit.edu']",True,programmable-bricks,1992-01-01,[],Programmable Bricks,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:30.944Z,"Programmable Paintings are a series of artworks that use electronic elements such as LED lights and microphone sensors as ""pigments"" in paintings. The goal is to blend traditional elements of painting—color, texture, composition—with these electronic components to create a new genre of time-based and interactive art.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/dandelion_logo.png,18143,--Choose Location,2017-10-16T15:13:03.200Z,"['leah@media.mit.edu', 'jieqi@media.mit.edu', 'joep@media.mit.edu']",True,programmable-paintings,2012-01-01,[],Programmable Paintings,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:31.025Z,We are creating consumer-grade appliances and authoring methodologies that will allow hallucinatory phenomena to be programmed and utilized for information display and narrative storytelling.,2019-02-01,[],https://dam-prod2.media.mit.edu/x/2019/04/16/01-holdens_eye.jpg,18151,--Choose Location,2020-06-10T16:24:45.769Z,"['vmb@media.mit.edu', 'novysan@media.mit.edu']",True,programmable-synthetic-hallucinations,2015-01-01,"['consumer-electronics', 'gaming', 'interfaces']",Programmable Synthetic Hallucinations,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:31.260Z,"This project creates an environment that allows young children access to programming. They can program a robot and see its movements but are also presented with a more abstract, graphical representation of the physical movement. In addition to the graphical image, Logo code that enables the movement is generated. Having the three versions of the same program allows children to transition from a concrete method of programming a robot to a symbolic programming language. The children can program with any of the methods and experiment with the relationships between physical and graphical representations.",2004-01-01,['future-of-learning-2'],,18167,E15-320,2016-12-05T00:16:46.156Z,['cavallo@media.mit.edu'],True,programming-continuum,2000-01-01,[],Programming Continuum,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:31.349Z,"We want to build programming systems that can converse with their users to build computer programs. Such systems will enable users without programming expertise to write programs using natural language. The text-based, virtual-world environments called the MOO (multi-user, object-oriented Dungeons and Dragons) allow their users to build objects and give them simple, interactive, text-based behaviors. These behaviors allow other participants in the environment to interact with those objects by invoking actions and receiving text messages. Through our natural-language dialogue system, the beginning programmer will be able to describe objects and the messages in MOO environments.",2014-09-01,['software-agents'],,18175,E15-383,2022-06-10T14:18:57.594Z,['lieber@media.mit.edu'],True,programming-in-natural-language,2003-09-01,[],Programming in Natural Language,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:31.595Z,"Project Voyager uses wirelessly networked devices to create Personal Shopping Assistants that deliver compelling Web services to customers in a supermarket.  Our research explores how to associate ""virtual"" services, such as personalized discounts, a recipe finder, a product recommendation agent, and mobile point of sales with products in a store. As such, Project Voyager aspires to build an ""Internet Presence"" for people, places, and things, thus building a bridge between the virtual and physical worlds we inhabit.",2001-12-30,"['counter-intelligence', 'personal-information-architecture']",,18199,E15-468,2016-12-05T00:16:45.981Z,['mike@media.mit.edu'],True,project-voyager,1999-12-31,[],Project Voyager,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:31.684Z,"Promise Tracker is a citizen-monitoring platform designed to help communities track issues they care about and use that information to advocate for change with local government, institutions or the press. Using a simple web application, community groups can design a mobile phone-based survey, distribute the survey to community members’ phones, collect data using a mobile app, visualize it on a map, and use the resulting data to advocate for change. We are currently partnering with civil society groups, universities, and government oversight agencies in Brazil who are implementing  Promise Tracker as part of multi-sector alliances to monitor public spending and services.  Key collaborators include: University of São Paulo's CoLaboratory for Development and ParticipationMinistry of Transparency, Oversight and the Comptroller-GeneralFederal University of Pará's Laboratory for Innovation and Oversight in the Public SectorSocial Observatory of Belém Project SOLHumanitas360",2018-12-31,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/13/IMG_1234_NhYcewq.JPG,18207,--Choose Location,2021-12-21T19:25:57.992Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'emreiser@media.mit.edu', 'joyab@media.mit.edu', 'ahope@media.mit.edu', 'jmwenda@media.mit.edu', 'adelineg@media.mit.edu']",True,promise-tracker,2013-01-01,[],Promise Tracker,PUBLIC,http://promisetracker.org,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:31.766Z,"Promiserver provides toolkits and services for online micro-contracts that are socially constructed, community enforced, and procedurally executed. The project aims to offer a sensible, lightweight, and agile promise system as an alternative to traditionally heavy legal commitments, and to facilitate new models of collaborative business by reducing transaction costs and improving market fluidity.",2007-09-01,['information-ecology'],,18215,E15-301,2016-12-05T00:16:46.133Z,['holtzman@media.mit.edu'],True,promiserver,2007-01-01,[],Promiserver,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:31.844Z,"Cooperation in a large society of self-interested individuals is notoriously difficult to achieve when the externality of one individual's action is spread thin and wide on the whole society (e.g., in the case of pollution). We introduce a new approach to achieving global cooperation by localizing externalities to one's peers in a social network, thus leveraging the power of peer-pressure to regulate behavior. Global cooperation becomes more like local cooperation.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/03/22/figure_externality_local.jpg,18223,--Choose Location,2020-07-23T18:50:11.314Z,['irahwan@media.mit.edu'],True,promoting-cooperation-through-peer-pressure,2015-01-01,[],Promoting cooperation through peer pressure,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:31.994Z,"Sensor networks permeate our built and natural environments, but our means for interfacing to the resultant data streams have not evolved much beyond HCI and information visualization. Researchers have long experimented with wearable sensors and actuators on the body as assistive devices. A user's neuroplasticity can, under certain conditions, transcend sensory substitution to enable perceptual-level cognition of ""extrasensory"" stimuli delivered through existing sensory channels. But there remains a huge gap between data and human sensory experience. We are exploring the space between sensor networks and human augmentation, in which distributed sensors become sensory prostheses. In contrast, user interfaces are substantially unincorporated by the body, our relationship to them never fully pre-attentive. Attention and proprioception are key, not only to moderate and direct stimuli, but also to enable users to move through the world naturally, attending to the sensory modalities relevant to their specific contexts.",,['responsive-environments'],,18239,--Choose Location,2019-04-19T14:31:47.208Z,"['gershon@media.mit.edu', 'joep@media.mit.edu']",True,prosthetic-sensor-networks-factoring-attention-proprioception-and-sensory-coding,2013-01-01,[],"Prosthetic Sensor Networks: Factoring attention, proprioception, and sensory coding",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:32.072Z,"The quality of communication, in all its various forms, is heavily dependent on the appropriate medium, timing, and level of trust between parties. Protocol is a tool that helps you, and society at large, understand and share preferencespersonal or professional protocolto communicate more effectively. Protocol is a dynamic and syndicated contact sheet, seen most often as an email signature or status update; it also functions as an independent site. More specifically, we see it as a preference page to share how and where you want to be contacted. Soon you will be able to embed your protocol in your site of choice (hey about.me, flavours.me, linkedin.com and wordpress.com).",2011-01-01,['information-ecology'],,18247,,2016-12-05T00:17:21.094Z,['holtzman@media.mit.edu'],True,protocol,2011-01-01,[],Protocol,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:32.149Z,"An assortment of everyday objects is given the ability to understand multitouch gestures of the sort used in mobile-device user interfaces, enabling people to use such increasingly familiar gestures to control a variety of objects, and to ""copy"" and ""paste"" configurations and other information among them.",2012-01-01,['object-based-media'],,18255,--Choose Location,2016-12-05T00:17:21.144Z,['vmb@media.mit.edu'],True,prototouch-multitouch-interfaces-to-everyday-objects,2011-01-01,[],ProtoTouch: Multitouch Interfaces to  Everyday Objects,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:32.227Z,"New technologies for recording neural activity, controlling neural activity, or building brain circuits, may be capable someday of serving in therapeutic roles for improving the health of human patients - enabling the restoration of lost senses, the control of aberrant or pathological neural dynamics, and the augmentation of cognition and empathy, through prosthetic means. High throughput molecular and physiological analysis methods may also open up new diagnostic possibilities. We are inventing new noninvasive methods for targetedly controlling brain dynamics in living human subjects, and also exploring novel ways of reading activity from the brain in noninvasive fashion. We are assessing, often in collaborations with other groups, the translational possibilities opened up by our technologies, exploring the safety and efficacy of our technologies in multiple animal models, in order to discover potential applications of our tools to various clinically relevant scenarios. New kinds of ""brain co-processor"" may be possible which can work efficaciously with the brain to augment its computational abilities, e.g. in the context of cognitive, emotional, sensory, or motor disability.",2020-07-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/2016/07/18/translation.large.jpg,18263,--Choose Location,2020-09-10T17:27:49.918Z,['esb@media.mit.edu'],True,prototype-strategies-for-treating-brain-disorders,2011-01-01,"['bioengineering', 'health', 'neurobiology', 'synthetic-biology', 'imaging']",Prototype strategies for treating brain disorders,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:11:32.303Z,"We have trouble controlling our consumer impulses, and there's a gap between our decisions and the consequences. When we pull a product off the shelf, do we know our bank-account balance, or whether we're over budget for the month? Our existing senses are inadequate to warn us. The Proverbial Wallet fosters a financial sense at the point of purchase by embodying our electronically tracked assets. We provide tactile feedback reflecting account balances, spending goals, and transactions as a visceral aid to responsible decision-making.",2013-09-01,['information-ecology'],,18271,E15-301,2016-12-05T00:16:46.178Z,"['holtzman@media.mit.edu', 'daniell@media.mit.edu', 'jkestner@media.mit.edu']",True,proverbial-wallets,2008-09-01,[],Proverbial Wallets,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:32.387Z,"Would you know if a dear, but seldom seen, friend happened to be on the same train as you? The Proximeter is both an agent that tracks the past and future proximity of one�s social cloud, and an instrument that charts this in an ambient display. By reading existing calendar and social-network feeds of others, and abstracting these into a glanceable pattern of paths, we hope to nuture within users a social proprioception and nudge them toward more face-to-face interactions when opportunities arise.",2009-09-01,['information-ecology'],,18279,E15-301,2016-12-05T00:16:46.281Z,"['holtzman@media.mit.edu', 'jkestner@media.mit.edu']",True,proximeter-an-ambient-social-navigation-instrument,2009-01-01,[],Proximeter: An Ambient Social Navigation Instrument ,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:32.467Z,"A crucial part of Montessori education is observation of the students, so teachers can assist individuals and structure the environment as needed. Our work aims to assist this observation by measuring proximity of students through Simblee COM sensors. We provide detailed visualizations in a dashboard-style interface to both teachers and parents. This dashboard helps teachers individualize their own methods to facilitate a child's growth in the classroom.",2017-05-01,['social-computing'],,18287,--Choose Location,2022-06-10T14:40:02.073Z,"['sdkamvar@media.mit.edu', 'saquib@media.mit.edu', 'aybose@media.mit.edu']",True,proximity-networks,2016-09-01,[],Proximity Networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:32.550Z,"We present PsychicVR, a proof-of-concept system that integrates a brain-computer interface device and virtual reality headset to improve mindfulness while enjoying a playful immersive experience. The fantasy that any of us could have superhero powers has always inspired us, and by using virtual reality and real-time brain activity sensing we are moving one step closer to making this dream real. We non-invasively monitor and record the electrical activity of the brain and incorporate this data into the VR experience using an Oculus Rift and the MUSE headband. By sensing brain waves using a series of EEG sensors, the level of activity is fed back to the user via 3D content in the virtual environment. When users are focused, they are able to make changes in the 3D environment and control their powers. Our system increases mindfulness and helps achieve higher levels of concentration while entertaining the user.",2022-08-01,"['advancing-wellbeing', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2018/10/18/muse_portada_A7IRlgV.jpg,18295,--Choose Location,2022-08-09T00:59:10.217Z,"['pattie@media.mit.edu', 'novysan@media.mit.edu', 'amores@media.mit.edu', 'xavib@media.mit.edu']",True,psychicvr,2015-09-01,"['augmented-reality', 'health', 'technology']",PsychicVR: Increasing mindfulness by using Virtual Reality and Brain Computer Interfaces,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:32.631Z,"The standard model of labor in economics assumes that work is aversive and that the employer, therefore, has to pay the employees money to compensate them for the disutility they incur when producing labor. While this basic model approximates the provision of some types of jobs (smelling shoes to diagnose odor problems, collecting garbage, lifting bricks), it seems to mis-characterize the motivations people have in taking jobs requiring more intellectual abilities, thinking, or creativity. Professors, physicians, teachers, and engineers are but a few examples of professions that fulfill aspects of one�s life that are not captured by the standard model. We aim to develop new frameworks, based on the psychology of labor, from which to understand the complex and important problem of compensation and its relationship to motivation, effort, and loyalty.",2008-01-01,['erationality'],,18303,E15-314,2016-12-05T00:17:21.172Z,[],True,psychology-of-labor,2004-01-01,[],Psychology of Labor,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:11:32.701Z,"Money is a wonderful invention. It lets people specialize, save, and trade. At the same time, money is very abstract, making it very hard to think about the ""shadow value"" of money (all other possible uses for money). In this project, we are investigating the ways in which individuals think about money, and in particular about the ways in which the psychology of money deviates from standard utility.",2007-01-01,['erationality'],,18311,E15-314,2016-12-05T00:16:46.262Z,[],True,psychology-of-money,2004-01-01,[],Psychology of Money,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:33.282Z,"PubPub reinvents publication to align with the way the web was designed: collaborative, evolving, and open. PubPub uses a graphical format that is deliberately simple and allows illustrations and text that are programs as well as static PDFs. The intention is to create an author-driven,  distributed alternative to academic journals that is tuned to the dynamic nature of many of our modern experiments and discoveries. It is optimized for public discussion and academic journals, and is being used for both.  It is equally useful for a newsroom to develop a story that is intended for both print and online distribution.",2019-08-31,"['ultimate-media', 'viral-communications', 'collective-learning']",,18383,--Choose Location,2020-08-25T17:01:34.587Z,"['hidalgo@media.mit.edu', 'trich@media.mit.edu', 'kzh@media.mit.edu', 'thariq@media.mit.edu', 'lip@media.mit.edu']",True,pubpub,2015-01-01,"['communications', 'networks']",PubPub,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:33.370Z,"Pulp-Based Computing is a series of explorations that combine smart materials, papermaking, and printing. By integrating electrically active inks and fibers during the papermaking process, it is possible to create sensors and actuators that behave, look, and feel like paper. These composite materials not only leverage the physical and tactile qualities of paper, but can also convey digital information, spawning new and unexpected application domains in ubiquitous and pervasive computing at extremely affordable costs.",2012-01-01,['fluid-interfaces'],,18391,E15-320,2016-12-05T00:17:21.207Z,"['pattie@media.mit.edu', 'marcelo@media.mit.edu']",True,pulp-based-computing-a-framework-for-building-computers-out-of-paper,2009-01-01,[],Pulp-Based Computing: A Framework for Building Computers Out of Paper,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:33.445Z,"The Pushpin Computing project demonstrates several concepts in distributed, embedded, and ubiquitous computing and dense sensor networks. Small pushpin-sized computers, each with two pins resembling thumbtacks, draw their power from a layered polyurethane/aluminum composite into which they are pushed. Communication between pushpins is local and neighbor-to-neighbor via IR. Current work with the Pushpin testbed includes distributed algorithm prototyping, building middleware to allow users to interface with the network, and integrating the Pushpins into a wider, more heterogeneous sensor network.",2005-01-01,['responsive-environments'],,18399,E15-344,2016-12-05T00:17:21.246Z,['joep@media.mit.edu'],True,pushpin-computing,2001-01-01,[],Pushpin Computing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:33.551Z,"How can one understand and visualize the lifestyle of a person on the other side of the world? Puzzlaef attempts to tackle this question through a mobile picture puzzle game, which users collaboratively solve with pictures from their lifestyles.",2012-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/puzzle-logo.jpg,18407,--Choose Location,2016-12-05T00:16:46.433Z,['geek@media.mit.edu'],True,puzzlaef,2011-01-01,[],Puzzlaef,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:33.631Z,"Qooqle allows people to reshape their interactions with computing and reorganize the world�s information through their casual conversations and habitual gestures. Qooqle combines mobile, cloud, and social media to draw people closer to computing and make computers more invisible. The multi-modal user interface of Qooqle allows people to engage with one another and the information world more naturally.",2012-01-01,['information-ecology'],,18415,,2016-12-05T00:16:46.462Z,['holtzman@media.mit.edu'],True,qooqle,2010-09-01,[],Qooqle,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:33.709Z,"Online games bring geographically separated people together in a virtual world. Board games bring people together in the physical world. QRtcullis is a game that does both. It is a place-based, multi-player, social game that can be played on any smartphone, tablet, or computer. It is a remix of the fort-explorer genre, updated to have a connection to the physical world through QR code-based markers. Play can be affected by real-world environment factors. Each fort can be populated with various objects that either help or hinder other players. Completing the fort may require solving a maze or other puzzles, answering riddles, or finding answers to questions about the real world. The goal is to find and complete as many forts as possible. QR codes will be placed throughout the Media Lab and on a screen in the Viral Spaces area where you can see and interact with all the players.",2011-01-01,"['information-ecology', 'viral-communications']",,18423,,2016-12-05T00:16:46.483Z,"['holtzman@media.mit.edu', 'lip@media.mit.edu']",True,qrtcullis,2011-01-01,[],QRtcullis,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:33.806Z,"Aerial imaging and sensor nodes each present a unique view point into the world. Using Unmanned Aerial Vehicles (UAVs) to navigate the same space shared by sensor networks, this project aims explore interaction between two historically disparate systems for purposes of both immersive experiences and scientific research. Users will remotely navigate landscapes using latest generation of head-mounted dsplays and see real-time sensor data as an augmented overlay with a real-time video stream from the UAV.",2016-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2017/05/17/Screen Shot 2017-05-17 at 7.07.45 PM.png,18431,--Choose Location,2017-05-17T23:11:48.250Z,"['vram@media.mit.edu', 'ddh@media.mit.edu', 'joep@media.mit.edu']",True,quadra-sense-confluence-of-uavs-enviornmental-sensor-networks-and-augmented-reality,2015-01-01,['systems'],"Quadra-Sense: Confluence of UAVs, Enviornmental Sensor Networks, and Augmented Reality",PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:33.884Z,"When bits meet atoms, new modes of computation, communication, and collaboration arise from the transition to a different world of physics: quantum mechanics. How do we build computers from atoms and molecules? Can we construct a universal, digital abstraction for describing quantum systems? These are the challenges facing our project.",2004-09-01,['quanta'],,18439,--Choose Location,2016-12-05T00:16:46.500Z,"['ike@media.mit.edu', 'neilg@media.mit.edu']",True,quanta,2000-01-01,[],quanta,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:11:33.964Z,"QUANTIFY is a generalized framework and JavaScript library to allow rapid multi-dimensional ""measurement"" of subjective qualities of media. The goal is to make qualitative metrics quantized. For everything from measuring emotional responses of content to the cultural importance of world landmarks, QUANTIFY helps to elicit the raw human subjectivity that fills much of our lives, and makes it programmatically actionable. ",2015-09-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/quantLogo.png,18447,--Choose Location,2016-12-05T00:16:46.569Z,"['hidalgo@media.mit.edu', 'trich@media.mit.edu', 'kzh@media.mit.edu', 'lip@media.mit.edu']",True,quantify,2014-01-01,[],QUANTIFY,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:11:34.158Z,"Inspired by previous work in the field of data sonification, we built a data-driven composition platform that enables users to map collision event information from experiments in high-energy physics to audio properties, and thus make music from real-time data. The tool is used for outreach purposes, allowing physicists and composers to interact with collision data through novel interfaces. Three real-time compositions were streamed from May 2016–July 2016. Two additional compositions are streamed in fall 2018. This project can inspire the development of strategic mappings that facilitate the auditory perception of hidden regularities in high-dimensional datasets, and one day evolve into a useful analysis tool for physicists as well, possibly for the purpose of monitoring slow control data in experiment control rooms. The project is accessible at Quantizer.media.mit.edu.",,[],https://dam-prod2.media.mit.edu/x/2018/09/27/Screen Shot 2018-09-27 at 3.39.20 PM.jpg,18463,--Choose Location,2018-10-09T20:36:43.077Z,"['cherston@media.mit.edu', 'joep@media.mit.edu']",True,quantizer-sonification-platform-for-high-energy-physics-data,2015-01-01,"['data', 'music', 'networks', 'sensors']",Quantizer: Sonification Platform for High-Energy Physics Data,PUBLIC,http://www.quantizer.media.mit.edu,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:34.251Z,"A single atom provides a superb single quantum bit. We are now building traps to contain and move single atoms using electrodynamic fields, so that computation can be done using narrow atomic resonances addressed by laser excitation.",2008-01-01,[],,18471,E15-427,2016-12-05T00:17:21.340Z,['ike@media.mit.edu'],True,quantum-computation-with-trapped-atoms,2004-09-01,[],Quantum Computation with Trapped Atoms,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:34.342Z,"The von Neumann architecture is the foundation of all modern microprocessor design. What is the analogue which will drive quantum computation, and enable efficient quantum processors? We have built a large-scale system for simulating and testing architectural designs for quantum computers, and are evaluating fault-tolerant quantum computation schemes.",2008-01-01,[],,18479,E15-427,2016-12-05T00:16:10.866Z,['ike@media.mit.edu'],True,quantum-computer-architecture,2003-09-01,[],Quantum Computer Architecture,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:34.428Z,"What new computation, communication, and cryptography capabilities are enabled by use of quantum physics?  We have shown that absolutely secure digital signatures can be created using quantum states; stored programs can also be encoded in quantum states, such that they can only be executed once (quantum states collapse after measurement). Currently, we are working on control techniques for quantum systems, and new quantum algorithms based on symmetry transforms.",2005-01-01,['quanta'],,18487,E15-427,2016-12-05T00:16:46.589Z,['ike@media.mit.edu'],True,quantum-information-science,2002-09-01,[],Quantum Information Science,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:34.597Z,"Inspired by the Danish fairy tale ""The Emperor's New Clothes"" and Lady Gaga�s Orbit dress, we have designed and implemented a costume, The Queen�s New Clothes, which appears plain to the naked eye but exhibits changing patterns on photos taken at different times and locations. The process of making this costume has taken us on a journey of exploring the digital aspect and dual status of fashion, fashion as a dynamically changing and embodied visual communication tool, and the relationship between the fashion trendsetter and the audience.",2012-01-01,['information-ecology'],,18495,,2016-12-05T00:16:46.522Z,['holtzman@media.mit.edu'],True,queens-new-clothes,2011-01-01,[],Queen's New Clothes,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:35.251Z,"QuitoLab will incorporate both architectural and CityScope LEGO models of the historic core of Quito to engage local and visiting communities in experiencing and understanding the city in creative, multisensory ways. The goal of the QuitoLab project is to use multiscalar models as educational and community-building tools to present a multidimensional image of the city, its history, and its potential for future development. Quito will be one of the first case studies for CityScope, Changing Places� platform for participatory urban design using LEGOs. CityScope uses 3D mapping technology to project urban data onto reconfigurable LEGO models. It creates a tangible, interactive platform that allows expert and non-expert stakeholders to understand and make informed decisions about the interaction of architecture, space use, mobility modes, energy and water networks, urban food production, movement of goods, data flows, and other urban systems.",2013-01-01,['changing-places'],,18527,--Choose Location,2016-12-05T00:16:46.641Z,"['kll@media.mit.edu', 'ramiroal@media.mit.edu']",True,quitolab,2013-01-01,[],QuitoLab,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:35.403Z,"Raconteur is a story-editing system for conversational storytelling that provides intelligent assistance in illustrating a story with photos and videos from an annotated media library. It performs natural language processing on a text chat between two or more participants, and recommends appropriate items from a personal media library to illustrate a story. A large common-sense knowledge base and a novel common-sense inference technique are used to find relevant media materials to match the story intent in a way that goes beyond keyword matching or word co-occurrence based techniques. Commonsense inference can identify larger-scale story patterns such as expectation violation or conflict and resolution, and helps a storyteller to chat and brainstorm personal stories with a friend.",2014-09-01,['software-agents'],,18535,E15-384,2022-06-10T14:18:57.604Z,['lieber@media.mit.edu'],True,raconteur-from-chat-to-stories,2009-09-01,[],Raconteur: From Chat to Stories,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:35.532Z,"Radical Atoms is our vision of interactions with future materials. Radical Atoms goes beyond Tangible Bits by assuming a hypothetical generation of materials that can change form and appearance dynamically, becoming as reconfigurable as pixels on a screen. Radical Atoms is a computationally transformable and reconfigurable material that is bidirectionally coupled with an underlying digital model (bits) so that dynamic changes of physical form can be reflected in digital states in real time, and vice versa.",,['tangible-media'],,18543,E15-344,2016-12-10T00:22:48.098Z,"['ishii@media.mit.edu', 'labrune@media.mit.edu', 'amerigo@media.mit.edu', 'daniell@media.mit.edu', 'x_x@media.mit.edu']",True,radical-atoms,2008-09-01,[],Radical Atoms,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:35.694Z,"We are flooded with information every instant; new email alerts, latest twitter updates, weather alerts, sports and the list goes on. While these alerts may intrude activities we're immersed in, there is still a desire to stay in touch with this ongoing noise. Radio Messenger is a new audio interface designed to deliver ambient and social information in a way to minimize the amount of time a user gets distracted from his primary task. This is achieved by interleaving this information with music listened to by the user in manner similar to a traditional radio. This will enable the user to listen and assimilate the alerts while tending to other tasks.  ",2009-01-01,['living-mobile'],,18551,E15-368,2016-12-05T00:16:46.624Z,['geek@media.mit.edu'],True,radio-messenger,2009-01-01,[],Radio Messenger,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:35.814Z,"What can you learn from listening to the radio all day? We are working on a music and language acquisition system that constantly monitors five genres of radio and extracts information about the incoming artists from the Web. The language data is fused with the music data to create machine representations of music description, which is then bootstrapped into lexical relations. The machine listener concurrently extracts musical structure from the audio with a view towards synthesis and intelligent retrieval.",2004-01-01,['music-mind-and-machine'],,18559,E15-491,2016-12-05T00:16:46.677Z,['bv@media.mit.edu'],True,radio-radio,2003-01-01,[],"Radio, Radio",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:11:35.934Z,"How many decisions does it take before you hear a desired piece of music on your iPod? First, you are asked to pick a genre, then an artist, then an album, and finally a song. The more songs you own, the tougher the choices are. To resolve these issues, we have turned the modern music player into an old analog radio tuner, the Radio-ish Media Player. No LCDs, no favorite channels, just a knob that will help you surf through channel after channel accompanied by synthesized noise. Radio-ish is our attempt to revive the lost art of channel surfing in the old analog radio tuner. Let music find you: your ears will tell you if the music is right. This project is not only a retrospective design, but also our reflection on lost simplicity in the process of digitalization. A mobile phone version is also available for demo.",2013-01-01,['living-mobile'],,18567,E15-310,2016-12-05T00:16:46.694Z,['geek@media.mit.edu'],True,radio-ish-media-player,2009-01-01,[],Radio-ish Media Player,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:36.070Z,"RadioActive is a mobile-based, asynchronous, audio-messaging platform. It features one-to-many discussions (think Usenet/Newsgroups) using speech instead of text. There are many design challenges in building a large-scale asynchronous audio conversation space.  How do you browse and navigate through the potentially large chat space? How do you skim messages? Leave responses? We are using a novel visual representation and combined interaction technique to deal with such issues.",2006-09-01,['sociable-media'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/radioactiveLogo.gif,18575,E15-390,2016-12-05T00:16:46.712Z,['judith@media.mit.edu'],True,radioactive,2004-09-01,[],RadioActive,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:36.220Z,"radiO_o is a battery-powered speaker worn by hundreds of party guests, turning each person into a local mobile sound system. The radiO_o broadcast system allows the DJ to transmit sounds over several pirate radio channels to mix sounds between hundreds of speakers roaming around the space and the venue's existing sound system.",2020-03-19,['playful-systems'],https://dam-prod2.media.mit.edu/x/files/Display/logo.jpg,18583,--Choose Location,2020-03-19T18:50:46.813Z,"['slavin@media.mit.edu', 'geppetto@media.mit.edu', 'novysan@media.mit.edu', 'tjlevy@media.mit.edu', 'cwwang@media.mit.edu']",True,radio_o,2014-01-01,[],radiO_o,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:36.661Z,"The Rainwater Wall augments and demonstrates the natural process of rainwater collection and dispersal. It is a freestanding wall that holds water storage bubbles and planters. Rainwater is collected on the roof, distributed by tubing to storage bubbles, and released to plants or returned to the earth. The wall constantly changes due to water retention and plant growth. The bubble wall is a manifestation of the temporal and physical processes of the exterior environment in the form of an interior micro-environment.",2009-01-01,[],,18615,Upper Atrium,2016-12-05T00:16:46.739Z,[],True,rainwater-wall,2009-01-01,[],Rainwater Wall,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:36.863Z,"""At the frontier, our liberty is stripped away...we enter the universe of control....We submit to scrutiny, to inspection, to judgment. We must be passive, docile. To be otherwise is to be suspect."" (Salman Rushdie, Step Across This Line) Random Search is a subtle, reactive undergarment. It logs, shares, and analyzes the experience of invasive airport searches on behalf of our silent, abiding, fearful bodies.",2006-01-01,['computing-culture'],,18631,E15-020C,2016-12-05T00:16:46.785Z,['csik@media.mit.edu'],True,random-search,2005-01-01,[],Random Search,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:36.969Z,"The values endorsed by vernacular architecture have traditionally promoted designs constructed and informed by and for the environment, while using local knowledge and indigenous materials. Under the imperatives and growing recognition of sustainable design, Rapid Craft seeks integration between local construction techniques and globally available digital design technologies to preserve, revive, and reshape these cultural traditions. ",2015-05-31,['mediated-matter'],,18639,--Choose Location,2021-05-21T13:14:06.488Z,['neri@media.mit.edu'],True,rapid-craft,2007-09-01,[],Rapid Craft,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:37.055Z,"Rapidnition is a new way of thinking about gesturally controlled interfaces.  Rather than forcing users to adapt their behavior to a predefined gestural interface, Rapidnition frees users to define their own gestures, which the system rapidly learns. The machine learning algorithms at the core of Rapidnition enable it to quickly infer a user�s gestural vocabulary, using a small number of user-demonstrated examples of each gesture. Rapidnition is capable of recognizing not just static postures but also dynamic temporal gestures. In addition, Rapidnition allows the user to define complex, nonlinear, continuous-mapping spaces. Rapidnition is currently being applied to the real-time recognition of musical gestures to rigorously test both the discrete and continuous recognition abilities of the system.",2013-09-01,['responsive-environments'],,18647,--Choose Location,2016-12-05T00:16:46.877Z,['joep@media.mit.edu'],True,rapidnition-rapid-user-customizable-gesture-recognition,2012-01-01,[],Rapidnition: Rapid User-Customizable Gesture Recognition,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:37.240Z,"Raycounting is a method for generating customized light-shading constructions by registering the intensity and orientation of light rays within a given environment. 3D surfaces of double curvature are the result of assigning light parameters to flat planes. The algorithm calculates the intensity, position, and direction of one or multiple light sources placed in a given environment, and assigns local curvature values to each point in space corresponding to the reference plane and the light dimension. Light performance analysis tools are reconstructed programmatically to allow for morphological synthesis based on intensity, frequency, and polarization of light parameters as defined by the user.",2017-05-31,['mediated-matter'],,18663,--Choose Location,2021-05-21T13:09:06.545Z,['neri@media.mit.edu'],True,raycounting,2010-01-01,"['design', 'art', 'biology', 'materials', 'covid19']",Raycounting,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:37.400Z,"Sleep is perhaps the most critical human activity for well-being, but seldom receives the priority it deserves. We get less rest than we need, and the respite we do achieve is often not in proper or comfortable postures. We present a smart pillow “RePose” to explore a new genre of soft, shape-changing auto-adaptive furniture. This new class of interactive ""Smart Furniture"" device themed around pillows, couches, and beds are designed to transform for, conform to, and inform the user as they rest. Using inflatable pouches and pressure-sensors, RePose detects uncomfortable or unergonomic usage and changes its shape and stiffness accordingly to promote good posture. The result is a pillow that is comfortable for everyone in every situation, and is capable of a variety of additional functionalities for sensing and actuation.",2016-09-01,"['advancing-wellbeing', 'tangible-media']",https://dam-prod2.media.mit.edu/x/2017/12/06/RePosePillow.png,18679,--Choose Location,2017-12-07T03:48:02.073Z,"['ishii@media.mit.edu', 'djfitz@media.mit.edu']",True,repose-smart-pillow,2016-01-01,"['robotics', 'design', 'virtual-reality', 'augmented-reality', 'health', 'human-machine-interaction', 'interfaces', 'internet-things', 'wellbeing']",RePose,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:37.567Z,"Bringing in knowledge from human discourse analysis and social cognition, we are developing autonomous agents that are capable of having a real-time face-to-face conversation with a human. These agents are human in form and communicate using both verbal and non-verbal modalities. Our current efforts focus on the integration of social and task dialogue, the ways in which people nonverbally signal topic shifts in their conversation, and studying the effectiveness of these agents on PDAs and compared to speech-only dialog systems.",2003-01-01,[],,18695,E15-320,2016-12-05T00:17:21.480Z,['bickmore@media.mit.edu'],True,rea-the-conversational-humanoid,1998-01-01,[],Rea: The Conversational Humanoid,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:37.638Z,"Reach merges inherently local communications with user requests or offers of services. It is built atop data from services users already use, like Facebook and Google Latitude. Reach is intended to demonstrate a flexible, attractive mobile interface that allows users to discover ""interesting"" aspects of the environment and to call upon services as needed. These can range from a broadcast offer to serve as a triage medic, to a way to share a cab or get help for a technical service problem like plugging into a video projector.",2012-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/reach/reachthumb.jpg,18703,--Choose Location,2016-12-05T00:16:46.903Z,['lip@media.mit.edu'],True,reach,2011-09-01,"['communications', 'social-media', 'wellbeing']",Reach,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:37.711Z,"ReachBand is a wireless wristband with an RFID reader that can detect user interactions with everyday objects, such as holding a book or opening a door. ReachBand enables the delivery of personalized services and just-in-time information to the user. It is used by various projects at the Lab, such as ReachMedia and Ambient Semantics. ReachBand v2 is being further developed, and will include updated electronics and communcation capabilities, as well as reduced dependence on a PC.",2010-01-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2016/10/06/files/pldb/Display/wristBand.jpg,18711,Pond,2016-12-05T00:17:21.497Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu']",True,reachband-an-rfid-wristband,2005-09-01,[],ReachBand: An RFID Wristband,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:37.938Z,"Read Out Loud is an application that empowers adults learning English to turn almost any reading material into an experience to help them learn. Learners can take a picture of a page of text; the app then scans in the page and presents the learner with a host of additional tools to facilitate reading. They can read the text aloud, which helps learners who are more comfortable with spoken English understand what is written. They can also select words to translate them into their native language. With this prototype, we want to give adult learners more agency to learn from material that focuses on subjects they care about, as well as increase access to English language learning material. Any book from the public library could become learning material with support in their native language. Read Out Loud is a prototype within the larger Making Learning Work project.",2016-06-01,['ml-learning'],https://dam-prod2.media.mit.edu/x/files/Display/rol_tile_4.jpg,18735,--Choose Location,2016-12-05T00:17:21.549Z,"['ps1@media.mit.edu', 'jnazare@media.mit.edu']",True,read-out-loud,2014-09-01,[],Read Out Loud,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:38.018Z,"The Passing On project uses data from 20 years of New York Times stories about society's heroes, leaders, and visionaries to ""reader-source"" improvements to Wikipedia. As readers explore compelling stories about notable women, content is generated to create new content and inspire the public to contribute those stories to Wikipedia.",2016-10-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/passing_on.png,18743,--Choose Location,2016-12-05T00:17:02.779Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'jnmatias@media.mit.edu']",True,readersourcing,2012-09-01,"['civic-media', 'data']",Readersourcing,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:38.097Z,"Current 3-D laser scanning systems require a lengthy interval to complete each scan and require a clear line-of-sight, thereby severing the physical model from its CAD representation, and not allowing most handheld interaction because of occlusion. This project proposes 3-D volumetric sensing using real-time tag tracking that allows the user to manipulate the physical model object while the tags are being scanned. This approach provides a seamless link between the real object and the virtual object, effectively converging the design and modeling processes for artists, architects, and engineers. The tag detection scheme uses magnetically coupled resonators similar to those developed for the Swept RF Tagging project. The current areas of investigation include reducing the dimensions of the tags, improving the signal-to-noise ratio of location measurement of tags in each axis, and increasing the number of trackable tags to form meaningful shapes and contours. Demonstrative content is being developed in collaboration with the MIT School of Architecture and Urban Planning.",2003-01-01,['responsive-environments'],,18751,E15-344,2016-12-05T00:16:46.957Z,['joep@media.mit.edu'],True,real-time-3-d-volumetric-sensing,2001-01-01,[],Real-Time 3-D Volumetric Sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:38.172Z,"Depression correlated with anxiety is one of the key factors leading to suicidal behavior, and is among the leading causes of death worldwide. Despite the scope and seriousness of suicidal thoughts and behaviors, we know surprisingly little about what suicidal thoughts look like in nature (e.g., How frequent, intense, and persistent are they among those who have them? What cognitive, affective/physiological, behavioral, and social factors trigger their occurrence?). The reason for this lack of information is that historically researchers have used retrospective self-report to measure suicidal thoughts, and have lacked the tools to measure them as they naturally occur. In this work we explore use of wearable devices and smartphones to identify behavioral, affective, and physiological predictors of suicidal thoughts and behaviors.",,"['advancing-wellbeing', 'affective-computing']",,18759,--Choose Location,2021-08-03T13:19:41.609Z,"['picard@media.mit.edu', 'sfedor@media.mit.edu']",True,real-time-assessment-of-suicidal-thoughts-and-behaviors,2015-01-01,['artificial-intelligence'],Real-time assessment of suicidal thoughts and behaviors,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:38.257Z,People are surprisingly predictable. We use real-time video analysis to extract patterns of behavior from crowds browsing demos in our lab space.  We can discover meaningful locations and sequences just from observing how people interact in the space.  We can even begin to predict what people might do next.,2011-01-01,['social-machines'],,18767,,2016-12-05T00:17:21.612Z,['dkroy@media.mit.edu'],True,real-time-behavior-analysis,2010-01-01,[],Real-Time Behavior Analysis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:38.332Z,"Complex data�such as neurophysiological recordings, or measures of human behavior, Internet, and general network data�are extremely difficult to analyze because of the dynamic nature of the high-dimensional set of interacting processes that generates the data. Accordingly, traditional statistical and data analysis methods�clustering, correlation, and so forth�can rarely create models sophisticated enough to explain the data without trying to explain noise, demanding astronomically sized datasets, or requiring enormous amounts of hand-tuning by insightful labor. We propose to design and develop a system that continuously generates novel data-modeling hypotheses and evaluates them in real time, testing models of ever-increasing complexity on data as it comes in.",2011-01-01,['synthetic-neurobiology'],,18775,E15-435,2016-12-05T00:16:10.893Z,"['esb@media.mit.edu', 'cwentz@media.mit.edu', 'caromk@media.mit.edu', 'jkinney@media.mit.edu', 'bdallen@media.mit.edu']",True,real-time-data-mining-and-perturbation,2009-01-01,[],Real-Time Data Mining and Perturbation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:38.409Z,"Cell-phone contact lists represent a very large peer-to-peer network. We are creating a cell-phone-based application that allows users to perform real-time searches on their local social network, against pieces of information that their contacts have provided about themselves. Our matchmaking agent uses Open Mind Common Sense to understand users' goals, and to expand logically on their queries.",2006-09-01,['software-agents'],,18783,E15-383,2016-12-05T00:17:21.642Z,['lieber@media.mit.edu'],True,real-time-searches-on-a-local-social-network,2004-01-01,[],Real-Time Searches on a Local Social Network,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:38.486Z,"The Reality Editor is a new kind of tool for empowering you to connect and manipulate the functionality of physical objects. Just point the camera of your smartphone at an object and its invisible capabilities will become visible for you to edit. Drag a virtual line from one object to another and create a new relationship between these objects. With this simplicity, you are able to master the entire scope of connected objects.",2017-06-30,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/02/22/RE05_42ypI8P.jpg,18791,--Choose Location,2018-04-30T14:19:37.322Z,"['pattie@media.mit.edu', 'heun@media.mit.edu', 'benolds@media.mit.edu', 'hobinjk@media.mit.edu']",True,reality-editor,2014-01-01,"['augmented-reality', 'consumer-electronics', 'interfaces', 'internet-things']",Reality Editor,PUBLIC,http://www.realityeditor.org,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:38.561Z,"The Reality Editor system supports editing the behavior and interfaces of so-called ""smart objects"": objects or devices that have an embedded processor and communication capability. Using augmented reality techniques, the Reality Editor maps graphical elements directly on top of the tangible interfaces found on physical objects, such as push buttons or knobs. The Reality Editor allows flexible reprogramming of the interfaces and behavior of the objects, as well as defining relationships between smart objects in order to easily create new functionalities. ",2015-09-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/image3.jpg,18799,--Choose Location,2016-12-05T00:17:21.670Z,"['pattie@media.mit.edu', 'heun@media.mit.edu', 'hobinjk@media.mit.edu']",True,reality-editor-programming-smarter-objects,2013-01-01,[],Reality Editor: Programming Smarter Objects,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:39.062Z,"In a digital, immersive world, e-book readers and flat displays are taking over what was once the realm of books. The reBook project attempts to revitalize traditional book production by adding a digital interface fabrication step to the conventional print and bind process. A reBook is a fusion of a 'real book' and an e-book; the cover is embedded with a memory module, wireless microprocessor, paper-based keypad, and flexible display. The reBook's coupling of digital content within a familiar design allows for reinterpretation of classic book functionality. We are exploring how new interactions such as search, bookmarking, physical copy and paste, and annotations could emerge with such a product.",2010-01-01,['fluid-interfaces'],,18855,--Choose Location,2016-12-05T00:16:46.327Z,"['pattie@media.mit.edu', 'linder@media.mit.edu']",True,rebook,2009-01-01,[],reBook,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:39.135Z,"Recap uses Glue to automatically create video casts, allowing users to specify how much time they have available and then intelligently filling that chunk of time with the top trending stories of the day.",2014-09-01,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/Screenshot%202014-10-02%2013.55.30.png,18863,--Choose Location,2016-12-05T00:16:47.091Z,"['dahlseng@media.mit.edu', 'lip@media.mit.edu']",True,recap,2014-01-01,[],Recap,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:39.214Z,"Recast is a media curation and distribution platform that enables anyone to create and distribute ""news programs"" that represent their views of the world from their own perspective. Recast provides a visual scripting interface similar to Scratch, where users can combine a series of logical blocks to query specific scene elements that present their views by drawing from arbitrary video contents, and constructing a story sequence. Recast uses the Constellation system as a backend for querying video content, and uses the Media Matrix as a content distribution platform.",2015-09-01,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/projects/glue/img/Glue_img.jpeg,18871,--Choose Location,2016-12-05T00:16:47.119Z,['lip@media.mit.edu'],True,recast,2013-09-01,[],Recast,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:39.374Z,"This research project is concerned with building computational models for the automatic recognition of affective expression in speech. We are in the process of completing an investigation of how acoustic parameters extracted from the speech waveform (related to voice quality, intonation, loudness and rhythm) can help disambiguate the affect of the speaker without knowledge of the textual component of the linguistic message. We have carried out a multi-corpus investigation, which includes data from actors and spontaneous speech in English, and evaluated the model's performance. In particular, we have shown that the model exhibits a speaker-dependent performance which reflects human evaluation of these particular data sets, and, held against human recognition benchmarks, the model begins to perform competitively.",2003-09-01,['affective-computing'],,18887,E15-001,2016-12-05T00:17:21.713Z,['picard@media.mit.edu'],True,recognizing-affect-in-speech,1998-01-01,[],Recognizing Affect in Speech,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:39.450Z,"Human beings have long shaped the physical environment to reflect designs of form and function. As an instrument of control, the human hand remains the most fundamental interface for affecting the material world. In the wake of the digital revolution, this is changing, bringing us to reexamine tangible interfaces. What if we could now dynamically reshape, redesign, and restructure our environment using the functional nature of digital tools? To address this, we present Recompose, a framework allowing direct and gestural manipulation of our physical environment. Recompose complements the highly precise, yet concentrated affordance of direct manipulation with a set of gestures, allowing functional manipulation of an actuated surface.",2011-09-01,"['tangible-media', 'viral-communications']",,18895,--Choose Location,2016-12-05T00:16:47.152Z,"['ishii@media.mit.edu', 'dlakatos@media.mit.edu']",True,recompose,2010-09-01,[],Recompose,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:39.610Z,"Location prediction is a critical building block in many location-based services and transportation management. This project explores the issue of next-location prediction based on the longitudinal movements of the locations individuals have visited, as observed from call detail decords (CDR). In a nutshell, we apply recurrent neural network (RNN) to next-location prediction on CDR. RNN can take in sequential input with no restriction on the dimensions of the input. The method can infer the hidden similarities among locations and interpret the semantic meanings of the locations. We compare the proposed method with Markov and a Naive Model proving that RNN has better accuracy in location prediction. ",,['human-dynamics'],,18911,--Choose Location,2019-04-19T14:52:06.657Z,"['sandy@media.mit.edu', 'yleng@media.mit.edu']",True,recurrent-neural-network-in-context-free-next-location-prediction,2016-01-01,"['data', 'transportation', 'machine-learning', 'computer-science']",Recurrent neural network in context-free next-location prediction,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:39.688Z,"With common-sense computing, we can discover trends in the topics that people are talking about right now. Red Fish, Blue Fish takes input in real time from many political blogs, and creates a visualization of what topics are being discussed by the left and the right.",2013-01-02,['digital-intuition'],,18919,,2022-06-10T13:46:47.136Z,[],True,red-fish-blue-fish,2010-01-01,[],"Red Fish, Blue Fish",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:11:39.760Z,"Red Ink is an open-source web service for ad hoc groups to share and interpret their collective financial data. It includes interfaces for defining transactions upon which to aggregate, methods of visualization, and public and private web publishing.",2011-01-01,['civic-media'],,18927,E15-001,2016-12-05T00:16:47.241Z,['csik@media.mit.edu'],True,red-ink,2009-09-01,[],Red Ink,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:39.835Z,"Red String of Fate is an East Asian mythology in which gods tie an invisible red string between those that are destined to be together. Sputniko! has collaborated with scientists from NIAS to genetically engineer silkworms to spin this mythical ""Red String of Fate"" by inserting genes that produce oxytocin, a social-bonding ""love"" hormone, and the genes of a red-glowing coral into silkworm eggs. Science has long challenged and demystified the world of mythologies: from Galileo's belief that the Earth revolved around the sun, to Darwin's theory of evolution and beyond—but in the near future, could science be recreating our mythologies? The film unravels a story around the protagonist Tamaki, an aspiring genetic engineer, who engineers her own ""Red Silk of Fate"" in the hopes of winning the heart of her crush, Sachihiko. However, strange, mythical powers start to inhabit her creation....",2010-05-01,['design-fiction'],https://dam-prod2.media.mit.edu/x/files/Display/redsikdisplay.jpg,18935,--Choose Location,2022-06-10T13:44:47.824Z,['sputniko@media.mit.edu'],True,red-silk-of-fate-tamakis-crush,2014-01-01,[],Red Silk of Fate—Tamaki's Crush,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:39.908Z,"Urban educational systems face myriad challenges. In developing countries, these challenges are accentuated by lack of resources and diminishing public confidence. Approaches to schools, informal learning centers, and community development need to change, but we lack concrete models and ideas. This project engages with several cities around the world to re-think, re-invent, and re-invigorate the learning environment for all children. We are creating centers of innovation for children, challenging mindsets about what education must be, and providing concrete examples of new learning. We are presently working in S�o Paulo, Curitiba, Campinas, Manaus, and Salvador (Brazil), where children are designing solutions for the problems of their cities and communities.",2004-01-01,['future-of-learning-2'],,18943,E15-320,2016-12-05T00:16:47.297Z,"['papert@media.mit.edu', 'cavallo@media.mit.edu']",True,redesigning-urban-learning-environments,2000-01-01,[],Redesigning Urban Learning Environments,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:40.121Z,"The world uses an estimated 20 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible, including the use of painkillers where appropriate. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals but are equally suited to laboratory and medical research. If successful, widespread adoption of these mice could drastically reduce the total amount of animal suffering in laboratories worldwide.",,['sculpting-evolution'],,18967,--Choose Location,2019-10-17T16:21:49.706Z,['esvelt@media.mit.edu'],True,reducing-suffering-in-laboratory-animals-3,2016-01-01,[],Reducing suffering in laboratory animals,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:40.182Z,"The world uses an estimated 100 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals, but that are equally suited to laboratory and medical research. Specifically, it will be possible for researchers to perfectly prevent these mice from experiencing either acute or chronic (neuropathic) pain as needed. If successful, widespread adoption of these mice could drastically reduce animal suffering in laboratories worldwide.",,['sculpting-evolution'],,18975,--Choose Location,2020-03-23T15:22:24.760Z,['esvelt@media.mit.edu'],True,reducing-suffering-in-laboratory-animals,2016-01-01,['biotechnology'],Reducing Suffering in Laboratory Animals,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:40.256Z,"We demonstrate a new technique that allows a camera to rapidly acquire reflectance properties of objects ""in the wild"" from a single viewpoint, over relatively long distances and without encircling equipment. This project has a wide variety of applications in computer graphics, including image relighting, material identification, and image editing.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/brdfrendering.jpeg,18983,--Choose Location,2019-04-19T18:34:19.350Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,reflectance-acquisition-using-ultrafast-imaging,2010-09-01,[],Reflectance acquisition using ultrafast imaging,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:40.332Z,"Most of us go through the chore of capturing our knowledge in a very perfunctory manner through some form of documentation. This is usually our knowledge-in-action and is restricted to certain learned theories, principles, and rules for resolving problems. However, reflection-in-action, a much deeper phenomena, and one from which we can accentuate our knowing, remains relatively unexplored. This work proposes the creation of an application that aids in the process of reflection-in-action. It will facilitate the process of documentation by utilizing the rich computational tools to which many people now have access. It provides an organization structure and taxonomy around which to compile tacit knowledge and its representation, allowing for exploration of such knowledge in a richer fashion. This will aid in bringing forth the intrinsic ""ifs"" and ""thens"" as well as generating the potential for serendipitous learning experiences. All of this is very useful in bringing some form of rigor into the practice of reflective inquiry.",2004-01-01,['electronic-publishing'],,18991,E15-320,2016-12-05T00:17:21.891Z,['walter@media.mit.edu'],True,reflective-practice-documentation,2004-09-01,[],Reflective Practice Documentation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:40.410Z,"ReflectOns are objects that help people think about their actions and change their behavior based on subtle, ambient nudges delivered at the moment of action. Certain tasks, such as figuring out the number of calories consumed, or amount of money spent eating out, are generally difficult for the human mind to grapple with. By using in-place sensing combined with gentle feedback and understanding of users' goals, we can recognize behaviors and trends, and provide a reflection of their own actions tailored to enable both better understanding of the repercussions of those actions, and changes to their behaviors to help them better match their own goals.",2012-09-01,['fluid-interfaces'],,18999,E15-320,2018-12-04T20:41:09.428Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu']",True,reflectons-mental-prostheses-for-self-reflection,2008-01-01,[],ReflectOns: Mental Prostheses for Self-Reflection,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:40.486Z,"The device that is proposed is a low cost simplification on the industrial sinking electro-discharge machine.   What is proposed is to remove the controlling actuators (usually stepping motors or servomotors) altogether and instead use the electromotive force of the spark discharge itself to drive a simple solenoid coil connected to the spark discharge in series, pulling the sparking electrode out of contact with the machined material when a discharge occurs.  This cycle is then biased by gravity or another constant force (such as a spring), making the process self-sustaining and requiring very little external control circuitry to operate.  This device would be useful for imprecise uses of EDM (such as burning out broken taps) and due to the simplicity of design and fabrication, priced affordably.",2012-09-01,[],https://dam-prod2.media.mit.edu/x/files/Display/FEDM.jpg,19007,,2016-12-05T00:17:21.912Z,"['neilg@media.mit.edu', 'neri@media.mit.edu']",True,reflex-electrical-discharge-machining,2012-01-01,[],Reflex Electrical Discharge Machining,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:40.631Z,"The Rehab Sound Sculptor is a motion-tracking interface that allows the researcher to quickly map a subject's movement to any aspect of sound control. The motivation for developing this interface is that it allows us to determine which attributes of sound are potentially more useful than others for providing feedback in motor rehabilitation. There has been a significant body of work, particularly with Parkinson's disease, that has shown the rehabilitative benefit of auditory feedback during physical therapy. However, little work has been done to determine exactly which features of sound are responsible for this benefit. With the Rehab Sound Sculptor interface, we can investigate the relative contributions of specific sound attributes to rehabilitation.",2005-01-01,['opera-of-the-future'],,19023,E15-443,2016-12-05T00:16:47.450Z,['tod@media.mit.edu'],True,rehab-sound-sculptor,2004-09-01,[],Rehab Sound Sculptor,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:40.783Z,"With skin conductance sensors, we map out what frustrates and excites customers as they shop�from layout to wanting to touch the product. Our work has helped a variety of large retailers innovate on what it means to shop. Findings have focused on reducing the stress of choices and learning while surprising customers in new ways. With the sensor technology we can pinpoint moments when customers are overwhelmed and then build out new ways to make retail engaging again.",2014-09-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/Slide1.JPG,19039,--Choose Location,2016-12-05T00:16:47.497Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",True,reinventing-the-retail-experience,2013-01-01,[],Reinventing the Retail Experience ,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:40.857Z,"Relational Agents are computational artifacts designed to build and maintain long-term, social-emotional relationships with their users. Central to the notion of relationship is that it is a persistent construct, spanning multiple interactions. Thus, Relational Agents are explicitly designed to remember past history and manage future expectations in their interactions with users. Since face-to-face conversation is the primary context of relationship-building for humans, our work focuses on Relational Agents as a specialized kind of embodied conversational agent (animated humanoid software agents that use speech, gaze, gesture, intonation, and other nonverbal modalities to emulate the experience of human face-to-face conversation). One major achievement was the development of a Relational Agent for health behavior change, specifically in the area of exercise adoption. A study involving 100 subjects interacting with this agent over one month demonstrated that the relational agent was respected more, liked more, and trusted more, and that these ratings were maintained over time (unlike for the non-relational agent, where they were not only significantly lower overall, but also declined over time.) People also expressed significantly greater ratings of perceived caring by the agent, and significantly more desire to keep working with the relational agent after the termination of the study.",2003-09-01,['affective-computing'],,19047,E15-001,2016-12-05T00:16:47.580Z,"['picard@media.mit.edu', 'bickmore@media.mit.edu']",True,relational-agents,2001-01-01,[],Relational Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:40.931Z,"Analogy is a powerful comparison mechanism, commonly thought to be central to human problem solving. Analogies like ""an atom is like the solar system"" enable people to effectively transfer knowledge to new domains. Can we enable computers to do similar comparisons? Prior work on analogy (structure mapping) provides guidance about the nature of analogies, but implementations of these theories are inefficient and brittle. We are working on a new analogy mechanism that uses instance learning to make robust, efficient comparisons.",2014-09-01,['software-agents'],,19055,E15-383,2022-06-10T14:18:57.619Z,['lieber@media.mit.edu'],True,relational-analogies-in-semantic-networks,2009-01-01,[],Relational Analogies in Semantic Networks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:41.005Z,"With the Relational Pillow project, we are trying to provide a simple, intimate, and personable communication medium between loved ones. The pillows are capable of sensing touch information, and displaying incoming touch data as a pattern of lights that show the outline ""drawn"" upon the remote pillow. Pillows can connect to each other over the network so that this sense of touch can be shared across long distances. The physical sensation of holding a pillow and interacting with it builds upon the idea of using the natural features of the object in order to acheive a deeper connection between the users, without interfering in the communication process itself.",2009-09-01,['fluid-interfaces'],,19063,E15-320,2016-12-05T00:16:47.476Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu']",True,relational-pillow,2004-01-01,[],Relational Pillow,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:41.082Z,"Relief is an actuated tabletop display, able to render and animate 3D shapes with a malleable surface. It allows users to experience and form digital models such as geographical terrain in an intuitive manner. The tabletop surface is actuated by an array of motorized pins, which can be addressed individually and sense user input such as pulling and pushing. Our current research focuses on utilizing freehand gestures for interacting with content on Relief.",2012-01-01,['tangible-media'],,19071,E15-344,2016-12-05T00:16:47.517Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu']",True,relief,2009-01-01,[],Relief,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:41.162Z,"Increasing numbers of networked appliances are bringing about new opportunities for control and automation. At the same time, an increase in multifunctional appliances is creating a complex and often frustrating environment for the end-user. Motivated by these opportunities and challenges, we are exploring the potential for sensor fusion to increase usability and improve user experience while retaining the user in the control loop. We have developed a novel, camera-less, multi-sensor solution for intuitive gesture-based indoor lighting control, called RElight. Using a wireless handheld device, the user simply points at a light fixture to select it and rotates his hand to continuously configure the dimming level. Pointing is a universal gesture that communicates one�s interest in or attention to an object. Advanced machine learning algorithms allow rapid training of gestures and continuous control that supplements gesture classification.",2013-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/RElight%20logo.png,19079,--Choose Location,2016-12-05T00:16:10.942Z,"['nanzhao@media.mit.edu', 'bmayton@media.mit.edu', 'joep@media.mit.edu']",True,relight-exploring-pointing-and-other-gestures-for-appliance-control,2012-01-01,[],RElight: Exploring pointing and other gestures for appliance control,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:41.239Z,"Remembrance Agents are a new class of applications that watch over a user's shoulder and suggest information relevant to the current situation. While query-based memory aids help with direct recall, remembrance agents are an augmented associative memory. For example, a database or search engine can answer direct questions like ""how do I get to Central Square?"" Remembrance Agents instead offer the kind of information you didn't even know enough to ask about, like ""there's a great coffee-shop just around the corner.""  
",2000-12-30,['software-agents'],,19087,E15-305A,2016-12-05T00:16:47.542Z,['pattie@media.mit.edu'],True,remembrance-agents,1994-12-31,[],Remembrance Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:41.312Z,"Remix and Robo are two sampler/sequencer controllers that allow children to engage in (1) storytelling, and (2) competitive endeavors with a modular robotic toy. We base this work on Topobo, a constructive assembly system with kinetic memory (the ability to record and play back physical motion). Remix is a tangible interface used to sample, organize, and manipulate several kinetic records. Robo is a modified game controller used to capture robotic motions, adjust global motion parameters, and execute kinetic recordings in real time. Remix and Robo facilitate new kinds of applications that build on children�s varying interests and social motivations. Whereas children typically play with building toys because of the joy of model-making or solving engineering problems, Remix and Robo allow users to cast their creations in performances and competitions. By introducing an expanded social scope, our work motivates learners to focus and reflect upon their understanding of dynamic physics concepts with Topobo.",2007-01-01,['tangible-media'],,19095,E15-344,2016-12-05T00:16:47.632Z,['ishii@media.mit.edu'],True,remix-and-robo-topobo,2005-01-01,[],Remix and Robo Topobo,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:41.393Z,"Remnant is a greeting card that merges the affordances of physical materials with the temporal malleability of digital systems to create, enshrine, and reinforce the very thing that makes a greeting personal: the hand of the sender. The card records both the timing and the form of the sender's handwriting when it is first used. At a later time, collocated output recreates the handwriting, allowing the invisible, memorized hand of the sender to write his or her message directly in front of the recipient.",2010-09-01,['fluid-interfaces'],,19103,E15-320,2016-12-05T00:16:47.656Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu']",True,remnant-handwriting-memory-card,2007-01-01,[],Remnant: Handwriting Memory Card,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:41.469Z,"Remot-IO is a system for mobile collaboration and remote assistance around Internet-connected devices. It uses two head-mounted displays, cameras, and depth sensors to enable a remote expert to be immersed in a local user's point of view, and to control devices in that user's environment. The remote expert can provide guidance through hand gestures that appear in real time in the local user's field of view as superimposed 3D hands. In addition, the remote expert can operate devices in the novice's environment and bring about physical changes by using the same hand gestures the novice would use. We describe a smart radio where the knobs of the radio can be controlled by local and remote users. Moreover, the user can visualize, interact, and modify properties of sound waves in real time by using intuitive hand gestures.",2016-12-31,"['terrestrial-sensing', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/files/Display/RemoteIO.png,19111,--Choose Location,2018-08-20T16:23:49.991Z,"['pattie@media.mit.edu', 'amores@media.mit.edu']",True,remot-io-a-system-for-reaching-into-the-environment-of-a-remote-collaborator,2015-01-01,[],Remot-IO: A System for Reaching into the Environment of a Remote Collaborator,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:41.548Z,"The MIT Media Lab collaborated with London-based theater group Punchdrunk to create an online platform connected to their New York City production of Sleep No More. In the live show, masked audience members explore and interact with a rich environment, discovering their own narrative pathways. We developed an online companion world to this real-life experience, through which online participants partner with live audience members to explore the interactive, immersive show together. Pushing the current capabilities of web standards and wireless communications technologies, the system delivered personalized multimedia content allowing each online participant to have a unique experience co-created in real time by his own actions and those of his onsite partner.  This project explored original ways of fostering meaningful relationships between online and onsite audience members, enhancing the experiences of both through the affordances that exist only at the intersection of the real and the virtual worlds.The first version of the extended Sleep No More was offered to a limited public in May 2012. Further elaborations and next steps are currently being evaluated.",2012-05-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/03/31/sleep_no_more-400x340.jpg,19119,--Choose Location,2017-04-03T19:51:26.732Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'benb@media.mit.edu', 'ejessop@media.mit.edu', 'gershon@media.mit.edu', 'akito@media.mit.edu', 'bmayton@media.mit.edu', 'jieqi@media.mit.edu', 'jhaas@media.mit.edu']",True,remote-theatrical-immersion-extending-sleep-no-more,2011-09-01,"['design', 'art', 'music', 'sensors', 'performance']","Remote Theatrical Immersion: Extending ""Sleep No More""",PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:41.637Z,"Renga is a Web-based system that allows children around the world to  collaboratively tell a story in real time. Users with only e-mail access to  the Internet can also play through the Renga e-mail interface.
",2000-12-30,['gesture-and-narrative-language'],,19127,,2016-12-05T00:17:22.015Z,[],True,renga-a-web-based-collaborative-storytelling-system,1999-12-31,[],Renga: A Web-Based Collaborative Storytelling System,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:41.789Z,"RePlace is a 3D data visualization platform that takes the normally invisible activity of sensors and data loggers and overlays that information as 3D sprites in the environment. RePlace uses an augmented reality viewfinder as a window into the data environment, and enables interaction with real-time and historic data feeds through the viewfinder.",2016-12-01,"['changing-places', 'city-science']",,19143,--Choose Location,2018-05-04T18:23:12.980Z,"['kll@media.mit.edu', 'csmuts@media.mit.edu']",True,OLD_replace,2015-09-01,[],RePlace,PUBLIC,,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:41.870Z,"In November 2014, Twitter granted Women, Action, and the Media (WAM!) authorized status to report harassment to the company. In three weeks, WAM! reviewers assessed 811 incoming reports of harassment and escalated 161 reports to Twitter, ultimately seeing Twitter carry out 70 account suspensions, 18 warnings, and one deleted account. This document presents quantitative and qualitative findings from this three-week project. Our findings focus on the people reporting and receiving harassment, the kinds of harassment that were reported, Twitter's response to harassment reports, the process of reviewing harassment reports, and challenges for harassment reporting processes.",2015-01-01,[],https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202015-10-14%20at%2013.49.16.png,19151,--Choose Location,2016-12-05T00:16:10.973Z,['jnmatias@media.mit.edu'],True,reporting-reviewing-and-responding-to-harassment-on-twitter,2015-01-01,[],"Reporting, Reviewing, and Responding to Harassment on Twitter",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:41.937Z,"An intelligent robot must build an understanding of its environment with minimal human mediation. We investigate the characterization of objects in terms of their affordances�the ways in which an agent can functionally interact with objects. To effect this goal, we represent objects as expectations with respect to sensorimotor schema�collections of data that describe how an agent expects its interaction with the world to affect its perception of the world. We are constructing novel robotic manipulators that have enhanced abilities to actively touch their environment to develop these ideas. This work will contribute to our larger effort of grounding natural language in the physical environment.",2005-01-01,['social-machines'],,19159,E15-441,2016-12-05T00:17:22.039Z,['dkroy@media.mit.edu'],True,representing-affordances-of-objects-through-active-touch,2004-01-01,[],Representing Affordances of Objects Through Active Touch,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:42.012Z,"The members of electronic communities are often unrelated to each other; they may have never met and have no information on each other's reputation.  This kind of information is vital in electronic commerce interactions, where the potential counterpart's reputation can be a significant factor in the negotiation strategy. In this project we are inventing reputation mechanisms that rely on collaborative rating and personalized evaluation of the various ratings assigned to each user.  While these reputation mechanisms are developed in the context of electronic commerce, we believe that they may have applicability in other types of electronic communities such as chatrooms, newsgroups, mailing lists, etc.",1999-12-30,['software-agents'],,19167,,2016-12-05T00:17:22.066Z,['pattie@media.mit.edu'],True,reputation-mechanisms,1996-12-31,[],Reputation Mechanisms,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:42.166Z,"We are experimenting with technology for low-cost, large-area input devices, using change-source tomography on an electrically resistive medium.",2007-01-01,['physics-and-media'],,19183,E15-023,2022-06-10T14:12:10.677Z,"['neilg@media.mit.edu', 'rehmi@media.mit.edu']",True,resistive-sheets,2005-01-01,[],Resistive Sheets,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:42.249Z,"This project investigates relationships between and within bodies. Architectural bodies and living bodies communicate in this project through sound, infrasound, and touch.",2004-09-01,['computing-culture'],,19191,E15-020C,2016-12-05T00:16:47.785Z,"['csik@media.mit.edu', 'monster@media.mit.edu']",True,resound,2002-01-01,[],ReSound,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:11:42.413Z,"Hydrogels are crosslink polymers that are capable of absorbing great amounts of water. They have been studied during the last 50 years, largely due to their hydrophilic character at ambient temperatures, which make them biocompatible and attractive for various biological applications. Nevertheless, in our project, we are interested in their hydrophilic-hydrophobic phase-transition, occurring slightly above room temperature. We investigate the mechanical and optical transformations at this phase transition–namely, their swelling, permeability, and optical transmission modification–as enabling responsive or passive dynamics for future product design.",2013-09-01,['mediated-matter'],,19207,,2016-12-14T01:46:36.641Z,['neri@media.mit.edu'],True,responsive-glass,2012-01-01,[],Responsive Glass,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:42.555Z,"A prime goal of ubiquitous computing is the embedding of sensing, communication, and computation into everyday objects. One consequence of this paradigm is that perhaps nothing will be lost again, allowing us to gain rapid access to the objects we want. This is particularly important in the age of digital storage media, where information content is often not obviously expressed by the appearance of packaging. The mechanical constraints of CDs, DVDs, and MiniDVs, for example, means that their content is abstracted into or scrawled onto anonymous, homogeneous containers. This is even true of books. How many times have we stared at a shelf full of books in the library only to find what we are looking for half an hour later? We have designed an active tagging system that responds to a coded optical beam from several meters away. The tags contain a minimalist microprocessor that ambiently operates in shutdown mode and, upon detecting particular frequency components in the AM-modulated interrogation beam, awakens to decode the incident digital message and produce an appropriate response. The lack of linear amplifiers means that these tags draw under 0.5 �A when sleeping, hence can operate up to 10 years on a lithium coin cell. The interrogator is a �flashlight,� with which one scans an area; when the light beam hits a tag programmed with a code that matches that sent by the interrogator, an on-tag LED flashes, indicating that the desired object is �found.""",2003-01-01,['responsive-environments'],,19215,E15-441,2016-12-05T00:17:22.092Z,['joep@media.mit.edu'],True,responsive-tagging-based-on-optically-triggered-microprocessor-wakeup,2001-01-01,[],Responsive Tagging Based on Optically Triggered Microprocessor Wakeup,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:42.724Z,"Tod Machover's latest opera premiered at the Houston Grand Opera in April and May 1999. It was Machover's first opera for traditional operatic forces, including ten solo singers, chorus, and orchestra. In addition, special keyboard-based hyperinstruments were developed to allow rich and sophisticated electronic sounds to be blended with the performers in the pit and on stage during the entire 2.5 hour performances. In addition, experiments were conducted to distribute electronic sound via multiple small, high-quality loudspeakers, placed in the pit with the orchestra. The result was an enhanced opera that sounded acoustic and physical while extending the sonic and dramatic expression of the operatic medium.",2001-12-30,['opera-of-the-future'],,19223,E15-494,2016-12-05T00:17:01.615Z,['tod@media.mit.edu'],True,resurrection,1999-12-31,[],Resurrection,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:42.909Z,"In a given organization, thousands of pages of material are printed only to be read once and discarded. We are developing a desktop printer and reusable paper substrate capable of producing a high-quality, high-contrast image, which may subsequently be erased and rewritten multiple times without any expendable materials, such as toner or chemicals. Current related investigations include the development of color substrates and the development of electronically addressable, extremely high-resolution color wall-coverings, public message boards, whiteboards, and art canvases.",2005-01-01,['molecular-machines'],,19231,E15-015,2016-12-05T00:17:22.120Z,"['jacobson@media.mit.edu', 'hamad@media.mit.edu']",True,rewritable-substrates-for-printing-and-large-scale-reflective-displays,2000-01-01,[],Rewritable Substrates for Printing and Large-Scale Reflective Displays,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:43.090Z,"Re�play is a self-documenting construction kit for children, allowing them both to share their designs with others and reflect on their own design process. Re�play consists of a set of angular construction pieces that can sense their connection and orientation. A virtual model is rendered in real time as a design is constructed, and an on-screen playback interface allows users to view models from multiple perspectives and watch how a design was assembled.",2012-09-01,['lifelong-kindergarten'],,19239,--Choose Location,2016-12-05T00:16:47.840Z,"['mres@media.mit.edu', 'ttseng@media.mit.edu']",True,replay,2011-09-01,[],Re�play,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:43.626Z,"People's daily lives are impacted by a great deal of dynamic environmental information, such as weather and traffic. Though most of this information is now available on the Internet, there isn't an easy way to access it while mobile. In addition, people do not always need to know this kind of information unless there is a significant change that may impact their current or future activities. Ringing in the Rain proposes a distributed, multi-agent architecture that uses GPS-enabled cell phones to build a mobile service development framework. The goal of this framework is to build mobile services to deliver timely changes in environmental information that could impact a user's current or future activities. A weather warning system is developed based on this framework as a demonstration to inform people to leave or detour in advance to avoid being caught in the rain.",2008-01-01,['living-mobile'],,19263,E15-368,2016-12-05T00:16:48.085Z,['geek@media.mit.edu'],True,ringing-in-the-rain,2007-01-01,[],Ringing in the Rain,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:44.565Z,"The experience of being in a crowd is visceral. We feel a sense of connection and belonging through shared experiences like watching a sporting event, speech, or performance. In online environments, though, we are often part of a crowd without feeling it. ROAR is designed to allow very large groups of distributed spectators to have meaningful conversations with strangers or friends while creating a sense of presence of thousands of other spectators. ROAR is also interested in creating opportunities for collective action among spectators and providing flexible ways to share content among very large groups. These systems combine to let you feel the roar of the crowd even if you're alone in your bedroom.",2012-09-01,['living-mobile'],,19311,--Choose Location,2016-12-05T00:16:47.950Z,['geek@media.mit.edu'],True,roar,2011-01-01,[],ROAR ,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:11:44.699Z,"RoBallet is a research project that brings together the arts, learning, and technology. Children extend the joy and creativity of expression through dance. They use technology to augment their choreography by programming interactive robots, animation, music, light, and images in their dance space. They outfit their bodies and the environment with computational devices and sensors so that their bodies can generate the activity of the space. Central to this vision is the insistence that children should control technology to serve their imaginations, and not be driven by it. The experience results in a deeper understanding of the three themes of RoBallet: arts as expression, technology as a means to serve expression, and learning as ""hard fun""�creative and enjoyable yet disciplined and purposeful.",2005-09-01,['future-of-learning'],,19319,E15-443,2016-12-05T00:16:48.148Z,['cavallo@media.mit.edu'],True,roballet,2003-01-01,[],RoBallet,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:44.871Z,"Robo Topobo is a controller that children can use to save, replay, and adjust playback of up to four Topobo recordings. Robo introduces improvisational performance, typical of video games, to a dynamic construction kit. With Topobo you can bring your creations to life, and with Robo you can direct them in a performance. To use Robo, a child builds a creation and physically programs it by twisting its body around. She can save a motion by pressing ""record"" and then one of four playback buttons. Pressing that same playback button will cause the recording to spontaneously play back. She can also play the motion backwards and two joysticks can change the speed and size of the motion. We are using Robo to create Topobo performances and to do ""battle bots"" style robot design competitions.",2006-01-01,['tangible-media'],,19327,E15-344,2016-12-05T00:16:48.109Z,['ishii@media.mit.edu'],True,robo-topobo,2005-09-01,[],Robo Topobo,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:45.026Z,"Robocrop combines sensors, networking, and plants to create a modular desktop gardening system, in which gardeners can have as much information and control as they wish. We are developing a kit of parts to enable a deeper understanding of the variables that effect plant growth in an accessible way�a MindStorms for nature.",2000-12-30,"['counter-intelligence', 'personal-information-architecture']",,19335,E15-068,2016-12-05T00:17:22.178Z,['mike@media.mit.edu'],True,robocrop,1999-12-31,[],Robocrop,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:45.600Z,"To serve us well, robots and other agents must understand our needs and how to fulfill them. To that end, our research develops robots that empower humans by interactively learning from them. Interactive learning methods enable technically unskilled end-users to designate correct behavior and communicate their task knowledge to improve a robot's task performance. This research on interactive learning focuses on algorithms that facilitate teaching by signals of approval and disapproval from a live human trainer. We operationalize these feedback signals as numeric rewards within the machine-learning framework of reinforcement learning. In comparison to the complementary form of teaching by demonstration, this feedback-based teaching may require less task expertise and place less cognitive load on the trainer. Envisioned applications include human-robot collaboration and assistive robotic devices for handicapped users, such as myolectrically controlled prosthetics.",2014-05-01,['personal-robots'],https://dam-prod2.media.mit.edu/x/files/Display/Thumbnail-TAMERNexiKeepConvoDist.jpg,19359,--Choose Location,2017-05-31T18:41:05.939Z,['cynthiab@media.mit.edu'],True,robot-learning-from-human-generated-rewards,2012-09-01,[],Robot Learning from Human-Generated Rewards,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:45.810Z,"Young Learner's Companion Developing robots' growth mindset and pro-curious behavior and fostering the same in young learners via long-term interactionA growth mindset and curiosity have significant impact on children's academic and social achievements. We are developing and evaluating a novel expressive cognitive-affective architecture that synergistically integrates models of curiosity, understanding of mindsets, and expressive social behaviors to advance the state-of the-art of robot companions. In doing so, we aim to contribute major advancements in the design of AI algorithms for artificial curiosity, artificial mindset, and their verbal and non-verbal expressiveness in a social robot companion for children. In our longitudinal study, we aim to evaluate the robot companion's ability to sustain engagement and promote children's curiosity and growth mindset for improved learning outcomes in an educational play context.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/17/Screen Shot 2016-10-03 at 11.31.22 AM_TlwcQ2Z.png,19367,--Choose Location,2020-04-03T20:15:42.863Z,"['cynthiab@media.mit.edu', 'haewon@media.mit.edu', 'safinah@media.mit.edu', 'akostrow@media.mit.edu']",True,robot-mindset-and-curiosity,2015-09-01,"['artificial-intelligence', 'kids', 'learning-teaching', 'social-robotics', 'long-term-interaction', 'technology', 'covid19']",Robot Mindset and Curiosity,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:46.017Z,"Using a material expressively requires knowledge and practice. Digital media can be more demanding in this respect; unlike clay or paint, they often lack entry points with which artists may immediately begin creating. Expanding on the Cricket toolkit previously developed at the Media Lab for robotic constructions, we created a suite of tools and activities to introduce artists to robotic/electronic media, allowing them to design and create early in the learning process yet fully supporting further explorations in the future.",2002-09-01,['lifelong-kindergarten'],,19375,E15-001,2016-12-05T00:17:22.268Z,['mres@media.mit.edu'],True,robotic-art-studio,2001-01-01,[],Robotic Art Studio,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:46.169Z,"Digital Materials are a way of designing and manufacturing. Rather than building large, monolithic, single-use components, we discretize the material into simple, repeating, functional bits. A discrete set of base elements are combined to form cellular lattices with bulk material properties. This lets us cheat: we can maximize the performance of our material by assembling high-performance sub-elements, and their reversibility maximizes the sustainability and post-life reusability of the product. With all of these discrete units, assembly becomes a chore, and automation becomes crucial. The structured nature of the lattice enables assembler robots to use the geometry of the lattice for locomotion and error-correction. Further, the structured nature of the discretized lattice lends itself to novel design and simulation tools that exploit functional representations of the geometry to open the design space to previously unthinkable regimes of simulation, topological design and manufacture path-planning.http://dma.cba.mit.eduhttp://dma.cba.mit.edu/mechanical/index.html",,[],,19383,--Choose Location,2017-03-30T19:52:24.235Z,"['neilg@media.mit.edu', 'mcarney@media.mit.edu']",True,robotic-assembly-of-discrete-cellular-lattices-digital-materials,2013-09-01,"['design', 'architecture', 'construction']",Robotic Assembly of Discrete Cellular Lattices (Digital Materials),PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:46.337Z,"The Robotic F.A.C.E. project explores the use of a physical object in the form of a face as a means of user interaction, taking advantage of socially intuitive facial expressions. As a testbed, we use an expressive robotic head, based on the mechanics of a commercial robotic toy that can convey socially strong, non-verbal facial cues to alert and notify. The head, which can be controlled via a powerful serial protocol, is capable of expressing most basic emotions not only statically, but also dynamically, as animation loops that vary some parameter, such as activity, over time. ",2005-01-01,['living-mobile'],,19391,E15-368,2016-12-05T00:17:22.315Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,robotic-face-facial-alerting-in-a-communication-environment,2003-01-01,[],Robotic F.A.C.E.: Facial Alerting in a Communication Environment,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:46.639Z,"Young children learn language not through listening alone, but through active communication with a social actor. Cultural immersion and context are also key in long-term language development. We are developing robotic conversational partners and hybrid physical/digital environments for language learning. For example, the robot Sophie helped young children learn French through a food-sharing game. The game was situated on a digital tablet embedded in a cafe table. Sophie modeled how to order food and as the child practiced the new vocabulary, the food was delivered via digital assets onto the table's surface. A teacher or parent can observe and shape the interaction remotely via a digital tablet interface to adjust the robot's conversation and behavior to support the learner. More recently, we have been examining how social nonverbal behaviors impact children's perceptions of the robot as an informant and social companion.",2014-09-01,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/17/cyber2-robots.png,19415,--Choose Location,2017-05-31T18:10:02.374Z,"['cynthiab@media.mit.edu', 'jakory@media.mit.edu', 'sooyeon6@media.mit.edu']",True,robotic-language-learning-companions,2010-01-01,"['robotics', 'kids', 'learning-teaching', 'social-robotics', 'language-learning']",Robotic Language Learning Companions,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:46.729Z,"The language and literacy skills of young children entering school are highly predictive of their long-term academic success. Children from low-income families are particularly at risk. Parents often work multiple jobs, giving them less time to talk to and read with their children. Parents might be illiterate or not speak the language taught in local schools, and they may not have been read to as children, providing less experience of good co-reading practice to draw upon. We are currently developing a robotic reading companion for young children, trained by interactive demonstrations from parents and/or educational experts. We intend for this robot to complement parental interaction and emulate some of their best practices in co-reading, building language and literacy through asking comprehension questions, prompting exploration, and simply being emotionally involved in the child's reading experience.",2016-06-01,[],https://dam-prod2.media.mit.edu/x/2016/10/17/dragons-animals-jayden.png,19423,--Choose Location,2017-06-07T16:36:47.771Z,"['cynthiab@media.mit.edu', 'jakory@media.mit.edu', 'samuelsp@media.mit.edu', 'dnunez@media.mit.edu']",True,robotic-learning-companions,2013-01-01,"['robotics', 'kids', 'learning-teaching', 'social-robotics', 'language-learning']",Robotic Learning Companions,PUBLIC,http://robotic.media.mit.edu/portfolio/cyberlearning/,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:46.813Z,"We are exploring new modalities of creative photography through robotics and long-exposure photography. Using a robotic arm, a light source is carried through precise movements in front of a camera. Photographic compositions are recorded as images of volumetric light. Robotic light 'painting' can also be inverted: the camera is moved via the arm to create an image 'painted' with environmental light. Finally, adding real-time sensor input to the moving arm and programming it to explore the physical space around objects can reveal immaterial fields like radio waves, magnetic fields, and heat flows.",2012-09-01,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/robotic_light_expressions_%2015.jpg,19431,,2016-12-05T00:17:05.179Z,"['stevenk@media.mit.edu', 'neri@media.mit.edu']",True,robotic-light-expressions,2012-01-01,[],Robotic Light Expressions,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:46.892Z,"We are investigating e-textiles and fiber-electronics to develop unique soft-architecture robotic components. We have been developing large area force sensors utilizing quantum tunneling composites integrated into textiles creating fabrics that can cover the body/surface of the robot and sense touch. By using e-textiles we shift from the metaphor of a sensing skin, often used in robotics, to one of sensing clothing. We incorporated apparel design and construction techniques to develop modular e-textile surfaces that can be easily attached to a robot and integrated into a robotic system. Adding new abilities to a robot system can become as simple as changing its clothes. Our goal is to study social touch interaction and communication between people and robots while exploring the benefits of textiles and the textile aesthetic.",2013-09-01,['personal-robots'],,19439,--Choose Location,2016-12-05T00:17:22.383Z,['cynthiab@media.mit.edu'],True,robotic-textiles,2010-01-01,[],Robotic Textiles,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:46.987Z,"In order to build machines with common sense we need to find ways to integrate a wide variety of specific ways to represent knowledge, reason with that knowledge, and arrange such resources into a larger architecture. To study this problem we are building a rich, simulated environment in which a pair of simulated robots live and perform various tasks: walking about, manipulating objects, building structures together, and conversing with each other about events in their world.",2006-01-01,['society-of-mind'],,19447,E15-385,2016-12-05T00:16:48.320Z,['minsky@media.mit.edu'],True,robots-with-common-sense,2000-01-01,[],Robots with Common Sense,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:11:47.155Z,"Roboverse is a physical artificial intelligence simulation. The environment supports rigid body physical simulation of wheeled and legged robots with default algorithms for handling the basic reactive control layers to move from one location to another by straight line, to observe objects in the local neighborhood of the robot's 2D position, to pick up, translate and rotate objects by using a servo interface. Higher level cognitive functions, such as speech and social problem solving were developed as part of Push Singh's cognitive architecture design, the Emotion Machine v1.0 (EM-1).",2007-09-01,"['society-of-mind', 'responsive-environments']",,19455,E15-353,2016-12-05T00:17:22.436Z,"['minsky@media.mit.edu', 'joep@media.mit.edu']",True,roboverse-physical-artificial-intelligence-simulation,1999-01-01,[],Roboverse: Physical Artificial Intelligence Simulation,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:47.640Z,"A robotic computer that moves its monitor ""head"" and ""neck,"" but that has no explicit face, is being designed to interact with users in a natural way for applications such as learning, rapport-building, interactive teaching, and posture improvement. In all these applications, the robot will need to move in subtle ways that express its state and promote appropriate movements in the user, but that don't distract or annoy. Toward this goal, we are giving the system the ability to recognize states of the user and also to have subtle expressions.",2008-09-01,['affective-computing'],,19479,E15-468,2016-12-05T00:16:11.022Z,['picard@media.mit.edu'],True,roco-a-robotic-desktop-computer,2003-01-01,[],RoCo: A Robotic Desktop Computer,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:47.999Z,"Learning is rooted in people and culture and bears fruit through the construction process, with shoots that branch into new areas. These principles of learning are experientially based, differing markedly from the concepts that require a disconnected accumulation of chunks of knowledge. In order for learning to become truly rooted, a person must have a deep emotional attachment to the subject area. Rooting and the possibilities for branching flow from a better understanding of affect, comfort, culture, and motivation. We have found that when individuals participate in shared activity, they develop new ideas for themselves (fruits) and the collaborative process transforms and shapes the organization of the community as a whole (shoots). Rather than thinking of learning and expertise as a body of acquired skills and knowledge unique to an individual, we adopt a more ""situated learning"" perspective, where the unit of analysis for expertise is the community itself. We are building technological affordances to rooted knowledge that serve as mentors and that foster the creative and idiosyncratic connections (new shoots) to learning that help community members progress through apprenticeship, guided participation, and participatory appropriation.",2004-09-01,"['gray-matters', 'future-of-learning-2', 'electronic-publishing']",,19503,Pond,2016-12-05T00:16:48.435Z,"['cavallo@media.mit.edu', 'walter@media.mit.edu']",True,roots-fruits-and-shoots,2003-09-01,[],"Roots, Fruits, and Shoots",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:48.120Z,"Rope Revolution is a rope-based gaming system for collaborative play. After identifying popular rope games and activities from around the world, we developed a generalized, tangible rope interface that includes a compact motion-sensing and force-feedback module that can be used for a variety of rope-based games, such as rope jumping, kite flying, and horseback riding. Rope Revolution is designed to foster both co-located and remote collaborative experiences by using actual rope to connect players in physical activities across virtual spaces.",2012-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202011-10-07%20at%205.50.57%20PM.png,19511,--Choose Location,2016-12-05T00:16:48.372Z,"['ishii@media.mit.edu', 'sdg1@media.mit.edu', 'liningy@media.mit.edu']",True,rope-revolution,2011-01-01,[],Rope Revolution,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:48.355Z,"Rottlace is a family of masks designed for Icelandic singer-songwriter Björk. Inspired by Björk’s most recent album—Vulnicura—the Mediated Matter Group explored themes associated with self-healing and expressing ""the face without a skin."" The series originates with a mask that emulates Björk’s facial structure and concludes with a mask that reveals a new identity, independent of its origin. What originates as a form of portraiture culminates in reincarnation.",2018-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/uuid/424abb52-bbf9-4cdf-978f-0e51de216381,19527,--Choose Location,2021-05-21T13:42:33.713Z,"['bader_ch@media.mit.edu', 'kolb@media.mit.edu', 'neri@media.mit.edu']",True,rottlace,2016-01-01,"['design', 'art', 'manufacturing']",Rottlace,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:48.723Z,"Rumble is an ambient notification platform to help you learn the locations of invisible urban infrastructures. Some example infrastructures you might want to learn are: subway lines, free WiFi hotspots, public restrooms, and so on. Android-based software monitors your location in the background; when you are near a resource you wanted to know about, a wearable peripheral device vibrates unobtrusively. If you decide you want to learn more about a certain feature, you can press the wearable against your head and hear information via a bone-conduction driver. The work's motivation is to help people optimize their everyday urban experience.",2009-01-01,['opera-of-the-future'],,19559,,2016-12-05T00:16:48.547Z,['tod@media.mit.edu'],True,rumble,2009-09-01,[],Rumble,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:48.819Z,"The scourge of cyberbullying has assumed worrisome proportions, with an ever-increasing number of adolescents admitting to having dealt with it either as a victim or a bystander. Anonymity and the lack of meaningful supervision in the electronic medium are two factors that have exacerbated this social menace. This project explores computational methods from natural language processing and reflective user interfaces to alleviate this problem.",,[],,19567,--Choose Location,2016-12-05T00:16:11.046Z,"['lieber@media.mit.edu', 'picard@media.mit.edu', 'kdinakar@media.mit.edu']",True,ruminati-tackling-cyberbullying-with-computational-empathy,2010-09-01,[],Ruminati: Tackling Cyberbullying with Computational Empathy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:48.928Z,"The spread of malicious or accidental misinformation in social media, especially in time-sensitive situations, such as real-world emergencies, can have harmful effects on individuals and society. In this work, we developed models for automated verification of rumors (unverified information) that propagate through Twitter. To predict the veracity of rumors, we identified salient features of rumors by examining three aspects of information spread: linguistic style used to express rumors, characteristics of people involved in propagating information, and network propagation dynamics. The predicted veracity of a time series of these features extracted from a rumor (a collection of tweets) is generated using Hidden Markov Models. The verification algorithm was trained and tested on 209 rumors representing 938,806 tweets collected from real-world events, including the 2013 Boston Marathon bombings, the 2014 Ferguson unrest, and the 2014 Ebola epidemic, and many other rumors about various real-world events reported on popular websites that document public rumors. The algorithm was able to correctly predict the veracity of 75% of the rumors faster than any other public source, including journalists and law enforcement officials. The ability to track rumors and predict their outcomes may have practical applications for news consumers, financial markets, journalists, and emergency services, and more generally to help minimize the impact of false information on Twitter.",2015-06-01,['social-machines'],https://dam-prod2.media.mit.edu/x/2017/10/16/ver_approach_2_3N1N0mP.jpg,19575,--Choose Location,2018-06-08T00:36:14.072Z,"['dkroy@media.mit.edu', 'soroush@media.mit.edu']",True,rumor-gauge-automatic-detection-and-verification-of-rumors-in-twitter,2013-04-15,['social-media'],Rumor Gauge: Automatic Detection and Verification of Rumors on Twitter,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:49.016Z,"RunLog is a simple Web application that keeps track of how much a user runs. For frequent runners, RunLog can be a useful tool to keep track of running goals. But RunLog was built to motivate people who spend most of their day working in front of a computer. Many people start running to get in shape as a New Year's resolution, and hopefully RunLog will help users maintain goals through the support of an online social network.",2008-01-01,['information-ecology'],,19583,E15-301,2016-12-05T00:16:48.458Z,[],True,runlog,2007-01-01,[],RunLog,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:49.103Z,"Safe & Sound is an application that enables a parent to define a ""secure zone"" for a child. A secure zone is an area where the parent considers the child to be safe. If the child ventures out of this zone, the parent will be alerted. He can then decide whether to place a call to the child or to log the child's coordinates (latitude, longitude, direction heading, and speed). The child will also receive an audio alert indicating that he is out of the defined boundary.",2004-01-01,['living-mobile'],,19591,E15-344,2016-12-05T00:16:48.485Z,['geek@media.mit.edu'],True,safe-sound,2001-01-01,[],Safe & Sound,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:49.188Z,"SAGE is an authoring environment that allows children to create interactive  storytellers, embodied in stuffed animals, with which they can talk and listen.",2000-12-30,['gesture-and-narrative-language'],,19599,,2016-12-05T00:17:22.539Z,[],True,sage-storyteller,1994-12-31,[],SAGE Storyteller,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:49.361Z,"The Salvage Table is a reuse system that educates participants about electronic items left for recycling. The Salvage Table breaks down complex circuitry into core, reusable pieces and provides information on how to reuse them. While the table is a main feature of the current trial implementation, a core component is crowd-sourcing. Users can visit salvage.media.mit.edu, browse through the items that have been left at the Salvage Table, and add missing meta-information (e.g., components on the boards, ways to use the chips or components). A long-term goal is to influence company policy for manufacturing parts so that modularization and reuse are prioritized. Eventually, companies may even re-harvest parts from older models for use in other products or in research labs. Produce less, salvage more.
",2010-01-01,['information-ecology'],,19615,,2016-12-05T00:16:48.571Z,['holtzman@media.mit.edu'],True,salvage,2010-01-01,[],Salvage,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:49.610Z,"The U.S. banking crisis and economic downturn has led to many people to seek assistance from social service agencies who'd not previously sought assistance. Same Boat lets people who've discovered helpful social services to share their discoveries with others who are in the same boat. Using the technologies in our What's Up toolkit, Same Boat is currently in the process of co-development in Wisconsin Falls, Wisconsin in partnership with the Community Foundation of South Wood County, Wisconsin. ",2012-01-01,['civic-media'],,19639,,2016-12-05T00:16:48.622Z,"['ethanz@media.mit.edu', 'leob@media.mit.edu']",True,same-boat,2011-09-01,[],Same Boat,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:49.684Z,"Sanctuary is an educational game to be played in pairs. It addresses topics in high-school biology and mathematics, and encourages players to become collaborative scientists with asymmetric interfaces and tools.",2013-09-01,['lifelong-kindergarten'],,19647,--Choose Location,2016-12-05T00:16:48.651Z,"['klopfer@media.mit.edu', 'jhaas@media.mit.edu']",True,sanctuary,2012-01-01,[],Sanctuary,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:49.764Z,"Sand:stone is a piece developed for the 7th Annual New York Digital Salon and subsequent installation in galleries across Europe. The physical installation is a box of sand with some stones in it. On a screen on the far side, there is a statue on a pedestal, reminiscent of Rodin's Thinker. The sound of wind blows by the desolate statue. As the participant moves the stones, a strain of music unfolds. The statue stretches and raises its arms in response to the movement of the stones. As different arrangements of stones are created, the statue takes on different feelings: a young man breathing in a bracing wind, a lonesome traveler bowed down by rain, a farmer bending to embrace the earth, a tortured zealot being burned at the stake. Through this installation we hope to investigate how character may be conveyed through music, light, and camera.",2000-12-30,['synthetic-characters'],,19655,,2016-12-05T00:16:48.675Z,[],True,sandstone,1998-12-31,[],sand:stone,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:49.840Z,"SandScape is a tangible interface for designing and understanding landscapes through a variety of computational simulations using sand. The simulations are projected on the surface of a sand model representing the terrain; users can choose from a variety of different simulations highlighting height, slope, contours, shadows, drainage, or aspect of the landscape model, and alter its form by manipulating sand while seeing the resulting effects of computational analysis generated and projected on the surface of sand in real time. SandScape demonstrates an alternative form of computer interface (tangible user interface) that takes advantage of our natural abilities to understand and manipulate physical forms while still harnessing the power of computational simulation to help in our understanding of a model representation.",2004-01-01,['tangible-media'],,19663,E15-344,2022-01-19T20:16:48.533Z,['ishii@media.mit.edu'],True,sandscape,2002-01-01,[],SandScape,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:50.183Z,"The Sustainable Access in Rural India project (SARI) seeks to show that viable markets exist for information and communication services in poor rural areas by inventing and deploying innovative technologies, assessments, and business models. The ultimate goal is to link these activities to sustainable human development objectives. SARI is part of the Digital Nations consortium of the MIT Media Lab and Harvard's Center for International Development. Other key partners include IIT-Madras and the I-Gyan Foundation.",2002-12-30,['edevelopment'],,19687,E15-391,2016-12-05T00:17:22.640Z,[],True,sari-sustainable-access-in-rural-india,1999-12-31,[],SARI: Sustainable Access in Rural India,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:50.356Z,Complicated models that can only be understood by experts block the communication process when decision makers have to choose between the different policies to be implemented.  SARS-TA is a new methodology for explaining and communicating the consequences of the various alternatives that governments will face in the battle against SARS. It is designed to provide both understanding and motivation to those in government who must make difficult decisions in a timely fashion by demonstrating the likely consequences of a lack of preparedness or delays in the implementation of specific recommendations.,2003-01-01,['electronic-publishing'],,19695,E15-320G,2016-12-05T00:16:48.778Z,[],True,sars-ta,2003-01-01,[],SARS-TA,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:11:50.792Z,"This project explores the relationship between empathy and civic engagement. We have designed and implemented a seven-part workshop to foster mutual understanding, collaborative problem-solving, and self-expression. The curriculum (which employs Scratch as a central tool) builds capacities in three areas: programming, storytelling, and perspective-taking. Throughout the workshop, participants use a variety of tools and techniques to engage in acts of personal expression by creating rich, interactive, multi-threaded narratives.",2010-01-01,['lifelong-kindergarten'],,19711,LEGO Learning Lab,2016-12-05T00:16:49.348Z,"['mres@media.mit.edu', 'kbrennan@media.mit.edu']",True,say-what,2008-01-01,[],Say What?!,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:51.029Z,"We demonstrate the design and implementation of a new versatile, scalable, and cost-effective sensate surface. The system is based on a new conductive inkjet technology, which allows capacitive sensor electrodes and different types of RF antennas to be cheaply printed onto a roll of flexible substrate that may be many meters long. By deploying this surface on (or under) a floor, it is possible to detect the presence and whereabouts of users through both passive and active capacitive coupling schemes. We have also incorporated GSM and NFC electromagnetic radiation sensing and piezoelectric pressure and vibration detection. We believe that this technology has the potential to change the way we think about covering large areas with sensors and associated electronic circuitry�not just floors, but potentially desktops, walls, and beyond.",2013-09-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/ubic404.jpg,19719,--Choose Location,2016-12-05T00:17:22.665Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,scalable-and-versatile-surface-for-ubiquitous-sensing,2011-01-01,[],Scalable and Versatile Surface for Ubiquitous Sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:51.231Z,"To adapt this vision for video to the rapidly evolving Internet, we are developing an architecture for the Shareable Media project which is scaleable and which will allow a range of users to augment and to play with the shared media objects. This architecture provides standardized APIs, which invite users to create their own interface applications for wired and wireless devices using both broadband and narrowband connections. This builds on I-View, a system developed by Pengkai Pan.",2002-12-30,"['gray-matters', 'interactive-cinema']",,19727,E15-351,2016-12-05T00:16:48.941Z,['gid@media.mit.edu'],True,scalable-architecture-for-shareable-media,1999-12-31,[],Scalable Architecture for Shareable Media,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:11:51.726Z,"Multicasting is one way to converge the delivery of broadcast video with the Internet. Last year, we built a protocol that used every receiver as a shared memory to allow personal music from an arbitrarily large set of channels. This year, we are adding scalable video to the multicast system to allow graceful degradation and variable resolution viewing. With SMD, on-demand delivery and time-shifting become not only feasible, but practical.",2001-12-30,['media-and-networks'],,19751,E15-344,2016-12-05T00:16:48.919Z,['lip@media.mit.edu'],True,scalable-multicasting,1999-12-31,[],Scalable Multicasting,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:51.876Z,"Our goals include novel gene logic and data logging systems, as well as DNA scaffolds that can be produced on commercial scales. State of the art in the former is limited by finding analogous and orthogonal proteins for those used in current single-layer gates and two-layered circuits. State of the art in the latter is constrained in size and efficiency by kinetic limits on self-assembly. We have designed and plan to demonstrate cascaded logic on chromosomes and DNA scaffolds that exhibit exponential growth.",,['molecular-machines'],https://dam-prod2.media.mit.edu/x/2016/10/21/MM-1.jpg,19759,,2017-08-29T04:20:18.205Z,"['jacobson@media.mit.edu', 'njakimo@media.mit.edu', 'lisanip@media.mit.edu']",True,scaling-up-dna-logic-and-structures,2012-09-01,"['bioengineering', 'synthetic-biology']",Scaling Up DNA Logic and Structures,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:52.011Z,"Scanner Grabber is a digital police scanner that enables reporters to record, playback, and export audio, as well as archive public safety radio (scanner) conversations. Like a TiVo for scanners, it's an update on technology that has been stuck in the last century. It's a great tool for newsrooms. For instance, a problem for reporters is missing the beginning of an important police incident because they have stepped away from their desk at the wrong time. Scanner Grabber solves this because conversations can be played back. Also, snippets of exciting audio, for instance a police chase, can be exported and embedded online. Reporters can listen to files while writing stories, or listen to older conversations to get a more nuanced grasp of police practices or long-term trouble spots. Editors and reporters can use the tool for collaborating, or crowdsourcing/public collaboration.",,"['future-of-news', 'fluid-interfaces', 'civic-media']",,19767,--Choose Location,2022-06-10T12:56:41.460Z,"['pattie@media.mit.edu', 'ethanz@media.mit.edu', 'achituv@media.mit.edu']",False,scanner-grabber,2014-09-01,[],Scanner Grabber,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:52.397Z,"Electronic commerce on the Web is thriving, but consumers still have trouble finding products to meet their needs and desires. We introduce a novel recommender system technique which works even when users don�t necessarily know exactly what they're looking for. Users describe a goal for a real-life scenario, e.g., ""I want something elegant to wear for my boss's birthday party."" A common-sense reasoning system maps between the stated goals and possibly relevant characteristics of the product. Reasoning is based on an 800,000-sentence common-sense knowledge base, and spreading activation inference. Scenario-oriented recommendation breaks down boundaries between products' categories, finds the ""first example"" for existing techniques like collaborative filtering, and helps promote independent brands. We describe our scenario-oriented fashion recommendation system, What Am I Gonna Wear?.",2007-01-01,"['software-agents', 'sociable-media']",,19791,E15-383,2016-12-05T00:16:49.063Z,['lieber@media.mit.edu'],True,scenario-oriented-recommendation,2005-09-01,[],Scenario-Oriented Recommendation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:52.511Z,"Current datebook programs such as Palm Desktop or Microsoft Outlook will give you a single audiovisual reminder a set period of time before your appointment is due. We are exploring the use of smell to convey information about upcoming events. Scent Reminder is an addition to Microsoft Outlook's Calendar program. When you make the appointment, it releases your chosen scent to help connect the stimulus with the event, and then releases the scent at the appropriate time to remind you. We have also started to explore gradually releasing more of a scent, or combinations of scents, over time.",2001-12-30,"['counter-intelligence', 'personal-information-architecture']",,19799,E15-068,2016-12-05T00:17:22.709Z,['mike@media.mit.edu'],True,scent-reminder,2000-12-31,[],Scent Reminder,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:52.615Z,"Scents Sense attempts to incorporate the recent developments in the field of electronic smell into the home. As the heart of the home, the kitchen presents an ideal setting for experimenting with this field. From preventing baked goods from burning (Burning Scentsations) to detecting and identifying spoiled produce, the possibilities are almost endless. A major goal of the project is to further ongoing smell research with the creation of the .snf file, a universal file format that will assist with the identification and classification of any sample being tested.",2001-12-30,"['counter-intelligence', 'personal-information-architecture']",,19807,E15-068,2016-12-05T00:16:49.088Z,['mike@media.mit.edu'],True,scents-sense,1999-12-31,[],Scents Sense,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:52.715Z,"The SCOOT Behavior system is a rich and flexible behavior architecture for constructing directable autonomous animated characters. Derived from studies of animal behavior and Marvin Minsky's ""Society of Mind"", the SCOOT behavior system comprises a few simple ""building blocks,"" and can be used to easily implement arbitrary models of action selection, behavioral motivations, emotions, and learning. The SCOOT behavior system allows us to easily construct characters that have distinct personalities and respond ""in character"" to the situations they encounter at any given moment.  It is the underlying behavior architecture used in Swamped!",1999-12-30,['synthetic-characters'],,19815,,2016-12-05T00:16:49.111Z,[],True,scoot-behavior-system,1996-12-31,[],SCOOT Behavior System,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:52.811Z,"A character's thoughts, intentions, and feelings are ultimately revealed through its motion. This project seeks to make it possible to combine key-framed animation, procedural animation, and user-driven animation in a seamless and expressive manner, such that the character always moves ""in character."" In particular, it focuses on the twin problems of spatial blending (i.e., compositing the motion of motor skills that affect different parts of the body simultaneously), and temporal blending (i.e., transitioning smoothly from one motor skill to another over time). In addition, borrowing ideas from Rose and Cohen, we are exploring the use of multi-target motion interpolation so as to be able to interpolate in ""adverb space,"" (i.e., given a sample of an ""very happy walk"" and a ""very sad walk"" be able to generate a ""slightly sad walk."") The SCOOT motor system was used to implement the motor systems of the individual characters in Swamped!",1998-09-01,[],,19823,--Choose Location,2016-12-05T00:16:49.158Z,[],True,scoot-motor-system,1996-01-01,[],SCOOT Motor System,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:52.899Z,"Scratch is the world's most popular coding community for kids. Millions of kids around the world are using Scratch to program their own interactive stories, games, and animations—and share their creations in an active online community. In the process, they are learning to think creatively, reason systematically, and work collaboratively, while also learning important mathematical and computational ideas. In the past year, more than 20 million people created projects with Scratch. (For information on who has contributed to Scratch, see the Scratch Credits page.)",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/10/24/Scratch5.jpg,19831,LEGO Learning Lab,2020-06-14T23:08:22.009Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'bowman@media.mit.edu', 'sdg1@media.mit.edu', 'ria@media.mit.edu', 'tmickel@media.mit.edu', 'cwillisf@media.mit.edu', 'mataylor@media.mit.edu', 'kaschm@media.mit.edu', 'colbygk@media.mit.edu', 'christan@media.mit.edu', 'tarmelop@media.mit.edu', 'morant@media.mit.edu', 'ascii@media.mit.edu', 'jbuitrag@media.mit.edu', 'millner@media.mit.edu', 'jmaloney@media.mit.edu', 'silver@media.mit.edu', 'ericr@media.mit.edu', 'kbrennan@media.mit.edu', 'shanemc@media.mit.edu', 'shrutid@media.mit.edu', 'eschill@media.mit.edu', 'rschamp@media.mit.edu', 'chrisg@media.mit.edu', 'sleggss@media.mit.edu', 'jwzimmer@media.mit.edu', 'otts@media.mit.edu', 'khanning@media.mit.edu', 'hisean@media.mit.edu', 'jaleesat@media.mit.edu', 'champika@media.mit.edu']",True,scratch,2002-01-01,"['robotics', 'design', 'art', 'artificial-intelligence', 'entertainment', 'gaming', 'kids', 'learning-teaching', 'music', 'storytelling', 'computer-science', 'makers', 'creativity', 'technology', 'alumni', 'public-health', 'code', 'covid19']",Scratch,PUBLIC,https://scratch.mit.edu,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:53.069Z,"With the Scratch Board, people can use sensors to control interactive stories and games that they create with the Scratch programming language. By connecting the physical and virtual, the Scratch Board extends the range of what people can design -- and extends what they learn in the process. The Scratch Board comes with several built-in sensors: a light sensor, sound sensor, touch sensor, and slider. It also has four ports where you can plug in your own resistance-based sensors. For example, you can create a Scratch program that controls music and animation on the computer based on your interactions with sensors connected to the Scratch Board.",2010-01-01,['lifelong-kindergarten'],,19839,E15-001,2016-12-05T00:16:48.808Z,"['mres@media.mit.edu', 'millner@media.mit.edu', 'jmaloney@media.mit.edu', 'bss@media.mit.edu']",True,scratch-board,2007-01-01,[],Scratch Board,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:53.211Z,"Scratch Community Blocks is an NSF-funded project that extends the Scratch programming language to enable youth to analyze and visualize their own learning and participation in the Scratch online community. With Scratch Community Blocks, youth in the Scratch community can easily access, analyze, and represent data about the ways they program, share, and discuss Scratch projects.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/scratch_data_blocks_logo.png,19847,--Choose Location,2016-12-05T00:17:22.736Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'sdg1@media.mit.edu', 'ascii@media.mit.edu']",True,scratch-data-blocks,2014-01-01,[],Scratch Community Blocks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:53.353Z,"Scratch Day (day.scratch.mit.edu) is a network of face-to-face local gatherings, on the same day in all parts of the world, where people can meet, share, and learn more about Scratch, a programming environment that enables people to create their own interactive stories, games, animations, and simulations. We believe that these types of face-to-face interactions remain essential for ensuring the accessibility and sustainability of initiatives such as Scratch. In-person interactions enable richer forms of communication among individuals, more rapid iteration of ideas, and a deeper sense of belonging and participation in a community. The first Scratch Day took place in 2009. In 2015, there were 350 events in 60 countries.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/10/24/Scratch-Day.jpg,19855,LEGO Learning Lab,2022-09-22T01:17:05.973Z,"['kaschm@media.mit.edu', 'sleggss@media.mit.edu', 'abisola1@media.mit.edu', 'mres@media.mit.edu', 'champika@media.mit.edu']",True,scratch-day,2009-01-01,"['gaming', 'kids', 'learning-teaching', 'makers']",Scratch Day,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:53.493Z,"The Scratch extension system enables anyone to extend the Scratch programming language through custom programming blocks written in JavaScript. The extension system is designed to enable innovating on the Scratch programming language itself, in addition to innovating with it through projects. With the extension system, anyone can write custom Scratch blocks that enable others to use Scratch to program hardware devices such as the LEGO WeDo, get data from online web-services such as weather.com, and use advanced web-browser capabilities such as speech recognition.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/10/24/ScratchDay3.jpg,19863,--Choose Location,2022-09-22T01:18:53.521Z,"['sdg1@media.mit.edu', 'cwillisf@media.mit.edu', 'ascii@media.mit.edu', 'khanning@media.mit.edu', 'ericr@media.mit.edu', 'sdruga@media.mit.edu']",True,scratch-extensions,2013-01-01,"['gaming', 'kids', 'learning-teaching']",Scratch Extensions,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:53.621Z,"The Arduino platform makes prototyping and tinkering with electronics open to more people, but its complicated programming language is a barrier to entry.  Building on the graphical-blocks programming language developed in the Scratch project, we are creating a new, more accessible way to program the Arduino, so that more people can become ""makers"" with electronics.",2009-09-01,"['lifelong-kindergarten', 'high-low-tech']",,19871,E15-001,2016-12-05T00:16:49.398Z,"['mres@media.mit.edu', 'leah@media.mit.edu', 'ericr@media.mit.edu', 'mellis@media.mit.edu']",True,scratch-for-arduino,2009-09-01,[],Scratch for Arduino,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:53.741Z,"Although we designed Scratch primarily as a means for personal expression, a growing number of high schools and colleges (including Harvard and Berkeley) are using Scratch as an introduction to computer science and programming. The Scratch4CS project explores the question: ""Can we extend Scratch so that it is suitable for a full-semester introduction to programming and computational thinking?"" This question is particularly relevant now since there are several initiatives underway to rethink introductory computer science courses and advanced-placement exams.",2010-01-01,['lifelong-kindergarten'],,19879,LEGO Learning Lab,2016-12-05T00:16:11.068Z,"['mres@media.mit.edu', 'jmaloney@media.mit.edu']",True,scratch-for-computer-science,2009-09-01,[],Scratch for Computer Science,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:53.854Z,"What if everyone could create their own interactive content in virtual worlds? We are putting the playful and intuitive features of Scratch into a new programming language for Second Life. We hope to make it easier for everyone to create their own interactive virtual pets, dancefloors, games, clothing, houses, and whatever else they can imagine.",2010-09-01,['lifelong-kindergarten'],,19887,E15-001,2016-12-05T00:16:49.435Z,"['mres@media.mit.edu', 'ericr@media.mit.edu']",True,scratch-worlds,2009-09-01,[],Scratch Worlds,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:53.964Z,"As Scratch proliferates through the world, there is a growing need to support learners. But for teachers, educators, and others who are primarily concerned with enabling Scratch learning, there is a disconnect between their needs and the resources that are presently available through the Scratch Web site. ScratchEd is an online environment for Scratch educators to share stories, exchange resources, ask questions, and find people.",2012-09-01,['lifelong-kindergarten'],,19895,E15-001,2016-12-05T00:16:49.456Z,"['mres@media.mit.edu', 'kbrennan@media.mit.edu']",True,scratched,2008-01-01,[],ScratchEd,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:54.063Z,"ScratchJr makes coding accessible to younger children (ages 5-7), enabling them to program their own interactive stories, games, and animations. To make ScratchJr developmentally appropriate for younger children, we revised the interface and provided new structures to help young children learn relevant math concepts and problem-solving strategies. ScratchJr is available as a free app for iPads, Android, and Chromebook. ScratchJr started as a collaboration between the MIT Media Lab, Tufts University, and Playful Invention Company, and it is now maintained by the Scratch Foundation. See scratchjr.org for more information.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/10/20/iTunesArtwork@2x.png,19903,--Choose Location,2023-03-18T14:20:53.493Z,"['mres@media.mit.edu', 'tmickel@media.mit.edu', 'sdg1@media.mit.edu', 'ascii@media.mit.edu', 'chrisg@media.mit.edu']",True,scratchjr,2011-09-01,"['robotics', 'design', 'kids', 'learning-teaching', 'language-learning', 'computer-science', 'alumni']",ScratchJr,PUBLIC,https://www.scratchjr.org,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:11:54.168Z,"ScratchR is a platform for sharing programmable media online, allowing people to publish their own interactive stories, games, and animations. ScratchR is the engine behind the Scratch online community, a social network of young programmers. Unlike other user-generated content communities, ScratchR makes it easy to reuse other people's creations to foster collaborative learning. ScratchR allows members to rate, comment, tag, and create galleries. ScratchR is to programmable media what YouTube is to videos.",2012-01-01,['lifelong-kindergarten'],,19911,LEGO Learning Lab,2016-12-05T00:16:49.480Z,['mres@media.mit.edu'],True,scratchr,2006-09-01,[],ScratchR,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:54.366Z,"Screen interactions have been shown to contribute to increases in stress, anxiety, and deficiencies in breathing patterns. Since better respiration patterns can have a positive impact on wellbeing, ScreenSpire improves respiration patterns during information work using subliminal biofeedback. By using subtle graphical variations that are tuned to attempt to influence the user subconsciously, user distraction and cognitive load are minimized. To enable a truly seamless interaction, we have adapted an RF-based sensor (ResMed S+ sleep sensor) to serve as a screen-mounted contact-free and respiration sensor. Traditionally, respiration sensing is achieved with either invasive or on-skin sensors (such as a chest belt); having a contact-free sensor contributes to increased ease, comfort, and user compliance, since no special actions are required from the user.",2016-12-31,"['advancing-wellbeing', 'fluid-interfaces']",,19927,--Choose Location,2017-08-25T11:59:51.368Z,"['pattie@media.mit.edu', 'achituv@media.mit.edu', 'changzj@media.mit.edu']",True,screenspire,2015-01-01,[],ScreenSpire,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:54.798Z,"The majority of today's communication technology focuses on communicating over distance, yet there are many ways technology can enrich face-to-face conversations. ""Second Messenger"" is a suite of interfaces, designed to augment a face-to-face interaction, that reveals simple observations about group dynamics. The goal of this project is to provide ways for a group to reflect upon its current interaction and improve upon it by considering a more diverse set of viewpoints in the discussion. The core focus of this project is evaluating the impact of these interfaces on a group. What is a group member's emotional reaction to the interfaces and the information revealed? How do these interfaces change the behavior of a group? ",2006-01-01,['electronic-publishing'],,19967,E15-309,2016-12-05T00:16:05.718Z,['walter@media.mit.edu'],True,second-messenger,2003-01-01,[],Second Messenger,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:54.874Z,"We have created a 3D motion-tracking system with automatic, real-time vibrotactile feedback and an assembly of photo-sensors, infrared projector pairs, vibration motors, and a wearable suit. This system allows us to enhance and quicken the motor learning process in a variety of fields such as healthcare (physiotherapy), entertainment (dance), and sports (martial arts).",,['camera-culture'],,19975,--Choose Location,2019-04-19T18:35:28.951Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,second-skin-motion-capture-with-actuated-feedback-for-motor-learning,2010-09-01,[],Second Skin: Motion capture with actuated feedback for motor learning,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:54.959Z,"An environment for creative collaboration is significant for enhancing human communication and expressive activities. We introduce Second Surface, a novel multi-user Augmented reality system that fosters a real-time interaction for user-generated contents on top of the physical environment. This interaction takes place in the physical surroundings of everyday objects such as trees or houses. Our system allows users to place three dimensional drawings, texts, and photos relative to such objects and share this expression with any other person who uses the same software at the same spot. Second Surface explores a vision that integrates collaborative virtual spaces into the physical space. Our system can provide an alternate reality that generates a playful and natural interaction in an everyday setup.",2012-09-01,['tangible-media'],,19983,--Choose Location,2016-12-05T00:16:08.404Z,"['ishii@media.mit.edu', 'heun@media.mit.edu']",True,second-surface,2012-01-01,[],Second Surface,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:55.128Z,"An online language learning tool and game with a purpose (GWAP) designed to simultaneously gather annotated speech and text data, useful for improving natural language processing (NLP) applications and serve as an English-language learning resource.",2016-08-31,['digital-intuition'],,19999,,2021-07-14T15:33:51.765Z,"['havasi@media.mit.edu', 'kmh@media.mit.edu']",True,second-language-learning-using-games-with-a-purpose,2013-01-01,[],Second-Language Learning Using Games with a Purpose,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:55.511Z,Distributed power meters monitor electricity consumption at each outlet and broadcast measurements over the power line for other nodes to read. The collected information is stored and graphed by a computer with a serial port connection to any of the nodes in the home. The computer can remotely shut down outlets to save power from appliances in standby mode.,2001-12-30,['personal-information-architecture'],,20039,,2016-12-05T00:16:24.785Z,['mike@media.mit.edu'],True,seegreen,2000-12-31,[],SeeGreen,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:55.587Z,"This project focuses on new methods for analyzing, visualizing, and interacting with large text and speech collections. We are exploring transparent and interactive knowledge representations, graphical representations that provide users with the gist of complex underlying data, and methods for displaying data at multiple scales. This work has a wide range of applications in data mining and knowledge management.",2006-09-01,['social-machines'],,20047,E15-483,2016-12-05T00:16:24.814Z,"['dkroy@media.mit.edu', 'decamp@media.mit.edu']",True,seeing-meaning,2004-01-01,[],Seeing Meaning,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:55.670Z,"AI programs perform inference -- they go beyond exactly what users tell them. They make assumptions, infer user goals, learn from experience, adapt according to context. But how do we tell what they're doing and why? We need to get ""inside the head"" of programs that use complex inference or machine learning techniques. This project works on visualizing the relationship between concepts, statements about those concepts, and inference which causes the program to learn new things. We also show how visualization  can be used to help ""debug"" inference. Does the program have sufficient knowledge to draw particular kinds of conclusions? How ""liberal"" (quick to jump to conclusions) or ""conservative"" (require quite a lot of evidence before concluding something) should the inference be?",,[],https://dam-prod2.media.mit.edu/x/files/Display/Inference-Visualization-Henke.png,20055,--Choose Location,2016-12-05T00:16:24.839Z,[],True,seeing-thought-visualizing-inference,2013-01-01,[],Seeing Thought: Visualizing Inference,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:55.735Z,"Now that mobile phones are starting to have 3D display and capture capabilities, there are opportunities to enable new applications that enhance person-person communication or person-object interaction. This project explores one such application: acquiring 3D models of objects using cellphones with stereo cameras. Such models could serve as shared objects that ground communication in virtual environments and mirrored worlds, or in mobile augmented reality applications.",2011-01-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/Fall11_seeitshareit_logo.png,20063,--Choose Location,2016-12-05T00:16:24.862Z,['geek@media.mit.edu'],True,seeit-shareit,2011-09-01,[],SeeIt-ShareIt,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:55.818Z,"Segu� is a browsing agent that can discern the user's changes of interest by examining browsing history. The agent represents the pages in your browsing history, not via a hierarchical list or graph, but by using a series of ""skeins"" which represent changes in interest over time. ""Your history is only the beginning...""",2003-12-30,['software-agents'],,20071,E15-301,2016-12-05T00:16:24.758Z,['lieber@media.mit.edu'],True,segu,1999-12-31,[],Segu�,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:55.960Z,"Selectricity (formerly HyperChad) is a Web-based voting system that supports anonymous and voter-verifiable balloting, and includes an election-methods library that implements a variety of election techniques, including several preferential systems. Unlike most voting projects, Selectricity does not attempt to address the issues raised in mainstream political elections. Instead, it provides a simple set of tools that small groups and organizations can use to incorporate computationally complex decision-making into new areas, and for purposes where they ordinarily would find such decision-making prohibitively complex. By supporting a variety of election methods, it provides a way for users to explore and compare the effects of different voting systems and, ultimately, come to better decisions.",2010-01-01,['computing-culture'],,20087,E15-020C,2022-06-10T13:40:30.319Z,['csik@media.mit.edu'],True,selectricity,2006-01-01,[],Selectricity,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:56.142Z,"The Self-Cam is a wearable camera apparatus that consists of a chest-mounted camera aimed at the wearer�s face. Self-Cam was designed to be used in conjunction with a belt-mounted computer and real-time mental-state inference software that can be used with visual, auditory, or tactile output as personal feedback for the wearer. As the camera faces inward, many privacy issues are avoided�only those who choose to wear the Self-cam appear in the recorded video. Head movement can be seen and analyzed alongside facial expressions because the system rests on the chest and the light, simple nature of the structure allows it to be worn without any physical discomfort. By wearing the Self-Cam, you can explore who you appear to be from the outside. The Self-Cam acts as an objective point of view that might help you to understand yourself in a different light.",2007-09-01,['affective-computing'],,20103,E15-443,2016-12-05T00:16:24.911Z,['picard@media.mit.edu'],True,self-cam,2006-01-01,[],Self-Cam,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:11:56.219Z,"A first-step toward origami robotics, I/O paper is a pair of origami papers in which the red (controller) paper senses how it is being folded and the white (output) paper follows. When the white paper is flipped over, blintz folding allows the paper to get up, wobble around, and even flip itself over. The microcontroller and circuitry is on the body of the red paper and the white paper is actuated by shape memory alloy.",2012-01-01,['high-low-tech'],,20111,,2016-12-05T00:16:24.946Z,"['leah@media.mit.edu', 'jieqi@media.mit.edu']",True,self-folding-origami-paper,2011-01-01,[],Self-Folding Origami Paper,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:56.294Z,"We are investigating a novel pixellated vibrotactile device based on the hardware and software infrastructure developed in previous MLE-UL projects. This project, however, introduces the innovation of being both input and output devices for human-computer interaction, simultaneously. The resulting device will be modular, act as both an input and output device, and be self-configuring. Plug-in software will be developed for application authoring in existing commercial and open source packages, and a number of use scenarios will be developed and evaluated. (Funded by the Higher Education Authority of Ireland.)",2004-09-01,['responsive-environments'],,20119,E15-344,2016-12-05T00:16:33.708Z,['joep@media.mit.edu'],True,self-organized-pixellated-vibrotactile-input-output-device,2002-01-01,[],Self-Organized Pixellated Vibrotactile Input-Output Device,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:56.370Z,"We have built a compact piezoelectric push button and associated minimal circuitry that is able to transmit a digital RFID code to the immediate region (50-100 foot radius) upon a single button push, without the need of batteries or other energy sources. Such devices have the potential of enabling the introduction of controls and interfaces into interactive environments without requiring any wiring, optical/acoustic lines of sight, or batteries.",2003-12-30,"['cc', 'responsive-environments']",,20127,E15-441,2016-12-05T00:16:25.016Z,"['geppetto@media.mit.edu', 'joep@media.mit.edu']",True,self-powered-wireless-push-button-controller,2000-12-31,[],Self-Powered / Wireless Push Button Controller,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:56.457Z,"We are interested in investigating designs for a self-revealing audio ballot. Traditional audio ballots are best described as awkward afterthoughts heavily tied to visual representation. We are exploring a multivocal, multichannel system that will allow a faster and more easily accessible interpretation of the same information contained in visual displays.",2007-09-01,['context-aware-computing'],,20135,E15-320,2016-12-05T00:16:25.075Z,[],True,self-revealing-interface-for-audio-ballots,2004-09-01,[],Self-Revealing Interface for Audio Ballots,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:56.535Z,"We are developing a multimodal processing system called Fuse to explore the effects of visual context on the performance of speech recognition. We propose that a speech recognizer with access to visual input may ""second guess"" what a person says based on the visual context of the utterance, thereby increasing speech recognition accuracy. To implement this idea, several problems of grounding language in vision (and vice versa) must be addressed. The current version of the system consists of a medium vocabulary speech-recognition system, a machine-vision system that perceives objects on a tabletop; a language acquisition component that learns mappings from words to objects and spatial relations; and a linguistically driven focus of visual attention. A corpus of naturally spoken, fluent speech was used to evaluate system performance; speech ranged from simple constructions such as ""the vertical red block"" to more complex utterances such as ""the large green block beneath the red block."" We found that integrating visual context reduces the error rate of the speech recognizer by over 30 percent. We are currently investigating implications of this improved recognition rate on the overall speech understanding accuracy of the system. This work has applications in contextual natural language understanding for intelligent user interfaces. For example, in wearable computing applications, awareness of the user's physical context may be leveraged to make better predictions of the user�s speech to support robust verbal command and control.",2003-09-01,['social-machines'],,20143,E15-483,2016-12-05T00:16:25.048Z,['dkroy@media.mit.edu'],True,semantic-priming-of-speech-recognition-using-visual-context,2003-01-01,[],Semantic Priming of Speech Recognition Using Visual Context,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:56.689Z,"We are building a prototype infrastructure for real-time collection and distribution of comprehensive information about road traffic and conditions on a network of streets. Small (2 cm diameter) cylinders placed just under the road surface count passing vehicles by detecting magnetic-field disturbances. Periodically, these sensors transmit an updated count by radio to a regional repeater, which relays them to a central processing station. Each package, which costs about $40, is powered by a single lithium battery, and can be installed in a few minutes without tearing up the road. The central station uses the sensor data to compute queue-length and departure-rate statistics for each road segment, and uses Little's Formula to assign a time in seconds to each road link. This database is published once a minute on the Internet. Vehicles will use CDPD or a similar wireless network standard to download the relevant sections of the database, and plan optimum (shortest-time or least-fuel) routes through the road network, informing the driver of suggested route changes in real time. We will also explore adding other minimally expensive sensors to this package, such as piezo pickups for detecting road vibration, and perhaps accident signatures and temperature and humidity sensors to determine local road conditions.",2000-12-30,"['cc', 'responsive-environments']",,20159,E15-357,2016-12-05T00:16:25.964Z,['joep@media.mit.edu'],True,sensate-roadbeds,1998-12-31,[],Sensate Roadbeds,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:56.772Z,"Senseboard is one facet of our using tangible media for manipulating abstract information. It allows the user to arrange small magnetic pucks on a grid, where each puck represents a piece of information to be organized, such as a message, file, bookmark, citation, presentation slide, movie scene, or newspaper story. As the user manipulates the physical puck, the corresponding digital information is projected onto the board. Special pucks may be placed
on the board to execute commands or request additional information. We seek to combine the benefits of physical manipulation (natural, fluid, rapid, two-handed, multi-person interaction) with the benefits we can get from computer augmentation (interactive commands, functions, queries, operations, importing and exporting data, and remote collaboration). We believe this type of interface is thus more effective for tasks involving organizing, grouping, and manipulating types of information that have no inherent physical representation, and it provides an example of a tangible interface for a typical ""knowledge worker"" task.",2002-12-30,['tangible-media'],,20167,E15-441,2016-12-05T00:16:27.366Z,"['ishii@media.mit.edu', 'jpatten@media.mit.edu']",True,senseboard,1999-12-31,[],Senseboard,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:57.016Z,"This project addresses the need for friendlier solutions to the issue of medication compliance. Sensepad is a portable platform which can automatically identify, sense, and track the medications which are placed on it.  Information is provided to the user via a graphical display embedded into the sensing surface.  By monitoring how often the medications  are used, this interface can provide the patient and/or doctor with a usage  history and warn the patient about possible adverse drug interactions. Future versions of this device will also be able to count pills and connect to the Internet via a modem or two-way pager. 
",2000-12-30,['physics-and-media'],,20191,,2016-12-05T00:16:33.986Z,"['neilg@media.mit.edu', 'fletcher@media.mit.edu']",True,sensepad-medication-monitor,1997-12-31,[],Sensepad Medication Monitor,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:57.095Z,"Sensetable is a system that wirelessly, quickly, and accurately tracks the positions of multiple objects on a flat display surface. The tracked objects have a digital state, which can be controlled by physically modifying them using dials or tokens. We have developed several new interaction techniques and applications on top of this platform. Our current work focuses on business supply-chain visualization using system-dynamics simulation.",2002-09-01,['tangible-media'],,20199,E15-344,2016-12-05T00:16:34.760Z,"['ishii@media.mit.edu', 'jpatten@media.mit.edu']",True,sensetable,1999-01-01,"['archives', 'history']",Sensetable,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:57.178Z,"We are developing a lower-cost portable system which allows passive RFID tags to be tracked on a flat surface. By creating a system that tracks the 2-D position of multiple tags, it is possible to create a variety of applications ranging from multi-user interactive tabletops as well as RFID smart shelves, using simple low-cost IC-tags.",2007-01-01,['tangible-media'],,20207,E15-344,2016-12-05T00:16:35.857Z,"['ishii@media.mit.edu', 'fletcher@media.mit.edu']",True,sensetable-ii-passive-tag-version,2006-01-01,[],"SenseTable II, Passive Tag Version",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:57.258Z,"We are developing low-cost, distributed sensor networks for grassroots environmental monitoring. ",2005-01-01,['computing-culture'],,20215,E15-020D,2016-12-05T00:16:49.802Z,['csik@media.mit.edu'],True,sensible-cities,2004-09-01,[],Sensible Cities,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:57.337Z,"Data mining of email has provided important insights into how organizations function and what management practices lead to greater productivity. But important communications are almost always face-to-face, so we are missing the greater part of the picture. Today, however, people carry cell phones and wear RFID badges. These body-worn sensor networks mean that we can potentially know who talks to whom, and even how they talk to each other. Sensible Organizations investigates how these new technologies for sensing human interaction can be used to reinvent organizations and management.",,[],,20223,E15-383,2016-12-05T00:17:02.975Z,"['sandy@media.mit.edu', 'orenled@media.mit.edu', 'amohan@media.mit.edu']",True,sensible-organizations,2006-09-01,[],Sensible Organizations,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:57.419Z,"Current sports-medicine practices for understanding the motion of athletes while engaged in their sport of choice are limited to camera-based marker tracking systems that generally lack the fidelity and sampling rates necessary to make medically usable measurements; they also typically require a structured, stable ""studio"" environment, and need considerable time to set up and calibrate. The data from our system provides the ability to understand the forces and torques that an athlete's joints and body segments undergo during activity. It also allows for precise biomechanical modeling of an athlete's motion. The application of sensor fusion techniques is essential for optimal extraction of kinetic and kinematic information. Also, it provides an alternative measurement method that can be used in out-of-lab scenarios. ",2014-01-01,['responsive-environments'],,20231,--Choose Location,2016-12-05T00:16:38.261Z,"['mtl@media.mit.edu', 'joep@media.mit.edu']",True,sensor-fusion-for-gesture-analyses-of-baseball-pitching,2013-01-01,[],Sensor Fusion for Gesture Analyses of Baseball Pitching,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:57.501Z,"The Sensor Network Application Retasking Framework (or SNARF) is a collection of tools for embedding a variant of the Python programming language into a sensor network. The goal is to make programming and interacting with sensor networks significantly easier. SNARF builds upon PyMite, an open source project for porting Python to microcontrollers.",2007-01-01,['responsive-environments'],,20239,E15-344,2016-12-05T00:16:49.748Z,['joep@media.mit.edu'],True,sensor-network-application-retasking-framework,2005-09-01,[],Sensor Network Application Retasking Framework,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:57.580Z,"Most sensory data has limited utility without location information, and manual node localization becomes impossible for large, inaccessable, or mobile sensor deployments. Thus, autonomous localization is crucial for many sensor-network applications. We are developing a distributed-localization algorithm for the PLUG indoor sensor network by analyzing commonly detected sound, light, and vibration sensory data from naturally occuring phenomena. The system enters active mode when its sensed region stays relatively silent and stable (assumed to be unoccupied). Otherwise, it stays in passive mode, with each node estimating its location by collecting sensory data and comparing it to synchronized data from other neighborhood nodes. In active mode, each node ocassionally generates predefined mimics of natural phenomena such as sonic transients, or manipulates an attached light source. The main features of this approach are distributed properties, lack of heavy infrastructure, unobtrusive exploitation of background phenomena, and application of junction trees for message passing.",2007-01-01,['responsive-environments'],,20247,E15-344,2016-12-05T00:16:49.770Z,['joep@media.mit.edu'],True,sensor-network-localization-from-natural-phenomena,2006-01-01,[],Sensor Network Localization from Natural Phenomena,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:57.735Z,"This project explores the wide-scale distribution of low-power, low-cost sensor nodes that can measure temperature, humidity, light levels, and human presence. These sensor nodes will enable buildings to react quickly and effectively to the changing needs of their inhabitants, automatically controlling, for example, heating/air conditioning, windows (opening and shades), and lighting. Total building power consumption can be reduced, and repair requests can be made automatically.",2010-01-01,['responsive-environments'],,20263,E15-344,2016-12-05T00:16:49.924Z,"['geppetto@media.mit.edu', 'joep@media.mit.edu']",True,sensor-enabled-active-buildings,2007-09-01,[],Sensor-Enabled Active Buildings,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:57.810Z,"A small number of studies support the notion of a functional relationship between movement stereotypy and arousal in individuals with ASD, such that changes in autonomic activity either precede or are a consequence of engaging in stereotypical motor movements. Unfortunately, it is difficult to generalize these findings as previous studies fail to report reliability statistics that demonstrate accurate identification of movement stereotypy start and end times, and use autonomic monitors that are obtrusive and thus only suitable for short-term measurement in laboratory settings. The current investigation further explores the relationship between movement stereotypy and autonomic activity in persons with autism by combining state-of-the-art ambulatory heart rate monitors to objectively assess arousal across settings; and wireless, wearable motion sensors and pattern recognition software that can automatically and reliably detect stereotypical motor movements in individuals with autism in real time.",2011-09-01,['affective-computing'],,20271,--Choose Location,2016-12-05T00:16:50.248Z,"['picard@media.mit.edu', 'mgoodwin@media.mit.edu']",True,sensor-enabled-measurement-of-stereotypy-and-arousal-in-individuals-with-autism,2008-01-01,[],Sensor-Enabled Measurement of Stereotypy and Arousal in Individuals with Autism,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:57.887Z,"Current unmotorized prostheses do not provide adequate energy return during late stance to improve level-ground locomotion. Robotic prostheses can provide power during late-stance to improve metabolic economy in an amputee during level-ground walking. This project seeks to improve the types of terrain a robotic ankle can successfully navigate by using command signals taken from the intact and residual limbs of an amputee. By combining these command signals with sensors attached to the robotic ankle, it might be possible to further understand the role of physiological signals in the terrain adaptation of robotic ankles.",,['biomechatronics'],,20279,--Choose Location,2021-09-17T19:23:38.145Z,['hherr@media.mit.edu'],True,sensor-fusions-for-an-emg-controlled-robotic-prosthesis,2009-01-01,[],Sensor-fusions for an EMG controlled robotic prosthesis,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:57.964Z,"SensorChimes aims to create a new canvas for artists leveraging ubiquitous sensing and data collection. Real-time data from environmental sensor networks are realized as musical composition. Physical processes are manifested as musical ideas, with the dual goal of making meaningful music and rendering an ambient display. The Tidmarsh Living Observatory initiative, which aims to document the transformation of a reclaimed cranberry bog, provides an opportunity to explore data-driven musical composition based on a large-scale environmental sensor network. The data collected from Tidmarsh are piped into a mapping framework, which a composer configures to produce music driven by the data.",,['responsive-environments'],,20287,--Choose Location,2019-04-19T14:33:18.956Z,"['eflynch@media.mit.edu', 'joep@media.mit.edu']",True,sensorchimes-musical-mapping-for-sensor-networks,2015-09-01,[],SensorChimes: Musical mapping for sensor networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:11:58.040Z,"These ""autonomous sensing devices,"" which combine sensors as information providers with wireless communication systems, have enormous implications for end users, and therefore tremendous market potential in key sectors such as healthcare, education, food quality control, and environmental monitoring. This National Centre for Sensor Research (NCSR)-Media Lab Europe (MLE)-Higher Education Authority of Ireland (HEA) project focuses on education (sensors integrated with LEGO Mindstorms and Crickets) and the environment (such as wireless networked data loggers for oxygen, carbon dioxide, temperature, and PH). (Funded by the Higher Education Authority of Ireland.)",2004-01-01,[],,20295,MLE,2016-12-05T00:16:53.696Z,[],True,sensors-for-science-education,2002-01-01,[],Sensors for Science Education,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:58.113Z,"SensorTape is a modular and dense sensor network in a form factor of a tape. SensorTape is composed of interconnected and programmable sensor nodes on a flexible electronics sub-strate. Each node can sense its orientation with an inertial measurement unit, allowing deformation self-sensing of the whole tape. Also, nodes sense proximity using time-of-flight infrared. We developed network architecture to automatically determine the location of each sensor node, as SensorTape is cut and rejoined. We also made an intuitive graphical interface to program the tape. Our user study suggested that SensorTape enables users with different skill sets to intuitively create and program large sensor network arrays. We developed diverse applications ranging from wearables to home sensing, to show low-deployment effort required by the user. We showed how SensorTape could be produced at scale and made a 2.3-meter long prototype.",,"['living-mobile', 'responsive-environments']",,20303,--Choose Location,2019-04-19T14:34:20.486Z,"['artemd@media.mit.edu', 'cindykao@media.mit.edu', 'joep@media.mit.edu']",True,sensortape-modular-and-programmable-3d-aware-dense-sensor-network-on-a-tape,2016-01-01,[],SensorTape: Modular and programmable 3D-aware dense sensor network on a tape,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:58.195Z,"Senspectra is a computationally augmented physical modeling toolkit designed for sensing and visualization of structural strain. The system functions as a decentralized sensor network; it consists of nodes that are each embedded with computational capabilities and a full-spectrum LED, which communicate with neighbor nodes to determine a network topology through a system of flexible joints. Each joint uses a simple optical occlusion technique as an omnidirectional bend-sensing mechanism to sense and communicate mechanical strain between neighboring nodes, while also serving as a data and power bus between nodes. The Senspectra infrastructure provides a flexible, modular, sensor-network platform whose primary application focuses on coupling physical-digital modeling techniques utilized in the architecture and industrial design disciplines with systems for structural engineering analysis, offering an intuitive approach for physical realtime finite element analysis.",2006-09-01,['tangible-media'],,20311,E15-344,2016-12-05T00:16:49.951Z,['ishii@media.mit.edu'],True,senspectra,2005-09-01,[],Senspectra,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:11:58.275Z,"Pigeons can perform impressive feats when trained with classical chaining, but these apparently sophisticated performances diminish in accuracy if the context changes or chained behavior is interrupted. Pigeons cannot easily start chaining in the middle of the sequence, omit steps, or modify the chained sequence to adapt to new conditions. We are studying the grey parrot's ability to transfer serial learning from an initial context to new contexts with minimal retraining, and to acquire knowledge beyond simple stimulus-response. While similar to classic operant-conditioning studies, here the parrot must demonstrate versatility in his behavior.",2001-12-30,['pet-projects'],,20319,E15-318,2016-12-05T00:16:49.979Z,['impepper@media.mit.edu'],True,serial-trhacking,1999-12-31,[],Serial TrHacking,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:58.562Z,"Interactive teaching tools strive to make subject-material more interesting through a hands-on approach to learning. Using video projection and shadow tracking, we want to expand the hands-on interaction approach to the whole body. Vehicles, based on ideas from the book ""Vehicles"" by Valentino Braitenberg, is a system for programming characters that can respond to sensed stimuli in a virtual environment. These characters are constructed similarly to the vehicle metaphor in the book, by connecting simple sensors to effectors. Users can then directly interact with their creations by stepping in front of the projected environment and casting a shadow. By using sensors and effectors with different behaviors in virtual environments with varying rules, we believe this will be a good tool for teaching science as well as some basic computer programming concepts.",2004-01-01,['fluid-interfaces'],,20351,E15-320,2016-12-05T00:16:50.061Z,['pattie@media.mit.edu'],True,shadow-vehicles,2004-01-01,[],Shadow Vehicles,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:58.638Z,"ShAir is a platform for instantly and easily creating local content-shareable spaces without requiring an Internet connection or location information. ShAir-enabled devices can opportunistically communicate with other mobile devices and optional pervasive storage devices such as Wi-Fi SD cards whenever they enter radio range of one another. Digital content can hop through devices in the background without user intervention. Applications that can be built on top of the platform include ad-hoc photo/video/music sharing and distribution, opportunistic social networking and games, digital business card exchange during meetings and conferences, and local news article-sharing on trains and buses.",2015-06-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/files/Display/icon4.png,20359,--Choose Location,2020-06-10T16:31:06.456Z,"['vmb@media.mit.edu', 'holtzman@media.mit.edu', 'bandy@media.mit.edu', 'arata@media.mit.edu']",True,shair-a-platform-for-mobile-content-sharing,2011-09-01,[],ShAir: A platform for mobile content sharing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:58.718Z,"Shake and Play is a family of building blocks for young children that encourage new forms of expressive and exploratory play. Some bricks can light up in any color, and others make sounds. All respond to different sensors, such as shaking, tilting, loudness and brightness. They also can record a sound or a pattern of colors and play it back.",2008-01-01,[],,20367,Cube,2016-12-05T00:16:50.090Z,"['mres@media.mit.edu', 'ericr@media.mit.edu']",True,shake-and-play,2008-01-01,[],Shake and Play,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:11:58.791Z,"Coordination of and communication between large numbers of individuals, especially in situations that are prone to change rapidly, requires a common output and a recognizable input. Shake4Action looks at how we can organize large groups by augmenting SMS, email, and phone calls with mobile gestures. This project builds a platform to receive information of varying types (such as keywords, touch tones, and gestures), and return information that can be re-interpreted on output by each participant.",2010-01-01,['information-ecology'],,20375,E15-301,2016-12-05T00:16:59.955Z,['holtzman@media.mit.edu'],True,shake4action-gestural-mobile-coordination,2009-01-01,[],Shake4Action: Gestural Mobile Coordination,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:58.870Z,"We are exploring ways to encode information exchange into preexisting natural interaction patterns, both between people and between a single user and objects with which he or she interacts on a regular basis. Two devices are presented to provoke thoughts regarding these information interchange modalities: a pair of gloves that requires two users to complete a ""secret handshake"" in order to gain shared access to restricted information, and a doorknob that recognizes the grasp of a user and becomes operational only if the person attempting to use it is authorized to do so.",2012-01-01,['object-based-media'],,20383,--Choose Location,2016-12-05T00:16:50.148Z,['vmb@media.mit.edu'],True,shakeonit,2010-01-01,[],ShakeOnIt,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:11:58.946Z,"In most �drop-on-demand� inkjet control schemes, a superheated bubble of liquid is used to propel a droplet or a piezoelectric crystal physically squeezes out a droplet at high speeds. These models rely on a reservoir of print media that is always �open� on one end for the droplet outlet. This makes the design of the system difficult for two reasons: the pore has to be small enough to hold back low-viscosity liquids by surface tension alone (~10um diameter), and the open nozzle leaves the ink exposed and prone to drying out.  We propose a new deposition mechanism based around a nozzle that is �plugged� by an actuating �stopper� made of shape memory wire backed by a positive internal fluid pressure. When the wire is actuated, the stopper is removed and the pressure of the fluid pushes one or more droplets out until the stopper is replaced.",2012-09-01,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/Niti%20inkjet.jpg,20391,,2016-12-05T00:16:50.171Z,"['jacobson@media.mit.edu', 'neilg@media.mit.edu', 'neri@media.mit.edu']",True,shape-memory-inkjet,2012-09-01,[],Shape Memory Inkjet,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:11:59.028Z,"Near-unity polarization of nuclear systems in atoms has been demonstrated via optical pumping. This relies on pumping specific absorption lines of atoms. Molecules are required to provide enough qubits for computations. Molecular absorption spectra are much more broadband, and therefore will require a more complex pumping signal in order to attain polarization. We will be investigating the feasibility of using shaped ultrafast optical pumping to improve nuclear polarization. High nuclear spin polarization is a route to increasing the number of qubits available and thereby the usefulness of NMR quantum computers.",2005-01-01,['physics-and-media'],,20399,E15-023,2016-12-05T00:17:00.385Z,['neilg@media.mit.edu'],True,shaped-ultrafast-pulse-optical-pumping-of-nmr-systems,2001-01-01,[],Shaped Ultrafast Pulse Optical Pumping of NMR Systems,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:11:59.106Z,"We are looking to better enable cooperation between individuals working in the same domain yet having differing goals. Tools for collaboration are mainly focused on groups with a single shared goal, however we observe cooperation between individuals occurring in much broader spheres. Our work will take the form of a networked programming environment where all code is shared; the goal is to develop tools and techniques that make it easy to leverage the work of others, as well as provide value (both social and technical) for making one's work available to the collective. In particular we want to make more visible the act of appropriation, and also make it easier for individuals to track the evolution of their contributions. We believe that by making appropriation more explicitly visible we can translate contributions into a form of social capital and thus encourage cooperation between otherwise unconnected individuals.",2017-05-01,['sociable-media'],,20407,E15-390,2022-06-10T14:41:30.050Z,['judith@media.mit.edu'],True,share-socio-technical-tools-for-loosely-bound-cooperation,2008-09-01,[],Share: Socio-Technical Tools for Loosely Bound Cooperation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:59.183Z,"SHARE is a robotic cognitive architecture focused on manipulating and understanding the phenomenon of shared attention during interaction. SHARE incorporates new findings and research in the understanding of nonverbal referential gesture, visual attention system research, and interaction science. SHARE's research incorporates new measurement devices, advanced artificial neural circuits, and a robot that makes its own decisions.",2017-05-31,['personal-robots'],,20415,--Choose Location,2017-10-15T17:57:01.084Z,"['cynthiab@media.mit.edu', 'ndepalma@media.mit.edu']",True,share-understanding-and-manipulating-attention-using-social-robots,2014-09-01,[],SHARE: Understanding and Manipulating Attention Using Social Robots,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:11:59.260Z,"Affordable, easy-to-use digital video equipment, widely available high-bandwidth connectivity, and the variability of distributed networks and messaging protocols provide an opportunity to expand the ways people utilize video-based storytelling. The Shareable Media project includes tool design, exploration in aesthetic structures, and methods for developing Shareable Media communities. The research infrastructure supports applications concerned with contextual browsing, context-sensitive display, and the formation of contextual communities. Four demonstrations highlight different aspects of the Shareable Media project: Individeo, developed by James Jung-Hoon Seo; PlusShorts, developed by Aisling Kelliher; M-Views, developed by Pengkai Pan; and Scaleable Architecture, developed by Cathy Lin and Pengkai Pan.",2002-09-01,"['gray-matters', 'interactive-cinema']",,20423,E15-368,2016-12-05T00:17:02.273Z,['gid@media.mit.edu'],True,shareable-media,2000-01-01,[],Shareable Media,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:11:59.572Z,"People keep their contacts, such as email addresses and phone numbers, in their computer address books and cell phones. Unless someone asks you for a contact, you probably don�t share the information. Is there a way to share this personal information that will benefit people in the same social circles? This research looks for opportunities to utilize and share contacts, based on user-defined contact groups, through P2P and Group2Group protocols on wireless/Internet networks.",2007-01-01,['living-mobile'],,20447,E15-384C,2016-12-05T00:17:02.353Z,['geek@media.mit.edu'],True,sharing-contacts-based-on-social-networks,2006-09-01,[],Sharing Contacts Based on Social Networks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:11:59.744Z,"In this installation you play the role of a shepherd who must herd a bunch of sheep with attitude through a course using Duncan H. Terrier (a.k.a. Duncan of Innisfree) as your sheepdog. Duncan and the sheep are fully autonomous, although Duncan responds to voice commands such as ""bye,"" ""away,"" ""steady,"" and ""down."" In fact, Duncan can be trained to respond to any set of utterances, in any accent, because rather than using speech recognition, he is looking at an acoustic pattern. Both the sheep and Duncan have minds of their own: for example, if the sheep get frightened, they may bolt or charge.",2001-12-30,['synthetic-characters'],,20455,E15-320,2016-12-05T00:16:50.294Z,[],True,sheepdog-trial-by-eire,1999-12-31,[],sheep/dog: Trial by Eire,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:00.070Z,"We present a new method for scanning 3D objects through a single-shot, shadow-based method. We decouple 3D occluders from 4D illumination using shield fields: the 4D attenuation function which acts on any light field incident on an occluder. We then analyze occluder reconstruction from cast shadows, leading to a single-shot light-field camera for visual hull reconstruction.",,['camera-culture'],,20471,E15-044A,2016-12-05T00:16:50.320Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,shield-field-imaging,2008-01-01,[],Shield Field Imaging,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:00.676Z,"ShowMe is an immersive mobile collaboration system that allows remote users to communicate with peers using video, audio, and gestures. With this research, we explore the use of head-mounted displays and depth sensor cameras to create a system that (1) enables remote users to be immersed in another person's view, and (2) offers a new way of sending and receiving the guidance of an expert through 3D hand gestures. With our system, both users are surrounded in the same physical environment and can perceive real-time inputs from each other.",2017-08-31,"['terrestrial-sensing', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/files/Display/ShowMe_View.png,20503,--Choose Location,2018-10-12T16:45:04.639Z,"['pattie@media.mit.edu', 'amores@media.mit.edu', 'xavib@media.mit.edu']",True,showme-immersive-remote-collaboration-system-with-3d-hand-gestures,2014-01-01,[],ShowMe: Immersive Remote Collaboration System with 3D Hand Gestures,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:00.857Z,"Expressive interactive characters call for expressive camera and lighting control. To show off our digital actors to best advantage, we are working on a real-time autonomous character, the ShutterBug, who will be the cinematographer in our virtual world. By combining the same techniques that we use to build our synthetic characters with inspiration from traditional film making, we seek to bring to life a dynamic camera system with an organic feel. Our autonomous camera creature captures the actions and emotions of our on-screen actors in an interesting and comprehensible manner, while simultaneously facilitating user interaction in the world. Current projects include adding an emotional system to the ShutterBug, and continuing our work in automatic lighting design. This work forms the basis of the camera control and lighting used in Swamped!
",1999-12-30,['synthetic-characters'],,20511,,2016-12-05T00:16:50.369Z,[],True,shutterbug-an-autonomous-camera-creature,1995-12-31,[],ShutterBug: An Autonomous Camera Creature,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:01.227Z,"Shybot is a personal mobile robot designed to both embody and elicit reflection on shyness behaviors. Shybot is being designed to detect human presence and familiarity from face detection and proximity sensing in order to categorize people as friends or strangers for interaction. Shybot also can reflect elements of the anxious state of its human companion through LEDs and a spinning propeller. We designed this simple social interaction to open up a new direction for intervention for children living with autism. We hope that from minimal social interaction, a child with autism or social anxiety disorders could reflect on and more deeply attain understanding about personal shyness behaviors, as a first step toward helping to make progress in developing greater capacity for complex social interactions.",2008-09-01,['affective-computing'],,20527,E15-443,2016-12-05T00:16:50.395Z,['picard@media.mit.edu'],True,shybot,2007-09-01,[],ShyBot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:01.397Z,"Consider each of our individual voices as a flashlight to illuminate how we project ourselves in society and how much sonic space we give ourselves or others. Thus, turn-taking computation through speaker recognition systems has been used as a tool to understand social situations or work meetings. We present SIDR, a deep learning-based, real-time speaker recognition system designed to be used in real-world settings. The system is resilient to noise, and adapts to room acoustics, different languages, and overlapping dialogues. While existing systems require the use of several microphones for each speaker or the need to couple video and sound recordings for accurate recognition of a speaker, SIDR only requires a medium-quality microphone or computer-embedded microphone.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/10/19/SIDR_im1.png,20535,--Choose Location,2016-12-05T00:17:02.465Z,"['tod@media.mit.edu', 'rebklein@media.mit.edu']",True,sidr-deep-learning-based-real-time-speaker-identification,2016-01-01,['machine-learning'],SIDR: Deep Learning-Based Real-Time Speaker Identification,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:01.743Z,"Signs is an instant messenger design philosophy and implementation. It aims to reduce confusion while increasing expressive power through a suite of novel design features.  These features provide native structural elements in the conversation that free the user from sub-optimally performing meta-linguistic tasks in the same text-only chat space.  Specifically, Signs aims to facilitate repair of sequencing problems, reduce ambiguity, and use the persistent chat log as a backdrop for new communication acts. ",2010-01-01,[],,20551,E15-318,2016-12-05T00:16:50.448Z,['judith@media.mit.edu'],True,signs,2005-01-01,[],Signs,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:01.901Z,"Collaborative group situations present opportunities for designing meaningful whole-body input and interaction techniques. In this project we focus on the action of sharing information in a group through posters and signs held up by its members and translating that action meaningfully based on the group context. Live audience events like reality-TV shows, sports events, and classrooms are example scenarios where this action is most commonly elicited. We attempt to automate the process of recognizing whenever people hold their signs up and feed this information back into the system or to remote audiences. In classroom situations, this tool would offer a quick way of gauging solutions to short questions which can then be projected back into the classroom. In the live-audience case, it offers a way of engaging remote spectators that can vote on all aggregated signs and projecting relevant information back into the live-audience space.",2013-09-01,[],https://dam-prod2.media.mit.edu/x/files/Display/signsup_logo.png,20559,--Choose Location,2016-12-05T00:16:50.423Z,[],True,signs-up,2012-01-01,[],Signs-Up!,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:02.054Z,The Silk Pavilion explores the relationship between digital and  biological fabrication on product and architectural scales.The primary  structure was created of 26 polygonal panels made of silk threads laid  down by a CNC (Computer-Numerically Controlled) machine.,2018-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2017/10/13/silkPavilion_4.jpg,20567,--Choose Location,2021-05-21T13:43:50.539Z,"['jlaucks@media.mit.edu', 'm_kayser@media.mit.edu', 'j_duro@media.mit.edu', 'neri@media.mit.edu', 'cdgu@media.mit.edu']",True,silk-pavilion,2012-09-01,"['design', 'architecture', 'art', 'synthetic-biology', '3d-printing', 'biology', 'materials', 'engineering']",Silk Pavilion,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:02.239Z,SilverWire is a tool for increasing socialization and augmenting communication among communities that actively publish content on the World Wide Web. It is designed to build bridges across community boundaries by comparing the works of different community publishers and by introducing members of different communities to each other. SilverWire also provides a way for conventional media companies to better serve these emerging community publications by integrating professionally created information feeds with community publications. The goal of the SilverWire system is to be a Web intermediary that makes communities more aware of other communities doing similar (or interestingly different) things.,2001-12-30,"['gray-matters', 'electronic-publishing']",,20575,E15-001,2019-02-06T21:48:54.409Z,['walter@media.mit.edu'],True,silverwire,1998-12-31,[],SilverWire,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:02.420Z,"Communication is an integral task for any distributed work group. As the work environment becomes less structured, it becomes harder for teams that are not co-located to coordinate and communicate about essential matters that might be integral to their productivity. More often than not, communication is not the only task in which an individual is engaged, but one that occurs in parallel and is closely dependent on many others. This project proposes a wireless, mobile, context-sensitive application for voice messaging and communication specifically designed for distributed work groups. The purpose of the system is to allow groups to communicate easily on a dedicated, ongoing communication channel. Groups can be composed of several mobile users using Pocket PCs and users connected over a telephone.",2004-01-01,['living-mobile'],,20583,E15-344,2016-12-05T00:16:50.497Z,['geek@media.mit.edu'],True,simphony-voice-group-communication,2003-01-01,[],SimPhony: Voice Group Communication,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:02.576Z,"The availability of cheap LEDs and diode lasers in a variety of wavelengths enables creation of simple and cheap spectroscopic sensors for specific tasks such as food shopping and preparation, healthcare sensing, material identification, and detection of contaminants or adulterants. ",2013-09-01,['object-based-media'],,20591,--Choose Location,2016-12-05T00:17:02.537Z,[],True,simple-spectral-sensing,2013-01-01,[],Simple Spectral Sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:02.722Z,"This category of Music Toy is being designed as an inexpensive, stand-alone, sound-producing, and manipulating device that gains power when networked with other Simple Things. Each Simple Thing is hand-held, and contains several buttons and a single continuous pressure controller. Sound memory and a simple processor allow the storage of waveforms, and their modulation or manipulation. IR links allow sound to be exchanged from one Simple Thing to another, as well as for Simple Things to be synchronized in large numbers. We are conducting experiments in the use of large numbers of Simple Things in physical proximity, and as remote music controllers via the Internet. In addition, we are exploring the use of Simple Things as a means of bringing personalized music and musical information between public performance experience spaces and home.",2000-12-30,"['toys-of-tomorrow', 'opera-of-the-future']",,20599,E15-483,2016-12-05T00:17:02.564Z,['tod@media.mit.edu'],True,simple-things,1999-12-31,[],Simple Things,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:02.885Z,"The Atmosphere installation presents a large cloud of information on a wide presentation screen that can be manipulated by three handheld devices mounted on plinths within the gallery space. The visitor takes one of the devices and physically moves it within a designated area, thereby manipulating the information presented both on the screen and the larger projection. A simple magnetic coupling system was developed to track the position and orientation of the handheld device. Signals are magnetically coupled into these tags from transmitting coils on each plinth. The magnitude of the coupling is inversely proportional to the distance from the coils and proportional to the sine of the angle between the tag and the normal of the coil. Using the magnitude of the recovered signal from each tag for each coil, it is possible to calculate both the position and (unsigned) orientation of the device relative to the reference frame formed by the coils. Atmosphere is a collaboration of the Aesthetics and Computation group, the Responsive Environments group, and Steelcase Inc.",2001-12-30,['responsive-environments'],,20607,E15-357,2016-12-05T00:16:11.134Z,"['geppetto@media.mit.edu', 'joep@media.mit.edu']",True,simple-low-cost-tracking-with-active-magnetic-tags,2000-12-31,[],"Simple, Low-Cost Tracking with Active Magnetic Tags",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:03.148Z,"This work focuses on the creation and prototyping of novel, mobile, interactive story experiences. The stories we create inhabit public space and invite the audience to participate and influence the story's progress as they move in, pass through, and experience that space. These stories are dynamic, responsive, and evolve as the audience plays with them in the public space. The stories are context aware: they are affected by changes to the surrounding environment, such as weather conditions, varying levels of audience activity, and time shifting, all of which influence the story through sensor-triggered story interfaces. The stories are organic, changing and growing as the audience becomes author and adds to the narrative. This collaboration (Trinity College Dublin and Media Lab Europe) brings together expertise in content creation, story design, wireless networks, and sensor technologies in the creation of these exploratory and novel story platforms. The project has implications outside its own application area as the provision of ever-changing information delivered to people in a meaningful and unobtrusive manner as they go about their daily lives is a challenging issue.",2003-01-01,['interactive-cinema'],,20623,E15-441,2017-09-13T21:00:16.905Z,['gid@media.mit.edu'],True,sin-scal-eile-thats-another-story,2002-01-01,[],Sin Sceal Eile (That's Another Story),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:03.273Z,"Singing Fingers allows children to fingerpaint with sound. Users paint by touching a screen with a finger, but color only emerges if a sound is made at the same time. By touching the painting again, users can play back the sound. This creates a new level of accessibility for recording, playback, and remixing of sound.",2015-01-01,['lifelong-kindergarten'],,20631,--Choose Location,2016-12-05T00:16:50.520Z,"['mres@media.mit.edu', 'silver@media.mit.edu', 'ericr@media.mit.edu']",True,singing-fingers,2010-01-01,[],Singing Fingers,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:03.398Z,"The human singing voice is the oldest musical instrument, yet it is one of the most difficult to simulate convincingly. Using signal processing and pattern-recognition techniques in combination with prior knowledge of the musical score, we are attempting to extract control parameters which capture the most perceptually significant features. This parameterized, structured model could then be efficiently transmitted and re-synthesized, resulting in a high-quality recreation of the original performance.",2005-01-01,['music-mind-and-machine'],,20639,--Choose Location,2016-12-05T00:17:02.684Z,['bv@media.mit.edu'],True,singing-voice-parameterization-and-re-synthesis,2000-01-01,[],Singing Voice Parameterization and Re-synthesis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:03.513Z,"Within the last few years, cellphone subscriptions have spread widely and now cover even the remotest parts of the planet. Adequate access to healthcare, however, is not widely available, especially in developing countries. We propose a new approach to converting cellphones into low-cost scientific devices for microscopy. Cellphone microscopes have the potential to revolutionize health-related screening and analysis for a variety of applications, including blood and water tests. Our optical system is more flexible than previously proposed mobile microscopes, and allows for wide field-of-view panoramic imaging, the acquisition of parallax, and coded background illumination, which optically enhances the contrast of transparent and refractive specimens.",,['camera-culture'],,20647,--Choose Location,2016-12-05T00:17:02.653Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,single-lens-off-chip-cellphone-microscopy,2012-01-01,[],Single Lens Off-Chip Cellphone Microscopy,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:03.619Z,"The ability to record images with extreme temporal resolution enables a diverse range of applications, such as time-of-flight depth imaging and characterization of ultrafast processes. Here we present a demonstration of the potential of single-photon detector arrays for visualization and rapid characterization of events evolving on picosecond time scales. The single-photon sensitivity, temporal resolution, and full-field imaging capability enables the observation of light-in-flight in air, as well as the measurement of laser-induced plasma formation and dynamics in its natural environment. The extreme sensitivity and short acquisition times pave the way for real-time imaging of ultrafast processes or visualization and tracking of objects hidden from view. ",,"['advancing-wellbeing', 'terrestrial-sensing', 'camera-culture']",,20655,--Choose Location,2016-12-05T00:17:02.706Z,"['raskar@media.mit.edu', 'barmak@media.mit.edu']",True,single-photon-sensitive-ultrafast-imaging,2015-01-01,[],Single-Photon Sensitive Ultrafast Imaging,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:04.029Z,"Skeleton is a set of Cocoa libraries optimized for fast development of research, and for professional applications dealing with the analysis of musical signals. Based on fundamentals of psychoacoustics, perception, and learning, the underlying framework consists of machine-listening and machine-learning tools, supported by flexible data structures and visualizations. Designed essentially as an alternative to more generic and slower tools such as Matlab, Skeleton should provide a robust, efficient, specific and yet open programming environment for deconstructing, structuring, and labeling audio, or for generating personalized music. Skeleton aims to combine various solid scientific approaches to music listening, learning, and cognition, to enable the development of consistent creative audio applications, and enhance music-making. Current applications include beat matching, music mosaicing, compression, and music textures.",2005-01-01,['opera-of-the-future'],,20687,E15-445,2016-12-05T00:16:50.638Z,"['tod@media.mit.edu', 'tristan@media.mit.edu']",True,skeleton,2003-01-01,[],Skeleton,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:04.115Z,"Skellig is an opera with music by Tod Machover and a libretto based on the best-selling novel for young people by David Almond. It premiered in the UK in November 2008. Besides blending acoustics and electronics, natural noise, and soaring melodies, Skellig also presents several live performance breakthroughs. A non-professional teenage chorus is used throughout, blended seamlessly with high-level professionals; this chorus is guided by an interactive ""sonic score"" that provides auditory cues, textures to imitate, and electronic reinforcement for the entire 100-minute show. In addition, specially designed ""ambisonics"" were developed to allow sound to emanate from the stage and engulf the audience in all dimensions, the first time such a technique has been used in a full-scale theatrical setting.",2009-01-01,['opera-of-the-future'],,20695,Hyperinstruments offices,2016-12-05T00:16:50.665Z,['tod@media.mit.edu'],True,skellig-a-surround-opera,2008-09-01,[],"Skellig: A ""Surround"" Opera",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:04.201Z,"The goal of this project is to allow designers to continue using the traditional drawing tools they love, but to digitally enhance the drawing space so that designers can explore and enhance their work in more resourceful ways. More specifically, we are looking at an application for architects. There is a natural connection between architects and this research, due to their work with layers of transparencies to sketch out their designs. Because architects work so frequently with layers, they end up dealing with tens of thousands of sketches, and organizing and utilizing such a large collection of sketches is difficult. This project proposes to digitally capture and organize the layers that architects create, in order to help architects document and utilize their creative process without losing the tactile media to which they are accustomed. Using a video camera and a projector installed in the sketch space, we can capture and track the layers as they are being developed. We are also designing ways to graphically and spatially organize the captured layers, and to allow architects to pull out and work with the old layers easily. Once the prototype is complete, we plan to evaluate it by inviting architects to use the system in their design studios. ",2004-01-01,['tangible-media'],,20703,E15-344,2016-12-05T00:16:50.564Z,['ishii@media.mit.edu'],True,sketch-interface,2003-01-01,[],Sketch Interface,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:04.384Z,"Skin and tissue perfusion measurements are important parameters for diagnosis of wounds and burns, and for monitoring plastic and reconstructive surgeries. In this project, we use a standard camera and a laser source in order to image blood-flow speed in skin tissue. We show results of blood-flow maps of hands, arms, and fingers. We combine the complex scattering of laser light from blood with computational techniques found in computer science.",,"['advancing-wellbeing', 'camera-culture']",https://dam-prod2.media.mit.edu/x/2016/10/19/perfusion.jpg,20719,--Choose Location,2016-12-14T20:46:59.692Z,"['raskar@media.mit.edu', 'guysatat@media.mit.edu']",True,skin-perfusion-photography,2014-09-01,['health'],Skin Perfusion Photography,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:04.478Z,"The skin is a versatile organ, capable of fine temporal and spatial discriminations. Touch is considered by many to be the most important of our senses, for without it we can have no sense of the physical boundaries of our own bodies or our surrounding physical environment. The Skinscape project introduces tools and a wearable that allow us to compose and perceive compositions designed for the skin. The wearable consists of a number of vibrotactile stimulators designed into a body suit. The stimulators are sequenced to play over the body according to the composer's intention. Cutaneous Grooves is a series of tactile and musical compositions designed to demonstrate the perceptual power of the system.",2002-12-30,"['gray-matters', 'interactive-cinema']",,20727,,2016-12-05T00:16:50.693Z,['gid@media.mit.edu'],True,skinscape-a-tool-for-tactile-composition,2000-12-31,[],Skinscape: A Tool for Tactile Composition,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:04.557Z,"Skrin is an exploration project on digitalized body skin surface using embedded electronics and prosthetics. Human skin is a means for protection, a mediator of our senses, and a presentation of our selves. Through several projects, we expand the expression capacity of the body's surface and emphasize the dynamic aesthetics of body texture by technological means. Working with conventional special effect makeup artists, we “hide” electronics into silicone which is applied onto skin and covered by cosmetics. The digitalized skin surface is connected with the affective experience, while the illuminated body is a representation of internal state.Working with bionic pop artist Viktoria Modesta, we deployed the project in Music Tech Festival Berlin 2016 and transformed her body as a canvas along with the performance.",2017-06-30,"['responsive-environments', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2016/10/25/Skrin-back.jpg,20735,--Choose Location,2018-12-05T14:35:37.656Z,"['pattie@media.mit.edu', 'katiav@media.mit.edu', 'xxxxxxin@media.mit.edu', 'joep@media.mit.edu']",True,skrin,2016-01-01,[],Skrin,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:12:04.641Z,"A basketball net incorporates segments of conductive fiber whose resistance changes with degree of stretch. By measuring this resistance over time, hardware associated with this net can calculate force and speed of a basketball traveling through the net. Applications include training, toys that indicate the force and speed on a display, dunk competitions, and augmented-reality effects on television broadcasts. This net is far less expensive and more robust than other approaches to measuring data about the ball (e.g., photosensors or ultrasonic sensors) and the only physical change required for the hoop or backboard is electrical connections to the net. Another application of the material is a flat net that can measure velocity of a ball hit or pitched into it (as in baseball or tennis); it can measure position as well (e.g., for determining whether a practice baseball pitch would have been a strike).",2014-01-13,['object-based-media'],https://dam-prod2.media.mit.edu/x/2017/08/15/IMAG0214.jpg,20743,--Choose Location,2020-01-13T19:51:49.415Z,"['vmb@media.mit.edu', 'novysan@media.mit.edu']",True,slam-force-net,2011-09-01,[],Slam Force Net,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:04.717Z,"Slap Snap Tap combines wearable sensors with physical block programming to enable enhanced expression through movement. By slapping on a set of sensor straps, snapping in code that links movement triggers to sound actions, and tapping the sensors to activate a play experience, users can combine motion and sound in creative ways. A dancer can create music through movement; an athlete can add emphasis to her performance; demonstrators can synchronize and amplify a chant; and anyone can create sound effects for life moments. Slap Snap Tap is a method of the Slay Play endeavor which aims to broaden participation in computational creation by using movement as a pathway into computational thinking.",2016-12-31,"['lifelong-kindergarten', 'civic-media']",https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202016-05-08%20at%204.49.14%20PM.png,20751,--Choose Location,2017-03-31T19:41:01.413Z,"['mres@media.mit.edu', 'ethanz@media.mit.edu', 'joyab@media.mit.edu']",True,slap-snap-tap,2016-01-01,[],Slap Snap Tap,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:04.933Z,"Nearly all classes of wearable and implantable biomedical devices depend on battery power for continuous operation. However, the life span of batteries is limited, rarely exceeding a few hours for wearables and a few years for implants. Consequently, battery replacements and, often times, surgical procedures are required to change the depleted batteries of implants, exposing people to high risks of surgical complications and/or high financial costs. This project seeks to develop conformal piezoelectric patches integrated to personal garments to extract energy from body movements such as motion of arms, fingers, and legs. The completion of this project could improve quality life for people and potentially provide environmentally friendly power.",,"['conformable-decoders', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2017/09/11/PZTdevice.jpg,20775,--Choose Location,2023-09-07T23:29:26.452Z,['canand@media.mit.edu'],True,conformal-piezoelectric-mechanical-energy-harvesters,2016-01-01,"['bioengineering', 'energy', 'health', 'wearable-computing', 'sensors', 'biotechnology']",Conformal Piezoelectric Mechanical Energy Harvesters: Mechanically Invisible Human Dynamos,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:06.734Z,"How can we show our 16-megapixel photos from our latest trip on a digital display? How can we create screens that are visible in direct sunlight as well as complete darkness? How can we create large displays that consume less than 2W of power? How can we create design tools for digital decal application and intuitive-computer aided modeling? We introduce a display that is high-resolution but updates at a low frame rate: a slow display. We use lasers and monostable light-reactive materials to provide programmable space-time resolution. This refreshable, high-resolution display exploits the time decay of monostable materials, making it attractive in terms of cost and power requirements. Our effort to repurpose these materials involves solving underlying problems in color reproduction, day-night visibility, and optimal time sequences for updating content.",2015-01-01,['camera-culture'],,20903,--Choose Location,2017-04-03T18:23:43.031Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,slow-display,2010-01-01,[],Slow Display,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:06.894Z,"Slurp is a tangible interface for manipulating abstract digital information as if it were water. Taking the form of an eyedropper, Slurp can extract (slurp up) and inject (squirt out) pointers to digital objects. We have created Slurp to explore the use of physical metaphor, feedback, and affordances in tangible interface design when working with abstract digital media types. Our goal is to privilege spatial relationships between devices and people while providing new physical manipulation techniques for ubiquitous computing environments. Current applications include tangible/graphical mediation, locative-media, multi-display reaching, and physicalization for online retail.",2008-01-01,['tangible-media'],,20911,E15-344,2019-02-15T03:08:31.211Z,['ishii@media.mit.edu'],True,slurp,2007-01-01,[],Slurp,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:07.261Z,"This project examines the creation of modular computational elements which  can be used to build smart rooms, linked meeting rooms, and other sensor- and display-equipped intelligent spaces. These units are intended  to tile the walls of a room, and act as a scalable, self-organizing system. Each tile incorporates networked communications, sensing, intelligence, and actuation/display. Cells coordinate their operations in order to provide complex sensing and display applications.  Most recently this platform has been the basis for a gesture-based distributed telecollaboration and simulation system. This is a joint project between the Media Laboratory and the Information and Communications University (ICU) in Korea.",2006-09-01,['object-based-media'],,20935,Garden Conference Room,2016-12-05T00:16:50.843Z,['vmb@media.mit.edu'],True,smart-architectural-surfaces,2003-01-01,[],Smart Architectural Surfaces,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:08.075Z,"Because of rising energy costs, of considerable interest in many parts of the world is the smart energy meter: a tool to let consumers know how much electricity they are using in their homes or offices at any given time. Instead of guessing at how much energy they are using and waiting to see their bill at the end of the month, users can directly observe the costs of, for example, leaving a computer on overnight, or how much they save by lowering the heat by one degree. The current project studies how consumers react to energy meters in their homes: how much and in what way they save energy, if these changes are lasting, and how to encourage further energy savings.",2008-01-01,['erationality'],,20991,E15-320A,2016-12-05T00:17:03.693Z,[],True,smart-energy-meters-and-consumer-behavior,2006-01-01,[],Smart Energy Meters and Consumer Behavior,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:08.244Z,"In the future, we will be able to paint pixels onto nearly any surface�but what will we use them for? Our smart megapixel display project aims to create a 100 megapixel display surface that is aware of the people around it and where they are looking. We will use this platform to explore collaborative work, software visualization, adaptive information display, and multi-resolution display.",2008-09-01,['design-ecology'],,20999,--Choose Location,2016-12-05T00:16:50.817Z,['dsmall@media.mit.edu'],True,smart-megapixel-display,2008-09-01,[],Smart Megapixel Display,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:08.427Z,"In this research project, we rethink the mobility dynamics in the city context, both from a physical and cognitive perspective, considering a bus line as a starting point to redesign multimodal urban transportation system. We consider a bus line as a complex environment composed of the bus as a public transportation system; the bus stop, where people meet, wait, connect; the city as the urban/architectural/cultural environment; the passengers with their needs; and the information/communication systems and devices. With this perspective, we explore how wireless interactive media technologies can be integrated into every level of a complex bus line, including physical and virtual public spaces and the information/communication system with passengers and urban environment, with the aim of increasing smart mobility and a more efficient information distribution.",2007-09-01,['smart-cities'],,21007,E15-001,2016-12-05T00:16:50.770Z,['federico@media.mit.edu'],True,smart-mobility,2005-09-01,[],Smart Mobility,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:09.753Z,"We are adding an olfactory dimension to storytelling in order to create more immersive and evocative experiences. Smell Narratives allows the authoring of a ""smell track,"" involving individual or proportionally mixed fragrance components. ",2019-10-01,"['ultimate-media', 'ce-20', 'future-storytelling', 'object-based-media']",,21095,--Choose Location,2020-06-10T16:32:00.577Z,['vmb@media.mit.edu'],True,smell-narratives,2014-09-01,[],Smell Narratives,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:09.876Z,"Our hectic and increasingly digital lives can have a negative effect on our health and wellbeing. Some authors have argued that we socialize less frequently with other people in person and that people feel increasingly lonely. Loneliness has been shown to significantly affect health and wellbeing in a negative way. To combat this, we designed a game, SmileCatcher, which encourages players to engage in in-person, social interactions and get others to smile. Participants wear a device that takes regular pictures of what is in front of them and the system analyzes the pictures captured to detect the number of smiles.",,"['advancing-wellbeing', 'fluid-interfaces']",,21103,--Choose Location,2016-12-05T00:16:44.188Z,"['pattie@media.mit.edu', 'nfarve@media.mit.edu']",True,smilecatcher,2014-01-01,[],SmileCatcher,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:10.019Z,"SmileSeeker is a novel, machine-vision system that captures and provides quantified information about nonverbal communication where social interactions naturally happen. For example, in banking services, tellers observe facial expressions, head gestures, and eye gaze of customers, but this tool lets them both observe their own expressions and analyze how these interact with those of the customer to influence their mutual experience.  The tool allows either real-time or offline feedback to help people reflect on what these interactions mean and determine how to elicit better experiences, such as true customer delight.  The first deployment of this project focuses on eliciting and capturing smiles, and doing so in a way that is respectful of both customer and employee feelings. This project will also explore ways to share this information and link it to outcomes such as banking fee reductions or donations to charity.",2009-01-01,['affective-computing'],,21111,E15-448,2016-12-05T00:17:04.322Z,['picard@media.mit.edu'],True,smileseeker-customer-and-employee-affect-tagging-system,2008-01-01,[],SmileSeeker: Customer and Employee Affect Tagging System,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:10.168Z,"SmileTracker is a system designed to capture naturally occurring instances of positive emotion during the course of normal interaction with a computer. A facial expression recognition algorithm is applied to images captured with the user's webcam. When the user smiles, both a photo and a screenshot are recorded and saved to the user's profile for later review. Based on positive psychology research, we hypothesize that the act of reviewing content that led to smiles will improve positive affect, and consequently, overall wellbeing.",2015-09-01,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/2016/10/20/PublicGallery.png,21119,--Choose Location,2016-12-05T00:17:02.371Z,"['picard@media.mit.edu', 'jaquesn@media.mit.edu', 'cvx@media.mit.edu']",True,smiletracker,2014-09-01,['health'],SmileTracker,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:10.322Z,"SMPL is a software architecture framework aimed at supporting rich synchronous and asynchronous collaboration among people across the Internet. The architecture allows different groups of users to collaborate through a wide range software, hardware, and data platforms. Applications powered by SMPL will not only be able to share processes and data, but also to harvest efficiently the information generated by the interaction of their users. SMPL presents a hybrid topology that simplifies the creation of new functionalities by facilitating the aggregation of different Internet services across a wide range of computing devices, in what is termed ""functionality composition.""",2005-09-01,['information-ecology'],,21127,E15-301,2016-12-05T00:16:11.164Z,[],True,smpl,2004-09-01,[],SMPL,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:10.474Z,"SMPL Services is a collection of basic software technologies built around the SMPL core that allows applications to go beyond the level of a single device. The first generation of distributed SMPL services encompasses data storage, user authentication, media rendering, annotation, sharing, search, interaction harvesting, and project management.",2005-01-01,['information-ecology'],,21135,E15-301,2016-12-05T00:16:51.238Z,[],True,smpl-services,2004-09-01,[],SMPL Services,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:10.730Z,"SnapN'Share, the first sample application that makes use of the Comm.unity platform, runs on wireless mobile devices and stationary computers, and enables users to generate and seamlessly share content with groups and communities as they come within close proximity of their peers. The system offers local filtering of content, exposing users to what is locally relevant, and also allowing new introductions to occur in the physical space. It enables communication based on groups and context�""Virtual Spaces"" are created for different social groups (friends, family, classmates). The system allows for the easy creation of ad hoc or time-limited groups (e.g., meeting attendees or people going on a trip together). SnapN'Share includes multiple features related to trust and identity, allowing the differentiation between strangers and familiar peers, and configuration of varying degrees of trust and privacy. Finally, the system offers all of the inherent features of the Comm.unity platform.",2008-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/snap_n_share_logo.jpg,21151,E15-483,2016-12-05T00:16:51.210Z,"['nadav@media.mit.edu', 'lip@media.mit.edu']",True,snapnshare,2007-01-01,[],SnapN'Share,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:10.863Z,"We are applying learnings from the SNAPSHOT study to the problem of changing behavior, exploring the design of user-centered tools which can harness the experience of collecting and reflecting on personal data to promote healthy behaviors--including stress management and sleep regularity. We draw on commonly used theories of behavior change as the inspiration for distinct conceptual designs for a behavior-change application based on the SNAPSHOT study. This approach will enable us to compare the types of visualization strategies that are most meaningful and useful for acting on each theory.",2016-12-01,"['advancing-wellbeing', 'affective-computing']",,21159,--Choose Location,2018-03-08T15:48:06.097Z,"['picard@media.mit.edu', 'akanes@media.mit.edu', 'sataylor@media.mit.edu']",True,snapshot-expose,2015-01-01,['wellbeing'],SNAPSHOT Expose,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:10.996Z,"The SNAPSHOT study seeks to measure Sleep, Networks, Affect, Performance, Stress, and Health using Objective Techniques. It is an NIH-funded collaborative research project between the Affective Computing and Collective Learning groups, and Harvard Medical School's Brigham & Women's hospital. Since fall 2013, we've run this study to collect one month of data every semester from 50 undergraduate students who are socially connected. We have collected data from about 250 participants, totaling over 7,500 days of data. We measure physiological, behavioral, environmental, and social data using mobile phones, wearable sensors, surveys, and lab studies. We investigate how daily behaviors and social connectivity influence sleep behaviors and health, and outcomes such as mood, stress, and academic performance. Using this multimodal data, we are developing models to predict onsets of sadness and stress. This study will provide insights into behavioral choices for wellbeing and performance.",,"['advancing-wellbeing', 'affective-computing']",,21167,--Choose Location,2019-05-24T08:52:52.725Z,"['picard@media.mit.edu', 'akanes@media.mit.edu', 'sataylor@media.mit.edu', 'jaquesn@media.mit.edu', 'ehinosa@media.mit.edu', 'cvx@media.mit.edu', 'orudovic@media.mit.edu', 'asma_gh@media.mit.edu', 'dlmocdm@media.mit.edu', 'fpeng@media.mit.edu', 'terumi@media.mit.edu']",True,snapshot-study,2012-01-01,"['health', 'wearable-computing', 'machine-learning', 'affective-computing', 'wellbeing']",SNAPSHOT Study,PUBLIC,https://snapshot.media.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:11.120Z,"Sneak is a hybrid digital tabletop game for two-to-four players about deception, stealth, and social intuition. Each player secretly controls one agent in a procedurally generated supervillain lair. Their mission is to find the secret plans and escape without getting discovered, shot, or poisoned by another player. To accomplish this, players must interact and blend in with a series of computer-controlled henchmen while keeping a close eye on their human opponents for any social cues that might reveal their identity. Sneak introduces a number of systems that are common in video games, but were impractical in tabletop games that did not deeply integrate a smartphone app. These include procedural map generation, NPC pathfinding, dynamic game balancing, and the use of sound.",2017-06-01,['playful-systems'],https://dam-prod2.media.mit.edu/x/files/Display/logo_fingers.png,21175,--Choose Location,2022-06-10T14:14:48.993Z,"['slavin@media.mit.edu', 'gregab@media.mit.edu']",True,sneak-a-hybrid-digital-physical-tabletop-game,2015-01-01,[],Sneak: A Hybrid Digital-Physical Tabletop Game,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:11.461Z,"The understanding and modeling of social influence on human economic behavior in city environments can have important implications. In this project, we study human purchase behavior at a community level and argue that people who live in different communities but work at similar locations could act as ""social bridges"" that link their respective communities and make the community purchase behavior similar through the possibility of social learning through face-to-face interactions.",,['human-dynamics'],,21199,--Choose Location,2016-12-05T00:17:04.354Z,"['sandy@media.mit.edu', 'singhv@media.mit.edu', 'xdong@media.mit.edu', 'suhara@media.mit.edu']",True,social-bridges-in-community-purchase-behavior,2015-09-01,[],Social Bridges in Community Purchase Behavior,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:11.558Z,"Buildings consume more than a third of the energy used in the United States, but most people have no sense of how much their actions can affect a building's energy use. We are testing the hypothesis that if people have a convenient way to record their energy use and learn ways to improve it, they will change their habits. We have created visualizations of HVAC use throughout the Media Lab to test this hypothesis in a large-scale space. The system uses touch-screen networked displays strategically placed throughout the building to convey real-time and historical temperature and thermostat settings, and ultimately electric usage. Not only can people see a heat map of their lab area, they can also observe trends and compare their energy usage to those in other areas.",2011-09-01,['viral-communications'],,21207,,2016-12-05T00:17:04.413Z,['lip@media.mit.edu'],True,social-energy,2010-09-01,[],Social Energy,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:11.752Z,"The Internet supports many great tools for communicating at a distance in order to maintain personal relationships and build social networks.  However, these tools rarely help us realize which relationships are strained by lack of attention. Social Garden explores using virtual plants as a metaphor for relationships, encouraging us to tend to our social connections as we do our gardens. By tracking and analyzing communications through email, instant messaging, social websites, SMS, and phone, Social Garden proposes to give feedback on how our relationships are flourishing or wilting, and organizes our social circles. We also explore the garden metaphor as a practical interface to browse and manage conversations and contacts.",2010-01-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/icon256.png,21223,E15-301,2016-12-05T00:16:51.322Z,"['holtzman@media.mit.edu', 'jkestner@media.mit.edu']",True,social-garden,2008-09-01,[],Social Garden,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:12.028Z,"Social Network Fragments is interested in exploring the structure of an individual's social network. The relationship between actors in one's network says a great deal about the individual, revealing how he or she segments his or her social circles to reflect different facets of his or her life and social identity. In our research, we use visualization techniques to reveal the network structure that people develop during their email correspondences, thereby creating the tools to help understand aspects of social interaction. Additionally, this research reveals how much information can be derived about a large group of people by analyzing one person's habits. This is particularly interesting given our desire to understand communities as a whole, not just individuals. Such research raises serious questions about privacy, representation, and analysis that must be addressed as we move forward.",2003-09-01,['sociable-media'],,21247,E15-383,2016-12-05T00:17:04.441Z,['judith@media.mit.edu'],True,social-network-fragments,2001-01-01,[],Social Network Fragments,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:12:12.113Z,"People are experts in social intelligence. Most know what is socially appropriate in a given situation, especially how to gauge if something is important enough to interrupt a conversation. People usually also know what kind of social situation they are in and, for example, if it is appropriate to take phone calls. We have built a system that allows an intelligent call-handling agent to ""ask"" the participants in a face-to-face conversation with the user in a very subtle way�a wirelessly actuated, vibrating finger ring�if an interruption from a mobile communication device would be appropriate. All involved people are given the possibility to ""veto"" an incoming communication anonymously in an equally subtle way, by touching their finger ring. This shifts the burden of deciding to interrupt or not away from the agent and toward the humans actually involved in a conversation.",2004-01-01,['living-mobile'],,21255,E15-368,2016-12-05T00:16:11.226Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,social-polling-of-immediate-surroundings,2003-09-01,[],Social Polling of Immediate Surroundings,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:12.192Z,"One of the most fundamental and important characteristics of children is their curiosity. Can an interaction with a robot elicit, guide, and promote curiosity? In this project we use the DragonBot platform and a tablet app in a language educational setting with children. We test the hypothesis that a personalized, curiosity-driven behavior of a robot behaving as a younger peer, can affect a child's own curiosity. We use an in-house developed app in which the child's interaction with figures is automatically transformed into a spoken and written story. The social robot reacts to the story in an emotional way and then asks the child questions about it. The child and robot then switch roles, so that the robot tells the story and the child asks questions. The research question is whether the robot's curiosity-driven behavior affects the child's curiosity.",2017-05-15,['personal-robots'],https://dam-prod2.media.mit.edu/x/files/Display/20140416_185315.jpg,21263,--Choose Location,2017-05-31T18:41:35.438Z,"['cynthiab@media.mit.edu', 'ggordon@media.mit.edu']",True,social-robot-as-a-younger-curious-peer,2014-01-01,[],Social Robot as a Younger Curious Peer,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:12.432Z,"We are using non-invasive measurement of social signals found in voice, body movement, and location to quantify symptoms in neurological disorders such as Parkinson's Disease.",2012-09-01,['human-dynamics'],,21287,,2016-12-05T00:16:51.495Z,['sandy@media.mit.edu'],True,social-signals-in-biomedicine,2011-01-01,[],Social Signals in Biomedicine,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:12.513Z,"The way we represent ourselves in social media is intangible. What we choose to wear is public to the world and we are aware of it. In contrast, what we post online about ourselves reaches thousands of people and generates social consequences, but it doesn’t feel that way. Is the current form of social media really making our relationships better? Current technologies are good at connecting people at a distance, but less so at connecting them within the same environment.Social textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging.Social Textiles embody who you are and dynamically reflect your shared interests with people nearby. They enable you to gain access to communities of people in the physical world and enhance social affordances and icebreaking interactions through wearable social messaging. Social Textiles can connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. They have the potential to enable members to bypass superficial or generic interests through ""filtering"" individuals, in order to tune social experiences toward people who are more compatible.",2015-01-01,"['tangible-media', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/files/Display/words.png,21295,--Choose Location,2018-10-12T16:50:34.930Z,"['ishii@media.mit.edu', 'pattie@media.mit.edu', 'amores@media.mit.edu', 'katsuyaf@media.mit.edu', 'viirj@media.mit.edu', 'changzj@media.mit.edu']",True,social-textiles,2014-09-01,[],Social Textiles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:12.602Z,"Social Transactions is an application that allows communities of consumers to collaboratively sense the market from mobile devices, enabling more informed financial decisions in a geo-local and timely context. The mobile application not only allows users to perform transactions, but also to inform, share, and purchase in groups at desired times. It could, for example, help people connect opportunistically in a local area to make group purchases, pick up an item for a friend, or perform reverse auctions. Our framework is an Open Transaction Network that enables applications from restaurant menu recommendations to electronics purchases.  We tested this with MIT's TechCASH payment system to investigate whether shared social transactions could provide just-in-time influences to change behaviors.",2012-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/otn_logo_text.png,21303,--Choose Location,2016-12-05T00:16:51.580Z,['lip@media.mit.edu'],True,social-transactionsopen-transactions,2009-09-01,[],Social Transactions/Open Transactions,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:12.684Z,"Our mission is to develop the computational techniques that will enable the design, implementation, and evaluation of ""relational"" robots, in order to encourage social, emotional, and cognitive growth in children, including those with social or cognitive deficits. Funding for the project comes from the NSF Expeditions in Computing program. This expedition has the potential to substantially impact the effectiveness of education and healthcare, and to enhance the lives of children and other groups that require specialized support and intervention. In particular, the MIT effort is focusing on developing second-language learning companions for pre-school aged children, ultimately for ESL (English as a Second Language).",2017-04-30,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/17/child-tega-2.png,21311,--Choose Location,2017-06-07T16:38:24.487Z,"['cynthiab@media.mit.edu', 'jinjoo@media.mit.edu', 'edith@media.mit.edu', 'jakory@media.mit.edu', 'lukulele@media.mit.edu', 'samuelsp@media.mit.edu', 'ggordon@media.mit.edu', 'michalg@media.mit.edu', 'havasi@media.mit.edu', 'sooyeon6@media.mit.edu']",True,socially-assistive-robotics-an-nsf-expedition-in-computing,2012-01-01,"['robotics', 'kids', 'learning-teaching', 'social-robotics', 'long-term-interaction']",Socially Assistive Robotics: An NSF Expedition in Computing,PUBLIC,http://robotshelpingkids.yale.edu/,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:12.859Z,"Professor Minsky continues to develop the theory of human thinking and learning called the ""Society of Mind,"" which tries to explain how various phenomena of mind emerge from the interactions among many different kinds of highly evolved brain mechanisms. In this way we can account for many aspects of common sense, imagination, and reasoning by analogy, as resulting from negotiations among systems that use different ways of representing knowledge. Similarly, it appears that we can explain many of the regularities found in natural languages as consequences of how those representations work, rather than as constraints that are externally imposed on interpersonal communications. This approach also suggests that some of what we call ""emotions"" are mechanisms required for managing conflicts among competing goals. We may need to construct similar systems when we begin to build smarter and more versatile machines.",2009-01-01,['society-of-mind'],,21327,E15-309,2022-06-10T13:10:40.315Z,['minsky@media.mit.edu'],True,society-of-mindthe-emotion-machine,2000-01-01,[],Society of Mind/The Emotion Machine,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:12:12.930Z,"Online public discussion spaces such as Usenet news groups are rich social environments. The social dynamics within the community are not obvious by looking at the strings of text-based content. Only a careful reading of the threads allows the viewer to discern complexities and nuances of social interactions. Expressive visualization, however, is an alternative medium for effectively conveying such information. In order to animate the dynamic social qualities found within the static data of a Usenet interface, motion is used as the communicative agent for this visual translation. The goal of the research is to isolate those elements that comprise visual motion, such as position, direction, speed, and time, in order to develop a visual language through which the social complexities of online communities can be communicated. A series of studies exploring this problem are being carried out using a theoretical framework inspired by cognitive and artistic precedents. We hope these investigations result in an understanding of how motion can be successfully employed as a visual language for social expression.",2003-01-01,['sociable-media'],,21335,E15-450,2016-12-05T00:17:04.538Z,['judith@media.mit.edu'],True,socio-kinetics,2002-01-01,[],Socio-Kinetics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:13.012Z,"We are designing new forms of situated displays and interfaces embedded in public spaces. Our current focus is on interfaces that can function as social sensors, capturing the dynamics of people in different locations. We design applications that engage with users with alternative models of input for content generation that foster collective behavior in public.",2017-05-01,['sociable-media'],,21343,E15-391,2022-06-10T14:41:30.088Z,['judith@media.mit.edu'],True,sociomedia-garden,2006-09-01,[],Sociomedia Garden,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:13.090Z,"The boundaries and fabric of human experience are continuously redefined by microorganisms, interacting at an imperceptible scale. Though hidden, these systems condition our bodies, environment, and even sensibilities and desires. The proposed works introduce a model of interaction in which the microbiome is an extension of the human sensory system, accessed through a series of biological interfaces that enable exchange. Biological Interfaces transfer discrete behaviors of microbes into information across scales, where it may be manipulated, even if unseen. In the same way the field of HCI has articulated our exchanges with electronic signals, Soft Exchange opens up the question of how to design for this other invisible, though present, and vital material.",2017-06-01,['playful-systems'],,21351,--Choose Location,2022-06-10T14:14:49.010Z,['slavin@media.mit.edu'],True,soft-exchange-interaction-design-with-biological-interfaces,2014-09-01,[],Soft Exchange: Interaction Design with Biological Interfaces,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:12:13.247Z,"The purpose of the Software Perception project is to develop fluid knowledge representations for the concepts needed to understand and create programs. These representations will power a visual editor to assist a person synthesizing new code through a visual dialogue. The system utilizes simulations to build models of many existing real-world metaphors used for programming models. In the other direction, visualization involves creating real-world analogues to abstract processes. The goal is to teach the system programming concepts and concepts based on real-world experiences, and the relation between the two.",2003-01-01,['aesthetics-computation'],,21367,E15-305A,2016-12-05T00:16:51.682Z,[],True,software-perception,2002-01-01,[],Software Perception,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:13.324Z,"Bitcoin generates net-new value from ""mining"" in a distributed network. In this work, we explore solar micro-mining rigs that transform excess energy capacity from renewable energy (hard to trade) into money (fungible). Each rig runs a small Bitcoin miner and produces Bitcoin dust for micropayments. We envision these micro-miners populating a highly distributed network, across rooftops, billboards, and other outdoor spaces. Where systematic or environmental restrictions limit the ability to freely trade the underlying commodity, micro-mining produces new economic viability. Renewable energy-based, micropayment mining systems can broaden financial inclusion in the Bitcoin network, particularly among populations that need a currency for temporary store of value and must rely on flexible electricity off the grid (e.g., unbanked populations in the developing world). This exploration seeds a longer-term goal to enable open access to digital currency via account-free infrastructure for the public good.",2017-06-06,"['digital-currency-initiative-dci', 'viral-communications']",https://dam-prod2.media.mit.edu/x/2017/03/23/SolarRig_BostonSkyline.jpg,21375,--Choose Location,2018-10-08T01:32:51.238Z,"['aekblaw@media.mit.edu', 'lip@media.mit.edu']",True,solar-micro-mining,2015-09-01,['blockchain'],Solar Micro-Mining,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:13.410Z,"In this project, human-built spaces such as parks are acoustically analyzed and then permanently and publicly labeled.",2005-09-01,['computing-culture'],,21383,E15-020A,2016-12-05T00:16:51.710Z,['csik@media.mit.edu'],True,sonic-authority,2005-09-01,[],Sonic Authority,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:12:13.486Z,"Building on well-developed data visualization techniques, audio is used to enhance understanding by improving the brain's ability to parse information in the scientific process. Sonification (aka auditory display) is the use of non-speech audio to convey information. In collaboration with the Media Lab, NASA is interested in using the human auditory system's powers of organizing and deconstructing sound for the purposes of scientific research and exploratory data analysis. Faced with increasingly voluminous and complex multimodal data and computations, researchers are seeking novel ways to optimize the conversion of information into knowledge. Several sonifications are under study, including interactive tours through the solar system, spectral data from Mars, sensor data from the Media Lab, new augmentations of the traditional orrery, and enhanced exploration of the Mandelbrot set. The eventual goal is to generalize these new sonification techniques for the display, exploration, and analysis of any large dataset.",2008-01-01,['music-mind-and-machine'],,21391,E15-484,2016-12-05T00:16:51.635Z,"['bv@media.mit.edu', 'joep@media.mit.edu']",True,sonic-scientist,2008-09-01,[],Sonic Scientist,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:13.569Z,"SonicLink is a fully decentralized, proximal communication framework for personal devices to seamlessly discover, connect, and interact with arbitrary public installations (such as digital billboards). It establishes connections based on audio proximity when you are not on the same network:  being near in physical space does not mean you are near in ""network space.""  SonicLink uses near-ultrasonic acoustic signals that permit devices and installations to discover each other. It also exploits peer-to-peer proximal wireless networking techniques for establishing a high-bandwidth, low-latency link between the device and the installation. Possible uses include �borrowing� a large-screen TV and a camera in the public space for personal video conferencing, presenting personal notifications to a public display, and taking over neon lights for visualizing music on a phone.",2012-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/logo.png,21399,--Choose Location,2016-12-05T00:16:51.732Z,['lip@media.mit.edu'],True,soniclink,2013-01-01,[],SonicLink,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:14.099Z,"Sound Cycles is a new interface for exploring, re-mixing, and composing with large volumes of audio content. The project presents a simple and intuitive interface for scanning through long audio files or pre-recorded music. Sound Cycles integrates with the existing Digital Audio Workstation for on-the-fly editing, audio analysis, and feature extraction.",,['opera-of-the-future'],,21455,--Choose Location,2016-12-05T00:17:04.683Z,"['tod@media.mit.edu', 'rebklein@media.mit.edu', 'holbrow@media.mit.edu']",True,sound-cycles,2015-01-01,[],Sound Cycles,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:14.251Z,"Sound designers, audio professionals, and musicians often spend time and energy looking for the right sound for a particular piece of music or sonic environment. Current sound synthesizers either contain numerous sound presets that are laborious to parse, or batteries of parameters to tweak without straightforward connections to one's intuitive expectation. We propose a sound retrieval and modification engine based on everyday words like ""bright,"" ""warm,"" and ""fat."" The perceptual sound synthesis engine is informed by a survey of musicians and listeners worldwide and can also be customized. This system allows dynamic tagging of sound material from online libraries, and ""sound sculpting"" based on common verbal descriptors instead of obscure numerical parameters.",2011-01-01,['music-mind-and-machine'],,21463,E15-310,2016-12-05T00:16:51.756Z,['bv@media.mit.edu'],True,sound-design-with-everyday-words,2005-09-01,[],Sound Design with Everyday Words,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:14.405Z,"This project will build comfortable technology and tools that a wearer can use to record their own speech in natural social settings, play it back, and learn about and reflect on how he sounds. The tools will include automated analysis of pitch patterns, loudness, and timing, to help highlight these non-linguistic features of speech. The toolkit can be used during, or in conjunction with, speech-language therapy for individuals on the autism spectrum to explore and reflect on natural social situations. It will also enable autism researches to collect speech data outside of laboratory settings to quantify progress with language and communication in natural social contexts. 
",2009-09-01,[],,21471,E15-443,2016-12-05T00:16:51.822Z,"['picard@media.mit.edu', 'mehoque@media.mit.edu', 'mgoodwin@media.mit.edu']",True,soundaffects,2007-01-01,[],SoundAffects,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:14.594Z,"Recently, multi-view display hardware has made compelling progress in graphics. Soundaround is a multi-viewer interactive audio system, designed to be integrated into unencumbered multi-view display systems, presenting localized audio/video channels with no need for glasses or headphones. Our technical work describes a framework for the design of multi-viewer interactive audio systems that is general and supports optimization of the system for multiple observation planes and room responses.",2012-01-01,"['information-ecology', 'camera-culture']",https://dam-prod2.media.mit.edu/x/files/Display/soundaround.png,21479,,2016-12-05T00:16:11.280Z,"['holtzman@media.mit.edu', 'raskar@media.mit.edu', 'olwal@media.mit.edu']",True,soundaround,2011-01-01,[],Soundaround,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:14.783Z,"Soundforms is a tabletop interface for collaborative composition. The shapes and sizes of objects are used as simple icons for controlling sound output, affording a playful interface for people to experiment together in a short sound composition.",2010-01-01,['fluid-interfaces'],,21487,,2016-12-05T00:16:51.859Z,"['pattie@media.mit.edu', 'pol@media.mit.edu']",True,soundforms,2009-09-01,[],SoundForms,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:15.137Z,"SoundSieve is an exciting new way to look at the structure of music, designed to create a visual �fingerprint that can provide the listener with instant information about any given piece. SoundSieve aims to counter the trend towards overly simplistic, flashy music visualization programs by creating a real-time visualization that is not only informative, but enlightening as to the underlying musical structure of the piece. From any MP3 file, SoundSieve uses simple, intuitive mapping to create a picture and video playback that highlight key audio features. In addition, SoundSieve allows the user to actively examine the piece: to zoom in on portions of interest, adjust playback, and add informative overlays. SoundSieve has recently been used to create customized animations that highlight particular features in a piece (e.g., Tod Machover�s ""The System"" for the opera ""Death and the Powers"").",2008-01-01,['opera-of-the-future'],,21503,E15-443,2016-12-05T00:17:04.793Z,['tod@media.mit.edu'],True,soundsieve,2007-01-01,[],SoundSieve,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:15.303Z,"SoundStrand is a music composition toy. It comprises a set of building blocks, each containing a musical motif. The blocks can be connected to each other to create a musical theme. They can also be manipulated with three degrees of freedom: elongation changes the rhythmic distribution of the notes; bending changes the direction of the melody; and twisting changes the harmonic context.",2012-09-01,['opera-of-the-future'],,21511,,2016-12-05T00:16:51.976Z,['tod@media.mit.edu'],True,soundstrand,2011-09-01,[],SoundStrand,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:15.468Z,"Sourcemap is the social network for supply chains, connecting producers, manufacturers, and consumers for end-to-end visibility. Consumers use Sourcemap to learn where things come from and what they're made of, including their social and environmental impact. Manufacturers use Sourcemap to trace products down to raw materials and plan more resilient, efficient supply chains. Visit www.sourcemap.com for more information.",2013-01-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/SourceMapLogo.jpg,21519,E15-348,2016-12-05T00:16:51.894Z,"['ishii@media.mit.edu', 'amerigo@media.mit.edu']",True,sourcemap,2010-01-01,[],Sourcemap,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:15.784Z,"Internet spam is an overwhelming disease of the 21st century. With very elaborate devices, we go to great length to avoid seeing spam: junk-mail filters, screening services, spam databases. SP4M attempts to turn the relationship we have with spam around. A microcontroller-based Webserver retrieves spam sent over the Internet in real time, and remotely feeds a continuous shuffled stream of it to public displays. Spam is no longer meaningless, time-consuming junk, but rather a diagnosis of cultural values and an alternate representation of societies.",2006-01-01,['computing-culture'],,21535,E15-020C,2016-12-05T00:16:52.041Z,['csik@media.mit.edu'],True,sp4m,2004-09-01,[],SP4M,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:12:15.927Z,"Space Saver is a system for importing physical spaces into virtual environments. The project provides tools that lower the complexity barrier to representing our physical spaces in a virtual world, in turn adding new levels of relevance to virtual representations. The system operates by amassing readings from a distance-sensing camera into models that can be imported into Second Life and other 3-D modeling software.",2008-09-01,['information-ecology'],,21543,E15-301,2016-12-05T00:16:52.101Z,['holtzman@media.mit.edu'],True,space-saver,2007-09-01,[],Space Saver,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:16.078Z,"SpaceBox explores the addressing of location-specific messages (text, voice, images) to an intended recipient or group of recipients. Its key features include tagging places of interest when a sender is physically present at that location and projecting messages to a specific location. The recipient would receive such messages only when in the vicinity of the tagged location.",2009-09-01,['living-mobile'],,21551,E15-383,2016-12-05T00:16:36.738Z,['geek@media.mit.edu'],True,spacebox-location-based-messaging,2008-09-01,[],SpaceBox: Location-Based Messaging,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:16.244Z,"We have seen an explosion of data, but the tools necessary to understand it have not kept pace. Humans have innate spatial memory and spatial organization abilities, but our interfaces into this ever-growing world of data rarely take into account a learned behavior of space. SpaceMarks changes that, creating an intuitive and consistent method to organize, learn, and collaborate with spatially aware resources: it provides a much-needed method for offloading the processing of our information into the world around us.",2010-01-01,['fluid-interfaces'],,21559,Pond,2016-12-05T00:16:11.333Z,['pattie@media.mit.edu'],True,spacemarks-brain-offloading-through-spatial-thinking,2009-09-01,[],SpaceMarks: Brain Offloading through Spatial Thinking,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:16.398Z,"When humans collaborate face-to-face, they use a wide range of nonverbal signals that support the conversation process. Many of these signals are not available when people have conversations online by exchanging text messages, or are distorted when they attempt conversation through direct transmission, such as video. This project presents a new approach to computer-mediated conversation where the missing signals are added based on a textual analysis of the message and the dynamic discourse context. Spark is the software architecture that supports the automated analysis of communicative function of text messages, the transformation of those functions into nonverbal cues and the performance of those cues by animated avatars.",2003-01-01,[],,21567,E15-320,2016-12-05T00:17:04.721Z,[],True,spark-avatar-augmented-online-collaboration,2002-01-01,[],Spark: Avatar Augmented Online Collaboration,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:16.525Z,"SparkInfo enables users to create, exchange, and augment their audiovisual elements in ways that are personally unique and sociable. SparkInfo provides a social space for the co-creation of audiovisual and multimedia resources. ",2012-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/sparkinfo2.jpg,21575,,2016-12-05T00:16:52.128Z,['holtzman@media.mit.edu'],True,sparkinfo,2012-01-01,[],SparkInfo,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:16.649Z,"Sparkler is the opening piece for Toy Symphony concerts composed by Tod Machover. A simple, sinuous melody weaves throughout the piece, beginning and ending in calmness, but rapidly and delicately undulating along the way. This melody varies continuously�pulling in its wake tiny fragments of remembered classical and popular tunes; expanding into more and more intricate textures; and finally being subsumed in a series of dense sonic masses at the work�s culminating moment. Sparkler explores many different relationships between orchestra and technology. Three keyboards�each with a specially designed controller for shaping timbre and articulation�play and modulate much of the electronics. Unlike many previous works where only solo instruments are amplified or electronically processed, here microphones capture the entire orchestral sound, which is analyzed live into �perceptual parameters� through software written by Tristan Jehan. These instrumental sound masses�which are performed with a certain freedom by players and conductor�generate and control (pushing, pulling, twisting, and morphing) complex electronic extensions, turning the whole ensemble into a kind of �hyperorchestra.� Sparkler reflects the energy, innocence, imagination, vulnerability, and rapid passing of childhood.",2003-01-01,['opera-of-the-future'],,21583,E15-445,2016-12-05T00:16:52.158Z,"['tod@media.mit.edu', 'tristan@media.mit.edu']",True,sparkler,2001-01-01,[],Sparkler,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:16.898Z,"A thing cannot be in two places at once. It cannot move through a solid object. If it goes from point A to point B, it must move through a series of intermediate, adjacent points. These are examples of common sense knowledge belonging to every young child, but heretofore absent in computers. This project involves designing a cognitive architecture for building systems with this kind of spatial and physical common sense.",2001-12-30,['society-of-mind'],,21599,E15-383,2016-12-05T00:17:04.747Z,['minsky@media.mit.edu'],True,spatial-and-physical-common-sense,1999-12-31,[],Spatial and Physical Common Sense,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:17.009Z,"In a modern urban setting, the organization of architectural space structures spatial proximity in the same fashion as peculiarities of natural landscape defined it for our nomadic primogenitors. Patterns of face-to-face interactions in a building are, to a large extent, defined when architects differentiate spaces in it for distinct functions and organize circulation between the spaces. Buildings of the future could be instrumented to inform communication devices carried by their inhabitants and visitors about the intercommunication configuration of spaces within and outside them, thus defining proximity in software. The goal of this project is to develop and evaluate a prototype wearable system for proximity-based synchronous voice messaging in a space configured for intercommunication. Potential applications range from wearable computers promoting ad hoc collaboration within organizations to communication systems for firefighters.",2003-09-01,['smart-cities'],,21607,--Choose Location,2016-12-05T00:17:04.813Z,[],True,spatial-aspects-of-mobile-ad-hoc-collaboration,2003-01-01,[],Spatial Aspects of Mobile Ad Hoc Collaboration,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:17.212Z,"This project uses spatialized audio and presentation of audio recordings from different points in space around the listener's head to enable more efficient listening. Examples include: browsing multiple streams of audio simultaneously, with computer-enhanced, selective attention modeling and automatic notification of salient events on secondary channels; spatial representation of a single audio recording to map time into position for more effective dynamic browsing; and auditory ""information landscapes"" which allow a listener to move in a virtual space among recordings.",2000-12-30,['living-mobile'],,21623,,2016-12-05T00:17:04.771Z,['geek@media.mit.edu'],True,spatial-listening-for-auditory-presentation,1993-12-31,[],Spatial Listening for Auditory Presentation,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:17.408Z,"We are developing a tool to allow users to place and recall virtual sound objects (music files, phone numbers) at 3-D spatial coordinates around us. As these locations are in fact imaginary, it frees us to explore visually independent interaction through audio. Two turns of the wrist, for pitch and yaw orientation, and a scroll of the thumb to provide a distance value, and we are exploring different notions of �file paths� and ""memory addressing.""",2008-01-01,['living-mobile'],,21639,E15-368,2016-12-05T00:16:52.280Z,['geek@media.mit.edu'],True,spatial-audio-interface,2007-01-01,[],Spatial-Audio Interface,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:17.500Z,"SpeakCup is a digital voice recorder. Rather than being controlled via buttons and blinking lights, SpeakCup uses shape to proscribe function. It takes the form of a small rubber disc with holes in its center on one side. When the holes are pressed in, forming a small cup, SpeakCup absorbs and contains sound. When the holes are pressed out, the stored sounds (embodied as fluxuating lights) are released.",2008-01-01,['tangible-media'],,21647,E15-344,2016-12-05T00:16:52.255Z,['ishii@media.mit.edu'],True,speakcup,2007-01-01,[],SpeakCup,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:17.590Z,"Speakeasy is a community-based telephone service that connects multilingual volunteers, non-English speaking immigrants, and social service agencies.  ",2008-01-01,['smart-cities'],,21655,E15-001,2016-12-05T00:16:52.303Z,[],True,speakeasy,2004-01-01,[],Speakeasy,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:17.673Z,"Speaker identification techniques (such as a Gaussian Mixture Model (GMM) and a Hidden Markov Model, using cepstral coefficients as features) have been applied to determine the question of whether individual marine mammals can be identified by their vocalizations alone. With a dataset of four killer whales uttering sounds previously classified as belonging to call type n2, and over 10 sounds from each individual, we have found a very high success rate of 80 to 100% correct for the six pairwise comparisons and around 78% correct for identification among all four individuals. The ability to identify marine mammals from their vocalizations alone, in addition to the theoretical interest for production mechanisms, is extremely valuable in the ability to track these mammals from remote locations where visual information is not present.",2010-01-01,['music-mind-and-machine'],,21663,E15-310,2016-12-05T00:17:04.849Z,"['bv@media.mit.edu', 'brown@media.mit.edu']",True,speaker-identification-of-marine-mammals,2009-01-01,[],Speaker Identification of Marine Mammals,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:17.756Z,"The ability to write has become completely identified with intellectual power, conflating the visual word with the nature of knowledge. One effect of graphocentrism is the limited path to literacy and learning. We propose some ways to re-value modes of composition that are closer to spontaneous oral language than to writing. We define a new term, spriting (speak + write = sprite), which serves some or all of the functions of writing (permanence, possibilities of editing, indexing, and scanning) but in a spoken modality, without the difficult transition to a different form of representation such as writing. We explore how spriting might support literacy learning and offer different ways to compose, using a design research intervention with novel spriting technology in two culturally and socio-economically diverse schools.  This work has implications for literacy learning, educational policy, writing software and speech dictation recognition technology, and Digital Talking Books (DTB) standards development.",2005-01-01,['future-of-learning'],,21671,E15-443D,2016-12-05T00:16:52.354Z,['papert@media.mit.edu'],True,speaking-on-the-record,2003-09-01,[],Speaking on the Record,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:17.839Z,"Motion sensing is of fundamental importance for user interfaces and input devices. In applications where optical sensing is preferred, traditional camera-based approaches can be prohibitive due to limited resolution, low frame rates, and the required computational power for image processing. We introduce a novel set of motion-sensing configurations based on laser speckle sensing that are particularly suitable for human-computer interaction. The underlying principles allow these configurations to be fast, precise, extremely compact, and low cost.",2015-01-01,['camera-culture'],,21679,--Choose Location,2017-04-03T18:24:45.671Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'olwal@media.mit.edu']",True,specklesense,2011-01-01,[],SpeckleSense,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:18.011Z,"Surface and object recognition is of significant importance in ubiquitous and wearable computing. While various techniques exist to infer context from material properties and appearance, they are typically neither designed for real-time applications nor for optically complex surfaces that may be specular, textureless, and even transparent. These materials are, however, becoming increasingly relevant in HCI for transparent displays, interactive surfaces, and ubiquitous computing. We present SpecTrans, a new sensing technology for surface classification of exotic materials, such as glass, transparent plastic, and metal. The proposed technique extracts optical features by employing laser and multi-directional, multi-spectral LED illumination that leverages the material's optical properties. The sensor hardware is small in size, and the proposed classification method requires significantly lower computational cost than conventional image-based methods, which use texture features or reflectance analysis, thereby providing real-time performance for ubiquitous computing.",2016-01-01,['camera-culture'],,21695,--Choose Location,2017-04-03T18:25:33.571Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'olwal@media.mit.edu', 'shiboxin@media.mit.edu', 'shigeoy@media.mit.edu']",True,spectrans-classification-of-transparent-materials-and-interactions,2013-09-01,[],SpecTrans: Classification of Transparent Materials and Interactions,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:18.097Z,"Conventional perception on wireless communications is that as more nodes are added, there could be more transmission interference. We demonstrate spectrum sharing with TV channels in a multicast setting, where more added nodes lead to less transmission power and less interference via cooperations among nodes. This demonstration will show that as more nodes are added in the network, the interference with TV signals due to spectrum sharing becomes almost imperceptible.",2005-09-01,['viral-communications'],,21703,E15-483,2016-12-05T00:16:11.451Z,['lip@media.mit.edu'],True,spectrum-sharing-with-tv-channels,2005-01-01,[],Spectrum Sharing with TV Channels,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:18.331Z,"Collection and analysis of longitudinal observational data of child behavior in natural, ecologically valid, non-laboratory settings holds significant benefits for advancing the understanding of autism and other developmental disorders. We developed the Speechome Recorder�a portable version of the embedded recording technology originally developed for the Human Speechome Project�to facilitate cost-effective deployment in special-needs clinics and homes. Recording child behavior daily in these settings will enable us to study developmental trajectories of autistic children from infancy through early childhood, as well as atypical dynamics of social interaction as they evolve on a day-to-day basis. Its portability makes possible potentially large-scale comparative studies of developmental milestones in both neurotypical and autistic children. Data-analysis tools developed in this research aim to reveal new insights toward early detection, provide more accurate assessments of context-specific behaviors for individualized treatment, and shed light on autism.",2014-01-01,['social-machines'],,21727,E15-441,2016-12-05T00:16:36.855Z,"['dkroy@media.mit.edu', 'soroush@media.mit.edu', 'decamp@media.mit.edu', 'mgoodwin@media.mit.edu']",True,speechome-recorder-for-the-study-of-child-development-disorders,2008-09-01,[],Speechome Recorder for the Study of Child Development Disorders,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:18.415Z,"We are adapting the video data collection and analysis technology derived from the Human Speechome Project for the retail sector through real-world deployments. We are developing strategies and tools for the analysis of dense, longitudinal video data to study behavior of and interaction between customers and employees in commercial retail settings. One key question in our study is how the architecture of a retail space affects customer activity and satisfaction, and what parameters in the design of a space are operant in this causal relationship.",2011-09-01,['social-machines'],,21735,E15-441,2016-12-05T00:16:52.428Z,"['dkroy@media.mit.edu', 'decamp@media.mit.edu']",True,speechome-video-for-retail-analysis,2008-09-01,[],Speechome Video for Retail Analysis,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:18.498Z,"spekter is an 8-bit analog input and output device capable of processing up to eight analog audio (ex: sensor) inputs simultaneously. The heart of the device is an off-the-shelf microprocessor core, featuring built-in Internet connectivity and a substantial amount of on-board memory. The intention is to create an easy and intuitive access to analog streams of data, such as audio. To this end, the device features an LED panel and a selector knob which allow the user to preview a value at any selected channel without the aid of a computer interface. On the flipside, a great degree of control over incoming data is afforded by a C-based programming environment that is interfaced to the device via serial connection. This combination provides for a powerful i/o tool intended as a means to streamline hardware development in the Aesthetics + Computation group.",2002-12-30,['aesthetics-computation'],,21743,E15-305A,2016-12-05T00:16:52.403Z,[],True,spekter,2001-12-31,[],spekter,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:18.573Z,"Emerging pervasive games use sensors, graphics and networking technologies to provide immersive game experiences integrated with the real world. Existing pervasive games commonly rely on a device screen for providing game-related information, while overlooking opportunities to include new types of contextual interactions like jumping, a punching gesture, or even voice to be used as game inputs. Spellbound is a physical mobile team-based game that aims to nurture a spirit of togetherness. Spellbound takes advantage of the above mentioned opportunities and integrates real-world actions like jumping and spinning with a virtual game world. It also replaces touch-based input with voice interaction and provides glanceable and haptic feedback using custom hardware in the true spirit of social play characteristic of traditional children’s games. Spellbound is a form of digital outdoor gaming that anchors enjoyment on physical action, social interaction, and tangible feedback.",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/12/05/movements.png,21751,--Choose Location,2022-06-10T14:01:02.047Z,"['sra@media.mit.edu', 'geek@media.mit.edu']",True,spellbound,2012-11-01,[],Spellbound,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:18.650Z,"SpendTrend is a ReflectOn designed to allow its user to reflect on spending habits. Often we spend based on the momentary buying capability, without considering the long-term outcomes. Most would not think twice about a $4 Starbucks latte, but over a year this amount becomes non-trivial. Likewise, we often fail to think about savings in the long term. By making trends visible at the moment of purchase, SpendTrend attempts to make users mindful of their behavior and long-term goals. SpendTrend is built into a credit card, and has embedded processing and communication. The SpendTrend reader informs the card of the details of the purchases. With accurate, fine-grained information about purchases, the card then computes and display feedback, while also acting as a collection device for receipt data. The card harvests power directly from the reader, and has no explicit charging needs.",2010-09-01,['fluid-interfaces'],,21759,E15-320,2016-12-05T00:16:52.468Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu']",True,spendtrend-reflecting-on-spending-habits,2008-01-01,[],SpendTrend: Reflecting on Spending Habits,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:18.725Z,"One future of media experience lies within a socially connected virtual world. VR started almost 30 years ago and is now wearable, real-time, integrated with sensing, and becoming transparent. Sphera realizes a socially driven 360-degree media space that includes ambient scenery, visual exploration, and integration with friends. By combining an Oculus Rift (VR heads-on display), Microsoft Kinect (depth sensor), and a natural voice command interface, we created a socially connected, 360-degree immersive virtual world for media exploration and selection, as well as big-data manipulation and visualization.",2015-09-01,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/spherical_system_01.png,21767,--Choose Location,2017-06-25T00:48:42.463Z,"['lip@media.mit.edu', 'amirl@media.mit.edu']",True,sphera,2013-09-01,[],Sphera,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:18.807Z,"Spheres and Splinters is a new work composed by Tod Machover for hypercello, electronics, and responsive visuals commissioned fo Faster than Sound at Aldeburgh Music. The work premiered with celllist Peter Gregson in the UK in 2010 and had its US premiere as part of FAST Festival of Art, Science, and Technology in celebration of MIT's 150th anniversary. Utilizing audio analysis and a multitude of wireless sensors on the cello and the bow that capture how the instrument is being played, the performer has control over transformations and extensions of the sound produced. This control extends into the ambisonic spatialization of sound in the performance space. The performance data is also used to produce realtime visual accompaniment on an array of LED strips surrounding the cellist.",2011-01-01,['opera-of-the-future'],,21775,,2016-12-05T00:16:29.010Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'benb@media.mit.edu']",True,spheres-and-splinters,2010-09-01,[],Spheres and Splinters,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:18.884Z,"The SpiderBot is a suspended robotic gantry system that provides an easily deployable platform from which to print large structures. The body is composed of a deposition nozzle, a reservoir of material, and parallel linear actuators. The robot is connected to stable points high in the environment, such as large trees or buildings. This arrangement is capable of moving large distances without the need for more conventional linear guides, much like a spider does. The system is easy to set up for mobile projects, and will afford sufficient printing resolution and build volume. Expanding foam can be deposited to create a building-scale printed object rapidly. Another material type of interest is the extrusion or spinning of tension elements, like rope or cable. With tension elements, unique structures such as bridges or webs can be wrapped, woven, or strung around environmental features or previously printed materials.",2014-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/Bot_3.png,21783,,2021-05-21T13:09:42.308Z,['neri@media.mit.edu'],True,spiderbot,2012-01-01,[],SpiderBot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:19.036Z,"Spin is a photography turntable system that lets you capture how your DIY projects come together over time. With Spin, you can create GIFs and videos of your projects that you can download and share on Twitter, Facebook, or any other social network.",2016-08-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/radar_no_background.png,21799,--Choose Location,2016-12-05T00:16:18.135Z,"['mres@media.mit.edu', 'ttseng@media.mit.edu']",True,spin,2015-01-01,['makers'],Spin,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:19.111Z,"Control of quantum systems is becoming increasingly relevant in magnetic resonance imaging, quantum computation, and quantum instrumentation. We are investigating the application of techniques from classical control theory to the quantum domain. Demonstrating open loop control via a proof of principle nuclear magnetic resonance experiments, we are working toward broad applications of control techniques to quantum mechanical engineering.",2002-01-01,['physics-and-media'],,21807,E15-023,2016-12-05T00:16:52.600Z,['neilg@media.mit.edu'],True,spingineering,2002-01-01,[],Spingineering,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:19.187Z,Spinner is a Lab-wide sensor network platform designed to detect and capture fragmented events of human behavior that can be collected and sequenced into a cohesive narrative conveying a larger overall meaning. This project also looks at the development of parametric models of narrative that can be mapped on to sensor-detectable elements of human activity.,2010-01-01,['responsive-environments'],,21815,E15-351,2016-12-05T00:16:52.622Z,['joep@media.mit.edu'],True,spinner,2006-09-01,[],Spinner,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:19.343Z,"The quantity and availability of video content is soaring due to the combination of television networks and the Internet. The aim of this project is to develop more effective means to manage, search, and translate video content. We are developing algorithms that interpret language in video (speech and closed caption text) by exploiting aspects of the non-linguistic context, or situation, conveyed by the accompanying video. We model situations by automatically finding patterns within low-level audio/video features that represent events. Event patterns are then mapped to words spoken in the video in order to create a �grounded� dictionary of word meanings. Our research focuses on sports video, in particular, on Major League Baseball games. We are exploring applications in multimedia search and video-based machine translation.",2008-01-01,['social-machines'],,21831,E15-483,2016-12-05T00:16:22.930Z,"['dkroy@media.mit.edu', 'mbf@media.mit.edu']",True,sports-video-search-using-situated-natural-language-processing,2006-09-01,[],Sports Video Search Using Situated Natural Language Processing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:19.761Z,"Exploring your city is a great way to make friends, discover new places, find new interests, and invent yourself. Spotz is an Android app where everyone collectively tags the places they visit and the places in turn tag them back. This means going to a place tagged ""fun"" increases your ""fun"" quotient. If people tag a spot ""geeky"" then going there pushes up your ""geeky"" score. This tag + location data is used to generate a tag-based persona for you that is constantly evolves based on where you go. You can also follow tags you like to find new ""fun"" places or find people with similar tag clouds as your own and experience new places together. In addition, you can create tags for individual items at places that others can track, for example, to find who has the #bestchocolatecake in town!",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/11/22/spotz.png,21855,--Choose Location,2022-06-10T14:01:02.068Z,"['sra@media.mit.edu', 'geek@media.mit.edu']",True,spotz,2011-09-01,[],Spotz,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:19.933Z,"SPRING is a custom-built hardware and software platform for children with neuro-differences. The system automates data acquisition, optimizes learning progressions, and encourages social, cognitive, and motor development in a positive, personalized, child-led play environment. The quantitative data and developmental trajectories captured by this platform enable systematic, mutli-modal, long-term studies of different therapeutic and educational approaches to autism and other developmental disorders, as well as a better understanding of motivation, engagement, and learning for the general population.",2021-08-01,"['ml-learning', 'affective-computing']",https://dam-prod2.media.mit.edu/x/2017/10/16/SPRING_ComplexMatch_star-cookie.png,21863,--Choose Location,2022-05-12T17:24:47.463Z,"['picard@media.mit.edu', 'ktj@media.mit.edu']",True,spring,2016-01-01,"['robotics', 'cognition', 'data', 'human-machine-interaction', 'kids', 'learning-teaching', 'wearable-computing', 'behavioral-science', 'affective-computing', 'autism-research']","SPRING: A Smart Platform for Research, Intervention, and Neurodevelopmental Growth",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:20.710Z,"Joysticks are a natural interface for controlling F-11s. Mice and keyboards suffice for document preparation. But what is the right interface to ""direct"" the actions of an interactive character, for example, a chicken with a mind of its own? In this case, an ordinary plush toy chicken, augmented with wireless sensing technology, may provide a more iconic and natural interface. In this project we are exploring novel interfaces that allow users to direct the behavior of interactive animated characters who have minds of their own. We are particularly interested in exploring how the innate intelligence of the characters can be used to facilitate the interpretation of the user's actions. Drawing on experience in mechanical and electrical engineering, gesture recognition, visual arts, sculpture, and animation, our group designs new toys to match our interactive worlds. In Swamped! we demonstrate the use of a wireless, plush-toy chicken to direct the actions of an animated chicken.",1999-12-30,['synthetic-characters'],,21903,,2016-12-05T00:16:11.524Z,[],True,squish-using-toys-to-direct-interactive-characters,1995-12-31,"['kids', 'archives', 'history']",Squish: Using Toys to Direct Interactive Characters,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:20.981Z,"StackAR explores the augmentation of physical objects within a digital environment by abstracting interfaces from physical to virtual implementations. StackAR is a Lilypad Arduino shield that enables capacitive touch and light base communication with a tablet. When pressed against a screen, the functionality of StackAR extends into the digital environment, allowing the object to become augmented by the underlying display. This creates an augmented breadboard environment where virtual and physical components can be combined and prototyped in a more intuitive manner.",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/StackAR.png,21919,--Choose Location,2016-12-05T00:16:52.665Z,['holtzman@media.mit.edu'],True,stackar,2012-01-01,[],StackAR,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:21.102Z,"Much of the recent interest in virtual worlds has focused on using the immersive properties of virtual worlds to recreate an experience like that of interacting face to face with other participants. This work instead focuses on how we can use the distinctive properties of virtual spaces to create experiences native to virtual worlds. I present two projects that have different perspectives on this concept. The first project, Information Spaces,  demonstrates how visualization of behavior in a 3-D meeting space can augment the meeting process and provide participants new behavioral ways to communicate. The second project, *space, is an abstract, 2-D virtual platform for prototyping and experimenting with virtual world experiences that provides a structure for changing properties of the virtual space to influence people's behavior in that space.",2008-09-01,['sociable-media'],,21927,E15-383,2016-12-05T00:17:04.269Z,['judith@media.mit.edu'],True,starspace-algorithmic-architecture-in-virtual-spaces,2007-01-01,[],starspace: Algorithmic Architecture in Virtual Spaces,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:21.228Z,"The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in ""maker"" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.",2016-08-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/Display/StartMaking!.jpg,21935,--Choose Location,2016-12-05T00:17:01.293Z,"['mres@media.mit.edu', 'chrisg@media.mit.edu', 'nrusk@media.mit.edu', 'jieqi@media.mit.edu', 'ttseng@media.mit.edu', 'mellis@media.mit.edu', 'ria@media.mit.edu']",True,start-making,2014-01-01,"['design', 'kids', 'learning-teaching', 'makers']",Start Making!,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:21.495Z,"Engineering in a limit of thermodynamic complexity, by compiling global optimizations into local dynamics in systems including analog logic, paintable computing, and quantum optics",2007-01-01,['physics-and-media'],,21951,E15-023,2022-06-10T14:12:10.690Z,['neilg@media.mit.edu'],True,statistical-mechanical-engineering,2005-01-01,[],Statistical-Mechanical Engineering,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:21.811Z,"We are developing a very intuitive and interactive platform to make complex information--especially science, technology, engineering, and mathematics (STEM) material--truly accessible to blind and visually impaired students by using a tactile device with no loss of information compared with printed materials. A key goal of this project is to develop tactile information-mapping protocols through which the tactile interface can best convey educational and other graphical materials.",,['fluid-interfaces'],,21967,--Choose Location,2016-12-05T00:16:52.685Z,"['pattie@media.mit.edu', 'namdev@media.mit.edu']",True,stem-accessibility-tool,2014-01-01,[],STEM Accessibility Tool,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:21.974Z,"A steptorial (""step tutorial"") is a new interaction strategy for learning complex topics. Conventional tutorials�such as Khan Academy-style videos, or interactive guided tours�require the student to cede control to the tutorial. Many students prefer to dive right in to try to learn how to do something by exploration, but they are quite likely to get lost or confused if the topic is complex. Steptorials are unique because they allow varying the autonomy of the user at every step. A steptorial has a control structure of a reversible programming language stepper. The user may choose, at any time, to be shown how to do a step, be guided through it, to try to attempt the task by themselves, or to return to a previous step. Steptorials introduce a new paradigm of mixed-initiative learning.",2014-09-01,['software-agents'],https://dam-prod2.media.mit.edu/x/files/Display/Steptorial-Justify-Screen.jpg,21975,--Choose Location,2022-06-10T14:18:57.629Z,"['lieber@media.mit.edu', 'cfry@media.mit.edu']",True,steptorials-a-new-interaction-technique-for-learning-complex-topics,2013-09-01,[],Steptorials: A New Interaction Technique for Learning Complex Topics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:22.372Z,"As virtual environments become more widely adopted for different tasks, it is critical that we develop techniques for designing spaces and interfaces that clearly suggest certain kinds of behavior for users, both in virtual environments and for people who interact with a virtual environment through a mixed-reality boundary. To explore these issues, we have designed and deployed a mixed-reality table soccer game in which teams of players�real and virtual�use two different interfaces to play a soccer-like game together.",2008-09-01,['sociable-media'],,21999,E15-383,2016-12-05T00:16:42.644Z,['judith@media.mit.edu'],True,stiff-peoples-league,2007-09-01,[],Stiff People's League,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:22.513Z,"Today, people can tell stories by composing, manipulating, and sequencing individual media artifacts using digital technologies. However, these tools offer little help in developing a story's plot. Specifically, when a user tries to construct her story based on a collection of individual media elements (videos, audio samples), current technological tools do not provide helpful information about the possible narratives that these pieces can form. Storied Navigation is a novel approach to this problem; media sequences are tagged with free-text annotations and stored as a collection. To tell a story, the user inputs a free-text sentence and the system suggests possible segments for a storied succession. This process iterates progressively, helping the user to explore the domain of possible stories. The system achieves the association between the input and the segments' annotations using reasoning techniques that exploit the WordNet semantic network and common-sense reasoning technology.",2014-09-01,['software-agents'],,22007,E15-383,2022-06-10T14:18:57.654Z,"['lieber@media.mit.edu', 'gid@media.mit.edu', 'barbara@media.mit.edu']",True,storied-navigation,2005-09-01,[],Storied Navigation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:22.633Z,"Designed with attention to the creation and expression of an historical perspective, Storied Objects present us with an object-centric view of their existence over time.  By acknowledging that an object's life can be reflected through storied output of its genealogy, history of social and cultural interactions, utilitarian features of the object, and many circumstances of which we are not aware, we explore methods for appropriate technological augmentation of an object, enabling the capture, reflection, sharing, and revealing of its stories. Through this approach, designers can enrich form and function of an object with narratives based on the historicity of everyday things. We have devised a case study, the Audio Bench, that considers how the capability for recording and expressing history can add aesthetic and cultural value�a �storiedness”—to the object. ",2007-09-01,['media-fabrics'],,22015,E15-368,2016-12-05T00:16:43.520Z,['gid@media.mit.edu'],True,storied-objects,2007-01-01,[],Storied Objects,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:12:22.750Z,Story Mat stores children's storytelling play by recording their voices and the movements of the toys they play with. These stories revive on the mat as other children play and tell their stories.,2001-12-30,['gesture-and-narrative-language'],,22023,,2016-12-05T00:16:36.906Z,[],True,story-mat,1997-12-31,[],Story Mat,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:22.867Z,"Analogy Space, a previous project under the Digital Intuition group, developed a technique of plotting concepts in a many-dimensional semantic space in order to identify clusters of concepts that are similar to each other.  Story Space will apply this technique to human narrative in order to provide a measure of similarity between different stories.  It has had preliminary success using datasets that are easily broken up into discrete events, such as ""how-to"" articles from the Internet.  The next steps involve using automatic event taggers to determine the progression of a story.",2020-02-28,['digital-intuition'],,22031,,2021-07-14T15:32:40.500Z,['havasi@media.mit.edu'],True,story-space,2012-09-01,[],Story Space,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:12:22.979Z,"With Story Trail, people record a piece of story on top of a toy trail piece. As a stuffed toy moves along the trail, it tells stories that were recorded on the trail. Each trail piece can be moved around or re-recorded to change both the physical and metaphoric direction of the story.",2001-12-30,['gesture-and-narrative-language'],,22039,E15-320,2016-12-05T00:16:45.429Z,[],True,story-trail,1999-12-31,[],Story Trail,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:23.078Z,"StoryBeads are an interactive wearable for constructing narratives through collecting, trading and sequencing images. Individual beads, labeled with metadata descriptions, act as containers for collections of images and related text. As images travel between users, the beads enable a transaction, which preserves the historical context, a small step toward memory of use.",2001-12-30,"['broadercasting', 'toys-of-tomorrow', 'interactive-cinema']",,22047,E15-427,2016-12-05T00:16:52.753Z,"['gid@media.mit.edu', 'barbara@media.mit.edu']",True,storybeads,1998-12-31,[],StoryBeads,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:23.186Z,"Giving opaque technology a glass house, Storyboards present the tinkerers or owners of electronic devices with stories of how their devices work. Just as the circuit board is a story of star-crossed lovers—Anode and Cathode—with its cast of characters (resistor, capacitor, transistor), Storyboards have their own characters driving a parallel visual narrative.",2017-06-01,"['social-computing', 'playful-systems']",https://dam-prod2.media.mit.edu/x/2016/12/14/storyboard_01_g2ollfv.png,22055,--Choose Location,2022-06-10T14:17:21.109Z,"['jbobrow@media.mit.edu', 'shantell@media.mit.edu', 'saquib@media.mit.edu']",True,storyboards,2014-09-01,[],Storyboards,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:23.295Z,"Exploring conductive inks as an expressive medium for narrative storytelling, StoryClip synthesizes electrical functionality, aesthetics, and creativity, to turn a drawing into a multimedia interface that promotes rich engagement with children.",2014-01-01,['high-low-tech'],,22063,,2022-06-10T13:57:55.528Z,['leah@media.mit.edu'],True,storyclip,2012-01-01,[],StoryClip,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:23.395Z,"StoryNet is a large database of story-scripts and inference tools that can be used for case-based common-sense reasoning. Since people are by nature experts at using story structures to organize and manage the complexity of life, we have turned to the general public to gather over a million story-scripts using various Web-based, story-building interfaces. As our acquisition systems are released and tested, we anticipate that our representations will be revised and enhanced as we learn from the human storytelling impulse and the success and limitations of the inference mechanisms we develop. StoryNet is one of a suite of common-sense tools at the Media Lab, including ConceptNet and LifeNet.",2006-01-01,['media-fabrics'],,22071,E15-368,2016-12-05T00:16:29.597Z,"['gid@media.mit.edu', 'barbara@media.mit.edu', 'minsky@media.mit.edu']",True,storynet,2003-01-01,[],StoryNet,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:23.496Z,"Stories, language, and art are at the heart StoryScape. While StoryScape began as a tool to meet the challenging language learning needs of children diagnosed with autism, it has become much more. StoryScape was created to be the first truly open and customizable platform for creating animated, interactive storybooks that can interact with the physical world. Download the android app and make your own amazing stories at https://storyscape.io.",2014-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/2017/09/19/field.jpg,22079,--Choose Location,2021-10-07T16:49:57.989Z,"['picard@media.mit.edu', 'micahrye@media.mit.edu', 'fergusoc@media.mit.edu']",True,storyscape,2012-01-01,[],StoryScape,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:23.578Z,"Storytelling Engines are computer programs that manipulate media objects based on their knowledge of content, rules for story progression, and potential for audience interaction. They can be designed to configure a presentation on-the-fly (selecting one material at a time), or they can be designed to select, assemble, and present a composite stream of scenes. Example engines include: ConTour, a system that uses the principle of a spreading activation network combined with positive-to-negative keyword weightings to locate the next relevant content segment; and Agent Stories, a system that assembles a story presentation based on the preferred characteristics of a storytelling agent.",2003-01-01,"['gray-matters', 'interactive-cinema']",,22087,E15-489,2016-12-05T00:16:52.923Z,['gid@media.mit.edu'],True,storytelling-engines,1986-01-01,[],Storytelling Engines,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:23.662Z,"Using the Preschool of the Future environment, children can create stories that come to life in the real world. We are developing interfaces that enable children to author stories in the physical environment–stories where robots are the characters and children are not only the observers, but also the choreographers and actors in the stories. To do this, children author stories and robot behaviors using a simple digital painting interface. By combining the physical affordances of painting with digital media and robotic characters, stories can come to life in the real world. Programming in this environment becomes a group activity when multiple children use these tangible interfaces to program advanced robot behaviors.",2012-09-01,['personal-robots'],,22095,--Choose Location,2018-03-29T17:09:20.173Z,['cynthiab@media.mit.edu'],True,storytelling-in-the-preschool-of-the-future,2010-09-01,[],Storytelling in the Preschool of the Future,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:23.746Z,"The Strata project explores the design of layered, electronically-augmented physical models that serve as tangible interfaces to specific dataspaces.  Strata's first application is an interface that embodies the physical structure of the Media Lab's home building.  This takes the form of a five-layer, translucent acrylic model woven with embedded lights, sensors, and computation.  This tangible interface will allow the building's networking and facilities staff to access, monitor, and control a dynamic representation of the building's physical and digital infrastructure.",2000-12-30,"['personal-fabrication', 'tangible-media']",,22103,E15-441,2016-12-05T00:16:52.843Z,"['ishii@media.mit.edu', 'jpatten@media.mit.edu']",True,strata,1998-12-31,[],Strata,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:23.834Z,"Strata/ICC is a computationally-augmented, two-meter-tall physical model of a 54-story skyscraper, serving as an interactive display of electricity consumption, water consumption, network utilization, and other kinds of building infrastructure.  Our approach pushes information visualizations into the physical world, with a vision of transforming large-scale physical models into new kinds of interaction workspaces.",2000-12-30,"['personal-fabrication', 'tangible-media']",,22111,E15-441,2016-12-05T00:16:48.345Z,['ishii@media.mit.edu'],True,strataicc,1999-12-31,[],Strata/ICC,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:23.930Z,"We are collecting ever more data about our environment, from above using high-end satellite networks, as well as from below with the cheap sensors built into our cellphones. This coverage has the potential to drastically change our perception of the fragile world we live in, and to make sense of such data we commonly use visualization techniques, enabling public discourse and analysis. However, when it comes to understanding complex Earth systems and causality chains�for instance the dramatic events leading up to the 2011 environmental disaster in Japan�can we find more engaging ways of interacting with geospatial data that go beyond static visualization? Stratarium is an interaction space that integrates various interface techniques to allow users to explore a layered model of our Earth in the form of part-physical, part-digital representations, and to understand the relationships between the planet�s surface shape and sensor data resulting from natural phenomena.",2013-01-01,['tangible-media'],,22119,--Choose Location,2016-12-05T00:16:52.955Z,['ishii@media.mit.edu'],True,stratarium,2013-01-01,[],Stratarium,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:24.168Z,"StreetScore is a machine learning algorithm that predicts the perceived safety of a streetscape. StreetScore was trained using 2,920 images of streetscapes from New York and Boston and their rankings for perceived safety obtained from a crowdsourced survey. To predict an image's score, StreetScore decomposes this image into features and assigns the image a score based on the associations between features and scores learned from the training dataset. We use StreetScore to create a collection of map visualizations of perceived safety of street views from cities in the United States. StreetScore allows us to scale up the evaluation of streetscapes by several orders of magnitude when compared to a crowdsourced survey. StreetScore can empower research groups working on connecting urban perception with social and economic outcomes by providing high-resolution data on urban perception.",2018-06-01,"['camera-culture', 'collective-learning']",https://dam-prod2.media.mit.edu/x/2016/10/21/streetscore_dynamic_15s_NfHNuHC.gif,22143,--Choose Location,2020-08-24T18:10:25.922Z,"['raskar@media.mit.edu', 'hidalgo@media.mit.edu', 'naik@media.mit.edu']",True,streetscore,2014-01-01,[],StreetScore,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:24.257Z,"Stress OutSourced (SOS) is a peer-to-peer network that allows anonymous users to send each other therapeutic massages to relieve stress. By applying the emerging concept of crowdsourcing to haptic therapy, SOS brings physical and affective dimensions to our already networked lifestyle while preserving the privacy of its members. SOS is an exploration and illustration of a new field of haptic social networking.",2009-01-01,['tangible-media'],,22151,E15-344,2016-12-05T00:16:52.985Z,"['ishii@media.mit.edu', 'x_x@media.mit.edu']",True,stress-outsourced,2008-09-01,[],Stress OutSourced,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:24.342Z,"A musical piece typically has a repetitive structure. Analysis of this structure will be useful for music segmentation, indexing, and thumbnailing. This project attempts to analyze automatically the repetitive structure of musical signals. Strategies for obtaining thumbnails of music based on the structual analysis results are also proposed.",2005-01-01,['music-mind-and-machine'],,22159,E15-484,2016-12-05T00:16:11.604Z,['bv@media.mit.edu'],True,structural-analysis-of-musical-signals-for-indexing-and-thumbnailing,2003-01-01,[],Structural Analysis of Musical Signals for Indexing and Thumbnailing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:24.424Z,"New, streaming, sound-encoding technologies are starting to fill the demand for low-bandwidth transmission of audio over the Internet, but such information-theoretic encoding schemes often don't allow for high-quality audio transmission or client-side sound manipulation. We are developing NetSound, a structured audio coding/decoding scheme (sort of a ""Postscript for audio"") that allows sound descriptors, algorithms, models, and schedulers to be sent efficiently over the Internet, and then altered, personalized, and reconstructed in real time at the receiver site with high fidelity. A set of tools based on this concept has been proposed by the Music, Mind and Machine group and accepted by the MPEG Consortium as part of the MPEG-4 international standard. We continue to develop and refine Structured Audio techniques in collaboration with researchers and industrial labs around the world.",2005-01-01,['music-mind-and-machine'],,22167,E15-401,2016-12-05T00:16:53.019Z,['bv@media.mit.edu'],True,structured-audio,1996-01-01,[],Structured Audio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:24.512Z,"Should students be prosecuted for innovative projects? In December 2014, four undergraduates associated with the Media Lab were subpoenaed by the New Jersey Attorney General after winning a programming competition with a bitcoin-related proof of concept. We worked with MIT administration and the Electronic Frontier Foundation to support the students and establish legal support for informal innovation. In September 2015, MIT announced the creation of a new clinic for business and cyberlaw.",2016-10-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/CodersKnowYourRights.png,22175,--Choose Location,2016-12-05T00:17:22.864Z,"['ethanz@media.mit.edu', 'jnmatias@media.mit.edu']",True,student-legal-services-for-innovation,2014-01-01,[],Student Legal Services for Innovation,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:24.593Z,"What is the relationship between the input children hear and the words they acquire? We are investigating the role of variables such as input word frequency and prosody in one child's lexical acquisition using the Human Speechome Project corpus. We are analyzing data from ages nine to 24 months, including the child's first productive use of language at about 11 months, ending at the child�s active use of a vocabulary with more than 500 words.",2010-09-01,['social-machines'],,22183,E15-483,2016-12-05T00:17:22.945Z,"['dkroy@media.mit.edu', 'soroush@media.mit.edu']",True,study-of-child-language-acquisition-in-the-human-speechome-project,2009-01-01,[],Study of Child Language Acquisition in the Human Speechome Project ,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:24.671Z,"How will gene drive systems evolve once released into the wild? Can they be reliably overwritten and blocked by immunizing reversal drives? Might they spread into related species? These are difficult questions because wild populations are so much larger than laboratory colonies, meaning critical evolutionary events would never be observed in the lab. We seek to develop nematode worms as a model system to help answer these questions. Nematodes are genetically tractable, reproduce twice each week, and are readily grown in populations numbering in the billions. This allows us to study drive systems intended for other organisms in nematodes. Synthetic site targeting, split drives, and ecological confinement will prevent spread into wild nematodes. Because nematodes are easy to culture and count using Foldscope microscopes, we intend to work with educators to enable students, museum-goers, and citizen scientists to participate in gene drive research.",,['sculpting-evolution'],,22191,--Choose Location,2019-04-17T18:56:53.592Z,"['esvelt@media.mit.edu', 'jmin01@media.mit.edu', 'codyg@media.mit.edu']",True,studying-the-evolution-of-gene-drive-systems,2016-01-01,"['bioengineering', 'genetics', 'biology']",Studying the evolution of gene drive systems,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:24.753Z,"Recent research in 3D user interfaces has pushed in two directions: immersive graphics and actuated tangible shape displays. We seek their hybrid by thinking about physical material density as a parameter in 3D rendering. We want to explore how digital models, handles, and controls can be rendered either as virtual 3D graphics or dynamic physical shapes, and move fluidly and quickly between these states, allowing physical affordances to be rendered only when needed. We were inspired by the different states of water: solid, gas, and liquid. We view digital computation and models as liquid, which can be vaporized into mid-air graphics, or solidified into dynamic physical shape. We also investigate transitions between solid and gas: sublimation and vaporization. To explore this, we have implemented a system which combines an actuated shape display and a spatial augmented reality display. This system can render physical shapes and volumetric graphics, co-located in the same space. We explore interaction techniques and motivating demonstration applications to explore 3D interaction between these boundaries. We also present results of a user study showing that freehand interaction with a physical shape display with co-located graphics outperforms direct interaction with only 3D graphics through a wand.",2013-09-01,['tangible-media'],,22199,--Choose Location,2016-12-05T00:17:22.885Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu', 'olwal@media.mit.edu']",True,sublimate,2012-01-01,[],Sublimate,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:24.856Z,"subTextile is a toolkit for behavioral textiles: the intersection of on-body computation and electronic textiles focusing on the interactive capability of electronic textiles. It provides a powerful visual programming language and hardware platform specifically designed to create complete behavioral textile systems. Using a rich, goal-oriented interface, subTextile makes it possible for technical novices to explore electronic textiles, while providing open-ended expandability to experts. As e-textiles mature, better tools and techniques are needed by artists and designers experimenting with these new materials. The subTextile project was created to support cross-pollination between technical and design disciplines in the hopes of fostering greater creativity and depth within the field.",2009-09-01,['fluid-interfaces'],,22207,E15-320,2016-12-05T00:16:29.182Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu']",True,subtextile-a-construction-kit-for-computationally-enabled-textiles,2006-09-01,[],subTextile: A Construction Kit for Computationally Enabled Textiles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:24.937Z,"Super Cilia Skin is a multi-modal interactive interface, conceived as a computationally enhanced membrane coupling tactile-kinesthetic input with tactile and visual output. An array of individual actuators (cilia) uses changes in orientation to display images or physical gestures as physical or tactile information.",2004-01-01,['tangible-media'],,22215,E15-344,2016-12-05T00:16:53.048Z,['ishii@media.mit.edu'],True,super-cilia-skin,2002-09-01,[],Super Cilia Skin,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:25.022Z,"A large portion of popular media is remixed: existing media content is spliced and re-ordered in a manner that serves a specific narrative. Super Cut Notes is a semi-comical content remix tool that allows a user to splice and combine the smallest bits of media: words. By tapping into the dataset of our group's SuperGlue platform, it has access to a huge dictionary of words created by SuperGlue's transcription module. Users are able to input a text of any length, choose video-bits of individual words that match their text, and create a video of their combination—in the style of cut-and-pasted ransom notes.",2017-06-06,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/Screen Shot 2016-03-10 at 10.41.32 AM.png,22223,--Choose Location,2018-10-09T01:52:24.513Z,"['weller@media.mit.edu', 'lip@media.mit.edu']",True,super-cut-notes,2016-09-01,[],Super Cut Notes,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:25.106Z,"When synthesizing ceramic powders for use in high-temperature superconductors, the bulk fraction of the synthesized powder that is actually superconductive is often low. In the specific case of YBaCuO 1-2-3 synthesis, the oxygen content of the sintered material is delicate (often destroyed by moisture) and critical to the observation of superconductivity above 77K (N2 boiling point). An apparatus is proposed that will preferentially filter out superconductive particles from non-superconductive particles from a finely ground powder (~100 um).  Filtered, superconductive material will then be sintered together (or drawn into a copper/brass carrying wire as is common with BSCCO) to yield a ceramic with higher bulk fraction superconductivity.  This apparatus would allow inexpensive superconductors to be fabricated with loose tolerances/purities on starter chemicals and firing apparatus.",2012-09-01,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/SPPD.jpg,22231,,2016-12-05T00:17:22.902Z,['neri@media.mit.edu'],True,superconductive-powder-purification-device,2012-01-01,[],Superconductive Powder Purification Device,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:25.182Z,"SuperGlue is a core news research initiative that is a ""digestion system"" and metadata generator for mass media. An evolving set of analysis modules annotate 14 DirecTV live news broadcast channels as well as web pages and tweets. The video is archived and synchronized with the analysis. Currently, the system provides named-entity extraction, audio expression markers, face detectors, scene/edit point locators, excitement trackers, and thumbnail summarization. We use this to organize material for presentation, analysis, and summarization. SuperGlue supports other news-related experiments.SuperGlue is a framework for media digestion and metadata generation. The digestion work flow also has applications for media more broadly including conversational ecommerce.",,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/Glue-8-3.png,22239,--Choose Location,2019-10-09T16:04:22.887Z,"['weller@media.mit.edu', 'lip@media.mit.edu', 'anderton@media.mit.edu', 'tejalw@media.mit.edu', 'mhjiang@media.mit.edu']",True,superglue,2014-09-01,[],SuperGlue,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:25.266Z,"Our smartphones take active attention while we use them to navigate streets, find restaurants, meet friends, and remind us of tasks. SuperShoes allows us to access this information in a physical, ambient form through a foot interface. SuperShoes takes us to our destination; senses interesting people, places, and events in our proximity; and notifies us about tasks, all while we immerse ourselves in the environment. We explore a physical language of interaction afforded by the foot through various tactile senses. By weaving digital bits into the shoes, SuperShoes liberates information from the confines of screens and onto the body.",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/supershoes.jpg,22247,--Choose Location,2016-12-05T00:16:29.225Z,['holtzman@media.mit.edu'],True,supershoes,2012-09-01,[],SuperShoes,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:25.569Z,"Adding augmented reality to the living-room TV, we are exploring the technical and creative implications of using a mobile phone or tablet (and possibly also dedicated devices like toys) as a controllable ""second screen"" for enhancing television viewing. Thus, a viewer could use the phone to look beyond the edges of the television to see the audience for a studio-based program, to pan around a sporting event, to take snapshots for a scavenger hunt, or to simulate binoculars to zoom in on a part of the scene. Recent developments include the creation of a mobile device app for Apple products and user studies involving several genres of broadcast television programming.",2015-06-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/files/Display/svsquare.jpg,22279,Garden Conference Room,2020-06-10T16:32:58.476Z,"['vmb@media.mit.edu', 'talfaro@media.mit.edu']",True,surroundvision,2009-09-01,[],SurroundVision,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:25.645Z,"Swamped! is an interactive experience in which instrumented plush toys are  used as a tangible, iconic interface for directing autonomous animated  characters. Each character has a distinct personality and decides in real  time what it should do based on its perception of its environment, its  motivational and emotional state, and input from its ""conscience,"" the  guest. By manipulating a stuffed animal corresponding to the character the  guest can influence how a given character acts and feels.    For example, the guest could have the character wave at another character  by waving its arm, make it squawk by squeezing it, or direct the  character's attention by moving the stuffed animal's head. The characters  incorporate a novel model of behavior and emotion, multi-target motion  interpolation and new techniques for real time graphics. Automatic camera  and lighting control helps reveal the emotional content of each scene.    By combining research in autonomous character design, automatic camera  control, sympathetic interfaces and action interpretation, Swamped! seeks  to create an evocative and novel experience.
",1998-12-30,['synthetic-characters'],,22287,,2016-12-05T00:16:53.081Z,[],True,swamped,1997-12-31,[],SWAMPED!,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:26.072Z,"Over two million speech-disabled people in the US alone rely on Augmentative and Alternative Communication (AAC) devices to help express their everyday thoughts, needs, and ideas. Some of these devices now support extensible vocabularies; however, the means of extension are too cumbersome to permit new symbols to be defined on demand, for immediate use in an utterance. On the time scale of a conversation, this shortcoming limits AAC users to a relatively small, fixed vocabulary. Symbolsnare is a gaze-speech interface enabling an AAC user to instantly extend the vocabulary of his or her device. New symbols are captured from the environment and imported into the user's AAC using an eye-tracking digital capture system triggered by button presses or simple vocalisations. By opening the vocabulary of AAC devices to the world, Symbolsnare empowers the speech-disabled to discuss anything they can see.",2004-01-01,['social-machines'],,22327,E15-4FL,2016-12-05T00:16:53.200Z,['dkroy@media.mit.edu'],True,symbolsnare,2004-01-01,[],Symbolsnare,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:26.258Z,"This work explores a technological answer to the perennial question ""What's for dinner?"" Few of us know with great certainty the exact food we crave; instead, we stew on the question and explore the nature of cravings with imaginative descriptions: ""I feel like something light, fresh, sophisticated, not too mushy; something influenced by Thai or Indian, something aromatic."" Synaesthetic Recipes is a visual search program that allows imaginative textual descriptions, using them to drive recipe recommendations. A database of 100,000 recipes is automatically annotated with common sense about food. An artificial intelligence robotic reader reads each recipe, and based on flavors of the ingredients and cooking procedures predicts how a food will look, taste, and smell. We are translating recipes into the rich descriptive vernacular of how people naturally conceptualize their cravings for food.",2006-09-01,['fluid-interfaces'],,22343,E15-320,2018-12-04T15:32:05.948Z,['pattie@media.mit.edu'],True,synaesthetic-recipes,2005-01-01,[],Synaesthetic Recipes,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:26.344Z,"Syncwalk is a platform for sound design in a geographic space: you place songs from your music library onto a map, and then you go out into the world and, as you walk from one place to another, you hear the songs you�ve placed along the way. The project explores different ways to design and share experiences centered around, but not wholly determined by, the spaces we transit and inhabit.",2010-01-01,['opera-of-the-future'],,22351,,2016-12-05T00:16:53.224Z,['tod@media.mit.edu'],True,syncwalk,2009-09-01,[],Syncwalk,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:26.427Z,"How can we encourage people to express themselves in new ways?  And how do people deal with objects that possess (presumed) agency? These two seemingly unrelated questions are brought together in the design and study of syngva, a robotic creature that moves in response to human non-speech vocal sounds. On the one hand, syngva is a creature that, through its movement, encourages the production of sounds that are otherwise silenced, bringing out what was unspeakable. On the other hand, syngva learns about these sounds over time, changing its motions and therefore prodding people to try different things. Thus, syngva serves as an anthropological probe, enabling us to better understand people's interactions with robotic creatures, while at the same time providing a potential space for unconventional experiences.",2007-09-01,['opera-of-the-future'],,22359,E15-445,2016-12-05T00:16:29.720Z,['tod@media.mit.edu'],True,syngva-encouraging-the-expression-of-the-unspeakable,2006-09-01,[],syngva: Encouraging the Expression of the Unspeakable,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:26.507Z,Carbon nanotubes consist of a graphite sheet of carbon atoms formed into tubes 1-10 nm in diameter. We are developing methods for synthesizing and manipulating the tubes and working to create novel electronic and electromechanical devices with them.,2003-09-01,"['personal-fabrication', 'nanoscale-sensing']",,22367,E15-015,2016-12-05T00:16:38.776Z,['scottm@media.mit.edu'],True,synthesis-and-manipulation-of-carbon-nanotubes,2000-01-01,[],Synthesis and Manipulation of Carbon Nanotubes,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:26.590Z,"The Synthetic Apiary proposes a new kind of environment, bridging urban and organismic scales by exploring one of the most important organisms for both the human species and our planet: bees. We explore the cohabitation of humans and other species through the creation of a controlled atmosphere and associated behavioral paradigms. The project facilitates Mediated Matter's ongoing research into biologically augmented digital fabrication with eusocial insect communities in architectural, and possibly urban, scales. Many animal communities in nature present collective behaviors known as ""swarming,"" prioritizing group survival over individuals, and constantly working to achieve a common goal. Often, swarms of organisms are skilled builders; for example, ants can create extremely complex networks by tunneling, and wasps can generate intricate paper nests with materials sourced from local areas.",2018-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2017/10/12/Mediated Matter Synthetic Apiary 01_l7FCDes.jpg,22375,--Choose Location,2022-03-11T15:59:02.935Z,"['ssunanda@media.mit.edu', 'm_kayser@media.mit.edu', 'j_duro@media.mit.edu', 'neri@media.mit.edu']",True,synthetic-apiary,2016-01-01,"['bioengineering', 'construction', 'environment', 'systems', 'urban-planning']",Synthetic Apiary,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:12:26.675Z,"This is a collection of smaller projects currently being pursued by the Synthetic Characters group. (1) C4.3 is the latest iteration of our toolkit for building adaptive and expressive synthetic characters. All of our projects are done using this Java- and C++-based toolkit, and the toolkit evolves as the lessons from the projects are incorporated into it. The system runs on Mac OS X and Windows 2000. (2) New Languages from Old are extensions to the Java programming language and the Java runtime custom-made for the problems faced by behavior designers. (3) Sequence, Sequences and More Sequences is a unification and diversification of our current sequence-learning strategies for new applications to behavior creation and motion understanding. (4) Stochastic Convolution Rendering creates new, painterly, expressive, and emphatically non-photorealistic graphics within our C4/magic graphics system. (5) Experiments in Intelligent Form are early sketches in force-field-based motor representations for synthetic characters, found far away from engineering concerns for optimality or the aesthetic concerns of 3-D computer graphics. (6) Expressive Inverse Kinematics: while the theory of inverse kinematics is well-understood, its application in real-time settings for synthetic characters that must always ""move in character"" remains problematic. In this project we are investigating new representations and approaches to inverse kinematics that allow the algorithm to take advantage of previously sampled motion. The hope is that by doing so, the algorithm can infer joint constraints, as well as the quality of movement that defines the character.",2004-01-01,['synthetic-characters'],,22383,E15-441,2016-12-05T00:16:53.249Z,[],True,synthetic-characters-skunk-works,2002-01-01,[],Synthetic Characters Skunk Works,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:26.836Z,We are using nanowires to build structures for synthetic photosynthesis for the solar generation of liquid fuels.,2013-01-01,['molecular-machines'],https://dam-prod2.media.mit.edu/x/files/Display/Synthetic%20Photosynthesis%20-%20schematic.jpg,22399,,2016-12-05T00:17:04.139Z,['jacobson@media.mit.edu'],True,synthetic-photosynthesis,2012-01-01,[],Synthetic Photosynthesis,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:26.916Z,"We are working on a new physical interactive system that makes it easier for kids to explore the behavior of dynamic systems. System Blocks, a set of computationally enhanced children's blocks made of wood and electronics, enables children to create and interact with systems that simulate real-life dynamic behavior, such as population growth or the delicate equilibrium of an ecosystem.",2004-09-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/system_blocks.jpg,22407,E15-120B,2016-12-05T00:16:53.328Z,['mres@media.mit.edu'],True,system-blocks,2002-01-01,[],System Blocks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:27.077Z,"We are working on Systems Thinking Blocks, a new modeling language for dynamic interconnected systems that can be simulated using digital computation. The language is implemented as a a Digital Manipulative, a playful set of wooden blocks with embedded digital computation, enabling children to experiment with different configurations of light patterns and in the process explore the generic structures of dynamic interconnected systems. ",2007-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/FB_logo.jpg,22423,E15-120B,2016-12-05T00:16:29.497Z,['mres@media.mit.edu'],True,systems-thinking-blocks,2005-09-01,[],Systems Thinking Blocks,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:27.156Z,"T(ether) is a novel spatially aware display that supports intuitive interaction with volumetric data. The display acts as a window affording users a perspective view of three-dimensional data through tracking of head position and orientation. T(ether) creates a 1:1 mapping between real and virtual coordinate space, allowing immersive exploration of the joint domain. Our system creates a shared workspace in which co-located or remote users can collaborate in both the real and virtual worlds. The system allows input through capacitive touch on the display with a motion-tracked glove. When placed behind the display, the user's hand extends into the virtual world, enabling the user to interact with objects directly.",2012-09-01,"['tangible-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/2020/06/09/7b0de68eabb195341021e450c41b961e.jpg,22431,--Choose Location,2020-06-12T17:58:52.512Z,"['ishii@media.mit.edu', 'dlakatos@media.mit.edu']",True,tether,2011-01-01,[],T(ether),PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:27.240Z,"T+1 is an application that creates an iterative structure  to help groups organize their interests and schedules.  Users receive instructions and send their personal information through mobile devices at discretized time steps, orchestrated by a unique, adaptive scheduling engine. At each time-step, T+1 takes several relevant factors of human interactions (such as participants' interests, opinions, locations, and partner matching schedules), and then computes and optimizes the structure and format of a group interactions for the next interval. T+1 facilitates consensus formation, better group dynamics, and more engaging user experiences by using a clearly visible and comprehensible process. We are planning to deploy the platform for both academic and political discussions, and will analyze how user opinions and interests evolve in time to understand its efficacy.",2012-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/t1_200x200.png,22439,--Choose Location,2016-12-05T00:16:53.397Z,"['borovoy@media.mit.edu', 'lip@media.mit.edu']",True,t1,2011-01-01,[],T+1,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:27.321Z,"The Toy Interface Construction Kit Learning Environment (T.I.C.K.L.E.) is a universal construction kit for the rest of us. It doesn't require 3D printers or CAD skills. Instead, it's a DIY social process for creating construction interoperability.",2012-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/TICKLE.png,22447,--Choose Location,2016-12-05T00:16:53.467Z,"['ethanz@media.mit.edu', 'ericr@media.mit.edu', 'rahulb@media.mit.edu', 'jnmatias@media.mit.edu']",True,tickle,2012-09-01,[],T.I.C.K.L.E.,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:27.400Z,"A low-cost UV-Vis spectrometer is being constructed for widespread use in developing nations. Consumer digital cameras have driven down the price of digital imaging chips; the spectrometer uses digital imaging to obtain a spectrum. This image is then sent to a computer and processed to produce an absorbance spectrum. Currently, the spectrometer distinguishes between colors and varying concentrations of test material and produces spectral data consistent with a commercial UV-Vis spectrometer. When finished, the spectrometer will be capable of food analysis and chemical and biochemical experiments. Recent and future work includes the development of IR spectrometers, FT-IR spectrometers, Raman spectrometers, and mass spectrometers.",2004-01-01,['physics-and-media'],,22455,E15-023,2016-12-05T00:16:53.507Z,['neilg@media.mit.edu'],True,table-top-chemistry,2002-01-01,[],Table-Top Chemistry,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:27.481Z,"Traditional NMR spectrometers are powerful and versatile, but cost hundreds of thousands of dollars and are complicated to operate. We are working on designing a table-top NMR system based on small permanent magnets and specialized electronics with a target price of around a thousand dollars. The original motivation for this was table-top quantum computing, but simple inexpensive NMR spectrometers would be broadly useful in medicine and other fields.",2003-12-30,"['personal-fabrication', 'silicon-biology', 'physics-and-media']",,22463,E15-023,2016-12-05T00:16:53.532Z,['neilg@media.mit.edu'],True,table-top-nmr,2000-12-31,[],Table-Top NMR,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:27.564Z,"Remember when we made a connection by handing someone a photo? Now we fiddle with too many cables, menus, and communication channels, and those individual connections get drowned out. Can we return to physical experiences while retaining the collective intelligence of the network? Tableau is a side table that stores and retrieves memories. It may put friends' photo postcards in the drawer, or post mementos to your online scrapbook. This is an example of task-centric computing, where the interface is distributed across connected physical objects. Apps that run in the cloud can weave available objects into environmental I/O, giving users computing experiences that fit into the flow of life.",2011-01-01,['information-ecology'],,22471,,2016-12-05T00:16:38.828Z,"['holtzman@media.mit.edu', 'jkestner@media.mit.edu']",True,tableau,2010-01-01,[],Tableau,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:27.953Z,"Singing on-pitch typically requires the ability to hear oneself and make fine motor corrections to the vocal cords. This hearing-adjusting feedback  loop is fine when a person can hear small pitch variations. But what about those who have trouble hearing when they are ""off""? We are investigating how the sense of touch can be used to improve a person's ability to sing on-pitch or follow a rhythm. Our system does active pitch-tracking of a singer, and gives immediate vibrational feedback indicating when he or she is off, and by how much. We hope that this system can be used as a training device, to improve singing performance, or to train new singing behaviors like harmonizing. Future work will also investigate how vibrotactile feedback can help timing-related musical performance, such as singing in rounds and drumming.",2003-01-01,['context-aware-computing'],,22511,E15-320,2016-12-05T00:16:40.338Z,[],True,tactile-feedback-for-singing-and-music-performance,2003-01-01,[],Tactile Feedback for Singing and Music Performance,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:28.110Z,"taggyMedia is an online interactive system that provides easier and more interesting ways of media search and exploration. By constucting a semantic network using a social tagging scheme, we are able to design a system that provides better tagging interface, and a variety of ways of exploration based on concepts, emotions, scenarios, etc. The system improves the current ""keyword-matching"" searching scheme to a much more intuitive and explorative one.",2006-09-01,['fluid-interfaces'],,22527,E15-320,2016-12-05T00:16:53.625Z,['pattie@media.mit.edu'],True,taggymedia,2007-01-01,[],taggyMedia,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:28.186Z,"TagMe is an end-user toolkit for easy creation of responsive objects and environments. It consists of a wearable device that recognizes the object or surface the user is touching. The user can make everyday objects come to life through the use of RFID tag stickers, which are read by an RFID bracelet whenever the user touches the object. We present a novel approach to create simple and customizable rules based on emotional attachment to objects and social interactions of people. Using this simple technology, the user can extend their application interfaces to include physical objects and surfaces into their personal environment, allowing people to communicate through everyday objects in very low-effort ways.",2018-01-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/TagMe_View.png,22535,--Choose Location,2018-10-11T18:48:35.308Z,"['pattie@media.mit.edu', 'amores@media.mit.edu', 'xavib@media.mit.edu', 'rboldu@media.mit.edu']",True,tagme,2014-09-01,[],TagMe,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:28.264Z,"In online environments, we often rely on some combination of pictures and pseudonyms to show others what kind of person we are. In more anonymous contexts, we tend to rely on non-representational images while on services like Facebook, identifiable pictures are the norm. In this project, we seek to turn the iconic profile picture into something more active and dynamic. With tailor, you can easily apply dynamic graphical effects and data-driven information to an existing profile picture on Twitter. Tailor will automatically update your profile picture based on these overlays. These overlays might include your current location, number of followers, or visualizations of your favorite team's recent record. Tailor will also support scenarios like automatically updating your picture when you upload pictures to Flickr, visually aging pictures the longer you have them, morphing between different pictures, or programmatically generating interesting abstract profile pictures.",2011-01-01,['living-mobile'],,22543,,2016-12-05T00:16:38.848Z,['geek@media.mit.edu'],True,tailor,2011-01-01,[],tailor,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:28.420Z,"Takeover TV heralds a new era of bar patronage where you and your like-minded friends are in charge of the screens. When you check in at a location, your likes and dislikes automatically influence what is being shown on local displays.  If you want more control, start a vote to pick a new show using your beer glass�or your iPhone. Create season-premiere nights for your favorite shows, or work with friends to define the types of shows that play at your local bars. Sick of watching sports?  Assemble enough fans of your favorite show at the local pub and take over the TV.",2012-01-01,['information-ecology'],,22559,,2016-12-05T00:16:53.721Z,['holtzman@media.mit.edu'],True,takeover-tv,2010-01-01,[],Takeover TV,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:28.578Z,"TalkBack is an answering machine with a little more smarts. It allows you to respond to your messages naturally as you get them. Next time you have a long list of messages on your answering machine, you do not have to remember all of them and respond to each one separately. Instead, you can record your responses as they come and send your responses back to the person who left you the message.",2003-01-01,['living-mobile'],,22575,E15-344,2016-12-05T00:16:53.754Z,['geek@media.mit.edu'],True,talkback,2001-01-01,[],TalkBack,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:28.655Z,"Talking In Circles is an audioconferencing chat environment. Unlike traditional text-based chat environments, Talking In Circles allows participants simply to speak to each other. Participants are represented as colored circles which accompany their speech with real-time graphical feedback.  In order to allow for 'mingling' and subgroup conversations, audio volume is regulated according to the the circles' location within the system, so that faraway speech is softer. Participants can also draw in their circles, permitting pictorial and gestural communication in addition to speech. The system also contains ""audio booths"" which play music and news to supplement conversation.",2000-12-30,['sociable-media'],,22583,,2016-12-05T00:16:11.735Z,['judith@media.mit.edu'],True,talking-in-circles,1998-12-31,[],Talking In Circles,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:28.730Z,"The talking trivet and oven mitt are digital enhancements of common household objects. By communicating important messages about a food's temperature, they make far better tools than a simple thermometer. The trivet uses an embedded computer to read time and temperature. It tells a user whether food needs re-warming (is under 90 degrees F); is hot and ready to take out of the oven and eat, but not overcooked; or is so hot that it will catch on fire (above 454 degrees F). This high-temperature alert provides an added safety feature for the home.",2002-09-01,"['counter-intelligence', 'context-aware-computing']",,22591,E15-320,2016-12-05T00:16:53.663Z,[],True,talking-trivetoven-mitt,2001-01-01,[],Talking Trivet/Oven Mitt,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:28.889Z,"This project explores how television fans appropriate video for personal expression and how technology can support such creative appropriation. Televisions do not have an equivalent to a Web browser's ""view source"" option; however, programs can be structured by their transcripts, which are embedded as closed captions in the signal of most shows. With our talkTV video-editing software, rearranging lines of dialogue automatically creates new scenes, thereby enabling television viewers to become authors and editors.",2005-01-01,['electronic-publishing'],,22607,E15-320G,2019-02-06T21:45:19.869Z,['walter@media.mit.edu'],True,talktv,2002-01-01,[],talkTV,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:29.145Z,"Tangible CityScape is a platform for users to explore the real 3D cityscape by changing parameters such as population, building capacity, traffic, energy consumption, and shadow simulation for collaborative review of urban planning. By integrating a 2.5D actuated shape display, immersive 2D displays, 3D projection mapping, and handheld AR, CityScape combines the strength of bits (pixels) and atoms (tangibles) to represent city models at different scales and translate the tangible view onto a larger, underlying data set.",2014-01-01,['tangible-media'],,22631,--Choose Location,2016-12-05T00:17:23.096Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu', 'heibeck@media.mit.edu']",True,tangible-cityscape,2013-09-01,[],Tangible CityScape,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:29.295Z,"Knowledge exists not only in our minds but in the world around us.  The field of tangible user interfaces aims to release knowledge inside computers into the physical world, and to extend our everyday world with graspable representations of knowledge as easily manipulated as everyday objects. Tangible programming provides a mechanism for directly manipulating procedures, configurations, and cybernetic behavior with our hands.    We have created a set of electronic, stackable LEGO bricks which reveal their order and identity.  We are using these bricks as a programming interface to a number of computational environments, including the behavior of children's toy cars and trains, microwave ovens (to specify the execution of a recipe), and music synthesizers.  
",1999-09-01,"['counter-intelligence', 'lifelong-kindergarten']",,22647,E15-068,2016-12-05T00:16:38.895Z,['mres@media.mit.edu'],True,tangible-programming-with-lego-bricks,1997-01-01,[],Tangible Programming with LEGO Bricks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:29.375Z,"This project is developing new tools for querying databases through the manipulation of physical objects. Our approach should allow multiple scientists, managers, or other groups of people to navigate, view, and discuss database contents collaboratively. We embody parameterized SQL queries as physical tokens. These tokens are manipulated, interpreted, and graphically augmented on a series of sliding racks. We map physical token sliders to parameter range selections; token adjacencies to Boolean AND/OR operations; and multiple racks to parenthetical groupings. The interface should support querying to a wide range of relational databases; initial target domains include product databases, media databases, network management, and bioinformatics.",2002-12-30,['tangible-media'],,22655,E15-441,2016-12-05T00:16:29.793Z,['ishii@media.mit.edu'],True,tangible-query-interfaces,2000-12-31,[],Tangible Query Interfaces,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:29.459Z,"Through human actions, conversations, and the creation of memories, physical spaces are transformed into social constructs and can become the arena for story construction and narrative development. This project explores how such narrative spaces can be created in a tangible form, allowing audiences to collectively reflect upon and navigate through complex, spatially structured, and multi-viewpoint stories. In Tangible Spatial Narratives, visual landscapes are projected onto the TViews interaction surface, providing a spatial framework for the overall story and the many perspectives and narrative threads that emerge as it unfolds. Users interact with the system by moving graspable pawns and other tangible objects around on the interaction surface. The interaction can often be a collaborative activity, in which several users work together to navigate through the story.",2005-01-01,['media-fabrics'],,22663,E15-368,2016-12-05T00:16:29.576Z,['gid@media.mit.edu'],True,tangible-spatial-narratives,2002-01-01,[],Tangible Spatial Narratives,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:29.541Z,"The Tangible StorySims project creates narrative simulation games that can be played out on the TViews display and interaction platform. The first of these, entitled ""The Blackjack Hack,"" is based on the story of a group of MIT students who made millions playing blackjack in casinos around the country. Viewers control the spotters on a blackjack card-counting team, and try to maximize their winnings as they engage in narrative play and social interaction around the platform.",2004-09-01,['media-fabrics'],,22671,E15-368,2016-12-05T00:16:53.893Z,['gid@media.mit.edu'],True,tangible-storysims,2003-09-01,[],Tangible StorySims,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:29.620Z,"Tangible Viewpoints is a storytelling application for interactive character-driven narratives that was designed for the TViews platform. The different segments of a multiple-point-of-view story are organized according to the character viewpoint they represent, as well as their place in the overall narrative. These segments can consist of various types of media (video, audio, images, text), and can present character development, action, and location with as much complexity as any scene of a film or chapter in a book. Graspable pawns are used to navigate through the multiple-viewpoint story. When a pawn is placed on the sensing surface, the story segments associated with its character's point-of-view are projected around it. Users select segments to be displayed on a nearby monitor, causing the narrative to advance and new segments to become available. An aura is also projected around each pawn to give a visual representation of the prominence of that viewpoint in the current telling of the story. Changes in the story space are reflected by dynamic changes in the projected graphics.",2002-12-30,"['interactive-cinema', 'tangible-media']",,22679,E15-368,2016-12-05T00:16:53.922Z,"['ishii@media.mit.edu', 'gid@media.mit.edu']",True,tangible-viewpoints,2000-12-31,[],Tangible Viewpoints,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:29.855Z,"People's tastes are their systems of aesthetic judgment and preference. Taste systems are generative; understanding the tastes of friends makes it possible to anticipate what book or movie they would like, or how they might feel about a particular news topic. We are pioneering a computational approach to automatically modeling the tastes of individuals and whole communities by mining their texts; blogs, social network profiles, and homepages; and analyzing the attitudes and preferences contained therein. By also applying machine-learning techniques, we generalize these models so that they can predict how a person would feel about hypotheticals, and even forecast tomorrow's trends.",2006-09-01,['fluid-interfaces'],,22703,E15-320,2018-12-04T14:44:11.037Z,['pattie@media.mit.edu'],True,taste-modeling-and-forecasting,2006-01-01,[],Taste Modeling and Forecasting,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:29.952Z,What if the taste of your toothpaste told you the weather? Leave your smart phone weather app in your pocket.  Tastes Like Rain dynamically alters the flavor and color of your morning toothpaste to give you today's temperature and weather. No LCD required.,2011-01-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/Display/tooth1sm.jpg,22711,,2016-12-05T00:16:53.782Z,['holtzman@media.mit.edu'],True,tastes-like-rain,2010-09-01,[],Tastes Like Rain,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:30.035Z,"We present Tattio, a fabrication process that draws from current body decoration processes (i.e., jewelry such as metallic temporary tattoos) for the creation of on-skin technology. The fabrication process generates functional components such as Near Field Communication (NFC) tags, while maintaining the aesthetics and user experience of existing metallic temporary tattoos. The fabrication process is low-cost, accessible, and customizable; we seek to enable individuals to design, make, and wear their own skin technology creations.",2016-03-01,['living-mobile'],,22719,--Choose Location,2016-12-05T00:17:23.072Z,"['cindykao@media.mit.edu', 'geek@media.mit.edu']",True,tattio,2015-09-01,[],Tattio,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:30.117Z,"As a direct way to visualize the impact of who you are (and what you eat), the electro-optic response of this display is controlled by the risk-factor levels in the bloodstream as an in situ form of risk-factor monitoring. Would you eat as many cheeseburgers if you could really SEE what they do to you?",2008-01-01,[],,22727,--Choose Location,2016-12-05T00:16:54.028Z,['mlj@media.mit.edu'],True,tattooed-displays,2005-09-01,[],Tattooed Displays,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:12:30.186Z,"New technologies for learning need to be accompanied by new ways of instruction. Teacher's LAB is an effort to bring digital technologies to teachers, to help them plan classroom activities and design learning interactions. Rather than making ""lesson plans"" that focus on procedural steps, Teacher's LAB allows teachers to create planning documents that focus on student interactions. What are good questions to ask students? How can they be encouraged to conduct self-directed learning? The hope is that such tools will encourage teachers to adopt new practices for teaching and instruction.",2001-12-30,['critical-computing'],,22735,E15-001,2016-12-05T00:16:54.051Z,[],True,teachers-lab,1999-12-31,[],Teacher's LAB,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:30.264Z,"Increasingly, software agents will need to detect certain kinds of semantically significant information that appear amidst a larger body of relatively unstructured information, for applications such as Web page analysis and data mining. Currently, the only way to express patterns of text is through grammars (a set of textual rules), which drive parsers (the recognition procedures). But grammars are difficult and error-prone to write. We are experimenting with an agent that learns recognition rules by example. The agent generates a set of hypotheses about the interpretation of the examples and dynamically displays them to the user, who may choose among them and edit them incrementally. This agent brings the power of parsing technology into the hands of non-expert users.",2007-01-01,['software-agents'],,22743,E15-383,2016-12-05T00:16:57.256Z,['lieber@media.mit.edu'],True,teaching-agents-to-recognize-text,2000-01-01,[],Teaching Agents to Recognize Text,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:30.340Z,"Many of us who enjoy what we do were motivated by an event or person that inspired us to learn, challenge, and question. We are building applications that attempt to convey that epiphany through cooperative learning and exploration. This year's work centers around ChessMaze, a strategic thinking game built in collaboration with International Chess Grand Master Maurice Ashley. ChessMaze challenges novices to learn the game as a team, and to develop generalizable strategic thinking. While playing their solo games, individual players contribute to a team database, where the same moves are grouped together, sorted based on popularity, and presented back to the community of players on a heat map.",2014-09-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/files/Display/Image005-480pixel.png,22751,--Choose Location,2016-12-05T00:16:54.080Z,['lip@media.mit.edu'],True,teamification-chessmaze,2013-01-01,[],Teamification: ChessMaze,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:30.421Z,"Musical interaction occurs through one of three modalities: performing, listening, or composing. This project investigates new ways in which computers and associated technologies may be applied to music education through each of these modalities. This project will apply current pedagogic theory and practice to the music toys developed for the ""Toy Symphony"" project. We will perform a major assessment of these instruments from an educational perspective, including design and implementation of workshop activities with children using the instruments, and evaluation of the learning outcomes in a variety of educational and cultural settings. (Funded by the Higher Education Authority of Ireland.)",2004-09-01,['opera-of-the-future'],,22759,E15-441,2016-12-05T00:16:54.106Z,['tod@media.mit.edu'],True,technology-and-creative-music-education,2002-01-01,[],Technology and Creative Music Education,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:30.594Z,"Tega is a new robot platform designed to support long-term, in-home interactions with children, with applications in early-literacy education from vocabulary to storytelling.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/01/28/TEGA_0059_QtsORP1.jpg,22775,--Choose Location,2020-01-08T17:49:00.641Z,"['cynthiab@media.mit.edu', 'jinjoo@media.mit.edu', 'lukulele@media.mit.edu', 'sooyeon6@media.mit.edu', 'haewon@media.mit.edu']",True,tega-a-new-robot-platform-for-long-term-interaction,2015-01-01,"['robotics', 'artificial-intelligence', 'kids', 'learning-teaching', 'social-robotics', 'technology']",Tega: A New Social Robot Platform,PUBLIC,http://www.jinjoolee.com,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:30.684Z,"We are developing a system that allows a group of distributed participants to direct the actions of a remote actor. This remote ""TeleActor"" is a telepresence agent, taking commands from the directors and sending audio and video back to them from the remote location. Research goals include designing interfaces for large groups of cooperating and competing individuals, developing complex telepresence command and feedback systems, and advancing our understanding of the interactions between the TeleActor and the real world. Our current focus is on TeleActor as a media agent for news broadcasting.",2004-09-01,['sociable-media'],,22783,E15-449,2016-12-05T00:16:54.178Z,"['judith@media.mit.edu', 'monster@media.mit.edu']",True,teleaction,2000-01-01,[],TeleAction,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:30.773Z,"Establishing telecenters (community computer centers that often provide a range of services) in developing countries requires careful evaluation of alternative hardware configurations, power sources and telecommunication services. Quite often the goal is to find a configuration whose recurrent cost can be sustained by the target community. The Telecenter Cost Estimator is a visual tool for exploring the impact of alternative telecenter configurations on capital and ongoing costs.",2002-12-30,"['e-markets', 'edevelopment']",,22791,E15-391,2016-12-05T00:16:54.241Z,[],True,telecenter-cost-estimator,1999-12-31,[],Telecenter Cost Estimator,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:30.855Z,"The Telecorrelator allows people to view multiple perspectives of real-time news events aligned by content as well as time. It reveals emphasis and timing and allows participants to discover points of view and important events. Currently, the Telecorrelator aligns broadcast news from four broadcast sources to visually reveal the emphasis and time  allocation they devote to a succession of events.",2014-09-01,"['ultimate-media', 'viral-communications']",,22799,--Choose Location,2016-12-05T00:16:54.212Z,['lip@media.mit.edu'],True,telecorrelator,2013-09-01,[],Telecorrelator,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:30.941Z,"Chit Chat Club is an experiment in bringing people together in both immediate and online spaces. It consists of a set of communication portals in the form of human-scale sculptures; these give the remote user equivalent presence among the co-located participants. Each sculpture features a different interaction style, allowing us to evaluate the social effect of different interfaces.",2006-09-01,['sociable-media'],,22807,E15-383,2016-12-05T00:17:01.780Z,['judith@media.mit.edu'],True,telematic-sculptures-chit-chat-club-ii,2006-01-01,[],Telematic Sculptures (Chit Chat Club II),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:31.033Z,"Mediated communication between remote social spaces is a relatively new concept. One current example of this interaction is video conferencing among people within the same organization. Large-scale video conferencing walls have begun to appear in public or semi-public areas such as workplace lobbies and kitchens. These connections provide a link via audio and/or video to another space within the organization. When placed in these spaces, they are often designed for casual encounters among people within that community. Thus far, communicating via these systems has not met expectations. We are exploring a different approach to linking spaces through the use of what we are defining as a social catalyst. These catalysts are incorporated into our installation, Telemurals. An ethnography study of Telemurals will be conducted between two graduate dormitories to see how the catalysts affect interaction.",2004-09-01,['sociable-media'],,22815,E15-468,2016-12-05T00:16:54.269Z,['judith@media.mit.edu'],True,telemurals,2002-01-01,[],Telemurals,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:31.118Z,"This project explores the utility of voice in a range of applications offering services to users of the telephone network. Topics being examined include voice mail, speech synthesis of e-mail, access to calendars and Rolodexes, and speech-based user interface to call-processing features, such as variable call forwarding. Visual- (on the workstation) and speech- (over the telephone) based applications offer differing views of the same underlying databases in an office environment.",1997-12-30,['living-mobile'],,22823,,2016-12-05T00:17:01.816Z,['geek@media.mit.edu'],True,telephone-based-voice-services,1988-12-31,[],Telephone-Based Voice Services,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:31.198Z,"The goal of the TeleReporter project is to design a system that allows for complete and open interaction between news providers and the news audience. The audience will, as a group, be able to ""direct"" the flow of a live news report by offering suggestions and requests on what actions to take, who to talk to, or what questions to ask. The TeleReporter is an extension of the Tele-Actor system, an ongoing research project in telepresence in development at the Media Laboratory and the University of California at Berkeley. The Tele-Actor interface allows remote users to share control of a human ""Tele-Actor,"" sending commands decided by majority vote to the actor and following progress through a live video feed streaming from a camera on the actor's person.",2002-09-01,['sociable-media'],,22831,E15-449,2016-12-05T00:17:01.837Z,"['judith@media.mit.edu', 'monster@media.mit.edu']",True,telereporter,2002-01-01,[],TeleReporter,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:31.385Z,"TeleScrapbook and TelePostcard are pairs of wirelessly connected electronic scrapbooks and electronic greeting cards, respectively. These electronic pages use I/O Stickers�adhesive electronic sensors and actuators�to allow users to design their own interfaces for remote communication. This project combines the creative affordances of traditional paper craft with electronic interactivity and long-distance communication. The simple, low-bandwidth connections made from these sensors and actuators leave room for users to design not only the look and function of the pages, but also the signification of the connections. By attaching I/O Stickers to these special books and greeting cards, users can invent ways to communicate with long-distance loved ones with personalized messages that are also connected in real time.",2011-09-01,"['personal-robots', 'high-low-tech']",,22839,,2016-12-05T00:17:02.152Z,"['cynthiab@media.mit.edu', 'leah@media.mit.edu', 'jieqi@media.mit.edu']",True,telescrapbook-and-telepostcard,2011-01-01,[],TeleScrapbook and TelePostcard,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:31.479Z,TeleStory is a tangible interface for children to learn vocabulary through gestural experimentation. Children pair characters in a story and influence their actions through gestural input on the Siftables platform. TeleStory allows children to influence the design of a television animation and learn the meanings of words by triggering actions in the narrative.,2009-09-01,['fluid-interfaces'],,22847,E15-320,2016-12-05T00:16:54.456Z,['pattie@media.mit.edu'],True,telestory,2008-09-01,[],TeleStory,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:31.660Z,"TeleTouch lets you touch and control from far. See thru your smartphone's camera and control EVERYTHING you see on the screen by touching it. Now you can interact with your appliances from far, in a fun way. Here it goes. You look at a scene thru the camera of your smartphone and control what you see - home appliances (TV, alarm, music player, ...), Open the door; or turn on that light. Just by touching it on screen. It's simple. TeleTouch lets you interact with your world with touch, but now from far.",2012-01-01,['fluid-interfaces'],,22863,--Choose Location,2018-10-12T16:52:12.814Z,"['pattie@media.mit.edu', 'pranav@media.mit.edu']",True,teletouch,2011-01-01,[],TeleTouch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:31.748Z,"This project explores how merging ubiquitous consumer electronics and the sociable Web can improve the user experience of these devices, increase the functionality of both, and help distribute content in a more sociable way.  Through custom software for digital video recorders and a Facebook application acting as a hub, we connect a community of television viewers via their televisions. By connecting these two technologies, the user can now see what her friends think of the shows available on her DVR, and automatically record her friends' favorites; in return, the user contributes her own viewing data back to the social network.",2008-01-01,['information-ecology'],,22871,E15-301,2016-12-05T00:16:29.826Z,['holtzman@media.mit.edu'],True,television-meets-facebook-social-networking-via-consumer-electronics,2007-09-01,[],Television Meets Facebook: Social Networking via Consumer Electronics,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:31.923Z,"TellTale is a story construction kit whose goal is to help young children create and experiment with the structure and content of personal narratives. The design consists of a number of modular body components and one head piece used to record and play audio segments created by a child, or by several children. The body parts can contain stories or story fragments that can be combined in different orders to create new narrative configurations, letting children experiment with plot, transitions, endings, and beginnings�basically anything they can imagine. In essence, it is a ""tangible audio processor"" for children who have stories to tell but who might not yet have the skills necessary to communicate their ideas in writing. TellTale's design will evolve as we explore with children how storytelling and personal narrative expression technologies of this sort can encourage the development of literacy skills.",2001-12-30,['gesture-and-narrative-language'],,22887,E15-320,2016-12-05T00:16:27.072Z,[],True,telltale-2,1999-12-31,[],TellTale,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:32.006Z,"TellTale is a story construction kit whose goal is to help young children create and experiment with the structure and content of personal narratives. The design consists of a number of modular body components and one head piece used to record and play audio segments created by a child, or by several children. The body parts can contain stories or story fragments that can be combined in different orders to create new narrative configurations, letting children experiment with plot, transitions, endings, and beginnings�basically anything they can imagine. In essence, it's a ""tangible audio processor"" for children who have stories to tell but who might not yet have the skills necessary to communicate their ideas in writing. TellTale's design will evolve as we explore with children how storytelling and personal narrative expression technologies of this sort can encourage the development of literacy skills.",2001-12-30,"['toys-of-tomorrow', 'tangible-media']",,22895,E15-441,2016-12-05T00:16:29.847Z,['ishii@media.mit.edu'],True,telltale,1999-12-31,[],TellTale,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:32.238Z,"Tensor displays are a family of glasses-free 3D displays comprising all architectures employing (a stack of) time-multiplexed LCDs illuminated by uniform or directional backlighting. We introduce a unified optimization framework that encompasses all tensor display architectures and allows for optimal glasses-free 3D display. We demonstrate the benefits of tensor displays by constructing a reconfigurable prototype using modified LCD panels and a custom integral imaging backlight. Our efficient, GPU-based NTF implementation enables interactive applications. In our experiments we show that tensor displays reveal practical architectures with greater depths of field, wider fields of view, and thinner form factors, compared to prior automultiscopic displays.",2015-09-15,['camera-culture'],,22919,--Choose Location,2016-12-05T00:17:03.939Z,"['raskar@media.mit.edu', 'gordonw@media.mit.edu', 'naik@media.mit.edu']",True,tensor-displays-high-quality-glasses-free-3d-tv,2012-01-01,[],Tensor Displays: High-Quality Glasses-Free 3D TV,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:32.484Z,"This project is a physical realization of the self-organizing algorithms work started by Rob Poor with Hyphos. TephraNet is a self-organizing network of 100 nodes deployed in the Hawaii Volcanoes National Park to monitor endangered species of plants and animals. The project is a collaboration with the University of Hawaii, which is using the data collected for microclimatology research.",2001-12-30,['personal-information-architecture'],,22943,E15-469,2016-12-05T00:17:23.128Z,['mike@media.mit.edu'],True,tephranet-self-organizing-wireless-sensor-network,1999-12-31,[],TephraNet: Self-Organizing Wireless Sensor Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:32.821Z,"Terra Incognita is a global news game and recommendation system. Terra Incognita helps you discover interesting news and personal connections to cities that you haven't read about. Whereas many recommendation systems connect you on the basis of ""similarity,"" Terra Incognita connects you to information on the basis of ""serendipity."" Each time you open the application, Terra Incognita shows you a city that you have not yet read about and gives you options for reading about it. Chelyabinsk (Russia), Hiroshima (Japan), and Dhaka (Bangladesh) are a few of the places where you might end up.",2014-06-01,['civic-media'],,22975,--Choose Location,2016-12-05T00:17:23.202Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'dignazio@media.mit.edu']",True,terra-incognita-1000-cities-of-the-world,2013-09-01,[],Terra Incognita: 1000 Cities of the World,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:32.900Z,"Although there have been great advances in the control of lower extremity prostheses, transitioning between terrains such as ramps or stairs remains a major challenge for the field. The mobility of leg amputees is thus limited, impacting their quality of life and independence. This projects aims to solve this problem by designing, implementing, and integrating a combined terrain-adaptive and volitional controller for powered lower limb prostheses. The controller will be able to predict terrain changes using data from both intrinsic sensors and electromyography (EMG) signals from the user; adapt the ankle position before footfall in a biologically accurate manner; and provide a torque profile consistent with biological ankle kinetics during stance. The result will allow amputees to traverse and transition among flat ground, stairs, and slopes of varying grade with lower energy and pain, greater balance, and without manually changing the walking mode of their prosthesis.",,['biomechatronics'],,22983,--Choose Location,2021-09-17T19:29:38.987Z,"['hherr@media.mit.edu', 'romka@media.mit.edu']",True,terrain-adaptive-lower-limb-prosthesis,2014-09-01,"['robotics', 'health', 'human-machine-interaction', 'prosthetics', 'prosthetic-design']",Terrain-Adaptive Lower Limb Prosthesis,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:33.053Z,"Teshima 8 Million Lab is the first Shinto shrine worshipping a genetically engineered life—a silkworm created in Sputniko!'s new work Red Silk of Fate—Tamaki's Crush. In the Shinto religion, ""Yaoyorozu"" (which literally means ""8 Million"") is a word used to describe the myriad of gods believed to reside in almost anything, such as the wind, the ocean, trees, and animals. Conceived by artist Sputniko!, Teshima 8 Million Lab sets out to create new members of Yaoyorozu, forming a mythology from emerging science and art. Far from the big city and located on a site blessed with an abundance of nature, the facility invites the exploration of alternative perspectives on our future of nature and beliefs, as science continues to move forward.",2010-05-01,['design-fiction'],https://dam-prod2.media.mit.edu/x/files/Display/Teshima8MillionLab_09.jpg,22999,--Choose Location,2022-06-10T13:44:47.836Z,['sputniko@media.mit.edu'],True,teshima-8-million-lab,2016-01-01,[],Teshima 8 Million Lab,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2016-07-15T00:12:33.130Z,"This project aims to build a powerful system as a scientific tool for bridging the gap in the literature by determining the dynamic biomechanics of the lower-limb joints and metabolic effects of physical interventions during natural locomotion. This system is meant for use in applying forces to the human body and measuring force, displacement, and other physiological properties simultaneously, helping investigate controllability and efficacy of mechanical devices physically interacting with a human subject.",,['biomechatronics'],,23007,--Choose Location,2019-04-17T19:22:13.955Z,"['hherr@media.mit.edu', 'kuan525@media.mit.edu', 'kenpasch@media.mit.edu']",True,tethered-robotic-system-for-understanding-human-movements,2011-01-01,[],Tethered robotic system for understanding human movements,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:33.211Z,"We study augmentation of the creative writing process through a new text editor. This text editor presents short-length snippets from books and Internet sources such as blogs and Twitter in the moment of the writing process. The snippets are selected in such a way as to be relevant to the content that the user is trying to produce. This text editor explores a number of ideas including: Is it possible to augment creativity by introducing just-in-time information? Is it more efficient to consume content in the moment of creation? Is it possible to develop a notion of ""books"" that do not have to be consumed in a serial process?",2013-01-01,['fluid-interfaces'],,23015,--Choose Location,2018-12-04T20:49:33.011Z,['pattie@media.mit.edu'],True,text-editor-for-augmented-writing,2013-01-01,[],Text Editor for Augmented Writing,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:33.288Z,"We are exploring ways to build sensors using a variety of crafting and needlework techniques, using affordable and available materials such as conductive threads, yarns, fabrics, and paints. These materials are used to sew, knit, crochet, embroider, and laminate, creating a range of textile-based sensors.",2011-09-01,['high-low-tech'],,23023,,2016-12-05T00:16:54.611Z,['leah@media.mit.edu'],True,textile-sensors,2010-01-01,[],Textile Sensors,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:33.366Z,"TextNet is a ""distributed"" wiki. While normal wikis are designed for groups to create a single document collaboratively, TextNet is built for authors writing multiple linked documents and texts with overlapping or partially overlapping content. TextNet helps writers to compute, track, and merge differences between diverged documents. The intention is to help writers easily collaborate where possible—while differing where necessary—in an ad hoc and lightweight manner. The project builds on thinking and tools from the world of distributed source management and version-control systems used in the free and open-source software community.",2008-01-01,['computing-culture'],,23031,E15-020C,2019-02-06T21:44:32.812Z,"['csik@media.mit.edu', 'walter@media.mit.edu']",True,textnet,2005-09-01,[],TextNet,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:33.770Z,"We have built several handheld devices that combine grasp and orientation sensing with pattern recognition in order to provide highly intelligent user interfaces. The Bar of Soap is a handheld device that senses the pattern of touch and orientation when it is held, and reconfigures to become one of a variety of devices, such as a phone, camera, remote control, PDA, or game machine. Pattern-recognition techniques allow the device to infer the user's intention based on grasp. Another example is a baseball that determines a user's pitching style as an input to a video game.",2008-09-01,['object-based-media'],,23071,Garden Conference Room,2016-12-05T00:17:03.965Z,['vmb@media.mit.edu'],True,the-bar-of-soap-grasp-based-interfaces,2007-01-01,"['design', 'human-computer-interaction', 'sensors', 'gesture-interface']","The ""Bar of Soap"": Grasp-Based Interfaces",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:33.848Z,"Affective Remixer is a real-time music-arranging system that reacts to immediate affective cues from a listener. Data was collected on the potential of certain musical dimensions to elicit change in a listener�s affective state using sound files created explicitly for the experiment through composition/production, segmentation, and re-assembly of music along these dimensions. Based on listener data, a probabilistic state transition model was developed to infer the listener�s current affective state. A second model was made that would select music segments and re-arrange ('re-mix') them to induce a target affective state. ",2006-01-01,['affective-computing'],,23079,--Choose Location,2016-12-05T00:16:54.709Z,['picard@media.mit.edu'],True,the-affective-remixer-personalized-music-arranging,2005-01-01,[],The Affective Remixer: Personalized Music Arranging,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:33.930Z,"The Alphabots are trans-fictional(xF) mobile and modular semi-autonomous robotic symbol set characters designed to play with preschool aged children (three to six years old).�In support of early development goals (literacy, numeracy and shape recognition) educators and parents can take an active role in co-designing playful learning interactions both on and off-screen.",2011-01-01,['personal-robots'],,23087,,2016-12-05T00:16:54.768Z,['cynthiab@media.mit.edu'],True,the-alphabots,2010-09-01,[],The Alphabots,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:34.122Z,"The future of mobile communications will certainly be one where devices will opportunistically connect to a variety of networks, obtaining and providing services on the fly. Our portable devices will carry our identity and act as intermediaries between users and their rich environments. The Amulet embodies this vision. We are developing this new device which instantiates a user's identity and provides a computing model that allows the device to securely exploit its context and surrounding infrastructure in an open, secure, and adaptable manner.",2009-01-01,['viral-communications'],,23095,E15-483,2016-12-05T00:16:54.838Z,['lip@media.mit.edu'],True,the-amulet,2008-01-01,[],The Amulet,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:34.294Z,"The Babbling Brook is an unnamed neighborhood creek in Waltham, MA, that winds its way to the Charles River. With the help of networked sensors and real-time processing, the brook constantly tweets about the status of its water quality, including thoughts and bad jokes about its own environmental and ontological condition. Currently, the Babbling Brook senses temperature and depth and cross-references that information with real-time weather data to come up with extremely bad comedy. Thanks to Brian Mayton, the Responsive Environments group, and Tidmarsh Farms Living Observatory for their support.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/phonograph-2.jpg,23103,--Choose Location,2022-06-10T13:35:30.024Z,"['ethanz@media.mit.edu', 'dignazio@media.mit.edu']",True,the-babbling-brook,2014-09-01,[],The Babbling Brook,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:34.463Z,"Mental wellbeing is intimately tied to both social support and physical activity. The Challenge is a tool aimed at promoting social connections and decreasing sedentary activity in a workplace environment. Our system asks participants to sign up for short physical challenges and pairs them with a partner to perform the activity. Social obligation and social consensus are leveraged to promote participation. Two experiments were conducted in which participants' overall activity levels were monitored with a fitness tracker. In the first study, we show that the system can improve users' physical activity, decrease sedentary time, and promote social connection. As part of the second study, we provide a detailed social network analysis of the participants, demonstrating that users' physical activity and participation depends strongly on their social community.",2019-07-30,"['advancing-wellbeing', 'affective-computing', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2016/10/18/graph_PostFull_partitions_prettier.png,23111,--Choose Location,2022-05-12T17:23:19.059Z,"['picard@media.mit.edu', 'pattie@media.mit.edu', 'nfarve@media.mit.edu', 'jaquesn@media.mit.edu']",True,the-challenge,2015-01-01,"['data', 'health']",The Challenge,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:34.628Z,"The Chandelier is a large-scale robotic musical instrument that is being developed for ""Death and the Powers."" Its 48 strings can be actuated both through powerful electromagnets, and tactilely (plucked like a harp or bowed like a cello). With the strings driven by electromagnets, the tactile player can also repeatedly damp strings or create overtones by carefully touching the strings' anti-nodes, creating a new intimacy between players, who play not just the same instrument, but the same strings. The Chandelier is composed of many systems�logic for control of music and lighting, networked servers, and playable interfaces�all built around an elegant, articulated skeletal structure which allows changes to the length, angle, and tensions of the strings. We are currently experimenting with playing it through new types of interfaces to take advantage of its unusual tuning and sonorities.",2010-09-01,['opera-of-the-future'],,23119,E15-443,2016-12-05T00:16:54.899Z,['tod@media.mit.edu'],True,the-chandelier,2006-09-01,[],The Chandelier,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:34.780Z,"In this project, learners construct computational models of how they would like to improve their communities. The basic premise is that students will perform a critical inquiry into the life, culture, and functioning of their cities, and create new models of how they would like some aspect to be. They can either address something they perceive as problematic (waste recycling, transportation, energy generation and consumption, employment, crime), or propose a model for a grand new idea to provide some elements desired but not previously possible or conceived (interactive public entertainment and art spaces for community, dynamic customizable clean transportation, instant playgrounds, responsive environments). They work in a variety of computational and traditional media. We are augmenting computational tools for learning, and building new support technologies for distance support and collaboration. The project also serves as a concrete model for alternative-learning environments and for teacher development.",2005-09-01,['future-of-learning'],,23127,E15-443,2016-12-05T00:16:11.814Z,"['ronmac@media.mit.edu', 'edith@media.mit.edu', 'cavallo@media.mit.edu']",True,the-city-that-we-wanta-cidade-que-a-gente-quer,2001-01-01,[],The City that We Want/A Cidade que a Gente Quer,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:35.086Z,"The Conductor's Jacket is a unique wearable device that measures physiological and gestural signals.  Together with  the Gesture Construction, a musical software system, it interprets these signals and applies them expressively in a  musical context.  Sixteen sensors have been incorporated into the Conductor's Jacket in such a way as to not  encumber or interfere with the gestures of a working orchestra conductor.  The Conductor's Jacket system gathers up to sixteen data channels reliably at rates of 3 kHz per channel, and also provides real-time graphical feedback.  Unlike many gesture-sensing systems it not only gathers positional and accelerational data but also senses muscle tension from several locations on each arm.  We will demonstrate the Gesture Construction, a musical software system that analyzes and performs music in real-time based  on the performer's gestures and breathing signals.  A bank of software filters extract several of the features that were found in the conductor study, including beat intensities and the alternation between arms.  These features are then used to generate real-time expressive effects by shaping the beats, tempos, articulations,  dynamics, and note lengths in a musical score.",1999-12-30,['affective-computing'],,23143,E15-384,2016-12-05T00:17:23.332Z,"['picard@media.mit.edu', 'tod@media.mit.edu']",True,the-conductors-jacket,1996-12-31,[],The Conductor's Jacket,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:35.210Z,"Fifty years ago, Doug Engelbart created a conceptual framework for augmenting human intellect in the context of problem-solving. We expand upon Engelbart's framework and use his concepts of process hierarchies and artifact augmentation for the design of personal intelligence augmentation (IA) systems within the domains of memory,  decision making, motivation, and mood. We propose a systematic design methodology for personal IA devices, to organize existing IA research within a logical framework, and to uncover underexplored areas of IA that could benefit from the invention of new artifacts.",2013-01-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/brain.png,23151,--Choose Location,2016-12-05T00:16:54.740Z,['pattie@media.mit.edu'],True,the-design-of-artifacts-for-augmenting-intellect,2012-09-01,[],The Design of Artifacts for Augmenting Intellect,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:35.329Z,"Dog is a programming language that makes it easy and intuitive to create social applications. A key feature of Dog is built-in support for interacting with people. Dog provides a natural framework in which both people and computers can be sent requests and return results. It can perform a long-running computation while also displaying messages, requesting information, or sending operations to particular individuals or groups. By switching between machine and human computation, developers can create powerful workflows and model complex social processes without worrying about low-level technical details. ",2017-05-01,['social-computing'],https://dam-prod2.media.mit.edu/x/files/Display/Dog%20Logo.png,23159,--Choose Location,2022-06-10T14:40:02.088Z,['sdkamvar@media.mit.edu'],True,the-dog-programming-language,2011-01-01,[],The Dog Programming Language,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:35.450Z,"With more than six billion people and 15 billion products, the world economy is anything but simple. The Economic Complexity Observatory is an online tool that helps people explore this complexity by providing tools that can allow decision makers to understand the connections that exist between countries and the myriad of products they produce and/or export. The Economic Complexity Observatory puts at everyone's fingertips the latest analytical tools developed to visualize and quantify the productive structure of countries and their evolution.",2016-10-22,['collective-learning'],https://dam-prod2.media.mit.edu/x/2016/08/23/observatory of economic complexity.png,23167,--Choose Location,2016-12-05T00:17:23.398Z,['hidalgo@media.mit.edu'],True,the-economic-complexity-observatory,2010-09-01,[],The Economic Complexity Observatory,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:35.743Z,"The Electome: Where AI meets political journalismThe Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “Horse Race of Ideas”). The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms identify those Tweets and stories specifically about the election, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:the share of conversation or coverage that any given issue or candidate commands on Twitter and in the news media, respectively—and how the two platforms are alignedwhich issues are most closely associated with each candidate on Twitter (via co-occurrent candidate/issue references in single Tweets)how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personalityspecific sub-topics and representative Tweets within broader conversations about specific issues or candidatesthe level of “incivility” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidatewho is influencing the public sphere election conversation (via a composite Twitter/media influence metrics)LSM’s deployment of Electome analytics has been supported by the Knight Foundation, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has: provided customized analysis to several Electome media outlets—including the Washington Post, Bloomberg News, CNN Politics and Fusion—as well as publishing its own analysis in Mediumcollaborated with the Commission on Presidential Debates to offer Electome analysis to the general election debates’ moderators and credentialed journalistsalso collaborated with the Roper Center for Public Opinion Research at Cornell University for integration of the Center’s polling in Electome analytics/dashboardand built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2017/02/21/image(2).png,23191,--Choose Location,2019-04-19T18:40:58.879Z,"['dkroy@media.mit.edu', 'soroush@media.mit.edu', 'soph@media.mit.edu', 'russell5@media.mit.edu', 'billp@media.mit.edu', 'pralav@media.mit.edu', 'mmv@media.mit.edu', 'pernghwa@media.mit.edu', 'schaad@media.mit.edu', 'aheyward@media.mit.edu']",True,the-electome-measuring-responsiveness-in-the-2016-election,2015-09-01,"['computer-vision', 'artificial-intelligence', 'data', 'entertainment', 'government', 'politics', 'social-media', 'social-science', 'machine-learning']",The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:35.854Z,"Individuals with autism are known to have difficulties connecting with other people, reciprocating social interactions, and being emotionally regulated by others. Yet, until recently, very little attention has been given to the way people interact together, in a system, rather than by themselves. We propose a new way to collect data on how caregivers and their children, with and without autism, affect and are affected by each other (i.e., how they ""sync up"" with one another), both in their behavior and in their physiology. We also introduce a customizable digital-physical smart toy platform that will allow us to test hypotheses and collect data about patterns of caregiver-child synchrony in a naturalistic and engaging environment. MIT and Northeastern are forging a new collaboration between smart toy technology and autism research that will help uncover how the social brain develops.",2018-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/2017/10/16/enTRAIN-autism-screenshot.png,23199,--Choose Location,2022-05-12T17:26:22.456Z,"['picard@media.mit.edu', 'ktj@media.mit.edu', 'oliverws@media.mit.edu', 'mgoodwin@media.mit.edu']",True,the-entrain-study,2016-09-01,[],The enTRAIN Study: Physiological synchrony in children with autism,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:36.183Z,"This project is exploring instrumenting everyone (The Flying Karamazov Brothers) and everything (their juggling clubs) in a large space (a stage), in order to let them work collaboratively as a giant musical instrument, display information in their juggling, and allow them to juggle in different worlds.",2001-12-30,['physics-and-media'],,23223,E15-023,2016-12-05T00:16:55.016Z,"['neilg@media.mit.edu', 'monster@media.mit.edu']",True,the-flying-karamazov-brothers,1998-12-31,[],The Flying Karamazov Brothers,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:36.313Z,"The Foodome addresses how to create deeper understanding and predictive intelligence about the relationships between how we talk and learn about food, and what we actually eat. Our aim is to build a food learning machine that comprehensively maps, for any given food, its form, function, production, distribution, marketing, science, policy, history, and culture (as well as the connections among all of these aspects). We are gathering and organizing a wide variety of data, including news/social content, recipes and menus, and sourcing and purchase information. We then use human-machine learning to uncover patterns within and among the heterogeneous food-related data. Long term, the Foodome is meant to help improve our understanding of, access to, and trust in food that is good for us; find new connections between food and health; and even predict impacts of local and global events on food.",2017-05-31,['social-machines'],https://dam-prod2.media.mit.edu/x/2016/10/21/Screen Shot 2016-10-21 at 12.54.46 PM.png,23231,--Choose Location,2017-10-16T15:28:49.377Z,"['dkroy@media.mit.edu', 'soroush@media.mit.edu', 'russell5@media.mit.edu', 'pralav@media.mit.edu', 'mmv@media.mit.edu', 'lukeglw@media.mit.edu']",True,the-foodome-building-a-comprehensive-knowledge-graph-of-food,2015-09-01,[],The Foodome: Building a Comprehensive Knowledge Graph of Food,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:36.538Z,"We are looking at the emotional experience created when children learn games. Why do we start games with the most boring part, reading directions? How can we create a product that does not create an abundance of work for parents? Key insights generated from field work, interviews, and measurement of electrodermal activity are: kids become bored listening to directions, ""it's like going to school""; parents feel rushed reading directions as they sense their children's boredom; children and parents struggle for power in interpreting and enforcing rules; children learn games by mimicking their parents, and; children enjoy the challenge of learning new games.",2012-01-01,['affective-computing'],,23247,--Choose Location,2016-12-05T00:16:36.607Z,"['picard@media.mit.edu', 'hedman@media.mit.edu']",True,the-frustration-of-learning-monopoly,2011-01-01,[],The Frustration of Learning Monopoly,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:36.641Z,"In the constant cycling between centralized and distributed processing in which the centralized VAX gave way to the distributed PC now giving way to the virtually centralized YouTube, the notion of tiny networked processors is still a new idea. Imagine an orchestra of virtual instruments that are sufficiently sensitive to each other as to induce the excitement we get from seeing a live orchestra trying to play together. Now suppose the audience is also free to jump up on stage and get involved. The susceptible processors might be $100 laptops or acoustically enhanced mobiles, or they may be greeting cards that have brought you all to a party in which all the voices of ""Happy Birthday"" are heard only if everyone comes.",2007-09-01,['music-mind-and-machine'],,23255,E15-484,2016-12-05T00:17:23.613Z,['bv@media.mit.edu'],True,the-future-of-collaborative-distributed-processing,2006-01-01,[],The Future of Collaborative Distributed Processing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:36.743Z,"This project builds a social, place-based information window into the Media Lab using 30 touch-sensitive screens strategically placed throughout the physical complex and at sponsor sites. The idea is get people to talk among themselves about the work that they jointly explore in a public place. We present Lab projects as dynamically connected sets of ""charms"" that visitors can save, trade, and explore. The GI demonstrates a framework for an open, integrated IT system and shows new uses for it.",,"['necsys', 'viral-communications']",,23263,--Choose Location,2023-08-30T14:30:34.284Z,"['jon@media.mit.edu', 'holtzman@media.mit.edu', 'borovoy@media.mit.edu', 'ypod@media.mit.edu', 'havasi@media.mit.edu', 'lip@media.mit.edu', 'mbletsas@media.mit.edu']",True,the-glass-infrastructure-gi,2010-01-01,[],The Glass Infrastructure (GI),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:36.861Z,"Two robot arms are in constant motion and hard at work. From a distance, they can be seen considering their tasks, communicating with each other, and struggling to make sense of their abstract mission. Participants are encouraged to approach the hands to engage in a dialog with the robots to offer assistance or advice. The project demonstrates a chatter system to simulate affective conversation and a parametric animation engine to provide dynamic, autonomous character-driven movement in the robots. ",2013-09-01,['personal-robots'],,23271,--Choose Location,2016-12-05T00:16:55.045Z,"['cynthiab@media.mit.edu', 'dnunez@media.mit.edu']",True,the-helping-hands,2013-01-01,[],The Helping Hands,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:37.163Z,"Most interactions between cultures require overcoming a language barrier, which is why multilingual speakers play an important role in facilitating such interactions. In addition, certain languages (not necessarily the most spoken ones) are more likely than others to serve as intermediary languages. We present the Language Group Network, a new approach for studying global networks using data generated by tens of millions of speakers from all over the world: a billion tweets, Wikipedia edits in all languages, and translations of two million printed books. Our network spans over eighty languages, and can be used to identify the most connected languages and the potential paths through which information diffuses from one culture to another. Applications include promotion of cultural interactions, prediction of trends, and marketing.",2016-10-22,['collective-learning'],https://dam-prod2.media.mit.edu/x/files/Display/lgn-demo-screenshot-square.png,23295,--Choose Location,2016-12-05T00:17:23.662Z,['hidalgo@media.mit.edu'],True,the-language-group-network,2012-01-01,[],The Language Group Network,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:37.505Z,"This project explores how agency affects decision-making, spending, and risk-taking. We are working to determine if agents behave differently when acting on behalf of others.",2009-09-01,[],,23319,--Choose Location,2016-12-05T00:17:23.683Z,[],True,the-money-of-others-agency-risk-taking-and-spending,2008-01-01,[],"The Money of Others: Agency, Risk-Taking, and Spending",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:12:37.817Z,"In this visual brainstorming, we present the next 30 years of VR in a set of concept designs.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/VR%20logo2.png,23335,--Choose Location,2016-12-05T00:16:55.169Z,"['raskar@media.mit.edu', 'barmak@media.mit.edu']",True,the-next-30-years-of-vr,2016-01-01,[],The Next 30 Years of VR,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:38.104Z,"Measuring development is difficult, and most of the widely accepted measures of progress, such as GDP per capita, or the U.N.�s Human Development Index, are aggregates that fail to consider people�s activities and achievements. Our civilization, however, has attained much more than providing services and purchasing power to an increasing population. Our history is punctuated by achievements, from the works of Leonardo da Vinci to the footsteps of Neil Armstrong. At the Observatory of Global Culture we are working on the creation of data, visualizations, and measures of cultural development with the goal of improving our understanding of the impact of culture. By producing these measures, we hope to highlight what our society has achieved and focus our social goals on concrete accomplishments that advance the development of human capacities, rather than more amorphous forms of aggregate growth.",2014-01-01,['collective-learning'],,23351,--Choose Location,2016-12-05T00:17:23.721Z,"['hidalgo@media.mit.edu', 'amy_yu@media.mit.edu', 'kzh@media.mit.edu']",True,the-observatory-of-global-culture,2012-01-01,[],The Observatory of Global Culture,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:38.240Z,"The Feast is an immersive, multi-sensory experience in four acts. Each act explores a different theme of eating, from mysterious to playful to communal. The Feast is not a normal dinner where people passively eat what's in front of them. In the late hours of the night, we will explore the active, creative role of the diner and build community around food and companionship.",2013-01-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/files/Display/feast%20logo.jpg,23359,--Choose Location,2016-12-05T00:16:55.236Z,"['tod@media.mit.edu', 'patorpey@media.mit.edu', 'benb@media.mit.edu', 'pip@media.mit.edu']",True,the-other-feast,2013-09-01,[],The Other Feast,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:38.626Z,"Telepresent robots are often pitched as a technology to extend the influence of those who already have money and power. We want to use robotic telepresence for the public good�broadening access, supporting public interest reporting, and funding access initiatives.",2014-09-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/the_peoples_bot.png,23383,--Choose Location,2016-12-05T00:16:55.263Z,"['ethanz@media.mit.edu', 'jnmatias@media.mit.edu']",True,the-peoples-bot,2014-01-01,[],The People's Bot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:38.742Z,"PlaceMap constructs a new model for maps that increases our awareness of the world around us by exploring new theoretical frameworks for spatial applications called user centered mapping. Grounded in our understanding of real-world models of spatial decision-making and the failings of traditional mapping applications, user-centered mapping provides the tools for a new approach to mapping that mirrors the way we think about space. By establishing a computational model of place we can begin formulate understanding of how to design the next generation of spatial applications.",2007-09-01,['context-aware-computing'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/pmlogo.png,23391,E15-320,2016-12-05T00:17:23.799Z,[],True,the-placemap-project-and-the-design-of-spatial-applications,2004-01-01,[],The PlaceMap Project and the Design of Spatial Applications,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:38.860Z,"In the spirit of Richard Feynman's playful approach to the world of ideas and things, we are developing a set of tools to enable everyone to have ""The Pleasure of Finding Things Out"" through rich Constructionist activities, in which one ""finds things out"" by ""making things"" they care about. Children and adults alike can use Crickets, LogoChips, and Towers to embed computational ""intelligence"" into the world of things, and in the process develop new ways of making and thinking about behaviors and functions. They can also use design tools like LaserLogo to create and shape the things in the world around them in new and more effective ways using computer-controlled, rapid-prototyping tools, such as laser-cutters, 3-D printers, machining tools, and embroidery, sewing, and pattern making machines. With these tools, the next generation will treat the whole world as a medium of expression: they will design patterns to put onto the surfaces of things, cut things out of a wide range of materials, and embed intelligence in them. In the process, they will discover new ways of mixing form and function by actively meshing the worlds of physical and computational ideas and artifacts together.",2004-01-01,['personal-fabrication'],,23399,E15-344,2016-12-05T00:16:55.288Z,[],True,the-pleasure-of-making-things,2000-01-01,[],The Pleasure of Making Things,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:38.966Z,"We used 15 months of data from 1.5 million people to show that four points--approximate places and times--are enough to identify 95 percent of individuals in a mobility database. Our work shows that human behavior puts fundamental natural constraints on the privacy of individuals, and these constraints hold even when the resolution of the dataset is low. These results demonstrate that even coarse datasets provide little anonymity. We further developed a formula to estimate the uniqueness of human mobility traces. These findings have important implications for the design of frameworks and institutions dedicated to protecting the privacy of individuals.",2016-10-22,"['human-dynamics', 'collective-learning']",https://dam-prod2.media.mit.edu/x/files/Display/mobility.png,23407,--Choose Location,2016-12-05T00:17:00.666Z,"['hidalgo@media.mit.edu', 'yva@media.mit.edu']",True,the-privacy-bounds-of-human-mobility,2012-09-01,[],The Privacy Bounds of Human Mobility,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:12:39.062Z,"Like many institutions, MIT is thinking about how to involve the MIT community in its space in Second Life. The Projects is a collaborative building system in Second Life, consisting of configurable pods owned by MIT community members that connect together into a high-level structure. The design of these pods and their interconnection has a number of important challenges unique to working in a three-dimensional, virtual environment like Second Life: presence, history, search, and navigation. Based on sensors embedded in the pods and rules about how pods can be connected, we can provide socially based services that will form the backbone of this virtual architecture.",2007-09-01,['sociable-media'],,23415,E15-383,2016-12-05T00:16:55.409Z,['judith@media.mit.edu'],True,the-projects,2007-01-01,[],The Projects,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:39.155Z,"The Reflectory is a storytelling and learning interface for children whose goal is to provide an environment in which children generate their own educational content by sharing personal written stories with a peer. Two children use the Reflectory to write and to exchange letters that together spark the appearance of an historical biography relevant to both children's stories. To create a feeling of privacy and security, the Reflectory's touch-sensitive screen has been encased in a traditional hard-cover volume. Opening this innocuous-looking book reveals a visually rich and engaging screen onto which children write their stories directly, using a stylus. The Reflectory will continue to develop as we work with children to discover how technologies that encourage and enhance collaborative storytelling can be powerful tools for self-expression, personalized learning, and language development.",2001-12-30,['gesture-and-narrative-language'],,23423,E15-320,2016-12-05T00:16:55.310Z,[],True,the-reflectory,1999-12-31,[],The Reflectory,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:39.246Z,"The Relative Size of Things is a low-cost 3D scanner for the microscopic world. It combines a webcam, a three-axis computer-controlled plotter, and image processing to merge hundreds of photographs into a single three-dimensional scan of surface features which are invisible to the naked eye.",2013-09-01,['fluid-interfaces'],,23431,--Choose Location,2016-12-05T00:17:00.732Z,"['pattie@media.mit.edu', 'marcelo@media.mit.edu']",True,the-relative-size-of-things,2011-01-01,[],The Relative Size of Things,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:39.429Z,"The Sound of Touch is a new instrument for real-time capture and sensitive physical stimulation of sound samples using digital convolution. The hand-held wand can be used to (1) record sound, then (2) brush, scrape, strike or otherwise physically manipulate this sound against physical objects. These actions produce sound in a manner that leverages peoples existing intuitions about sonic properties of physical materials. The Sound of Touch permits real-time exploitation of the sonic properties of a physical environment, to achieve a rich and expressive control of digital sound that is not typically possible in electronic sound synthesis and control systems.",2008-09-01,['fluid-interfaces'],,23447,E15-320,2016-12-05T00:17:03.736Z,['pattie@media.mit.edu'],True,the-sound-of-touch,2007-01-01,[],The Sound of Touch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:39.681Z,"The Tower is a modular development system for designing and prototyping computational devices. Physically, the Tower consists of a primary foundation layer with a central processor. At this point, two foundations are available, equipped with either the PIC16F877 microcontroller from Microchip Inc.�, or the Rabbit� 2300 processor from Rabbit Semiconductor�. Additional modules can be added to the stack as needed by specific applications, and a special prototyping layer allows for simplified design of new modules for the system. Currently, layers are being produced that allow for sensor readings, servo-motor control, MIDI music playback, data collection, network connectivity, and RF and infrared communication. In addition to the currently available layers, we are actively designing a set of hardware and software development tools to make it easy for anyone to add their own layers to the system as an application's needs require.",2004-01-01,['personal-fabrication'],,23471,E15-344,2016-12-05T00:17:23.839Z,[],True,the-tower-a-modular-electronics-design-environment,2001-01-01,[],The Tower: A Modular Electronics Design Environment,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:39.762Z,Themail visualizes the content of a person's email archive over time. The project focuses on the unique content found in interactions between the owner of the email archive and each contact.,2017-05-01,['sociable-media'],,23479,E15-383,2022-06-10T14:41:30.109Z,['judith@media.mit.edu'],True,themail,2005-01-01,[],Themail,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:39.840Z,We are using intelligent interfaces to spatially organize streams of incoming information into coherent themes. Theme Stream helps us to deal with the ever-increasing streams of information generated around us by applying both supervised and unsupervised machine-learning techniques to the layout of information in the user interface.,2010-01-01,['fluid-interfaces'],,23487,E15-320,2016-12-05T00:16:11.893Z,['pattie@media.mit.edu'],True,theme-stream-visualizing-complex-time-based-information,2009-01-01,[],Theme Stream: Visualizing Complex Time-Based Information,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:39.918Z,"This work focuses on bringing powerful concepts from wave optics to the creation of new algorithms and applications for computer vision and graphics. Specifically, ray-based, 4D lightfield representation, based on simple 3D geometric principles, has led to a range of new applications that include digital refocusing, depth estimation, synthetic aperture, and glare reduction within a camera or using an array of cameras. The lightfield representation, however, is inadequate to describe interactions with diffractive or phase-sensitive optical elements. Therefore we use Fourier optics principles to represent wavefronts with additional phase information. We introduce a key modification to the ray-based model to support modeling of wave phenomena. The two key ideas are ""negative radiance"" and a ""virtual light projector."" This involves exploiting higher dimensional representation of light transport.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/logo_ALF.png,23495,E15-320,2019-04-19T18:36:26.091Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,theory-unifying-ray-and-wavefront-lightfield-propagation,2009-01-01,[],Theory unifying ray and wavefront lightfield propagation,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:40.087Z,"Two of the most important traits of environmental hazards today are their invisibility and the fact that they are experienced by communities, not just individuals. Yet we don't have a good way to make hazards like chemical pollution visible and intuitive.  SeeBoat and the thermal fishing bob seek to visceralize rather than simply visualize data by creating a physical data experience that makes water pollution data present in communities. SeeBoat is a remote control boat with sensors (temperature, turbidity, conductivity, pH) that measure local water quality and LEDs that display the data on site by changing color in real time. Data is also logged to be physically displayed elsewhere and can be further recorded using long-exposure photos. Making environmental data experiential and interactive will help both communities and researchers better understand pollution and its implications.The Thermal Fishing Bob is an early version of this tool that has a spherical form factor and focuses on measuring water temperature as a marker for combined sewer overflows (CSOs) that may pollute rivers.  This project began in partnership with Sara Wylie (Northeastern University) in Spring 2015. Early work included Thermal Fishing Bob workshops, design iteration, prototyping, system testing with users in the Mystic River and Charles River, long exposure photography events, and further concept development. In Spring of 2017, Perovich and Wylie began a collaboration with Roseann Bongiovanni of GreenRoots, an environmental justice community group in Chelsea, MA, to test and iterate on the devices so they best suit the environmental and social context in the local community.  As part of this process, Perovich continued to develop the technical side of the project to create SeeBoat, a remote control boat based system, including sensors for turbidity, conductivity, pH, radio based data communication, and designs for and early implementation of an Android app for collecting and viewing quantitative sensor data. Perovich, Wylie, and Bongiovanni are also pursuing related routes of research and community engagement around open access environmental data, the politics of space, community based data installations, and evaluating individual and group learning through extended participatory action research projects. A publication describing their first year of collaboration can be found in their paper:  Laura J. Perovich, Sara Wylie, Roseann Bongiovanni (2018) Pokémon Go, pH, and projectors: Applying transformation design and participatory action research to an environmental justice collaboration in Chelsea, MA, Cogent Arts & Humanities, 5:1, 1-22. (Link to PDF.) In July of 2018, the team began to collaborate with high school students and staff at the Microsoft Garage Makerspace to test the ease of fabrication of SeeBoat in a more general audience and to continue development of the SeeBoat Android app for numeric data display. Thanks to ECO, David Ortiz, Adela Gonzalez, Leo Martinez, GreenRoots staff, Don Blair, Catherine D’Ignazio, the Boston University Law Clinic, and Dr. Sharon Harlan for their support and input on this project. Thanks to MIT undergraduates Sophia Struckman, Rod Bayliss, Robert Henning, and Claudia Chen who contributed to the technical aspect of these workshops and citizen science tool development, photographers Jorge Valdez and Shirin Adhami,  the Wylie Lab at Northeastern University, Dr. V. Michael Bove and members of the Object-Based Media group at the MIT Media Lab, the MIT Arts Scholars, the Public Lab community,  Mare Librum, the MIT Sailing Pavilion, and the Council for the Arts at MIT.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/files/Display/IMAG3993.jpg,23511,--Choose Location,2022-06-10T14:09:49.237Z,['perovich@media.mit.edu'],True,thermal-fishing-bob-in-place-environmental-data-visualization,2015-01-01,"['art', 'environment', 'sensors']",SeeBoat (Thermal Fishing Bob): In-place environmental data visualization,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:40.247Z,"While technologists scramble to develop technologies for production and storage of environmentally friendly electricity, we must consider our personal roles in conserving energy. Thighmaster balances comfort and discomfort to achieve sustainable change. In addition to potentially decreasing a user's energy use, Thighmaster can also relieve the less easily measured�but no less real�feeling of individual powerlessness in the face of accelerated climate change. The system consists of a personal techno-garter, inspired by the ""Opus Dei"" cilice popularized in Dan Brown's novel The DaVinci Code, worn on the thigh, that communicates wirelessly to a set of low-power sensors measuring the wearer's personal energy consumption. If the wearer's electricity use exceeds a certain limit, the device plunges stainless-steel thorns into the thigh to remind users of both their complicity in the planet's demise and mortality.",2008-01-01,['computing-culture'],,23527,E15-020D,2016-12-05T00:16:55.569Z,['csik@media.mit.edu'],True,thighmaster,2007-01-01,[],Thighmaster,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:12:40.722Z,"This Is How is a platform for connecting makers with small businesses through stories. Small businesses share their stories in the form of video bytes in which they explain what they do and why, what their requirements and constraints are, and what kinds of issues they have. Makers can then annotate the video, ask further questions, and propose solutions for issues. The video is passed through SuperGlue for annotation and to categorize and find commonalities among requests.",,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/milky_video.png,23575,--Choose Location,2016-12-05T00:16:55.625Z,"['weller@media.mit.edu', 'lip@media.mit.edu']",True,this-is-how,2016-09-01,[],This Is How,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:40.871Z,"Search engines often return millions of results to a query. Organizing these results is a challenge, particularly for visual imagery. Search engines typically use textual annotations rather than the visual characteristics of the images to perform the search. ThoughtSort uses gaze detection and inferred points of interest to dynamically adjust the results of a search query. The user implicitly steers the system by showing more visual interest in some results than others. With ThoughtSort, search becomes a more dynamic experience as results self-adjust before the user's eyes. This application is part of a framework which aims to provide developers with the necessary tools to create dynamic and considerate content that can adjust to the natural responses of the user.",2012-01-01,['information-ecology'],,23591,,2016-12-05T00:16:55.687Z,['holtzman@media.mit.edu'],True,thoughtsort,2011-09-01,[],ThoughtSort,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:40.946Z,"MRI and CAT-scan cameras gather 3-D data, but holography offers the only way of examining those images in fully 3-D form. This project explores the new image-processing, editing, and rendering tools that are needed to make these complex 3-D images quickly and accurately interpretable by physicians.",2004-01-01,[],,23599,E15-441,2016-12-05T00:17:24.032Z,[],True,three-dimensional-capture-and-virtual-cinematographer,2003-01-01,[],Three-Dimensional Capture and Virtual Cinematographer,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:41.023Z,"Much social interaction goes on at a door's threshold: you poke your head in for a quick chat with a colleague, or you pass through to enter a meeting or to follow up a previous conversation. Sensors and a voice recognition system outside and inside the digitally enhanced threshold indicate a person's approach and identity. This ""smart"" threshold can also be used to help manage meeting size or meeting scheduling. It does so by using electronically enhanced doormats that would know and communicate how many people enter or leave a room. Voice interface can identify visitors and reschedule them if the room's occupant is not there. A graphical interface in the room can let the room's owner approve the entrance of a visitor.",2001-09-01,['context-aware-computing'],,23607,E15-320,2016-12-05T00:16:55.711Z,[],True,threshold,2000-01-01,[],Threshold,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:41.099Z,"Throwing Light is an exploration into computational display systems that surpass the traditional rectangular screen. Using conventional, off-the-shelf video projectors and surveillance cameras, combined with robotic mounts and CG generated images, our system will throw a parallax-corrected projected image anywhere in an architectural space. The system will allow for a tight interaction between space, mobile viewers, and the output. While our initial applications are directed towards generating narratives that are linked to a specific site, there are many potential implications for human-computer interactions, such as augmented reality applications, mobile work-screens, and real-world agents.",2004-01-01,['computing-culture'],,23615,E15-20C,2016-12-05T00:17:24.059Z,['csik@media.mit.edu'],True,throwing-light,2001-01-01,[],Throwing Light,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:41.329Z,"Tid'Zam is an ambient sound analysis system for outdoor environments. It is a component of the Tidmarsh Farms project which monitors the environmental evolution of an industrial cranberry farm during its ecological restoration of wetland. Tid'Zam analyzes the audio streams generated by the deployed microphones in the wild in order to detect the sonic events happening on the site, such as bird calls, insects, frogs, rain, storms, car noise, human voices, and more. This system is used to cross-validate other sensors for weather monitoring to identify, geolocalize, and track present wildlife and bird specimens over time. It also controls the audio mixers in order to mute or change the gain on noisy microphones.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2017/05/17/Capture du 2017-03-29 16-26-01_mOqXb8e.png.1400x1400.jpg,23639,--Choose Location,2017-10-02T15:14:53.312Z,"['ddh@media.mit.edu', 'duhart@media.mit.edu', 'joep@media.mit.edu', 'gershon@media.mit.edu', 'sfr@media.mit.edu']",True,tidzam,2016-01-01,[],Tid'Zam,PUBLIC,http://deep-resenv.media.mit.edu:8080,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:42.036Z,"The Tiles were a construction kit, in which powerful computation and communication were built into kid-scale blocks. Children experimented with dynamic patterns, wrote programs that moved from Tile to Tile, and  created rich arrangements of overlapped atoms and bits. This project grew out of our research into mobile code approaches to network programming. Our  goal was to create ""communities of toys,"" creating new possibilities for interactive play. 
",1999-09-01,['lifelong-kindergarten'],,23711,E15-120h,2016-12-05T00:16:55.873Z,['mres@media.mit.edu'],True,tiles,1995-01-01,[],Tiles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:42.116Z,This shape-sensing quilt consists of a communicating grid of textile tilt sensors.  It showcases the wide variety of techniques we have developed for constructing fabric-based sensors and circuits.,2011-01-01,['high-low-tech'],,23719,E15-368,2016-12-05T00:17:24.110Z,['leah@media.mit.edu'],True,tilt-sensing-quilt,2010-01-01,[],Tilt-Sensing Quilt,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:42.193Z,"Time Out is an experimental user interface system for addressing cyberbullying on social networks. A Reflective User Interface (RUI) is a novel concept to help users consider the possible consequences of their online behavior, and assist in intervention or mitigation of potentially negative/harmful actions.",2014-09-01,['software-agents'],,23727,,2022-06-10T14:18:57.667Z,"['lieber@media.mit.edu', 'kdinakar@media.mit.edu']",True,time-out-reflective-user-interface-for-social-networks,2011-01-01,[],Time Out: Reflective User Interface for Social Networks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:12:42.273Z,"Computer games and learning environments increasingly involve humans relating to computational avatars and robots. Intelligence-modeling software requires real-time interaction between the parts, with channel capacities that must match the best of human-human performance. Many multi-modal activities challenge the real-time communication and comprehension speeds between participants. This project aims to enhance human-machine and machine-machine communication capacities between entities in order to encourage new models of interaction. ",2009-09-01,['music-mind-and-machine'],,23735,E15-485A,2016-12-05T00:17:24.338Z,"['bv@media.mit.edu', 'jmaloney@media.mit.edu']",True,time-critical-networks-for-interaction-design,2008-01-01,[],Time-Critical Networks for Interaction Design,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:12:42.349Z,"Our architecture takes a hybrid approach to microwaves and treats them like waves of light. Most other work places antennas in a 2D arrangement to directly sample the RF reflections that return. Instead of placing antennas in a 2D arrangment, we use a single, passive, parabolic reflector (dish) as a lens. Think of every point on that dish as an antenna with a fixed phase-offset. This means that the lens acts as a fixed set of 2D antennas which are very dense and spaced across a large aperture. We then sample the focal-plane of that lens. This architecture makes it possible for us to capture higher resolution images at a lower cost.",,['camera-culture'],,23743,--Choose Location,2016-12-05T00:16:55.816Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'michaf@media.mit.edu']",True,time-of-flight-microwave-camera,2014-09-01,[],Time-of-Flight Microwave Camera,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:42.435Z,"To maintain our relationships with family and friends, it's important to share our daily lives. TimelessWords is an integrated, calendar-based life-sharing system to enhance emotion-driven daily communication. Users can easily pre-record voice messages via a handheld device, and send the messages to other community members. TimelessWords users have a shared calendar. Messages will be delivered at scheduled moments to express timeless care.",2008-01-01,['living-mobile'],,23751,E15-384C,2017-11-15T17:28:00.712Z,['geek@media.mit.edu'],True,timelesswords,2007-09-01,[],TimelessWords,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:42.512Z,"Urban spaces have different uses and different histories for the people who move through and use them. Those layered histories can enrich our understanding of spaces and provide common ground for collaboration and understanding for people... but only if we can make them visible. Timenesia allows people to annotate the spaces they live in, recording their stories and memories and attaching them to locations in a neighborhood. When you encounter a Timenesia mark, you can explore it via calling a toll-free phone number, by scanning a QR code, or by taking a tour using Google Maps or a custom smart phone application. After exploring memories and meanings of a specific place, you can add your reflections and annotations to the conversation. The system is in use in Dorchester, MA to offer tours of the Field's Corner neighborhood. ",2012-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/logo-square.gif,23759,,2016-12-05T00:17:24.178Z,"['ethanz@media.mit.edu', 'leob@media.mit.edu', 'rahulb@media.mit.edu']",True,timenesia,2011-01-01,[],Timenesia,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:42.659Z,"Distributed meetings present a set of interesting challenges to staying engaged and involved. Because one person speaks at a time, it is easy (particularly for remote participants) to disengage from the meeting undetected. However, non-speaking roles in a meeting can be just as important as speaking ones, and if we could give non-speaking participants ways to contribute, we could help support better-run meetings of all kinds. Tin Can collects background tasks like taking notes, managing the agenda, sharing relevant content, and tracking to-dos in a distributed interface that uses meeting participants' phones and laptops as input devices, and represents current meeting activities on an iPad in the center of the table in each meeting location. By publicly representing these background processes, we provide meeting attendees with new ways to contribute and be recognized for their non-verbal participation.",2012-01-01,['living-mobile'],,23775,--Choose Location,2016-12-05T00:16:55.897Z,['geek@media.mit.edu'],True,tin-can,2010-01-01,[],Tin Can,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:42.737Z,"Classroom discussions may not seem like an environment that needs a new kind of supporting technology. But we've found that augmenting classroom discussions with an iPad-based environment to help promote discussion, keep track of current and future discussion topics, and create a shared record of class keeps students engaged and involved with discussion topics, and helps restart the discussion when conversation lags. Contrary to what you might expect, having another discussion venue doesn't seem to add to student distraction; rather it tends to focus distracted students on this backchannel discussion. For the instructor, our system offers powerful insights into the engagement and interests of students who tend to speak less in class, which in turn can empower less-active students to contribute in a venue in which they feel more comfortable.",2012-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/files/Display/potential_logo_2_small.png,23783,--Choose Location,2016-12-05T00:16:55.989Z,['geek@media.mit.edu'],True,tin-can-classroom,2011-01-01,[],Tin Can Classroom,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:42.818Z,"TinkRBook is a storytelling system that introduces a new concept of reading, called textual tinkerability. Textual tinkerability uses storytelling gestures to expose the text-concept relationships within a scene. Tinkerability prompts readers to become more physically active and expressive as they explore concepts in reading together. TinkRBooks are interactive storybooks that prompt interactivity in a subtle way, enhancing communication between parents and children during shared picture-book reading. TinkRBooks encourage positive reading behaviors in emergent literacy: parents act out the story to control the words onscreen, demonstrating print referencing and dialogic questioning techniques. Young children actively explore the abstract relationship between printed words and their meanings, even before this relationship is properly understood. By making story elements alterable within a narrative, readers can learn to read by playing with how word choices impact the storytelling experience. Recently, this research has been applied in developing countries.",2014-12-31,['personal-robots'],,23791,E15-468,2017-05-31T18:21:26.258Z,"['cynthiab@media.mit.edu', 'dnunez@media.mit.edu']",True,tinkrbook-reinventing-the-reading-primer,2009-01-01,[],TinkRBook: Reinventing the Reading Primer,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:42.898Z,"TinyProjector is a very small portable character projector, based on inexpensive laser diodes, projecting a single line of text onto nearby walls and tables. TinyProjector is useful for projecting text from portable and wearable devices such as cell phones and PocketPCs that will be connected, for example, via a wireless serial connection.",2003-01-01,['living-mobile'],,23799,E15-368,2016-12-05T00:16:55.921Z,"['stefanm@media.mit.edu', 'geek@media.mit.edu']",True,tinyprojector,2000-01-01,[],TinyProjector,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:43.184Z,"ToDoGo is a system for managing to-do lists on mobile devices. ToDoGo understands how to-do list entries relate to a user's everyday life activities, and can give them location-aware help in scheduling events and finding places to accomplish tasks.",2008-09-01,['software-agents'],,23831,E15-383,2016-12-05T00:17:24.283Z,['lieber@media.mit.edu'],True,todogo,2008-01-01,[],ToDoGo,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:43.734Z,"Complex biological systems such as brain circuits are extended 3-D structures made out of nanoscale building blocks such as proteins, RNAs, and lipids, which are often organized with nanoscale precision. This presents a fundamental tension in biology—to understand a biological system like a brain circuit, you might need to map a large diversity of nanoscale building blocks, across an extended spatial expanse. We are developing a new suite of tools that enable the mapping of the location and identity of the molecular building blocks of complex biological systems such as the brain, aiming to map out the architecture of such systems with enough precision to understand how the structures of biological systems lead to function and dysfunction. One of the technologies we are developing, expansion microscopy (ExM), enables large 3D objects to be imaged with nanoscale precision, by physically expanding preserved biological systems (in contrast to all previous microscopies, that magnify light from the sample via lenses). We are working to improve expansion microscopy further, and are working, often in interdisciplinary collaborations, on a suite of new labeling and analysis techniques that exploit the biochemical freedom enabled by the expanded state. We are also applying expansion microscopy to the scalable mapping of complex biological systems, including brain circuits. Such brain circuit maps may be detailed enough to enable detailed computer simulations of neural circuits. Finally, we are extending and applying such tools to the early detection and understanding of complex diseases such as cancers and autoimmune diseases, and to the analysis of aging.",2020-07-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/2016/07/15/mapping.large.jpg,23887,--Choose Location,2020-09-10T17:27:11.570Z,['esb@media.mit.edu'],True,tools-for-mapping-the-molecular-structure-of-the-brain,2012-01-01,"['art', 'artificial-intelligence', 'bioengineering', 'civic-media', 'cognition', 'health', 'neurobiology', 'synthetic-biology', 'imaging', 'social-science', 'biology', 'biomechanics', 'technology', 'cognitive-science', 'biotechnology', 'engineering']",Tools for mapping the molecular architecture and wiring of the brain,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:43.807Z,"The brain is a three-dimensional, densely-wired circuit that computes via large sets of widely distributed neurons interacting at fast timescales. In order to understand the brain, ideally it would be possible to observe the electrical activity, and other intra- and intercellular signaling pathways, of many neurons—and ideally entire brains—with as great a degree of precision as possible, so as to understand the neural codes and dynamics that are produced by the circuits of the brain. Our lab and our collaborators are developing a number of innovations—such as new fluorescent reporters of cellular signals such as voltage, and new robotic and nanotechnological probes—to enable such analyses of neural circuit dynamics. These tools will hopefully enable pictures of how neurons work together to implement brain computations, and how these computations go awry in brain disorder states. Such neural observation strategies may also serve as detailed biomarkers of brain disorders or indicators of potential drug side effects. These technologies may, in conjunction with optogenetics, enable closed-loop neural control technologies, which can introduce information into the brain as a function of brain state (""brain co-processors""), enabling new kinds of circuit characterization tool as well as new kinds of advanced brain-repair prosthetic. To build these tools, we are developing supporting approaches such as robots and molecular strategies for multidimensional directed evolution of protein-based tools in mammalian cells.",2020-07-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/2016/07/18/20120506135845-0_0.jpg,23895,--Choose Location,2020-09-10T17:28:06.441Z,['esb@media.mit.edu'],True,tools-for-recording-high-speed-brain-dynamics,2011-01-01,"['robotics', 'bioengineering', 'health', 'neurobiology', 'synthetic-biology', 'imaging', 'cognitive-science', 'engineering']",Tools for recording high-speed brain dynamics,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:43.881Z,"Our translation assistant applies common-sense logic to the problem of translating speech in real time from one language to another. Using speech recognition combined with a software translator to do word-by-word translation is not feasible because speech recognition is notorious for poor results. Word-by-word translation requires grammatically correct input to translate accurately. Therefore, translating speech that is potentially already fraught with errors is not expected to be good. Our translation assistant works around these problems by using the context of the conversation as a basis for translation; it takes the location and the speaker as input to establish the circumstances, and then uses a common-sense knowledge network to do topic-spotting using key words from the conversation. It only translates the most likely topics of conversation into the target language.  This system does not require perfect speech recognition, yet enables end-users to have a sense of the conversation.",2005-09-01,['living-mobile'],,23903,E15-384C,2016-12-05T00:17:24.439Z,"['lieber@media.mit.edu', 'geek@media.mit.edu']",True,topic-spotting-common-sense-translation-assistant,2004-09-01,[],"Topic-Spotting, Common-Sense Translation Assistant",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:43.959Z,"Topobo is a 3D constructive assembly system embedded with kinetic memory�the ability to record and play back physical motion. Unique among modeling systems is Topobo�s coincident physical input and output behaviors. By snapping together a combination of passive (static) and active (motorized) components, users can quickly assemble dynamic, biomorphic forms such as animals and skeletons, animate those forms by pushing, pulling, and twisting them, and observe the system repeatedly playing back those motions. For example, a dog can be constructed and then taught to gesture and walk by twisting its body and legs. The dog will then repeat those movements.",2005-01-01,['tangible-media'],,23911,E15-344,2016-12-05T00:16:56.107Z,['ishii@media.mit.edu'],True,topobo,2003-01-01,[],Topobo,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:44.037Z,"This project provides new ways to work with Topobo motion sequences. The Backpacks are a series of physical components with a knob that can be attached to a Topobo Active. The Backpacks will modify the way your recorded motion plays back. For instance, by turning the knob on the Bigger/Smaller (Amplitude) Backpack, you can make the motion larger or smaller. Other Backpacks include ""Time Delay,"" ""Faster-Slower,"" and ""Position Offset."" The Position Offset Backpack has light sensors instead of a knob, allowing users to build creatures that can walk towards or away from light. The Backpacks introduce ideas about symbolic representation and feedback to Topobo.",2007-09-01,['tangible-media'],,23919,E15-344,2016-12-05T00:17:24.457Z,['ishii@media.mit.edu'],True,topobo-backpacks,2004-01-01,[],Topobo Backpacks,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:44.277Z,"One of the most difficult aspects of using Radio-Frequency Identification chips is determining what tag is being read when. By sending the energy, as well as the data, to interrogate a tag through the human body, it is possible to associate gestures with tags. This allows user interfaces to be based on manipulating tangible icons, and makes it possible to pick up the bit associated with an object at the same time you pick up its atoms.",2001-12-30,['physics-and-media'],,23943,,2016-12-05T00:16:56.178Z,['neilg@media.mit.edu'],True,touch-tags,1997-12-31,[],Touch Tags,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:44.352Z,"The Touch-Phone was developed to explore the use of objects to mediate the emotional exchange in interpersonal communication. Through an abstract visualization of screen-based color changes, a standard telephone is modified to communicate how it is being held and squeezed. The telephone receiver includes a touch-sensitive surface which conveys the user's physical response over a computer network. The recipient sees a small colored icon on his computer screen which changes in real time according to the way his conversational partner is interacting with the telephone object.  
",2001-12-30,['affective-computing'],,23951,E15-383,2016-12-05T00:16:56.209Z,['picard@media.mit.edu'],True,touch-phone,1997-12-31,[],Touch-Phone,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:44.428Z,"TouchCounters are computational tags that track the use of physical objects. TouchCounters sense activity through magnetic, acceleration, and infrared sensors, and indicate their status on bright LED displays. Through magnetic snap connectors, TouchCounters can be networked to a Web server that generates use histograms for each object. Just as ""hit counters"" indicate frequency of use for Web pages, TouchCounters indicate usage frequency for physical objects.",2000-12-30,['tangible-media'],,23959,E15-441,2016-12-05T00:17:24.490Z,['ishii@media.mit.edu'],True,touchcounters-interactive-electronic-labels,1997-12-31,[],TouchCounters: Interactive Electronic Labels,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:45.106Z,"Touch�Sensitive is a wearable device that leverages stress and provides comfort for people on the move. It is determined by how the body reacts to the bag-load user's support, and we position this research within wearable devices that consider body specifics in their design. A series of observations using ethnographic methods informed us of how our apparel can soothe people who suffer from bag overload and can connect to the technologies users carry.",2009-01-01,['tangible-media'],,23991,E15-350,2016-12-05T00:16:56.253Z,['ishii@media.mit.edu'],True,touchsensitive,2007-01-01,[],Touch�Sensitive,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:45.277Z,"A new method to detect and distinguish between different types of fluorescent materials. The suggested technique has provided a dramatically larger depth range compared to previous methods; thus it enables medical diagnosis of body tissues without removing the tissue from the body, which is the current medical standard. It uses fluorescent probes, which are commonly used in medical diagnosis. One of these parameters is the fluorescence lifetime, that is the average time the fluorescence emission lasts. The new method can distinguish between different fluorescence lifetimes, which allows diagnosis of deep tissues. Locating fluorescence probes in the body using this method can, for example, indicate the location of a tumor in deep tissue, and classify it as malignant or benign according to the fluorescence lifetime, thus eliminating the need for X-ray or biopsy.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2018/03/29/sketch.jpg,23999,--Choose Location,2018-03-29T20:34:54.336Z,"['raskar@media.mit.edu', 'guysatat@media.mit.edu', 'barmak@media.mit.edu']",True,towards-in-vivo-biopsy,2014-09-01,[],Towards In-Vivo Biopsy,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:45.453Z,"Toy Symphony combines children, virtuosic soloists, composers, and symphony orchestras around the world to alter radically how children are introduced to music, as well as to redefine the relationship between professional musicians and young people. A complete set of Music Toys will be distributed to children in each host city (including Berlin, Dublin, Glasgow, Manchester/London, and Tokyo), where children will be mentored to create their own sounds and compositions for toys and traditional instruments. A pedagogy for using these Music Toys to teach and to instill a love for musical creativity will also be developed. Final concerts will be presented in each host city including children's compositions and specially commissioned works by young composers, to be performed by children, soloists, and orchestra, playing Music Toys, Hyperinstruments, and traditional instruments.",2006-01-01,['opera-of-the-future'],,24007,E15-483,2016-12-05T00:16:56.277Z,"['tod@media.mit.edu', 'laird@media.mit.edu', 'tristan@media.mit.edu', 'ariane@media.mit.edu']",True,toy-symphony,2000-01-01,"['artificial-intelligence', 'music', 'ethics', 'technology']",Toy Symphony,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:45.597Z,"We are designing and testing an automatic system to calculate the usefulness of items on Web pages. Using this system, Web sites can adjust the order of presentation, for example of reviews on a book-selling site, so that the most interesting and useful reviews filter to the top. The system works by observing the dwell-times and locations of the cursor while users browse the site, and so requires no deliberate action on the part of its users or administrators. The system can be used passively to gather information, or to feed the interest levels of past users forward in order to improve the experience of future visitors.",2006-01-01,['fluid-interfaces'],,24015,E15-320,2016-12-05T00:16:56.309Z,"['pattie@media.mit.edu', 'holtzman@media.mit.edu', 'sajid@media.mit.edu']",True,tracking-usefulness,2006-09-01,[],Tracking Usefulness,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:45.737Z,"This project attempts to address the practical problems involved with extracting behavioral information from large, multi-camera video corpora.  Ultra-dense video recordings offer new possibilities for in-depth, quantitative analysis of human behavior, with applications ranging from child development research to determining how people are affected by different retail environments.  Despite the growing sophistication of computer vision systems being developed for person tracking, gesture recognition, and object identification, these technologies remain error prone.  Accurate video annotation still requires substantial human input.  In order to analyze the hundreds of thousands of hours of video collected for the Human Speechome Project, we have developed a new software system for semi-automatically annotating longitudinal, multi-track video data.  This system combines computer vision algorithms with a novel interface design to enable human annotators to generate and edit video annotations with speed and accuracy.",2010-09-01,['social-machines'],,24023,E15-441,2016-12-05T00:17:24.594Z,"['dkroy@media.mit.edu', 'decamp@media.mit.edu']",True,trackmarks-semi-automatic-video-annotation,2008-09-01,[],TrackMarks: Semi-Automatic Video Annotation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:45.877Z,"Trackmate is an open-source initiative to create an inexpensive, do-it-yourself tangible tracking system. The Trackmate tracker allows any computer to recognize tagged objects and their corresponding position, rotation, and color information when placed on a surface. With over 280 trillion possible visual markers, each tagged object can be unique in the world and work seamlessly across any Trackmate system.  Trackmate sends all object data via LusidOSC (a protocol layer for unique spatial input devices), allowing any LusidOSC-based application to use the spatial object information.",2009-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/trackmate_072hat.png,24031,E15-344,2016-12-05T00:16:56.231Z,"['ishii@media.mit.edu', 'labrune@media.mit.edu', 'daniell@media.mit.edu']",True,trackmate,2009-01-01,[],Trackmate,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:46.033Z,"In the tradition of baseball cards, Pogs, and Beanie Babies, we created a new generation of toys that let children trade bits instead of atoms. Imagine a child teaching her robotic toy a new dance step. Then, when she is playing with a friend, her toy can ""teach"" this new dance to her friend's toy. Later, her friend can modify this dance a little, and pass it on to another friend. The creator of the dance can check the Internet to see how far her dance has spread (""150 toys know my dance"" or ""It spread all the way to Japan!""). Trading digital (rather than physical) objects has distinct advantages: kids can more easily author, copy, modify, and trace them. We  studied how such toys can give children a richer understanding of their social network, and help them play a more active role in constructing ""kid culture."" We explored a wide range of Tradable Bits, including pieces of music, animations, stories, and digital creatures.",2001-09-01,['lifelong-kindergarten'],,24039,E15-120B,2016-12-05T00:16:11.966Z,"['mres@media.mit.edu', 'borovoy@media.mit.edu', 'bss@media.mit.edu']",True,tradable-bits,1998-01-01,[],Tradable Bits,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:46.377Z,"The Trainer Piano actively moves your fingers as you learn how to play. Employing active magnetic force below the keys of a working piano, the Trainer Piano provides users with kinesthetic input that augments their normal motor learning process. By providing a ""feel"" for what a user is supposed to play, the Trainer Piano minimizes the amount of time necessary to learn new motor patterns and acquire new motor skills. The core idea behind this project lies in the hypothesis that computer-controlled force can be used to teach students how to play an instrument at a faster and more efficient rate than would occur in an unaided learning environment.",2008-01-01,['opera-of-the-future'],,24055,E15-443,2016-12-05T00:16:56.360Z,['tod@media.mit.edu'],True,trainer-piano,2007-01-01,[],Trainer Piano,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:46.529Z,"We collaborated with NIAS (National Institute of Agricultural Science) to genetically engineer silkworms to develop new kinds of silk for future fashion. For an exhibition at Tokyo's Gucci Gallery, we designed a Nishijin-Kimono dress, working with NIAS's glowing silk (created by injecting the genes of a glowing coral and jellyfish into silkworm eggs) . ",2010-05-01,['design-fiction'],https://dam-prod2.media.mit.edu/x/files/Display/displayImage.png,24063,--Choose Location,2022-06-10T13:44:47.869Z,['sputniko@media.mit.edu'],True,tranceflora-amys-glowing-silk,2015-01-01,"['art', 'bioengineering', 'fashion', 'genetics']",Tranceflora—Amy's Glowing Silk,PUBLIC,http://sputniko.com/2015/04/amyglowingsilk,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:12:46.802Z,"The concept of story, or narrative, is closely associated with the understanding of self. Human beings use the narrative thread to interpret reality: what is happening to us in events, in our interaction with others, and in relation to a thing we construct�an object, a dialogue, a story. Interactive networks provide a means of connecting and communicating remotely. They also provide an opportunity for maintaining and comparing records of use or transactions. By embedding transactional hooks in digital environments, we can enhance the power of the communication, opening the door to porous story-scapes, collectibles and tradeables, affectively enhanced presentations, or economic pro-rating of responsive narrative.",2002-12-30,['interactive-cinema'],,24079,,2016-12-05T00:16:12.097Z,"['gid@media.mit.edu', 'barbara@media.mit.edu']",True,transactional-storytelling,1998-12-31,[],Transactional Storytelling,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:46.936Z,"TRANSFORM fuses technology and design to celebrate its transformation from still furniture to a dynamic machine driven by a stream of data and energy. TRANSFORM aims to inspire viewers with unexpected transformations and the aesthetics of the complex machine in motion. First exhibited at LEXUS DESIGN AMAZING MILAN (April 2014), the work comprises three dynamic shape displays that move over one thousand pins up and down in real time to transform the tabletop into a dynamic tangible display. The kinetic energy of the viewers, captured by a sensor, drives the wave motion represented by the dynamic pins. The motion design is inspired by dynamic interactions among wind, water, and sand in nature, Escher's representations of perpetual motion, and the attributes of sand castles built at the seashore. TRANSFORM tells of the conflict between nature and machine, and its reconciliation, through the ever-changing tabletop landscape.",,"['terrestrial-sensing', 'tangible-media']",https://dam-prod2.media.mit.edu/x/2016/10/24/DSC03216-retouched_RuWn9UJ.jpg,24087,--Choose Location,2023-08-11T16:55:33.816Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu']",True,transform,2013-09-01,"['design', 'human-computer-interaction', 'sensors', 'community', 'technology', 'assistive-technology']",TRANSFORM,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:47.142Z,"Introducing TRANSFORM, a shape-changing desk. TRANSFORM is an exploration of how shape display technology can be integrated into our everyday lives as interactive, transforming furniture. These interfaces not only serve as traditional computing devices, but also support a variety of physical activities. By creating shapes on demand or by moving objects around, TRANSFORM changes the ergonomics and aesthetic dimensions of furniture, supporting a variety of use cases at home and work: it holds and moves objects like fruit, game tokens, office supplies, and tablets, creates dividers on demand, and generates interactive sculptures to convey messages and audio.",,['tangible-media'],,24095,--Choose Location,2019-04-17T19:36:00.162Z,"['ishii@media.mit.edu', 'daniell@media.mit.edu', 'lajv@media.mit.edu', 'ken_n@media.mit.edu', 'viirj@media.mit.edu']",True,transform-adaptive-and-dynamic-furniture,2014-09-01,[],TRANSFORM: Adaptive and dynamic furniture,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:47.336Z,"The Transformative Copy Suite is a pair of applications that deal with the social history and evolution of media files. The first, Infinite Animation, is an exquisite-corpse style collaborative animation tool that allows users to collectively generate and revise an open-ended animation consisting of drawing and text; the second, Remote Whistles, is a mobile music project where users can record audio files with their cell phones and pass them on to friends. Each recipient can remix or rerecord parts of the piece.",2017-05-01,['sociable-media'],,24103,E15-389,2022-06-10T14:41:30.131Z,['judith@media.mit.edu'],True,transformative-copy-suite,2007-01-01,[],Transformative Copy Suite,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:47.689Z,"""Trash Talk"" is a distributed mesh network of decidedly inexpensive speaker nodes that play audio messages. Each node contains a microphone and speaker, and can be used to program the message that will be rebroadcast by the network. With several Trash Talk nodes, one can easily and quickly deploy a freely accessible messaging platform in public spaces. Nodes are intended to be cheap enough that the deployments can be considered disposable, and usable in spaces where authoritarian presences restrict democratic speech.",2008-01-01,['living-mobile'],,24119,E15-320,2016-12-05T00:16:56.407Z,['geek@media.mit.edu'],True,trash-talk,2007-01-01,[],Trash Talk,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:47.854Z,"A fundamental question has arisen out of our current effort to create systems for digital media content authoring over the Web: ""How do we manage all of this content?""  More importantly, how do we manage it efficiently over the Web in a cross-platform manner? A suite of core technologies is being developed that includes a universal secure login architecture, Web-based storage lockers, seamless back-end database connectivity and transfigurability of all digital media types, and an engine for visible common sense to enable a new kind of markup paradigm. Impact areas for this project include handheld devices, Web-hosting services, and digital video/still image management.",2005-01-01,['information-ecology'],,24127,E15-301,2016-12-05T00:16:29.892Z,[],True,treehouse-studio,2003-09-01,[],Treehouse Studio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:48.034Z,"This project explores the creation and use of a physical/digital  construction kit. Triangles is a physical computer interface in the form of a construction kit of identical, flat, plastic triangles. The triangles connect together both physically and digitally with magnetic, conducting connectors. When the pieces contact one another, specific connections can trigger specific digital events, allowing a simple but powerful means of physically interacting with digital information. Users can create both two- and three- dimensional patterns whose exact configuration is known to the computer. Triangles is a flexible system in that it can be reconfigured at hardware and software levels for a variety of uses. It is versatile, with applications from storytelling and education to rapid prototyping of human-computer interfaces.",2000-12-30,['tangible-media'],,24135,E15-441,2016-12-05T00:16:41.029Z,['ishii@media.mit.edu'],True,triangles,1995-12-31,[],Triangles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:48.180Z,"The Tribble project demonstrates the application of dense, distributed sensor networks to electronic skins. The Tribble is a one-foot diameter sphere tiled by a 32-node systolic sensor network, with each node capable of multi-modal sensing (pressure, light, sound, temperature) and actuation (glowing, purring). There is no central controller�all behavior evolves from communication between proximate patches of electronic skin. Current research on this platform is exploring algorithms for distributed sensor calibration.",2005-01-01,['responsive-environments'],,24143,E15-344,2016-12-05T00:17:24.658Z,['joep@media.mit.edu'],True,tribble-the-robotic-interactive-ball-based-living-entity,2003-01-01,[],Tribble: The Robotic Interactive Ball-Based Living Entity,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:48.324Z,"The proliferation of smartphones and wearable sensors is creating very large data sets that may contain useful information. However, the magnitude of generated data creates new challenges as well. Processing and analyzing these large data sets in an efficient manner requires computational tools. Many of the traditional analytics tools are not optimized for dealing with large datasets. Tributary is a parallel engine for searching and analyzing sensor data. The system utilizes large clusters of commodity machines to enable in-memory processing of sensor time-series signals, making it possible to search through billions of samples in seconds. Users can access a rich library of statistics and digital signal processing functions or write their own in a variety of languages.",2015-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/Tributary_logo.svg,24151,--Choose Location,2022-05-12T17:08:35.520Z,"['picard@media.mit.edu', 'yadid@media.mit.edu']",True,tributary,2013-09-01,[],Tributary,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:48.469Z,"Tricorder Net is a handheld sensor-network interface inspired by Star Trek's tricorder, a device that provided Science Officer Spock with information about the environment by simply pointing the device and twiddling its knobs. However, unlike Star Trek's tricorder, which had sensors embedded in the handheld device, Tricorder Net's intelligence comes from querying a sensor network distributed throughout the environment.",2008-09-01,['responsive-environments'],,24159,E15-344,2016-12-05T00:17:24.741Z,['joep@media.mit.edu'],True,tricorder-net-mobile-browsing-of-ubiquitous-sensor-network-information,2006-09-01,[],Tricorder Net: Mobile Browsing of Ubiquitous Sensor Network Information,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:48.653Z,"We have developed a camera system that captures movies at an effective rate of approximately one trillion frames per second. In one frame of our movie, light moves only about 0.6 mm. We can observe pulses of light as they propagate through a scene. We use this information to understand how light propagation affects image formation and to learn things about a scene that are invisible to a regular camera.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/tmp_frame128.jpg,24167,,2016-12-05T00:17:00.920Z,['raskar@media.mit.edu'],True,trillion-frames-per-second-camera,2011-01-01,[],Trillion Frames Per Second Camera,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:48.845Z,"Trisk is a humanoid robot that integrates speech input, visual perception, and active touch in order to interact with humans and its environment. It can understand and obey natural language commands, and will soon be able to answer questions. The robot is a platform for designing new algorithms and multimodal knowledge representations for sensory-motor grounded language use. This research takes steps towards social robots that can coordinate activities with human partners using natural language and gesture.",2008-09-01,['social-machines'],,24175,E15-483,2016-12-05T00:17:24.777Z,"['dkroy@media.mit.edu', 'soroush@media.mit.edu', 'tsourk@media.mit.edu']",True,trisk-a-conversational-robot,2004-01-01,[],Trisk: A Conversational Robot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:49.030Z,"The building blocks we grow up with and the coordinate systems we are introduced to at an early age shape the design space with which we think. Complex systems are difficult to understand because they often require transition from one coordinate system to another. We could even begin to say that empathy is precisely this ability to map easily to many different coordinates. Troxes is a building blocks kit based on the triangle, where kids get to build their building blocks and then assemble Platonic and Archimedean solids.",2017-06-01,['playful-systems'],https://dam-prod2.media.mit.edu/x/2016/12/14/trox_header_wide.png,24183,--Choose Location,2022-06-10T14:14:49.053Z,['jbobrow@media.mit.edu'],True,troxes,2014-09-01,[],Troxes,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:49.203Z,"We are developing a system for inferring safety context on construction sites by fusing data from wearable devices, distributed sensing infrastructure, and video. Wearable sensors stream real-time levels of dangerous gases, dust, noise, light quality, precise altitude, and motion to base stations that synchronize the mobile devices, monitor the environment, and capture video. Context mined from these data is used to highlight salient elements in the video stream for monitoring and decision support in a control room. We tested our system in a initial user study on a construction site, instrumenting a small number of steel workers and collecting data. A recently completed hardware revision will be followed by further user testing and interface development.",2013-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/logo2.png,24191,--Choose Location,2016-12-05T00:16:12.131Z,"['gershon@media.mit.edu', 'bmayton@media.mit.edu', 'joep@media.mit.edu']",True,truss-tracking-risk-with-ubiquitous-smart-sensing,2011-01-01,[],TRUSS: Tracking Risk with Ubiquitous Smart Sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:49.376Z,"Truth Goggles attempts to decrease the polarizing effect of perceived media bias by forcing people to question all sources equally by invoking fact-checking services at the point of media consumption. Readers will approach even their most trusted sources with a more critical mentality by viewing content through various ""lenses"" of truth.",2013-09-01,['information-ecology'],,24199,--Choose Location,2018-05-14T18:06:26.935Z,"['holtzman@media.mit.edu', 'schultzd@media.mit.edu']",True,truth-goggles,2011-01-01,[],Truth Goggles,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-07-15T00:12:49.877Z,"Bringing broadcast to the Internet redefines it to a participatory medium. Centralized distribution does not scale for technical and economic reasons; instead, we distribute distribution to the receivers. We do this with Rewire, a dynamic protocol with a completely distributed architecture that has evolved from the VidTorrent project. The protocol builds an elastic, self-healing overlay that scales organically, with no upfront investment. Latency and effects on the network are minimized with gradual self-optimization.",2008-01-01,['viral-communications'],,24223,E15-483,2016-12-05T00:16:56.502Z,['lip@media.mit.edu'],True,tv-rewired,2007-01-01,[],"TV, Rewired!",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:50.056Z,"TViews is an integrated display and interaction table designed for collaborative exploration of multimedia applications in home and classroom environments. The platform is extensible at both hardware and software levels, and invites a variety of interactive applications including media content browsing, multi-viewpoint storytelling, and narrative simulation games. This work was supported by Samsung Electronics.",2005-01-01,['media-fabrics'],,24231,E15-368,2016-12-05T00:16:56.530Z,['gid@media.mit.edu'],True,tviews,2001-01-01,[],TViews,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:50.228Z,"Twinkle lets you program using crayons, LEGO bricks, or anything that has colors, like a striped shirt or fall leaves. Compose a song with markers, program a robot by drawing instructions on paper, or create a custom interface just by doodling.",2010-01-01,['lifelong-kindergarten'],,24239,LEGO Learning Lab,2016-12-05T00:16:56.560Z,"['mres@media.mit.edu', 'silver@media.mit.edu', 'ericr@media.mit.edu']",True,twinkle,2009-01-01,[],Twinkle,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:50.398Z,"The vast amounts of user-generated content on the Web produce information overload as frequently as they provide enlightenment. Twitter Weather reduces large quantities of text into meaningful data by gauging its emotional content. This Website visualizes the prevailing mood about top Twitter topics by rendering a weather-report-style display. Comment Weather is its counterpart for article comments, allowing users to gauge sentiment without leaving the page. Supporting Twitter Weather is a user-trained Web service that aggregates and visualizes attitudes on a topic.",2013-09-01,['information-ecology'],,24247,E15-301,2016-12-05T00:17:00.873Z,"['holtzman@media.mit.edu', 'jkestner@media.mit.edu']",True,twitter-weather,2009-01-01,[],Twitter Weather,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:12:50.854Z,"TxTMob is a text-message service developed for political activists engaged in direct action and mass mobilizations. Approximately 6,000 protestors used TxTMob during the Democratic and Republican National Conventions of 2004. TxTMob was also used by demonstrators in Kiev during Ukraine's Orange Revolution, and by activists during George W. Bush's 2005 inauguration in Washington, DC.",2008-01-01,['smart-cities'],,24271,E15-001,2016-12-05T00:16:56.618Z,[],True,txtmob,2004-09-01,[],TxTMob,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:50.990Z,"Social Networking Services (SNS) such as MySpace and Facebook are increasing in popularity. They encourage and enable users to communicate with previously unknown people on an unprecedented scale. Now our increased social sphere is requiring us to potentially distinguish between legitimate and illegitimate strangers. Automatically rejecting 'unwanted' messages has long been the function of spam filtering; however, as we move from Viagra ads in email to Betty the saleswoman in SNS, identifying a nuisance requires placing value judgments. Nuisances take the shape of legitimate social humans and commercial entities, and sometimes both�what is Britney Spears?  We seek to redefine spam and how people are labeled more in the context of SNS by looking at users as prototypes.",2008-01-01,['sociable-media'],,24279,E15-390,2016-12-05T00:17:24.818Z,['judith@media.mit.edu'],True,typecasting,2007-01-01,[],Typecasting,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:12:51.249Z,"We have turned our building into a living laboratory by distributing 45 Ubiquitous Media Portals throughout our facility. These sensor-rich platforms capture video and stereo audio in addition to measuring nearby motion, temperature/humidity, light levels, and IR detection of active badges. The portals also work as Zigbee base stations, enabling communication and localization of various low-power, wireless, wearable devices that our group and others have developed for on-body sensing and identification/tracking of people. They also feature a small touch-screen display and speaker, allowing them to render and present dynamic graphics and information.",2010-01-01,['responsive-environments'],,24295,--Choose Location,2016-12-05T00:16:56.685Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,ubiquitous-media-portals,2008-01-01,[],Ubiquitous Media Portals,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:51.392Z,"The ubiquitous sensor network and media explorer integrates sound, images, and diverse data sampled throughout the Media Lab into a visual, aural and haptic experience. Real time data drives the system, allowing the observer to sense the overall activity of the Media Lab. Sound and images are streamed into our system from a selection of 45 sensor nodes distributed throughout our Lab. Observers can interact with the piece, both observing the generic activity of the Media Lab and drilling down to browse media streamed from particular locations. The installation reflects the essence of the Media Lab as a holistic entity.",2010-01-01,['responsive-environments'],,24303,,2016-12-05T00:16:29.911Z,"['nanwei@media.mit.edu', 'joep@media.mit.edu']",True,ubiquitous-sensor-network-navigator-and-media-explorer,2009-09-01,[],Ubiquitous Sensor Network Navigator and Media Explorer,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:12:51.505Z,"This project demonstrates how sensor networks can couple the real world with virtual worlds. A complete sensor network system (hardware and software) is being deployed for this purpose. Applications include interactively browsing large sensor data sets, expressing affect in a virtual social environment, and, in general, more closely coupling the real and virtual in order to enhance both.",2006-09-01,['responsive-environments'],,24311,E15-344,2016-12-05T00:17:24.864Z,['joep@media.mit.edu'],True,ubiquitous-sensor-networks-and-virtual-worlds,2006-01-01,[],Ubiquitous Sensor Networks and Virtual Worlds,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:51.606Z,"uCom (the ""u"" stands for ""ubiquitous"") is a follow-on to our iCom (Media Lab/Media Lab Europe, 1999) system for connecting architectural spaces to enable collaboration by distributed groups. uCom takes advantage of input/output resources (e.g., displays, cameras, speakers, sensors) already in place; it is not restricted to one ""window"" in one location, but rather creates multiple video and audio portals between the spaces; it scales in richness as the number of input and output devices increase; and it has both a real-world and a virtual-world presence. The virtual-world model can be accessed by users in other places, or for replay of past events. uCom also enables the aggregating of sensor data to allow processes to draw higher-level inferences (e.g., for understanding the actions of the community of users), monitoring individual users (e.g., for health or disability reasons), or augmenting the space with additional virtual information.",2010-01-01,['object-based-media'],,24319,Garden Conference Room,2016-12-05T00:16:56.752Z,['vmb@media.mit.edu'],True,ucom,2008-01-01,[],uCom,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:51.818Z,"Although RFID technology is well established, there are some niches where RFID doesn't perform well (in the presence of metallic objects, or in applications that afford very limited surface area). We have developed an optical RFID tag, based on extrapolations of our ""FindIT Flashlight"" technology, that avoids these difficulties. Using our approach, a user searches for a particular tag by scanning the area with a flashlight, a common real-world activity extrapolated to the world of information. With minute power draw regulated via quasi-passive wakeup, the tags should last through the shelf-life of their small embedded battery. The tags will also have the ability to optically talk back to the reader, enabling two-way communication.",2006-01-01,['responsive-environments'],,24335,E15-344,2016-12-05T00:17:24.933Z,['joep@media.mit.edu'],True,ultra-low-power-sendreceive-optical-rfid-tag,2005-01-01,[],Ultra-Low Power Send/Receive Optical RFID Tag,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:52.021Z,"Traditional medical ultrasound assumes that we are imaging ideal liquids. We are interested in imaging muscle and bone as well as measuring elastic properties of tissues, all of which are places where this assumption fails quite miserably. Interested in cancer detections, Duchenne muscular dystrophy, and prosthetic fitting, we use tomographic techniques as well as ideas from seismic imaging to deal with these issues.",,['camera-culture'],,24351,--Choose Location,2016-12-05T00:17:24.963Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'michaf@media.mit.edu']",True,ultrasound-tomography,2014-01-01,[],Ultrasound Tomography,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:52.115Z,"Some of the world's most pressing problems can be traced to inequity between people, both in the present and over time. Long periods of inequity can lead to both social and environmental degradation. In its 2011 Human Development Report (HDR), the United Nations incisively examines some of the complex relationships between socioeconomic equity and environmental sustainability. To examine some of the patterns in this report, we created �Networks in Equity and Sustainability,� a visualization tool that shows, via a network graph, how nations are multi-dimensionally linked. Examining linkages with this tool can illuminate potential partnerships between cultures. As the tool is expanded in the future, it will support intercultural discussions in the global effort for a world that is more equal today and more sustainable over time.",2012-09-01,"['information-ecology', 'collective-learning']",https://dam-prod2.media.mit.edu/x/2016/10/06/files/Display/Thumbnail.jpg,24359,,2016-12-05T00:17:00.970Z,"['holtzman@media.mit.edu', 'hidalgo@media.mit.edu']",True,un-hdr-networks-in-sustainability-and-equity,2011-09-01,[],UN HDR: Networks in Sustainability and Equity,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:12:52.213Z,"We present a novel framework to extend the dynamic range of images called Unbounded High Dynamic Range (UHDR) photography with a modulo camera. A modulo camera could theoretically take unbounded radiance levels by keeping only the least significant bits. We show that with limited bit depth, very high radiance levels can be recovered from a single modulus image with our newly proposed unwrapping algorithm for natural images. We can also obtain an HDR image with details equally well preserved for all radiance levels by merging the least number of modulus images. Synthetic experiments and experiments with a real modulo camera show the effectiveness of the proposed approach.",,['camera-culture'],,24367,--Choose Location,2019-04-19T18:38:01.471Z,"['raskar@media.mit.edu', 'naik@media.mit.edu', 'shiboxin@media.mit.edu', 'hangzhao@media.mit.edu']",True,unbounded-high-dynamic-range-photography-using-a-modulo-camera,2014-09-01,[],Unbounded high dynamic range photography using a modulo camera,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:52.310Z,"In order to extend the Digital Intuition group's ability to understand human language, a module that fills in the gaps of current technology must be developed to understand dialogue. This module will be based on data from the Switchboard human-human dialogue corpus, as well as a dataset of recorded dialogues between parents and children while reading an interactive e-book created by the Lab's Personal Robots group. The goal is for the module to be able to identify the emotion and mood of the dialogue in order to make inferences about what parents and children generally talk about when reading the book, and to make suggestions about additional conversation topics. Conversations between an adult and child while reading a book can greatly contribute to the learning and development of young children.",2020-02-28,['digital-intuition'],,24375,,2021-07-14T15:32:24.523Z,['havasi@media.mit.edu'],True,understanding-dialogue,2012-09-01,[],Understanding Dialogue,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:52.399Z,"Humanity has harnessed evolution to sculpt domesticated animals, crops, and molecules, but the process remains a black box. Which combinations of evolutionary parameters will enable us to discover the best solutions? We plan to answer this question by performing massively parallel directed evolution experiments. Our system  titled ""Phage- and Robotics-Assisted Near-Continuous Evolution"" (PRANCE) uses phage-assisted continuous evolution (PACE), a method of building synthetic ecosystems in which billions of fast-replicating viruses compete to optimize a molecular function of our choice. This system allows for the simultaneously evolution  of many different molecules and running of experiments in parallel, each with real-time fitness monitoring and customized evolutionary conditions such as mutation rate, selection stringency, and evolutionary goal-switching. We are currently using these methods to systematically characterize the relationship between evolutionary parameters and outcomes. ",,"['sculpting-evolution', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/02/14/Jimmy_robot cover.png,24383,--Choose Location,2023-09-07T22:41:23.875Z,"['esvelt@media.mit.edu', 'erikad@media.mit.edu', 'echory@media.mit.edu', 'dgretton@media.mit.edu', 'bscwang@media.mit.edu']",True,understanding-molecular-evolution,2016-01-01,"['robotics', 'environment', 'genetics', 'biology', 'microbiology']",Understanding Molecular Evolution,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:52.518Z,"We are providing our tools to the community, and also using them within our lab, to analyze how specific brain mechanisms (molecular, cellular, circuit-level) give rise to behaviors and pathological states. These studies may yield fundamental insights into how best to go about treating brain disorders.",2020-07-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/files/uploads/analyzingneuralcircuits.small.jpg,24391,--Choose Location,2020-09-10T17:26:40.874Z,['esb@media.mit.edu'],True,understanding-normal-and-pathological-brain-computations,2011-01-01,"['bioengineering', 'health', 'neurobiology']",Understanding normal and pathological brain computations,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:52.901Z,"Unhangout is an open source platform for running large-scale, participant-driven events online.Each event has a landing page, which we call the lobby. When participants arrive, they can see who else is there and chat with each other. Hosts can welcome their community and do introductions in a video window that gets streamed into the lobby. Participants can then join breakouts, which are small group video chats, for in-depth conversations, peer-to-peer learning, and collaboration on projects.Unhangout facilitates participant-driven, community-based learning, rather than top-down information transfer.For more information visit https://unhangout.media.mit.edu.",2023-05-25,"['ml-learning', 'digital-learning-studio']",https://dam-prod2.media.mit.edu/x/2017/02/10/unhangout-big-wbg.png,24407,--Choose Location,2023-05-25T22:05:27.065Z,"['ps1@media.mit.edu', 'srishti@media.mit.edu', 'kamcco@media.mit.edu', 'yumikom@media.mit.edu']",True,unhangout,2013-09-01,"['learning-teaching', 'open-source']",Unhangout,PUBLIC,https://unhangout.media.mit.edu,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:53.089Z,"uniMorph is an enabling technology for thin-sheet shape-changing interfaces. It affords actuation powered by environmental temperature changes, as well as computational control of actuation. Additionally, the composite allows a seamless integration of electronic components. We developed a digital fabrication method to enable easy creation of composites with actuation patterns of high accuracy.",2015-09-01,['tangible-media'],,24415,--Choose Location,2016-12-05T00:16:12.153Z,"['ishii@media.mit.edu', 'heibeck@media.mit.edu', 'basheer@media.mit.edu', 'clarkds@media.mit.edu']",True,unimorph-thin-film-shape-changing-interfaces,2014-09-01,[],uniMorph: Thin-Film Shape-Changing Interfaces,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:53.260Z,"The aim of UniPlug is to make it *really easy* to configure and update systems of networked devices without the need for manual intervention by administrative staff. Currently, a lot of time and manual effort goes into configuring and installing systems. UniPlug includes a safety framework to ensure that a new device added to a system does not decrease the safety of the system. UniPlug devices are context oriented. They convey context information and seek configuration and behaviour information that the user has specified previously. The user uses a web browser or an application on a device such as a cellphone to configure device behaviour. Since UniPlug is a generic framework, it could be used in an Operating Room, and at the same time be used for configuring devices in a barber's salon or the phone system of an enterprise.",2007-09-01,['viral-communications'],,24423,E15-468d,2016-12-05T00:16:56.916Z,['lip@media.mit.edu'],True,uniplug,2006-09-01,[],UniPlug,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:12:53.430Z,"Everyone knows the world is not a perfect place.  Love and slack
though, fill in many painful gaps.  In this project, an electronic device
negotiates the pathologically noisy environment of New York City,
exposing its rhythm and harmony.  Successor to noise cancellation headphones, Universal Slack draws on numerous Media Lab technologies
to turn the sound around us into music.  Comfortably integrated 
into the wearer's bodyspace, worn like an iPod, it analyzes audio, finds 
order, reduces noise, harmonizes and soothes.

",2005-01-01,[],,24431,E15-020A,2016-12-05T00:16:56.955Z,['csik@media.mit.edu'],True,universal-slack,2005-09-01,[],Universal Slack,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:53.843Z,"We are exploring the potential of low-cost, networked sensing and actuation technologies to improve urban agriculture, promote public health, and foster deep connections between people and the natural environment.",2008-01-01,['smart-cities'],,24455,E15-001,2016-12-05T00:17:25.086Z,[],True,urban-spaces-and-structures,2007-09-01,[],Urban Spaces and Structures,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:53.989Z,"Fashion signals are important in displaying one�s quality of access to information. Electronic fashions (information that may circulate within blog circles or online communities) experience rapid fashion cycles as the information is easily disseminated and regenerated. Physical fashions (such as clothing) are unable to update as quickly as information flow due to their physical structure. Urbanhermes is an augmented messenger bag that aims to incorporate the fluid, expressive signals of electronic fashion into the constrained, material-based environment of physical fashion. The bag is able to change its dynamic, temporal display within the context of its social environment, providing a versatile means for face-to-face signaling. By accelerating the physical fashion cycle, Urbanhermes facilitates more meaningful and communicative representations of self-identity.",2007-09-01,['sociable-media'],,24463,E15-383,2016-12-05T00:16:56.862Z,['judith@media.mit.edu'],True,urbanhermes,2005-09-01,[],Urbanhermes,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:54.337Z,"When computers automatically generate video game environments, the result is often as simplistic as a randomly generated maze. Using the Open Mind Common Sense knowledge base, we are exploring ways to automatically generate virtual environments that are immersive and intelligently designed. By allowing video games to dynamically create their own environments, developers will be able to reduce the time they currently spend crafting environments by hand and focus on higher-level design issues. More importantly, games that are capable of changing their own environments will feel less static, resulting in a more believable experience and an increased replay value.",2007-01-01,['software-agents'],,24479,E15-383,2016-12-05T00:16:12.189Z,['lieber@media.mit.edu'],True,using-common-sense-reasoning-in-video-game-design,2004-01-01,[],Using Common-Sense Reasoning in Video Game Design,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:54.521Z,"Tim Berners-Lee envisions information on the Web being given well-defined meaning, creating the Semantic Web, which would then enable user-interface agents to assist with complex, multi-step, context-dependent tasks, such as arranging travel, appointments, and online purchases. Such agents are assumed to achieve their tasks by composing Web services that perform the functions of individual steps. However, to date, much of the work on the Semantic Web has concerned how to describe and represent the individual services and resources, and not what the agents do to compose them. To avoid hard-coding each task the user wishes to perform, we must give the agent the ability to dynamically compose Web Services according to what will satisfy the user's goals. We are using a large common-sense knowledge base to allow user interface agents to anticipate user goals and flexibly connect them with Web services that can accomplish those goals.",2006-09-01,['software-agents'],,24487,E15-383,2016-12-05T00:16:41.689Z,['lieber@media.mit.edu'],True,using-common-sense-reasoning-to-enable-the-semantic-web,2003-09-01,[],Using Common-Sense Reasoning to Enable the Semantic Web,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:54.664Z,"Current voice-recognition software relies on statistical techniques to determine which words a user has said. This project attempts to leverage the semantic context of what the user has said previously to improve future predictions. We are using OMCSNet, a semantic network created from the Open Mind Common Sense knowledge base, to disambiguate phonetically similar words and improve overall recognition accuracy.",2006-09-01,['software-agents'],,24495,E15-383,2016-12-05T00:17:25.107Z,['lieber@media.mit.edu'],True,using-common-sense-reasoning-to-improve-voice-recognition,2004-01-01,[],Using Common-Sense Reasoning to Improve Voice Recognition,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:54.978Z,"Our voice is an important part of our individuality. From the voices of others, we understand a wealth of non-linguistic information, such as identity, social-cultural clues, and emotional state. But the relationship we have with our own voice is less obvious. We don't hear it the way others do, and our brain treats it differently from any other sound. Yet its sonority is deeply connected with how we are perceived by society and how we see ourselves, body and mind. This project is composed of software, devices, installations, and thoughts used to challenge us to gain new insights on our voices. To increase self-awareness, we propose different ways to extend, project, and visualize the voice. We show how our voices sometimes escape our control, and we explore the consequences in terms of self-reflection, cognitive processes, therapy, affective features visualization, and communication improvement.",,"['advancing-wellbeing', 'opera-of-the-future']",,24511,--Choose Location,2019-04-17T20:07:10.276Z,"['tod@media.mit.edu', 'rebklein@media.mit.edu']",True,using-the-voice-as-a-tool-for-self-reflection,2013-01-01,['voice'],Using the voice as a tool for self-reflection,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:12:55.341Z,"This project, a collaboration between the Media Lab, the United States Postal Service, and the Lear Corporation, explores the application of Media Lab technologies in the context of a widely-used postal delivery truck, the two ton. The intention is to improve driver performance and to address a variety of safety issues.",2003-01-01,['context-aware-computing'],,24527,E15-489,2016-12-05T00:17:01.012Z,[],True,usps-postal-concept-truck,2002-01-01,[],USPS Postal Concept Truck,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:55.533Z,"We are developing statistical tools for understanding, modeling, and predicting self-harm by using advanced probabilistic graphical models and fail-soft machine learning in collaboration with Harvard University and Microsoft Research.",2017-12-31,"['advancing-wellbeing', 'affective-computing']",,24535,--Choose Location,2022-05-12T17:17:26.504Z,"['picard@media.mit.edu', 'kdinakar@media.mit.edu']",True,valinor-mathematical-models-to-understand-and-predict-self-harm,2014-01-01,[],Valinor: Mathematical models to understand and predict self-harm,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:12:55.730Z,"Variable Reality is an augmented reality system designed for reading digital and physical books more intuitively and efficiently. Through a head-worn display device such as Oculus Rift, the user is able to instantly access and display any desired book contents onto either a real book or a hand, depending on the need and affordability. Quick hand gestures integrated with the system further facilitate natural user interactions.",2018-09-01,['living-mobile'],,24543,--Choose Location,2022-06-10T14:01:01.922Z,['geek@media.mit.edu'],True,variable-reality-interaction-with-the-virtual-book,2014-09-01,[],Variable Reality: Interaction with the Virtual Book,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:55.910Z,"Complications of prosthetic leg use in persons with lower extremity amputation often occur at the prosthetic socket, and includes delayed wound healing, recurrent skin ulcerations, and pressure damage to soft tissues. Such complications can result in limited mobility, which further contributes to conditions such as obesity, musculoskeletal pathologies, and cardiovascular disease. Conventional prosthetic socket fabrication is an artisanal process requiring substantial human hours, financial cost and patient involvement for evaluation. Computer aided design (CAD) and computer aided manufacturing (CAM) methods have been explored as an alternative. However, these tools have not reached full clinical efficacy and do not inform the design in a data-driven sense since the actual design process remains a manual and experience-based procedure. The long-term goal of our research is to develop a fully-quantitative process for prosthetic socket design and production that requires minimal patient involvement and can be delivered at affordable price points. A pre-print for our novel patient-specific and data-driven computational framework for the automated design of biomechanical interfaces is presented here. Optimization of the design of biomechanical interfaces is complex since it is affected by the interplay of the geometry and mechanical properties of both the tissue and the interface. The proposed framework is presented for the application of transtibial amputee prostheses where the interface is formed by a prosthetic liner and socket. Conventional socket design and manufacturing is largely artisan, non-standard, and insufficiently data-driven, leading to discrepancies between the quality of sockets produced by different prosthetists. Furthermore, current prosthetic liners are often not patient-specific. The proposed framework involves: A) non-invasive imaging to record patient geometry, B) indentation to assess tissue mechanical properties, C) data-driven and automated creation of patient-specific designs, D) patient-specific finite element analysis (FEA) and design evaluation, and finally E) computer aided manufacturing. Uniquely, the FEA procedure controls both the design and mechanical properties of the devices, and simulates, not only the loading during use, but also the pre-load induced by the donning of both the liner and the socket independently. Through FEA evaluation, detailed information on internal and external tissue loading, which are directly responsible for discomfort and injury, are available. Further, these provide quantitative evidence on the implications of design choices, e.g. : 1) alterations in the design can be used to locally enhance or reduce tissue loading, 2) compliant features can aid in relieving local surface pressure. The proposed methods form a patient-specific, data-driven and repeatable design framework for biomechanical interfaces, and by enabling FEA-based optimization reduces the requirement for repeated patient involvement in the currently manual and iterative design process.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2017/03/23/socketDesign_rHGZQD4.png,24551,--Choose Location,2019-04-17T19:03:36.507Z,"['hherr@media.mit.edu', 'branger@media.mit.edu', 'kmoerman@media.mit.edu', 'danask@media.mit.edu', 'lfreed@media.mit.edu']",True,variable-impedance-prosthetic-vipr-socket-design,2010-09-01,[],Automated and data-driven computational design of subject-specific prosthetic sockets,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:12:56.069Z,"Beginning in the early twentieth century, composition branched out into a variety of new representations, the most common being the graphical score.  Cage's _Variations II_ is a prime example, utilizing only dots and lines as its basis.  A potential downside to these new methods of representation is the intense burden it places on the performer to define the parameters for the realization.  We have created an interactive version of Cage's piece, called here _Variations 10b_, where a performer can change the score and get immediate feedback as to the result.  We hope that both listeners and performers will develop a more nuanced understanding of the score through the use of the interface.",2007-01-01,['opera-of-the-future'],,24559,E15-445,2016-12-05T00:16:57.094Z,['tod@media.mit.edu'],True,variations-10b,2006-01-01,[],Variations 10b,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:56.385Z,"We wish to address two aspects of vending machine design. The first is the paying system and thus the role of the machine in the market. The second is the interface and interaction with the customer. We are starting with four interface projects: 1) make the vending machine glass a semitransparent display that is touch enabled; 2) use a ""magic wand"" to point and click to purchase items and view information about them; 3) interface the vending machine with a PDA; and 4) introduce speech recognition to the machine.",2001-12-30,"['counter-intelligence', 'personal-information-architecture']",,24575,E15-068,2016-12-05T00:16:12.432Z,['mike@media.mit.edu'],True,vending-machine-challenge,1999-12-31,[],Vending Machine Challenge,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:56.530Z,"Ventus is an attempt to address the problem of Medical Compliance.  Asthma medication can be dangerous if used improperly.  By tracking when it is  being used, Ventus aims to help asthma sufferers everywhere improve their health and self-awareness.",1999-12-30,['personal-information-architecture'],,24583,,2016-12-05T00:17:25.216Z,['mike@media.mit.edu'],True,ventus-the-intelligent-inhaler,1998-12-31,[],Ventus: The Intelligent Inhaler,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2016-07-15T00:12:57.079Z,"The Victorian Laptop explores the interaction between physical objects and virtual worlds, conversational storytelling, and the use of computers to encourage and enhance communication. It merges (and extends) the capabilities of modern laptop computers with the aesthetic and evocative  qualities of an antique writing box. It makes readable the memories and histories that are associated with old and valued objects as it links the present-day traveler to those who have stood before in the same spot.",2000-12-30,['gesture-and-narrative-language'],,24615,,2016-12-05T00:16:57.184Z,[],True,victorian-laptop,1996-12-31,[],Victorian Laptop,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:57.212Z,"Can display nanostructures be applied to all surfaces in a room?  We explore growing them (like mold), and their interactions with light and the presence of electric fields. Beamed standing waves can selectively excite and not-excite nano-surface areas to create the display. These nano-surfaces could also function as holographic displays, or displays that have complex lenses embedded into them that allow light to be ""thrown"" to other areas of the room, onto bodies, or yes, thin air.",2008-01-01,[],,24623,--Choose Location,2016-12-05T00:16:57.283Z,['mlj@media.mit.edu'],True,video-displays-on-every-surface,2005-09-01,[],Video Displays on Every Surface,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:57.328Z,"Long-distance families are increasingly staying connected with free video conferencing tools. However, the tools themselves are not designed to accommodate children's or families' needs. We explore how play can be a means for communication at a distance. Our Video Play prototypes are simple video-conferencing applications built with play in mind, creating opportunities for silliness and open-ended play between adults and young children. They include simple games, such as Find It, but also shared activities like book reading, where users' videos are displayed as characters in a story book.",2011-09-01,['tangible-media'],,24631,--Choose Location,2016-12-05T00:16:57.304Z,['ishii@media.mit.edu'],True,video-play,2010-01-01,[],Video Play,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:57.566Z,"This work exploits the spontaneity and connectivity speed afforded by mobile devices, combined with the organizational structure of a Weblog, where brief captured experiences can be expanded and further reflected upon. A group of friends are using their cell phones to correspond and exchange mediated messages with one of their friends who is in the hospital undergoing a medical procedure. In introducing this approach to documenting the recovery process, we strive to create an evolving, ""architectured,"" story process that is built up of many voices and interpretations. On an immediate level, there is a desire to connect and extend our physical reach and to provide comfort and companionship to a friend, which is met by the friend's wish to help remove the alienating distance of the hospital and its attendant medical procedures. The Weblog additionally makes visible the stages of recovery, delineating individual progress made in the context of the continuing day-to-day goings-on of the rest of the group. This experiment will be extended as we introduce MOVITS, a custom-designed application for the cell phone that offers a methodology for gently interrogating lived experiences, where the time-lag between experience, interpretation, representation, and display becomes, on some levels, miniscule.",2005-01-01,['media-fabrics'],,24647,E15-320B,2016-12-05T00:16:57.328Z,['gid@media.mit.edu'],True,video-weblogs,2002-01-01,[],Video Weblogs,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:57.789Z,"Using the Eye Society network of robotic cameras, this project seeks to capture three-dimensional representations of static interior spaces for use with holographic displays. The project directs robotic cameras to gather images needed to generate novel 2-D renderings of the scene, directing cameras to move so that they can look behind objects that might be obstructing parts of their view.",2007-09-01,['object-based-media'],,24663,E15-368,2016-12-05T00:17:25.352Z,['vmb@media.mit.edu'],True,view-planning-for-camera-networks,2005-09-01,[],View Planning for Camera Networks,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:12:58.196Z,"The Village Area Network (VAN) provides information and communication services to an entire rural community, supporting a range of roaming and fixed services. Schools, rural clinics, government offices, police stations, agricultural cooperatives, and local residences will be empowered with access to information and new means to communicate with each other and the world. The first VAN is being implemented in the rural community of Bohechio, Dominican Republic. Last year, a LINCOS telecenter was installed in Bohechio. The VAN will be centered around a LINCOS multipurpose community telecenter and will extend the LINCOS facilities with mobile handheld and fixed wireless services in strategic local centers such as schools and hospitals.",2002-12-30,['edevelopment'],,24695,,2016-12-05T00:17:01.120Z,[],True,village-area-network,2000-12-31,[],Village Area Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:58.312Z,"The Village Visualizer is visualization of a learner�s social network. It highlights people with similar interests and projects, so that users can find others with common interests, communicate about projects, collaborate, and find resources. The system is a demonstration of how visualizations can address the particular needs of communities of learners, principally children and adolescents. ",2007-01-01,['lifelong-kindergarten'],,24703,Cube,2016-12-05T00:16:38.926Z,"['mres@media.mit.edu', 'sylvan@media.mit.edu']",True,village-visualizer,2006-01-01,[],Village Visualizer,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:58.590Z,"The vinylMP3 project brings together new digital technology with old vintage analog technology. It was designed to look, sound, and behave just like a normal old vintage record player, but is, in fact, a transparent, physically-controlled, computer-based MP3 player.",2002-12-30,['opera-of-the-future'],,24719,E15-445,2016-12-05T00:16:57.416Z,"['tod@media.mit.edu', 'tristan@media.mit.edu']",True,vinylmp3,2001-12-31,[],VinylMP3,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:12:59.319Z,"Broadband architecture is optimized for asymmetric, client-server computing applications. While ample downstream capacity is offered to subscribers, the upstream capacity is limited. As a result, emerging peer-to-peer and dynamic applications cannot easily reach residential end-users; innovation is limited in the edge of the network. The objective of the Viral Broadband project is to build a viral network that compliments existing broadband infrastructure in order to provide symmetric networking capabilities.  We experiment with
parasitic wireless network infrastructure deployed from the bottom up
by end users.",2005-01-01,['viral-communications'],,24751,E15-495,2016-12-05T00:16:57.448Z,['lip@media.mit.edu'],True,viral-broadband,2004-09-01,[],Viral BroadBand,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:59.621Z,"In the real world, people clothe themselves in garments whose cut and design encodes information about the social identity of the wearer. This encoding changes temporally as the design spreads throughout a population; this is the basis of the cultural phenomenon known as ""fashion."" On the Web, people embellish their homepages with links, pictures and other items that exhibit similar patterns of dispersion. We are developing tools to track, analyze and visualize these patterns; our goal with this research is to further understanding of how the Web functions as a social environment.   
",2000-12-30,['sociable-media'],,24767,,2016-12-05T00:16:12.466Z,['judith@media.mit.edu'],True,virtual-fashion-cultural-dispersion-on-the-world-wide-web,1998-12-31,[],Virtual Fashion: Cultural Dispersion on the World Wide Web,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-07-15T00:12:59.760Z,"Virtual Gaza is a space where ordinary Palestinians under siege can describe their experiences in their own words, and where the destruction of the Gaza strip can be documented by those experiencing it directly. The diary entries, photographs, and video material gathered have been contributed by residents of Gaza.",2010-01-01,['computing-culture'],,24775,E15-020C,2016-12-05T00:16:57.561Z,['csik@media.mit.edu'],True,virtual-gaza,2009-09-01,[],Virtual Gaza,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:12:59.901Z,"The virtual messenger system acts as a portal to subtly communicate messages and pass information between the digital, virtual, and physical worlds, using the Media Lab�s Glass Infrastructure system. Users who opt into the system are tracked throughout the Media Lab by a multimodal sensor network. If a participating user approaches any of the Lab�s Glass Infrastructure displays they are met by their virtual personal assistant (VPA), who exists in DoppelLab�s virtual representation of the current physical space. Each VPA acts as a mediator to pass on any messages or important information from the digital world to the user in the physical world. Participating users can interact with their VPA through a small subset of hand gestures, allowing the user to read any pending messages or notices, or inform their virtual avatar not to bother them until later.  ",2014-01-01,['responsive-environments'],,24783,--Choose Location,2016-12-05T00:16:57.587Z,['joep@media.mit.edu'],True,virtual-messenger,2012-09-01,[],Virtual Messenger,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:00.142Z,"Virtual Towing allows a city to rapidly collect and redistribute vehicles, by enabling a single driver to tow multiple vehicles without mechanically connecting them. Given the principles of one-way vehicle sharing in the CityCar concept, the need to stabilize the city's vehicle fleet metabolism is critical to properly balancing vehicle demand and location. Virtual towing builds upon the (digital) wheel robot architecture and combines a multitude of sensing technologies in order to maximize security and efficiency. The key issues of adoption and system autonomy are being explored in this project. A small-scale, radio-controlled vehicle has been built to test the control of the wheel robots and will serve as a testing platform for virtual towing.",2009-01-01,['smart-cities'],,24799,E15-001,2016-12-05T00:16:57.680Z,"['rchin@media.mit.edu', 'ypod@media.mit.edu']",True,virtual-towing,2006-01-01,[],Virtual Towing,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:13:00.259Z,"Canopy Climb is a virtual climbing interface for a rainforest tree. Pulling on the ropes allows the visitor to 'climb' up and down the tree. At the base of the ropes is a log which has a scroll mouse. Stepping on this log will display more information about the organisms in the tree. This is written in HTML, so that museums and schools can easily create their own vertical interfaces such as a journey to the bottom of the sea, to outer space, down a mine shaft, or up a skyscraper. New plastic, vacuum-formed molds for the base of the tree have been made as an exploration of low-cost rapid prototyping tools.",2003-01-01,['context-aware-computing'],,24807,E15-320,2016-12-05T00:16:57.609Z,[],True,virtual-tree,2000-01-01,[],Virtual Tree,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:13:00.480Z,"Vast regions of the world are unmapped by com­mercial services, and communities living there are digitally invisible. Visible Communities is a system that combines what local people using smartphones see on the ground with what computers can detect from satellite images, to create an interactive map at a fine resolution that continuously improves. The map captures both spatial and social data: houses and the paths connecting them, and the households living there and their relationships.Enabling communities to put themselves on the map is a powerful way to increase their own visibility, and in turn serves institutional needs to improve infrastructure planning and humanitarian aid delivery. Existing approaches to do community-driven mapping either require outside experts to facilitate, or the results are lower-tech and not easy to keep up to date. In collaboration with Partners in Health (PIH), and supported by the MIT Tata Center, we are piloting this social machine in a sparsely populated, hilly region with a Community Health Worker (CHW) network in Burera, Rwanda.The smartphone app enables CHWs to self-map their communities. We are intentionally designing an intuitive pre-literacy touch interface, enabling a wide range of users to participate without training. By removing barriers for people at the base of the socio-economic pyramid and designing with social dynamics in mind, we hope to unlock existing, self-motivated human potential.",2017-05-31,['social-machines'],https://dam-prod2.media.mit.edu/uuid/7a68099a-4371-489d-9dff-ca4f1d8e2ae9,24823,--Choose Location,2017-10-16T15:35:47.765Z,"['dkroy@media.mit.edu', 'schaad@media.mit.edu', 'koehrsen@media.mit.edu']",True,visible-communities,2016-01-01,"['design', 'human-machine-interaction', 'machine-learning', 'mapping', 'gis']",Visible Communities,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:00.579Z,"The VisiGeek displayer enables the visual comparison of otherwise non-visual datasets. The visualizer highlights differences in the data using patterns, letter-shapes and motion. Applications of this type of visualization could include affinity-matching when searching for newsgroups or chat rooms to join, or visual sorting tasks such as identifying a desirable place to live using a back-end service such as a demographic database. 

The demonstration project's immediate goal is to allow a viewer to visually compare ""Geek Codes"" of two individuals to uncover differences and similarities.",2000-12-30,['software-agents'],,24831,,2016-12-05T00:16:57.648Z,['judith@media.mit.edu'],True,visigeek,1998-12-31,[],VisiGeek,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:00.675Z,"Computer vision is a class of technologies that lets computers use cameras to automatically stitch together panoramas, reconstruct 3D geometry from multiple photographs, and even tell you when the water's boiling. For decades, this technology has been advancing mostly within the confines of academic institutions and research labs. Vision on Tap is our attempt to bring computer vision to the masses.",2010-09-01,['camera-culture'],https://dam-prod2.media.mit.edu/x/files/Display/Screen%20shot%202010-05-24%20at%207.30.33%20PM.png,24839,E15-320,2016-12-05T00:16:57.704Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,vision-on-tap,2009-01-01,[],Vision on Tap,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:13:00.875Z,"Mobile devices with cameras have enough processing power to do simple machine-vision tasks, and we are exploring how this capability can enable new user interfaces to applications. Examples include dialing someone by pointing the camera at the person's photograph, or using the camera as an input to allow navigating virtual spaces larger than the device's screen.",2012-01-01,['object-based-media'],,24855,Garden Conference Room,2016-12-05T00:17:25.506Z,['vmb@media.mit.edu'],True,vision-based-interfaces-for-mobile-devices,2009-01-01,[],Vision-Based Interfaces for Mobile Devices,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:00.968Z,"VisionBlocks is an on-demand, in-browser, customizable, computer-vision application-building platform for the masses. Even without any prior programming experience, users can create and share computer vision applications. End-users drag and drop computer vision processing blocks to create their apps. The input feed could be either from a user's webcam or a video from the Internet. VisionBlocks is a community effort where researchers obtain fast feedback, developers monetize their vision applications, and consumers can use state-of-the-art computer vision techniques. We envision a Vision-as-a-Service (VaaS) over-the-web model, with easy-to-use interfaces for application creation for everyone.",,['camera-culture'],,24863,--Choose Location,2018-10-20T01:02:47.699Z,"['raskar@media.mit.edu', 'naik@media.mit.edu']",True,visionblocks,2011-01-01,[],VisionBlocks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:13:01.135Z,"Visiphone is a communication sculpture that visualizes the sounds flowing between two distant locations. A continuous audio connection between the two placesallows the inhabitants to talk informally and easily while Visiphone's graphics express the dynamics of the conversation, providing visual feedback that one's voice has carried sufficiently and indicating the presence of those on the other end. The rendering is evocative rather than technical; our goal in building the Visiphone has been to create an aesthetic object that enhances sociable awareness.   
",1999-12-30,['sociable-media'],,24879,,2016-12-05T00:17:25.550Z,['judith@media.mit.edu'],True,visiphone,1997-12-31,[],VisiPhone,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:13:01.370Z,"Visual Who is a tool for visualizing an electronic community. Using data such as mailing list subscriptions, it creates a spring-based model of the patterns of affiliation within the community. By varying the groups chosen as anchor points, the user can explore interactively the underlying social and organizational structures. The goal of Visual Who is to make the social patterns of an electronic community visible. A large-scale installation version of the project is being built in collaboration with the Responsive Environments group. There, the visualization is displayed on the LaserWall and music and sound accompany the motion of the anchors and names.",2002-12-30,['sociable-media'],,24903,E15-449,2016-12-05T00:16:57.780Z,['judith@media.mit.edu'],True,visual-who,1993-12-31,[],Visual Who,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:13:01.451Z,"VisualCV is an iconic representation of a person's career, exposing spatio-temporal patterns such as relocations and changes of position, and context information such as a person's impact on her environment. Similar to Tufte's ""Sparklines,"" the visualization does not use an explicit metric, but instead  suggests trends and shows patterns. At an individual level, this graphic representation has few advantages over a traditional CV; instead, it is meant for overview and comparison of a large number of CVs�with hundreds of applications, patterns become immediately visible.",2017-05-01,['sociable-media'],,24911,E15-390,2022-06-10T14:41:30.150Z,['judith@media.mit.edu'],True,visualcv,2006-09-01,[],VisualCV,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-07-15T00:13:01.687Z,"This project presents a multimodal speech recognition system for real world scene description tasks. Given a visual scene, the system dynamically biases its language model based on the content of the visual scene and visual attention of the speaker. Visual attention is used to focus on likely objects within the scene. Given a spoken description the system then uses the visually biased language model to process the speech. The system uses head pose as a proxy for the visual attention of the speaker. Readily available standard computer vision algorithms are used to recognize the objects in the scene and automatic real-time head pose estimation is done using depth data captured via a Microsoft Kinect. The system was evaluated on multiple participants. Overall, incorporating visual information into the speech recognizer greatly improved speech recognition accuracy. The rapidly decreasing cost of 3D sensing technologies such as the Kinect allows systems with similar underlying principles to be used for many speech recognition tasks where there is visual information.",2013-09-01,['social-machines'],,24935,--Choose Location,2016-12-05T00:16:12.500Z,"['dkroy@media.mit.edu', 'soroush@media.mit.edu']",True,visually-grounding-speech-recognition-systems-using-kinect,2012-01-01,[],Visually grounding speech recognition systems using Kinect ,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:13:01.764Z,"In this project we demonstrate an application of our work in understanding natural language about spatial scenes. All 3-D modeling applications face the problem of letting their users interact with a 2-D projection of a 3-D scene. Rather than the common solutions that include multiple views, and selective display and editing of the scene, we employ our language learning and understanding research to allow for speech-based selection and manipulation of objects in the 3-D scene. We demonstrate such an interface based on our Bishop project for the 3-D modeling application Blender.",2004-01-01,['social-machines'],,24943,E15-483,2016-12-05T00:16:12.534Z,['dkroy@media.mit.edu'],True,visually-grounded-language-understanding-for-the-interface-of-an-interactive-design-system,2003-09-01,[],Visually-Grounded Language Understanding for the Interface of an Interactive Design System,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:01.843Z,"The Vocal Augmentation and Manipulation Prosthesis (VAMP) is a gesture-based, wearable controller for live-time vocal performance. This controller allows a singer to capture and manipulate single notes that she sings, using a gestural vocabulary developed from that of choral conducting. By drawing from a familiar gestural vocabulary, this controller and the associated mappings can be more intuitive to both performer and audience. This instrument was inspired by the character of Nicholas in Death and the Powers.",2010-01-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/files/Display/vamp_glove1%20crop.jpg,24951,--Choose Location,2016-12-05T00:17:25.874Z,"['tod@media.mit.edu', 'ejessop@media.mit.edu']",True,vocal-augmentation-and-manipulation-prosthesis-vamp,2009-01-01,[],Vocal Augmentation and Manipulation Prosthesis (VAMP),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:13:01.921Z,Expressive Performance for Body-Mind WellbeingThe Vocal Vibrations music is now available for exclusive download from Bowers & Wilkins. Vocal Vibrations was exhibited at Le Laboratoire Cambridge in March 2015. The original installation at Le Laboratoire Paris ran from March to September 2014.,,"['advancing-wellbeing', 'opera-of-the-future']",https://dam-prod2.media.mit.edu/x/files/Display/vocal_vibrations.jpg,24959,--Choose Location,2018-10-19T19:52:05.771Z,"['tod@media.mit.edu', 'ejessop@media.mit.edu', 'rebklein@media.mit.edu', 'holbrow@media.mit.edu']",True,vocal-vibrations-expressive-performance-for-body-mind-wellbeing,2011-09-01,[],Vocal Vibrations,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:13:02.014Z,"In order to utilize digitized speech (a telephone message, recording of a meeting, lecture, etc.) more effectively as a computer-data-type, it is important to understand speech structure and allow rapid scanning of the audio data. Structure is derived from pauses, speaker changes and turn-taking, emphasis, and intonational cues. Presentation may be serial, including audio time-compression, or parallel, using both ears to listen to multiple audio streams simultaneously.",1999-12-30,['living-mobile'],,24967,,2016-12-05T00:17:25.686Z,['geek@media.mit.edu'],True,voice-as-data,1999-12-31,[],Voice as Data,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:13:02.088Z,"An interconnected musical network for a museum setting, Voice Patterns allows for up to four participants to create and transform musical motives with their voices, and to share these motives with peers by synchronizing play patterns.",2005-09-01,['opera-of-the-future'],,24975,E15-441,2016-12-05T00:16:57.839Z,['tod@media.mit.edu'],True,voice-patterns,2002-01-01,[],Voice Patterns,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:13:02.251Z,"VoIP Drupal is an innovative framework that brings the power of voice and Internet-telephony to Drupal sites. It can be used to build hybrid applications that combine regular touchtone phones, web, SMS, Twitter, IM, and other communication tools in a variety of ways, facilitating community outreach and providing an online presence to those who are illiterate or do not have regular access to computers. VoIP Drupal will change the way you interact with Drupal, your phone, and the web.",2013-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/new-voipdrupal-logo.png,24991,--Choose Location,2016-12-05T00:17:25.721Z,"['ethanz@media.mit.edu', 'leob@media.mit.edu', 'rahulb@media.mit.edu']",True,voip-drupal,2010-01-01,[],VoIP Drupal,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:02.413Z,"Vojo.co is a hosted mobile blogging platform that makes it easy for people to share content to the web from mobile phones via voice calls, SMS, or MMS. Our goal is to make it easier for people in low-income communities to participate in the digital public sphere. You don't need a smart phone or an app to post blog entries or digital stories to Vojo; any phone will do. You don't even need Internet access: Vojo lets you create an account via SMS and start posting right away. Vojo is powered by the VozMob Drupal Distribution, a customized version of the popular free and open-source content management system that is being developed through an ongoing codesign process by day laborers, household workers, and a diverse team from the Institute of Popular Education of Southern California (IDEPSCA).vojo.co",2015-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/sites/all/themes/vojo_generic/logo-full.png,25007,--Choose Location,2016-12-05T00:17:25.771Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'elplatt@media.mit.edu']",True,vojoco,2012-01-01,[],Vojo.co,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:02.512Z,"This project focuses on giving transtibial amputees volitional control over their prostheses by combining electromyographic (EMG) activity from the amputees' residual limb muscles with intrinsic controllers on the prosthesis. The aim is to generalize biomimetic behavior of the prosthesis, making it independent of walking terrains and transitions. ",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/files/Display/olli_pldb.png,25015,,2021-09-17T19:11:18.056Z,"['hherr@media.mit.edu', 'olli@media.mit.edu']",True,volitional-control-of-a-powered-ankle-foot-prosthesis,2012-09-01,[],Volitional control of a powered ankle-foot prosthesis,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:13:02.705Z,"The VozMob Drupal Distribution is Drupal customized as a mobile blogging platform. VozMob has been designed to make it easy to post content to the web from mobile phones via voice calls, SMS, or MMS. You don't need a smart phone or an app to post blog entriesany phone will do. VozMob allows civic journalists in low-income communities to participate in the digital public sphere. Features include groups, tags, geocoding and maps, MMS filters, and new user registration via SMS. Site editors can send multimedia content out to registered users' mobile phones. VozMob Drupal Distribution is developed through an ongoing codesign process by day laborers, household workers, and students from the Institute of Popular Education of Southern California (IDEPSCA.org). The project received early support from the Annenberg School for Communication and Journalism at the University of Southern California, Macarthur/HASTAC, Nokia, and others. ",2013-09-01,['civic-media'],,25031,--Choose Location,2016-12-05T00:17:25.844Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu']",True,vozmob,2008-01-01,[],VozMob,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:02.804Z,"VR Codes are dynamic data invisibly hidden in television and graphic displays.  They allow the display to present simultaneously visual information in an unimpeded way, and real-time data to a camera. Our intention is to make social displays that many can use at once; using VR codes, users can draw data from a display and control its use on a mobile device.  We think of VR Codes as analogous to QR codes for video, and envision a future where every display in the environment contains latent information embedded in VR codes.",,['viral-communications'],,25039,,2018-05-01T15:29:34.350Z,['lip@media.mit.edu'],True,vr-codes,2011-01-01,[],VR Codes,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:13:02.902Z,"Tired of waiting for public transportation to arrive?  Wait Loss will tell  you the expected arrival time of your favorite bus or train so you can be  more productive. 
",1998-12-30,['personal-information-architecture'],,25047,,2016-12-05T00:17:25.958Z,['mike@media.mit.edu'],True,wait-loss,1997-12-31,[],Wait Loss,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:02.991Z,"Wall of Now is a multi-dimensional media browser of recent news items. It attempts to address our need to know everything by presenting a deliberately overwhelming amount of media, while simplifying the categorization of the content into single entities. Every column in the wall represents a different type of entity: people, countries, states, companies, and organizations. Each column contains the top-trending stories of that type in the last 24 hours. Pressing on an entity will reveal a stream of video that relates to that specific entity. The Wall of Now is a single-view experience that challenges previous perceptions of screen space utilization towards a future of extremely large, high-resolution displays.",,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/2019/04/05/wall-of-now.png,25055,--Choose Location,2019-10-21T21:18:09.527Z,"['weller@media.mit.edu', 'lip@media.mit.edu', 'anderton@media.mit.edu', 'mhjiang@media.mit.edu']",True,wall-of-now,2015-01-01,[],Wall of Now,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-07-15T00:13:03.082Z,"A wall that responds to its surroundings. By listening and interacting with the people around it the wall retains a memory of its experiences. The wall communicates with its visitors through audible sound and infrasound, sometimes speaking with itself to remember and feel. 
",2004-09-01,['computing-culture'],,25063,E15-020C,2016-12-05T00:16:57.885Z,"['csik@media.mit.edu', 'monster@media.mit.edu']",True,wall-of-sound,2003-01-01,[],Wall of Sound,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:13:03.167Z,"How will life change as our surfaces become dynamic information displays?  How will we use space tastefully and what kinds of interactions will be possible? Wall Paper explores the ability to selectively display information based on proximity and user attention, making it possible to delve deeper into information as people interact with the wall.  By intelligently switching modes based on user behavior, the wall avoids being the cause of information overload despite the vast amount of information it can provide.",2011-09-01,['information-ecology'],,25071,,2016-12-05T00:16:57.921Z,['holtzman@media.mit.edu'],True,wall-paper,2010-09-01,[],Wall Paper,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:03.253Z,"Walter's Helmet is a bike helmet project. The project seeks to establish mediated information with the minimum of user distraction. As a hands-free interface, the helmet will listen to the audio, imagery, and motion of the environment. The bicyclist can make phone calls or give dictation, or listen to music or other information. The helmet attempts to provide the user with the most information and the least interruptions. It uses a horn, and front, back, and side lighting for visibility and signaling. The helmet is designed to intrude slightly with unusual external information; for example, it will tell you when a car is approaching.",2007-09-01,['context-aware-computing'],,25079,E15-320,2016-12-05T00:16:19.831Z,[],True,walters-helmet,2002-01-01,[],Walter's Helmet,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:03.496Z,"WATCH is a system that attempts to measure the possible influence that a new time-management interface will have on improving the habits of a user. Users set goals for each of the activities detected by the app. Detected activities include physical activity and time spent in pre-defined locations. An Andriod app (WATCH) on their personal phones is able to track their activities (running, walking, and sitting) as well as their GPS location. Their progress in comparison to their goals is displayed on their home screens as a pie chart.",,"['advancing-wellbeing', 'fluid-interfaces']",,25103,--Choose Location,2016-12-05T00:16:58.002Z,"['pattie@media.mit.edu', 'nfarve@media.mit.edu']",True,watch,2015-01-01,[],WATCH,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:03.578Z,"Microsoft Kinect has popularized the use of body gestures to control games and animations, but only expert programmers can create applications for the Kinect. With our new video-motion extensions to Scratch, it is possible for everyone, even novice programmers, to create Kinect-like games and animations using only a standard webcam. Our extension uses computer-vision algorithms based on optical flow to track motion from real-time video, allowing Scratch programmers to access the amount and direction of motion across the whole scene and also under individual objects. ",2012-01-01,['lifelong-kindergarten'],,25111,,2016-12-05T00:16:12.583Z,"['mres@media.mit.edu', 'jmaloney@media.mit.edu']",True,watch-me-move,2011-09-01,[],Watch Me Move!,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:03.656Z,"Recording your reaction to a short video is becoming the new gossip; famous watchers get as many as 750,000 views. We attempt to transform this utterly useless and talentless event into a socially constructive alternative to simultaneous, synchronized, group viewing. Any user can opt in to be recorded and added to the shared, collective viewing experience. No talent or skills required.",2017-06-30,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/files/Display/Screen%20Shot%202016-03-30%20at%208.56.04%20AM.png,25119,--Choose Location,2018-10-09T18:03:44Z,"['suzwang@media.mit.edu', 'weller@media.mit.edu', 'lip@media.mit.edu']",True,watch-people-watch,2016-01-01,[],Watch People Watch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:13:03.817Z,"Communication is much more than the direct transfer of information. It is an interactive collaborative act including potentially rich verbal and non-verbal cues. To date, telecommunication has focused on verbal communication at a distance, restricting the non-verbal situational information. With the existing infrastructure, it is necessary to first contact a person before receiving feedback on what they are doing. Current communication devices do not provide a persistent sense of presence of those with whom we connect, nor do they create opportunities for additional communication with them. WatchMe�a platform for mobile communication and awareness in the form of a watch�addresses these limitations. ",2006-01-01,['living-mobile'],,25135,E15-384C,2016-12-05T00:16:58.029Z,['geek@media.mit.edu'],True,watchme,2002-01-01,"['wearable-computing', 'archives', 'history']",WatchMe,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:04.042Z,"This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.",2019-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/files/Display/IMG_0020.JPG,25151,--Choose Location,2021-05-21T14:38:07.678Z,"['dumo@media.mit.edu', 'j_duro@media.mit.edu', 'neri@media.mit.edu', 'jvanzak@media.mit.edu', 'yjtai@media.mit.edu', 'asling@media.mit.edu', 'nah6cz@media.mit.edu', 'bader_ch@media.mit.edu']",True,water-based-additive-manufacturing,2014-01-01,"['architecture', 'synthetic-biology', '3d-printing']",Water-Based Additive Manufacturing,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:13:04.231Z,"WaterBot is a persuasive water conservation device that presents �just-in-time"" context-sensitive feedback to users. Its goal is to motivate behavior change regarding water conservation while using the bathroom sink. The system collects water usage information from flow sensors and presents subtle prompts at the point of behavior. These prompts are designed to be unobtrusive in the form of illumination, graph bars, and auditory feedback. WaterBot uses several persuasive techniques, such as law of contrast, positive reinforcement, variable schedule of reinforcement, social validation, scarcity, curiosity, and challenge. The demonstration includes a video  showing its different behaviors while installed on a sink.",2005-01-01,"['counter-intelligence', 'context-aware-computing']",,25159,E15-320,2016-12-05T00:16:12.618Z,[],True,waterbot-a-persuasive-technology-to-motivate-water-conservation,2003-09-01,[],WaterBot: A Persuasive Technology to Motivate Water Conservation,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:04.577Z,"It's common for us to attach different emotions to each moment in our lives, but when we capture and interact with them, we are often constrained to flat, 2D encapsulations of video, audio, or photographic recordings. In this digital age we often look at memories through a screen with hundreds of digital files, further removing the event's emotion. What if we could experience this emotional energy again, and dynamically interact with it in infinitely complex ways? We present a concept and prototype that explores a novel physical-visual language of dynamic, emotionally expressive waveforms, designed to transform how we perceive different forms of energy in our daily lives. With the power of computation hidden within the physical materials of the interface, we create an interactive form that takes one form of energy and transmutes it into a waveform as its output, or Wave Alchemy.",2012-09-01,"['living-mobile', 'fluid-interfaces', 'viral-communications']",,25175,--Choose Location,2018-12-04T20:33:12.363Z,"['pattie@media.mit.edu', 'sujoy@media.mit.edu', 'geek@media.mit.edu', 'lip@media.mit.edu']",True,wave-alchemy-perception-and-reminiscence-of-expressive-moments-through-waves,2012-09-01,[],Wave Alchemy: Perception and Reminiscence of Expressive Moments through Waves,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:04.754Z,"In a high-population-density city, inhabitants must be prepared to defend their own personal space. Technologies that increase personal productivity are on the rise, even though they may intrude on others. The unavoidable reaction is to create technologies that counteract other people's devices. Wave Bubble counters the all-too-familiar annoyance of loud ring tones and overt cell-phone conversations in public. This wearable device blocks all radio frequencies in the PCS/GSM cellular phone bands. By activating it, the user creates a bubble of low-reception in their personal space, which repels cell-phone users (or, if they choose, deactivates RFID tag readers or GPS tracking devices). When worn openly, wearers make the statement that they reject the intrusion of these technologies into their everyday lives.",2005-01-01,['computing-culture'],,25183,E15-020D,2019-10-21T15:41:48.154Z,['csik@media.mit.edu'],True,wave-bubble,2003-09-01,[],Wave Bubble,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:13:04.909Z,"Electrodermal activity (EDA) recording is a powerful, widely used tool for monitoring psychological or physiological arousal. However, analysis of EDA is hampered by its sensitivity to motion artifacts. We propose a method for removing motion artifacts from EDA, measured as skin conductance (SC), using a stationary wavelet transform (SWT). We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level (SCL) and skin conductance responses (SCRs). The goodness-of-fit of the model was validated on ambulatory SC data. We evaluated the proposed method in comparison with three previous approaches. Our method achieved a greater reduction of artifacts while retaining motion-artifact-free data.",2018-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/2016/10/23/wavelet-denoising.png,25191,--Choose Location,2022-05-12T17:21:39.018Z,"['picard@media.mit.edu', 'akanes@media.mit.edu', 'sfedor@media.mit.edu', 'sataylor@media.mit.edu', 'jaquesn@media.mit.edu', 'cvx@media.mit.edu']",True,wavelet-based-motion-artifact-removal-for-electrodermal-activity,2015-01-01,[],Wavelet-based motion artifact removal for electrodermal activity,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:05.090Z,"This project explores the idea that some ""intelligence"" is encoded in the habits that people assume in daily life. Adopting someone else's habits might allow you to break out of a personal rut, glean some success tactics from someone you admire, or to empathize with someone you care about. This is a wearable system with a Google Calendar backend that actively alerts users to perform a habit based on the events of their calendar.",2013-01-01,['fluid-interfaces'],,25199,--Choose Location,2016-12-05T00:17:26.036Z,['pattie@media.mit.edu'],True,wear-someone-elses-habits,2013-01-01,[],Wear Someone Else's Habits,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:13:05.269Z,"The societal problem of physical abuse persists in part because of isolation and concealment. Emerging technologies have been adapted by abusers for methods of control and by victims for methods of resistance. This project examines the intimate position that wearable technologies have with our bodies and explores the design of a wearable computer system that could record and document physical forces to the body in an effort to quantify physical abuse. This apparel or smart clothing system utilizes large area, fabric-based pressure sensors to categorize and measure the intensity and patterns of forces to the wearer�s body. As textile-based user interfaces find their way into clothing, the opportunity for computers to identify physical abuse will become apparent. This work will function both as a proof of concept and as a surveying stake to demonstrate a possible field of future investigation.",2009-01-01,['computing-culture'],,25207,E15-001,2016-12-05T00:17:26.064Z,['csik@media.mit.edu'],True,wearable-as-witness,2007-01-01,[],Wearable as Witness,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-07-15T00:13:05.454Z,"The UbER-Badge is a small, wearable-computing platform for experimentation with distributed systems, and the analysis and enhancement of group social interaction. The UbER-Badge is equipped with multiple communication channels, including an RF channel for broadcast communication, and an IR channel for more intimate dialogues. It is also equipped with a large LED matrix display, vibratory feedback, audio input and output, and data memory. It is powered by three in-system reprogrammable microprocessors. It is designed to be easily augmented by other technologies to enable a wide span of research throughout the Media Lab and the sponsor community. It was used in October 2004 at the TTT sponsor event, providing a suite of different applications.",2005-09-01,['responsive-environments'],,25215,E15-344,2016-12-05T00:17:26.188Z,['joep@media.mit.edu'],True,wearable-badge-for-distributed-systems-with-multiple-channels-of-communication,2003-01-01,[],Wearable Badge for Distributed Systems with Multiple Channels of Communication,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-07-15T00:13:05.814Z,"Wearable Body Organs are a series of digitally controlled, electromechanical wearable devices that respond to a set of psychologically and socially critical needs. Each device is adaptable to its current user and situation, and in this way WBOs are made by users and their ever-changing influences. ScreamBody, HoldBody, CryBody, FightBody�rather than being hidden and made to go unnoticed as may be the trend with our ever-shrinking mobile devices, WBOs are designed for visible, active, personal, and social engagement.",2006-09-01,['computing-culture'],,25231,E15-020C,2016-12-05T00:16:58.225Z,"['csik@media.mit.edu', 'monster@media.mit.edu']",True,wearable-body-organs,1997-01-01,[],Wearable Body Organs,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:13:06.367Z,"We have created a system for data collection, annotation, and feedback that is part of a longer-term research interest to gather data to understand more about stress and the physiological signals involved in its expression. First, we built a wearable apparatus for gathering data that allows the user to include as many accurate labels (annotations) as possible while going about natural daily activities. Gathering annotations is disruptive and likely to increase stress (thus interfering with the signals being measured). We hypothesized that empathetic ways of interrupting would be less stressful than non-empathetic and found significant effects on many of user's self-reported items such as preference for the more empathetic system, and also on behavioral items, such as the estimated number of times they were interrupted (significantly lower when system was more empathetic.)  ",2004-09-01,['affective-computing'],,25255,E15-001,2016-12-05T00:17:26.245Z,['picard@media.mit.edu'],True,wearable-relational-devices-for-stress-monitoring,2003-01-01,[],Wearable Relational Devices for Stress Monitoring,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:06.555Z,"This project is a system of compact, wearable, wireless sensor nodes, equipped with full six-degree-of-freedom inertial measurement units and node-to-node capacitive proximity sensing. A high-bandwidth, channel-shared RF protocol has been developed to acquire data from many (e.g., 25) of these sensors at 100 Hz full-state update rates, and software is being developed to fuse this data into a compact set of descriptive parameters in real time. A base station and central computer clock the network and process received data. We aim to capture and analyze the physical movements of multiple people in real time, using unobtrusive sensors worn on the body. Applications abound in biomotion analysis, sports medicine, health monitoring, interactive exercise, immersive gaming, and interactive dance ensemble performance.",2013-09-01,['responsive-environments'],,25263,E15-344,2016-12-05T00:17:26.277Z,"['mtl@media.mit.edu', 'joep@media.mit.edu']",True,wearable-wireless-sensor-system-for-sports-medicine-and-interactive-media,2005-01-01,[],"Wearable, Wireless Sensor System for Sports Medicine and Interactive Media",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:13:06.754Z,"We are exploring the use of wearable objects for capturing emotion. When the user experiences a particular emotion, she initiates the wearable object to generate unique haptic sensations that come to be associated with that particular emotion. We explore the use of these haptic-emotion capture devices triggered by natural gestures such as the knee-slapping funny gesture and the congratulatory high-five gesture.",2013-01-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/files/Display/emotion_capture.JPG,25271,--Choose Location,2016-12-05T00:16:58.477Z,['pattie@media.mit.edu'],True,wearables-for-emotion-capture,2013-01-01,[],Wearables for Emotion Capture,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:07.311Z,"Webbed Footnotes is a Web annotation system that enables users to have a discussion about, around, and within the space of a Web page. Webbed Footnotes' approval-based moderation system promotes contributions that other users find useful and interesting.",2017-05-01,['sociable-media'],,25295,E15-391,2022-06-10T14:41:30.171Z,['judith@media.mit.edu'],True,webbed-footnotes,2004-09-01,[],Webbed Footnotes,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:07.654Z,"This project presents a weight-changing device based on the transfer of mass. We chose liquid metal and a bi-directional pump to inject into or remove from from the mass of a target object. The liquid metal is about six times heavier than water, and is thus suitable for effective mass transfer. We also combine the device with a dynamic volume changing function to achieve programmable mass/volume at the same time. We explore three potential applications enabled by weight-changing devices. This technique opens up a new direction in human science regarding user interfaces with dynamic weight/volume changing.",2014-01-01,['tangible-media'],,25311,--Choose Location,2016-12-05T00:16:58.541Z,"['ishii@media.mit.edu', 'liningy@media.mit.edu']",True,weight-and-volume-changing-device,2013-01-01,[],Weight and Volume Changing Device,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:08.013Z,"The Wetpaint project investigates new interfaces for exploring the history of a work of visual art. We are seeking intuitive metaphors for touch-screen interaction, including virtually scraping through the multispectral scans of an ancient painting, and pulling, stretching, and tearing through a virtual canvas. The interaction techniques refined through Wetpaint are being built into a Web-based tool for leveraging collective intelligence toward the pursuit of art diagnostics.",2010-01-01,['tangible-media'],,25327,E15-348,2016-12-05T00:16:58.609Z,"['ishii@media.mit.edu', 'amerigo@media.mit.edu', 'x_x@media.mit.edu']",True,wetpaint,2008-01-01,[],Wetpaint,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:08.303Z,"We are automating recognition of positive/negative experiences (valence) and affect from facial expressions. We present a toolkit, Acume, for interpreting and visualizing facial expressions whilst people interact with products and/or concepts.",2011-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/files/Display/Acume_Pic.png,25343,,2016-12-05T00:17:26.380Z,['picard@media.mit.edu'],True,what-do-facial-expressions-mean,2009-09-01,[],What Do Facial Expressions Mean?,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:08.441Z,"Information that is spatially or temporarily detached from us is hard to relate to our own personal lives. In addition, we are confronted with more and more information, and we need tools to simplify and abstract this vast amount of data. Using Facebook as an example, we have created a tool for personalized data visualization. Your Facebook friends and acquaintances become stand-ins for the entire world population, and serve as a more easily comprehensible representation for this vast data set. It generates a sense of empathy and connection to what is usually a bodiless and abstract set of numbers. This project is part of the Face the Facts series, which experiments with different ways to make abstract statistical data more tangible using unusual, personalized data-visualization techniques.",2010-01-01,['fluid-interfaces'],,25351,,2016-12-05T00:16:12.666Z,"['dsmall@media.mit.edu', 'pattie@media.mit.edu']",True,what-if-the-world-were-your-n-facebook-friends,2010-01-01,[],What If the World Were Your n Facebook Friends?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-07-15T00:13:08.673Z,"More than a billion people a month visit YouTube to watch videos. Sometimes, those billion people watch the same video. What We Watch is a browser for trending YouTube videos. Some videos trend in a single country, and some find regional audiences.  Others spread across borders of language, culture, and nation to reach a global audience.  What We We watch lets us visualize and explore the connections between countries based on their video viewing habits.http://whatwewatch.mediameter.org/",2016-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2016/10/20/What_We_Watch.png,25367,--Choose Location,2016-12-05T00:16:55.775Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'elplatt@media.mit.edu']",True,what-we-watch,2013-01-01,"['data', 'networks']",What We Watch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:13:08.790Z,"The system consists of virtual representations of a handful of people that react to what a person is currently reading, writing, or talking about. These ""digital personas"" are constructed automatically by analyzing personal texts (such as interviews/blogs/messages posted by the person being modeled) using natural language processing techniques and the Open Mind Common Sense knowledge base. The ""What Would They Think?"" system has several applications. For example, it can help a person form a deep understanding of a community that is new to them by constantly showing them the attitudes and disagreements of strong personalities of  that community. Other applications include virtual ""mentors"" or ""guides,"" and knowledge-sharing in communities and workgroups.",2009-01-01,['fluid-interfaces'],,25375,E15-320,2016-12-05T00:16:58.673Z,['pattie@media.mit.edu'],True,what-would-they-think,2003-01-01,[],What Would They Think?,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:08.904Z,"What's Up is a neighborhood news system that combines power of the telephone and of the Web to make it easier for youth to organize community events and find out what is happening in the place where they live. By dialing a central number, youth can send and receive voicemail messages, publish audio community announcements, create voicemail groups, add events to a shared community calendar and more. When field tested in Lawrence, MA, What's Up helped increase awareness of and accessibility to important local youth resources, provided youth with opportunities to express their opinions about their neighborhoods and, with that, contributed to transform Lawrence into a community that is both friendlier and more empowering for young people.",2007-01-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/files/pldb/Display/whatsupdisc.jpg,25383,E15-020A,2016-12-05T00:16:58.731Z,"['mres@media.mit.edu', 'csik@media.mit.edu', 'leob@media.mit.edu']",True,whats-up-2,2005-09-01,[],What's Up,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:09.010Z,"What's Up is a set of tools designed to allow people in a small geographic community to share information, plan events, and make decisions, using media that's as broadly inclusive as possible. The platform incorporates low-cost LED signs, online and paper event calendars, and a simple yet powerful phone system that is usable with the lowest-end mobile and touch tone phones. ",2012-09-01,['civic-media'],,25391,--Choose Location,2016-12-05T00:16:58.859Z,"['ethanz@media.mit.edu', 'leob@media.mit.edu', 'rahulb@media.mit.edu']",True,whats-up,2011-09-01,[],What's Up,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:09.307Z,"""Where The Hel"" is a pair of helmets: plain and funky. The funky helmet is 3D printed; the plain helmet visualizes proximity to the funky helmet as a function of signal strength, via an LED light strip. The funky helmet contains an Xbee and a GPS Radio. Its position is tracked via a web app. The wearer of the plain helmet can track the funky one via the web app and the LED strip on his helmet. These helmets are potential iterations towards a more developed HADR (Humanitarian Assistance and Disaster Relief) helmet system.",2013-09-01,['information-ecology'],https://dam-prod2.media.mit.edu/x/files/%7Earlduc/images/portfolio/WhereTheHel/WhereTheHel_Thumb.jpg,25415,--Choose Location,2016-12-05T00:16:59.058Z,['holtzman@media.mit.edu'],True,where-the-hel,2012-01-01,[],Where The Hel,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:13:09.496Z,"Many Twitter users post questions with hashtags like #twoogle or #lazyweb. Is this an effective way to find expert answers beyond one's network, or are questions mostly answered by friends?Rzeszotarski, J. M., Spiro, E. S., Matias, J. N., Monroy-Hernández, A., & Morris, M. R. (2014, April). Is anyone out there?: unpacking Q&A hashtags on twitter. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems (pp. 2755-2758). ACM.",2013-09-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/hashtag%20paper.png,25431,--Choose Location,2016-12-05T00:17:26.454Z,"['ethanz@media.mit.edu', 'jnmatias@media.mit.edu']",True,who-answers-questions-on-twitter,2013-01-01,['social-media'],Who Answers Questions on Twitter?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:13:09.584Z,"Many people play strategy games against computer opponents, but don't have a clear understanding of how they function. Building your own Deep Blue is a formidable challenge, but is perhaps a good way to learn the computer science and mathematics of decision making. Since most people aren't computer scientists, they need a way to describe game strategies so they can be executed by a computer they can play against. This research features a graphic toolkit for designing strategy-games and computer opponents, tailored to support learning artificial intelligence techniques for deciding which next moves are better than others.  The demonstration of the toolkit is complemented by initial results of a study of how kids articulate their strategies and modify them after seeing their effect on game play.",2005-09-01,['electronic-publishing'],,25439,E15-320G,2019-02-06T21:45:08.761Z,['walter@media.mit.edu'],True,whos-got-game,2003-09-01,[],Who's Got Game?,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:09.744Z,"Wildflower is an open-source approach to Montessori learning. Its aim is to be an experiment in a new learning environment, blurring the boundaries between home-schooling and institutional schooling, between scientists and teachers, between schools and the neighborhoods around them. At the core of Wildflower are nine principles that define the approach.  The Wildflower approach has been implemented by several schools, which serve as a research platform for the development of Montessori materials that advance the Montessori Method, software tools that enable Montessori research, and social software that fosters the growth and connection of such schools.",2017-05-01,['social-computing'],,25455,--Choose Location,2022-06-10T14:40:02.101Z,['sdkamvar@media.mit.edu'],True,wildflower-montessori,2013-09-01,"['kids', 'learning-teaching']",Wildflower Montessori,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:09.818Z,"This project illustrates the shared spaces and resources between urban wildlife and citizens. It presents interactive tools to enhance human knowledge of our urban wildlife surroundings, as well as methods for mapping ambient devices to outdoor urban-wildlife-generated events.",2009-01-01,[],,25463,Upper Atrium,2016-12-05T00:17:26.507Z,['labrune@media.mit.edu'],True,wildurban-shared-spaces-between-wildlife-and-citizens,2009-01-01,[],WildUrban: Shared Spaces Between Wildlife and Citizens ,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:09.893Z,"This project describes the design and development of a system that will help to increase safety and security using cell phones. This system has two main components: a master phone application to assist people who need to take care of their loved ones, and a slave phone application to provide help to care-recipients who need attention from their caregivers. This system applies location awareness (GPS), awareness of social activities (communication activity and proximity with close peers), and peer-to-peer data communication as its core technologies.",2008-01-01,['living-mobile'],,25471,E15-368,2016-12-05T00:16:59.239Z,['geek@media.mit.edu'],True,will-you-help-me,2005-09-01,[],Will You Help Me?,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:09.973Z,"While computer data can live virtually anywhere, we are still faced with the mundane tasks of document management such as uploading, sharing, and syncing between locations. Window Wallet aims to remove the burden of managing your data across screens, computers, and devices by turning your portable device into a virtual wallet of data and software. This project looks at developing an interface that facilitates this process, acting on your mobile device as a virtual conduit between local data and data in the cloud. It allows you both to access and transfer documents independent of your physical location.",2010-01-01,['information-ecology'],,25479,E15-301,2016-12-05T00:16:59.219Z,['holtzman@media.mit.edu'],True,window-wallet,2008-09-01,[],Window Wallet,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:10.052Z,"We are developing a system for wireless capture of gesture, useful, for example, when interacting with a wearable display or other mobile computing environment where a keyboard isn't appropriate.  We are putting small RFID-queried passive sensors on the fingertips of a user, interrogated by a reader mounted near a sleeve, for example.  This allows the fingers to be free while enabling simple gesture, such as finger curl and touch, to be reliably detected.  We are developing hardware to show a working demonstration of this concept, together with a gestural vocabulary appropriate to this interface.",2011-01-01,[],,25487,,2016-12-05T00:17:26.539Z,['joep@media.mit.edu'],True,wireless-finger-gesture-capture-with-passive-tag-sensing,2010-01-01,[],Wireless Finger Gesture Capture with Passive Tag Sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:10.121Z,"Motion and gait analyses have a number of applications in the medical field. For example, change in gait over extended time is used in neurological exams to diagnose dementias. Changes in gait over short periods of time may be indicative of a number of conditions, such as the onset of a stroke and progression of congestive heart failure. Continuous monitoring of gait could be useful for physical therapy and rehabilitation. For instance, patients who have undergone knee replacement therapy are advised to place less weight on the corresponding foot; feedback when excessive weight is applied could help prevent re-injury. Gait feedback could also be used for athletic analysis, both to provide complex gait information to elite athletes and to provide real-time exercise feedback and incentives to everyday athletes. Precise gait analysis is currently done in a motion lab; however, these large labs are expensive to maintain and costly to use. We are developing a wireless wearable system that is capable of making many inexpensive measurements. These measurements may have less precision, but may provide more information by collecting data throughout the day in the patient's home environment. Our system will be optimized for gait analysis and for detecting medically relevant gait characteristics.",2003-01-01,['responsive-environments'],,25495,E15-344,2016-12-05T00:17:26.572Z,['joep@media.mit.edu'],True,wireless-wearable-system-for-gait-evaluation,2000-01-01,[],Wireless Wearable System for Gait Evaluation,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:10.201Z,"WISE is a Web system in which children can engage in conversation with interactive storytellers. These storytellers draw children out, listen to their personal stories, and then offer a traditional story in return. Children can also create their own WISE characters online and share them with others around the world.",2001-12-30,['gesture-and-narrative-language'],,25503,,2016-12-05T00:16:59.278Z,[],True,wise,1996-12-31,[],WISE,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:10.419Z,"This work has a double focus. First, considerable research has centered on designing computer tools and interfaces, ""smart rooms,"" and computer-based interactive systems for human adults and children, but few people realize the potential for extending this work to include animals. After many years of studying the intelligence of animals, we believe that such an extension to the animal kingdom is not only justified, but viable. Toward this end, we have been designing ""Pet Projects,"" a collection of computer-based tools to enhance pet-human interactions and the lives of pet animals. Second, these tools can be used to explore further the cognitive abilities of animals. Such work is important because understanding how creatures with brains so different from humans can process information can help us design better artificial learning systems.",2001-12-30,"['toys-of-tomorrow', 'synthetic-characters']",,25527,E15-325,2016-12-05T00:16:59.340Z,['impepper@media.mit.edu'],True,woofers-tweeters,1999-12-31,[],Woofers & Tweeters,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:10.503Z,"WordPlay is a mutual tabletop interface for the generating, organizing, and exploring new ideas. Participants add terms to the table by speaking, modify the properties (size, position, rotation) of the terms with their fingers, and find related concepts through a simple set of gestures (stretching, merging, circling).",2009-01-01,['fluid-interfaces'],,25535,E15-320B,2016-12-05T00:16:47.982Z,['pattie@media.mit.edu'],True,wordplay,2008-01-01,[],WordPlay,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:10.580Z,"We are exploring the work piece as an interface to digital fabrication and digital modeling. Possible advantages include a more intuitive workflow and a simpler learning curve. As the first step, we are putting together a prototype of a PCB workbench that allows the user to bring up information about a feature on the PCB from the linked CAD data, by pointing a custom pointing device to the desired trace or component.",2014-09-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/projector.png,25543,--Choose Location,2016-12-05T00:16:59.367Z,"['pragun@media.mit.edu', 'joep@media.mit.edu']",True,work-piece-as-interface,2013-01-01,[],Work Piece as Interface,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:10.738Z,"Muscles are biological actuators with unique properties compared to traditional mechanical actuators. The ability to simultaneously modify stiffness while providing power as well as the potential for self-repair make their use desirable when considering the design of robotic systems. Improving our understanding of how muscles work as actuators, struts, and springs is essential for both robotic design and for understanding animal locomotion. Unfortunately, our understanding of the energetics of muscles is incomplete, and traditional methods for studying muscles have inherent limitations. Our group has developed a novel apparatus that allows us to test muscles in unique ways by tethering the muscle to a movable platform coupled to a computer-simulated load. With this setup, we can vary the parameters of the ""virtual load"" to address more complex and relevant questions, and examine the dynamic interactions between muscles.",2009-09-01,['biomechatronics'],,25559,E15-054,2016-12-05T00:17:26.609Z,"['hherr@media.mit.edu', 'eswart@media.mit.edu']",True,workloop-energetics-of-muscles-and-interactive-loads,2008-01-01,[],Workloop Energetics of Muscles and Interactive Loads,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:13:10.823Z,"World Lens informs users about newsworthy events that are both popular and obscure. It is a front page that is both navigable and scalable, allowing one to discover as well as track ongoing events. We array in-depth news information across a large multitouch display organized by time, coverage, and geography. Elements are drawn from blogs, the web, newspapers, magazines, and television. Each is presented by a front page that tells the literal story. Readers can fly through the news space, mark items for interest, and activate each. News data is gathered and analyzed by our Glue system, which generates frame-by-frame metadata for video and page analysis for other online material.",2015-01-01,"['ultimate-media', 'viral-communications']",,25567,--Choose Location,2016-12-05T00:16:59.435Z,['lip@media.mit.edu'],True,worldlens,2014-01-01,[],WorldLens,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:13:10.906Z,"The WoW Pod is an immersive architectural space that provides and anticipates all life needs of the World of Warcraft player. Outfitted with toilet throne, hydration system, and meals at the ready, the WoW Pod makes daily human function possible without ever stepping away from the game. The project received grants from the Council for the Arts at MIT and the SHASS's Peter de Florez Fund for Humor.",2009-09-01,['tangible-media'],,25575,E15-344,2016-12-05T00:16:59.468Z,['ishii@media.mit.edu'],True,wow-pod,2008-09-01,[],WoW Pod,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:10.992Z,"While many wearable sensors have been developed, few are actually worn by people on a regular basis. WristQue is a wristband sensor that is comfortable and customizable to encourage widespread adoption. The hardware is 3D printable, giving users a choice of materials and colors. Internally, the wristband will include a main board with microprocessor, standard sensors, and localization/wireless communication, and an additional expansion board that can be replaced to customize functionality of the device for a wide variety of applications. Environmental sensors (temperature, humidity, light) combined with fine-grained indoor localization will enable smarter building infrastructure, allowing HVAC and lighting systems to optimize to the locations and ways that people are actually using the space. Users' preferences can be input through buttons on the wristband. Fine-grained localization also opens up possibilities for larger applications, such as visualizing building usage through DoppelLab and smart displays that react to users' presence.",2013-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/files/Display/wristque_icon.png,25583,--Choose Location,2016-12-05T00:17:26.640Z,"['bmayton@media.mit.edu', 'joep@media.mit.edu']",True,wristque-a-personal-wristband-for-sensing-and-smart-infrastructure,2011-01-01,[],WristQue: A Personal Wristband for Sensing and Smart Infrastructure,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-07-15T00:13:11.072Z,"Many people don't finish high school. We are studying the context, student population, and technical resources of an urban, community-based adult education program to understand the enormous challenges involved in delivering education there. We are introducing a computer-based curriculum to see how it might meet the social and contextual challenges to teaching and learning in this environment. We are particularly interested in seeing if both existing speech technology (such as speech dictation software) and novel technology we will design and introduce can improve students' writing performance. Through multiple design cycles, we aim to create a new piece of software that helps novice adult writers understand and use a writing process�many for the first time. Part of this software will involve the capture and use of recorded talk, which the student can use as a direct resource in generating text. Our measurable goal is to improve the quality of writing in adult education.",2003-01-01,[],,25591,E15-312,2016-12-05T00:17:26.667Z,[],True,writing-technology-in-adult-education,2002-01-01,[],Writing Technology in Adult Education,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:11.765Z,"The Context Manager is a generalized framework for connecting a variety of pervasive sensor technologies and output modalities to intelligent context-aware services. It focuses on binding together technologies that can be used to sense activity and interaction to services in a device- and collocation-agnostic way, and then use the context information, along with other personalization data that may be part of the user's profile, to provide highly relevant, just-in-time information and recommendations. The output is produced in a generalized form that can then be rendered into different modalities, such as WAP (cell phone output), audio, or a regular computer screen. The Context Manger can also push highly relevant information to the user, consolidating the point-of-contact between a multitude of services and the user in a consistent, manageable way. This infrastructure will be part of several projects, including ReachMedia, Invisible Media, and the UbER-Badge Personalization Project.",2007-01-01,['fluid-interfaces'],,25647,E15-320,2018-12-04T14:52:42.080Z,"['pattie@media.mit.edu', 'sajid@media.mit.edu']",True,xlink-extensible-context-management-layer,2004-09-01,[],xLink: Extensible Context Management Layer,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:12.822Z,"This project has developed a software agent that finds people who have never met, but share similar interests, and introduces them to each other. Such introductions can automatically form interest groups and coalitions, and can be used to locate someone knowledgeable in a particular area. Each participant runs a copy of the agent, and these individual copies find each other, as appropriate, on the network and begin the introduction process. The project is an experiment in creating a decentralized, fault-tolerant application that handles potentially sensitive information (such as people's mail, their personal files, or lists of their particular interests) in a responsible and privacy-protecting fashion, using cryptographic and other techniques. The goal is ubiquitous deployment across the Internet.",2000-12-30,['software-agents'],,25703,,2016-12-05T00:16:42.865Z,['pattie@media.mit.edu'],True,yenta-matchmaking-agents,1994-12-31,[],Yenta: Matchmaking Agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:13:13.671Z,"You Are Here is an experiment in microurbanism.  In this project, we are creating 100 maps each of 100 different cities. Each map gives a collective portrait of one aspect of life in the city, and is designed to give communities meaningful micro-suggestions of what they might do to improve their city. The interplay between the visualizations and the community work they induce creates a collective, dynamic, urban-scale project.",2017-05-01,['social-computing'],https://dam-prod2.media.mit.edu/x/2017/04/03/Screen Shot 2017-04-03 at 12.04.01 PM.png,25759,--Choose Location,2022-06-10T14:40:02.118Z,"['sdkamvar@media.mit.edu', 'srife@media.mit.edu', 'yonatanc@media.mit.edu', 'pranavr@media.mit.edu', 'manassra@media.mit.edu', 'zhangjia@media.mit.edu']",True,you-are-here,2013-01-01,"['data', 'urban-planning', 'mapping']",You Are Here,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:13.796Z,"We are working on ways to detect sensitive themes in the online discussions between teenagers on social networks, and to match them to similar experiences shared publicly by other teenagers. For example, imagine a thread between two teenagers on a social networking website; if the theme of their discussion involves ""pressure and regret conditioned on sex,"" we can show them stories by other teenagers who have publicly talked about pressure and regret based on sex. We're trying to help teenagers in difficult situations, helping them to feel that they are not alone, and that there have been other teenagers in similar plights.",2014-09-01,['software-agents'],,25767,--Choose Location,2022-06-10T14:18:57.682Z,"['lieber@media.mit.edu', 'kdinakar@media.mit.edu']",True,you-too,2011-09-01,[],You Too!,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-07-15T00:13:14.007Z,"A designated room in the Media Lab will be equipped with a module that includes an occupancy sensor, speakers, and processor. This apparatus will produce an acoustic space that reflects the activity of squirrels when they don�t detect human presence. It is possible to trick your imaginary friend into coming out when room is occupied, but the occupant must be very still and silent.",2009-01-01,[],,25783,Bartos Theater,2016-12-05T00:16:59.843Z,[],True,your-imaginary-friend,2009-01-01,[],Your Imaginary Friend,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:14.092Z,"The Boston Youth and Community Connections Program examined ways of using new technologies to help break down the social and cultural barriers that have historically existed between various communities in the Boston area. We engaged a diverse group of young people age 12 and up, from qualitatively different communities, in a series of face-to-face and online activities.  This project was a collaborative effort with the Center for Reflective Community Practice in the MIT Department of Urban Studies and Planning.",2000-09-01,['lifelong-kindergarten'],,25791,E15-120B,2016-12-05T00:17:26.722Z,['mres@media.mit.edu'],True,youth-and-community-connections,1999-01-01,[],Youth and Community Connections,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-07-15T00:13:14.187Z,"OPAL is a project to allow for private data to be used in privacy-conscientious ways for good. Collaborating companies can use OPAL's open platform and algorithms behind their own firewalls to extract key development indicators. OPAL grew out of the recognition that accessing big data sources for research and policy purposes has been a conundrum. To date, data held by private companies, such as large-scale mobile phone data, have been accessed and analyzed externally, either through data challenges, or through bilateral agreements. While these types of engagements offered evidence of big data's promise and demand, these modalities limit the full realization of its potential. By ""sending the code to the data"" rather than the other way around, OPAL seeks to address these challenges and develop data services on the basis of greater trust between all parties involved.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/files/Display/Logo-Opal-code.jpg,25799,--Choose Location,2016-10-24T19:44:56.655Z,"['sandy@media.mit.edu', 'yva@media.mit.edu']",False,yvas-untitled-project-2,2016-01-01,[],OPAL: Privacy-Conscientious Use of Mobile Phone Data,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-07-15T00:13:14.615Z,"We introduce Zensei, an implicit sensing system that leverages bio-sensing, signal processing, and machine learning to classify uninstrumented users by their body’s electrical properties. Zensei could allow many objects to recognize users. E.g., phones that unlock when held, cars that automatically adjust mirrors and seats, or power tools that restore user settings.We introduce wide-spectrum bioimpedance hardware that measures both amplitude and phase. It extends previous approaches through multi-electrode sensing and high-speed wire- less data collection for embedded devices. We implement the sensing in devices and furniture, where unique electrode configurations generate characteristic profiles based on user’s unique electrical properties. Finally, we discuss results from a comprehensive, longitudinal 22-day data collection experiment with 46 subjects. Our analysis shows promising classification accuracy and low false acceptance rate.More information at http://zensei.technologyThis project was completed in collaboration with Takram London and Google ATAP.",2017-12-01,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/05/03/01_zensei_main (1).jpg,25847,--Choose Location,2017-05-09T18:23:47.552Z,"['raskar@media.mit.edu', 'olwal@media.mit.edu', 'rohan@media.mit.edu', 'munehiko@media.mit.edu']",True,zensei,2013-09-01,"['human-machine-interaction', 'interfaces', 'member-company', 'internet-things']","Zensei: Embedded, Multi-Electrode Bioimpedance Sensing for Implicit, Ubiquitous User Recognition",PUBLIC,http://zensei.technology,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-07-15T00:13:14.697Z,"The Zero Car is a testing platform and chassis for the Wheel Robots project. Zero Car is a customizable and modular platform that houses the energy source (batteries), control system, and mechanical connections to smart wheel assemblies. The Zero Car will also be the future testing platform for our virtual towing simulation. Zero Car comes in multiple sizes: bicycle, tricycle, 4-wheels, and mini. The mini-Zero tests four omni-directional wheels attached to a chassis and wireless RC controller.",2009-01-01,['smart-cities'],,25855,E15-001,2016-12-05T00:17:26.770Z,"['rchin@media.mit.edu', 'ypod@media.mit.edu']",True,zero-car,2004-01-01,[],Zero Car,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-07-15T00:13:14.782Z,"Imagine a physical object that floats through the air, unconstrained by gravity, which can move freely to display the motion of a planet in orbit or the flight path of an airplane. We introduce ZeroN, a new tangible interface element based on a levitated object that can be moved freely in a three-dimensional space to represent and control the dynamic status of computation.  Users can see, feel, and control computation through interaction with ZeroN. One can place and move the ZeroN in the air to animate characters' movements or simulate physics in a tangible form.",2012-01-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/files/Display/Picture%2014.png,25863,--Choose Location,2019-04-09T13:41:09.022Z,"['ishii@media.mit.edu', 'rehmi@media.mit.edu']",True,zeron-levitated-tangible-interface,2010-01-01,"['imaging', 'space', 'gesture-interface', 'zero-gravity']",ZeroN: Levitated Tangible Interface,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-07-15T00:13:14.858Z,"In robotics, the emerging field of electronic textiles and fiber-electronics represents a shift in morphology from hard and rigid mechatronic components toward a soft-architecture�and more specifically, a flexible planar surface morphology. It is thus essential to determine how a robotic system might actuate flexible surfaces for donning and doffing actions. Zipperbot is a robotic continuous closure system for joining fabrics and textiles. By augmenting traditional apparel closure techniques and hardware with robotic attributes, we can incorporate these into robotic systems for surface manipulation. Through actuating closures, textiles could shape-shift or self-assemble into a variety of forms.",2013-09-01,['personal-robots'],,25871,--Choose Location,2016-12-05T00:17:26.789Z,['cynthiab@media.mit.edu'],True,zipperbot-robotic-continuous-closure-for-fabric-edge-joining,2012-01-01,[],Zipperbot: Robotic Continuous Closure for Fabric Edge Joining,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-07-15T00:13:14.934Z,"This project is currently promoting a survey of data from the East Side (Zona Leste) of the city of Sao Paulo, Brazil. The aim is to detect the landscape dynamics: infrastructure and urban planning, critical landscapes, housing, productive territory, recycling, and public space. The material will be made available on a digital platform, accessible by computers and mobile devices: a tool specially developed to enable local communities to disseminate productive and creative practices that occur in the area, as well as to enable a greater participation in the formulation of public policies. ZL Vortice is an instrument that will serve to strengthen social, productive, and cultural networks of the region.",2016-01-01,['civic-media'],https://dam-prod2.media.mit.edu/x/files/Display/diagrama-zlvortice-02.jpg,25879,--Choose Location,2017-01-08T20:05:40.680Z,"['ethanz@media.mit.edu', 'adelineg@media.mit.edu']",True,zl-vortice,2015-01-01,[],ZL Vortice,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-09-27T14:49:47.518Z,"Mission Wildlife is a research collaboration between San Diego Zoo Global and the MIT Center for Civic Media to explore the potential for interactive technologies in conservation education. In particular, we used augmented reality (AR) to focus visitors’ attention towards survival threats to endangered species. The project was deployed during the 100th anniversary year (2016) of the San Diego Zoo. Visitors competed against each other to trigger 3D animations from animal signage in the zoo and shared results on social media to spread awareness about conservation issues. This case study demonstrates how AR can be tied to efforts to expand awareness of social issues.",2017-06-20,[],https://dam-prod2.media.mit.edu/x/2016/09/27/mission wildlife.jpg,42312,,2018-05-04T10:49:30.350Z,"['ethanz@media.mit.edu', 'oi7@media.mit.edu', 'gabem@media.mit.edu']",True,mission-wildlife,2016-06-20,"['augmented-reality', 'civic-media', 'civic-technology', 'crowdsourcing', 'entertainment', 'environment', 'gaming', 'holography', 'learning-teaching', 'social-media', 'interfaces', 'long-term-interaction']",Mission Wildlife,PUBLIC,http://oi7.me,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-10-12T19:23:25.590Z,"Animated GIFs are widely used on the Internet to express emotions, but automatic analysis of their content is largely unexplored. To help with the search and recommendation of GIFs, we aim to predict  how their emotions will be perceived by humans based on their content. Since previous solutions to this problem only utilize image-based features and lose all the motion information, we propose to use 3D convolutional neural networks (CNNs) to extract spatiotemporal features from GIFs. We evaluate our methodology on a crowdsourcing platform called GIFGIF with more than 6,000 animated GIFs, and achieve better accuracy than any previous approach in predicting crowdsourced intensity scores of 17 emotions. We have also found that our trained model can be used to distinguish and cluster emotions in terms of valence and risk perception.",2020-07-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2016/10/23/deep-gif.png,44835,,2022-05-12T17:30:29.546Z,"['cvx@media.mit.edu', 'picard@media.mit.edu']",True,deep-gif,2016-10-18,"['crowdsourcing', 'social-media']",Predicting perceived emotions in animated GIFs with 3D convolutional neural networks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-10-12T21:32:40.807Z,"Smart Communal Spaces is a project from MIT Media Lab Dubai Workshop 2016, which was deployed and filmed at the Dubai Museum of the Future. It explores how Mixed Reality could enhance team communication in shared office spaces by looking into the interplay of Slack and HoloLens. While existing co-working spaces provide the benefits of openness and flexibility, they sacrifice our privacy and personal experience. However, Mixed Reality enables us to project holograms into our physical reality, visually and auditorily reorganizing how bits and atoms exist around us. In the demo video, we try to imagine and tell a story of how human-computer interactions might look like in an office when team communication tools are operated in a spatial context without any streaming device. We broke down the most important elements in the office space by researching the core functions of a team communication software and several holographic applications.
                    
                Design Lead: Chrisoula Kapelonis, Technology Lead: Poseidon Hai-Chi Ho Students: Rajeev Mylapalli, Yazan Fanous, Lamees Alhashimi, Moza Al Naimi, Esra'a Alsanie, Asalah ArankiSpecial Thanks: Joichi Ito, Noah Raford, Nick DuPey, Ashley ShafferMIT Media Lab, IDEO, Wamda, Dubai Museum of the Future",2017-08-29,[],https://dam-prod2.media.mit.edu/x/2017/01/12/Smart Communal Spaces 2.png,44859,,2018-05-04T10:51:42.799Z,"['kapeloni@media.mit.edu', 'oi7@media.mit.edu']",True,smart-communal-spaces,2016-08-29,[],Smart Communal Spaces,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-10-13T19:34:20.907Z,The Social Robot Toolkit aims to provide a platform for children to learn through playful interaction. The social robot (Soro) toolkit allows preschool children to experiment with computational concepts while teaching a social robot new rules. The toolkit also provides a platform for learning interpersonal skills through the use of storytelling that integrates interpersonal and computational concepts. This harnesses preschoolers' natural interest in social interaction to familiarize them with new concepts.,2017-04-30,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/13/sorofeatureimageResize.jpg,44981,,2017-06-05T16:13:22.057Z,"['randiw12@media.mit.edu', 'haewon@media.mit.edu', 'michalg@media.mit.edu', 'cynthiab@media.mit.edu', 'edith@media.mit.edu']",True,soro,2014-08-23,[],Social Robot Toolkit,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-14T05:34:55.448Z,"Amoeba Wall: a context aware wall system. Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area. Award-A' Design Award 2017 - Gold Prize-Honorable Mention - Tomorrow Workplace Competition by METROPOLISPublication-H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9",2017-11-30,"['changing-places', 'city-science']",https://dam-prod2.media.mit.edu/x/2019/02/10/AmoebaWall_1m.gif,45029,,2019-02-11T15:09:42.775Z,"['alonsolp@media.mit.edu', 'honghaod@media.mit.edu', 'oi7@media.mit.edu']",True,_amoeba,2016-04-22,"['architecture', 'human-machine-interaction']",Amoetecture,PUBLIC,http://rnd.studio/project/amoeba-wall,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2016-10-14T14:46:54.477Z,"Multimodal textile sensate media as an expressive and deformable musical interfaceIn the area of intelligent textiles, we are exploring a multi-modal, fabric-based, stretchable sensate surface for physical interaction media, specifically as deformable musical interface. The fabric keyboard consists of multi-layer textile sensors machine-sewn in a keyboard pattern, and it detects different stimuli such as touch, pressure, stretch, proximity, and electric field. This allows users to explore physical and non-contact gestures for expressive on-body and on-surface musical performance. We've also developed additional textile-based inputs such as ribbon controller, trackpad, and fur for more expressive control. This soft sensate surface contributes toward developing seamless, self-aware, and washable media.",2018-01-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2017/06/15/IMG_6054-F.jpg,45074,,2021-04-13T01:54:11.486Z,"['joep@media.mit.edu', 'irmandy@media.mit.edu']",True,FabricKeyboard,2016-05-27,"['design', 'fashion', 'music', 'sensors', 'interfaces']",FabricKeyboard,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-14T16:12:45.924Z,"This project aims to show a different picture of the data behind the news, looking at how we analyze, represent, and interact with it. Video content is constantly created and added to the public archives, but there is never time to watch it all. News Graph explores a new method for interacting with news media. By analyzing the words that are said, extracting entities that appear, and finding the connections between them, we are able to map connections between video segments. Each connection represents two entities that were mentioned in the same video segment, and a video segment can be mapped to a number of connections.",2019-09-30,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/2016/12/08/NewsGraphHero.png,45088,,2019-04-18T01:15:48.619Z,"['jasrub@media.mit.edu', 'lip@media.mit.edu']",True,news-graph,2016-04-01,"['data', 'networks', 'interfaces', 'machine-learning']",News Graph,PUBLIC,http://news-graph.um-dokku.media.mit.edu,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-10-14T16:38:37.097Z,"The news is probably one of the first things people check in the morning, but how much does what you know and understand about the world depend on your news source? Will you view the world differently if you head over to CNN instead of BBC?“Perspectives” presents top news stories from many points of view. The viewer can easily see the different perspectives and get the whole story.",2018-06-06,"['ultimate-media', 'viral-communications']",https://dam-prod2.media.mit.edu/x/2016/10/14/logo.jpg,45098,,2018-10-17T23:15:04.211Z,"['jasrub@media.mit.edu', 'lip@media.mit.edu']",True,perspectives,2016-08-01,"['civic-media', 'storytelling']",Perspectives,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-10-14T17:12:14.714Z,"How can people learn advanced motor skills such as front flips and tennis swings without starting from a young age? The answer, following the work of Masters et. al., we believe, is implicitly. Implicit learning is associated with higher retention and knowledge transfer, but that is unable to be explicitly articulated as a set of rules. To achieve implicit learning is difficult, but may be taught using obscured feedback—that is, feedback that does not directly describe the result of an action.With AUFLIP , we sought to provide auditory feedback to help newcomers learn front flips. We created a wearable system with a simplified  model of a front flip that compares a user’s time to peak rotation  against their ideal time. As the user approaches their ideal  performance, the system begins playing a chord, only completing the  chord if the user manages to rotate at their ideal peak time. We tested  this system by integrating it into an environment where professional  coaches teach novices how to perform front flips; we found preliminary  results suggesting that users wearing the device exhibited implicit  learning. ",2018-08-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2019/01/15/0_AUFLIP_MAIN.JPG,45108,,2020-06-09T17:30:55.226Z,['dvlevine@media.mit.edu'],True,AUFLIP_Feedback,2016-10-14,[],AUFLIP,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-17T15:56:32.970Z,"Children’s oral language skills in preschool can predict their academic success later in life. Helping children improve their language and vocabulary skills early on could help them succeed later, in middle and high school. Learning language is also a very social, interactive activity. Learning language also takes time. Social robots could have great impact in this area, since they can leverage the same kinds of social cues and presence that people use. In this work, we asked whether a sociable robotic learning/teaching companion could supplement children’s early long-term language education. Children played with the robot for two months. The robot was designed as a social character, engaging children as a peer, not as a teacher, within a relational, dialogic context. The robot targeted the social, interactive nature of language learning through a storytelling game that the robot and child played together. The game was on a tablet: the tablet showed a couple of characters that the robot or child could move around while telling their story. During the game, the robot introduced new vocabulary words and modeled good story narration skills.Furthermore, because children may learn better when appropriately challenged, we asked whether a robot that matched the “level” or complexity of the language it used to the general language ability of the child might help children improve more. The robot told easier or harder stories based on an assessment of the child’s general ability. This work is supported by the NSF Expeditions in Computing award in Socially Assistive Robots.",2014-09-01,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/17/session8-p004b.jpg,45430,,2020-09-23T16:39:07.445Z,"['cynthiab@media.mit.edu', 'jakory@media.mit.edu']",True,storytelling-companion,2013-09-01,"['robotics', 'kids', 'learning-teaching', 'storytelling', 'social-robotics', 'long-term-interaction', 'language-learning']",Storytelling Companion,PUBLIC,http://robotic.media.mit.edu/portfolio/storytelling-companion,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0
False,2016-10-17T17:56:28.691Z,"Prior research with preschool children has established that book reading, especially when children are encouraged to actively process the story materials through dialogic reading, is an effective method for expanding young children’s vocabulary. A growing body of research also suggests that social robots have potential as learning companions and tutors for young children’s early language education. Social robots are new technologies that combine the adaptability, customizability, and scalability of technology with the embodied, situated world in which we operate.In this project, we asked whether a social robot can effectively engage preschoolers in dialogic reading. Given that past work has shown that children can and do learn new words from social robots, we investigate what factors modulate their learning. In particular, we looked at whether the verbal expressiveness of the robot impacted children’s learning and engagement during a dialogic reading activity. This project was funded by an NSF Cyberlearning grant.",2017-09-01,"['ml-learning', 'personal-robots']",https://dam-prod2.media.mit.edu/x/2016/10/17/vlcsnap-2016-10-15-10h26m19s77.png,45457,,2019-03-02T01:42:38.931Z,"['jakory@media.mit.edu', 'cynthiab@media.mit.edu', 'sooyeon6@media.mit.edu', 'haewon@media.mit.edu']",True,robot-vocal-expressiveness,2015-09-01,"['robotics', 'kids', 'learning-teaching', 'social-robotics', 'language-learning']",Robot Expressiveness Affects Children's Learning,PUBLIC,http://robotic.media.mit.edu/portfolio/robot-vocal-expressivity,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-17T18:36:39.208Z,"Early literacy plays an important role in a child's future. However, the reality is that over 57 million children have no access to a school and another 100 million attend such inadequate schools that they will remain functionally non-literate.Curious Learning is an open platform that addresses the deployment and learning challenges faced by under-resourced communities, particularly their limited access to literacy instruction.We are developing a system of early literacy apps, games, toys, and robots that will triage how children are learning, diagnose literacy deficits, and deploy dosages of content to encourage app play using a mentoring algorithm that recommends an appropriate activity given a child's progress. Currently, over 200 Android-based tablets have been sent to children around the world; these devices are instrumented to provide a very detailed picture of how kids are using these technologies. We are using this big data to discover usage and learning models that will inform future educational development.  The open-source software enables any Android device to be transformed into a literacy mentor. This platform is presently deployed in Ethiopia, Uganda, India, South Africa, and rural United States.The open-source tablet software enables data collection across the deployment sites.  By employing a data-driven approach to understanding learning behaviors across cultures and contexts, this project seeks to design and develop a personalized, adaptive learning platform.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/17/P1040264-2263.JPG,45469,,2019-04-17T18:41:03.019Z,"['cynthiab@media.mit.edu', 'nikhita@media.mit.edu', 'pcuellar@media.mit.edu']",True,curious-learning,2016-09-01,"['robotics', 'data', 'learning-teaching', 'language-learning']",Curious Learning: Understanding learning behaviors for early literacy,PUBLIC,http://www.curiouslearning.org/,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-10-17T18:54:58.229Z,"When learning from human partners, infants and young children will pay attention to nonverbal signals, such as gaze and bodily orientation, to figure out what a person is looking at and why. They may follow gaze to determine what object or event triggered another's emotion, or to learn about the goal of another's ongoing action. They also follow gaze in language learning, using the speaker's gaze to figure out what new objects are being referred to or named.In this project, we examine whether young children will attend to the same social cues from a robot as from a human partner during a word learning task, specifically gaze and bodily orientation.",2015-12-31,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/17/kid-with-robot.png,45477,,2017-06-07T16:38:44.917Z,"['cynthiab@media.mit.edu', 'jakory@media.mit.edu', 'sooyeon6@media.mit.edu']",True,children,2014-09-01,"['robotics', 'kids', 'learning-teaching', 'social-robotics', 'language-learning']",Children Use Nonverbal Cues to Learn from Robots,PUBLIC,http://robotic.media.mit.edu/portfolio/children-use-nonverbal-cues-learn-robots/,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-17T18:56:37.594Z,"This project aims to create an agent that�using common-sense reasoning�predicts tasks users may be interested in adding to their to-do lists, based on the context of appointments that they are adding to their calendars. For instance, if the user makes the calendar appointment ""go fishing,"" the agent may recommend the task ""pack fishing equipment."" By automatically generating potential tasks, the agent saves the users time, and assists them in remembering actions that they may commonly forget.",2006-09-01,['software-agents'],,45486,E15-383,2016-10-17T18:56:37.860Z,['lieber@media.mit.edu'],False,anticipating-user-tasks-using-common-sense-reasoning,2004-09-01,[], Anticipating User Tasks Using Common-Sense Reasoning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-10-17T21:48:57.254Z,"Tidmarsh is a 600-acre former cranberry farm near Plymouth, MA that has undergone a restoration to wetland. We have instrumented the site with an extensive network of custom low-power environmental sensor nodes, microphones, and cameras. The data from the network is made available in real time and has enabled a number of explorations into the ways that people can experience and learn from large-scale, long-term sensor installations.See sensor data, listen to live audio, and watch live camera feeds on the Tidmarsh website.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2016/10/17/node.jpg,45512,,2020-06-14T03:32:39.944Z,"['bmayton@media.mit.edu', 'joep@media.mit.edu']",True,low-power-wireless-environmental-sensor-node,2016-06-01,"['ecology', 'environment', 'networks', 'sensors', 'climate-change']",Low-power wireless environmental sensor network,PUBLIC,https://tidmarsh.media.mit.edu,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2016-10-18T15:32:16.064Z,"Content and curriculum lie at the heart of every educational system, learning environment, and learning technology. Yet the infrastructure to support this main pillar is severely lacking, and largely consists of a scattered set of elements: research on learning constructs, various curricula and syllabi, loosely tied together by jurisdiction-based standards documents (i.e. the Common Core State Standards, Finland’s education standards, etc.). These elements evolved before the digital era, based on different constituent's needs. However, the disjointedness of these structures leave the management of knowledge on learning constructs—and the design/access of learning tools and technologies for these learning constructs—in a messy, challenging state, unable to successfully carry us into the 21st century and the future of learning technologies and educational systems.The LearningGraph is a research initiative that uses systems engineering practices to reengineer this infrastructure, using learning sciences frameworks and modern data modeling and ontological design. The goal of the project is to create a unified data model that supports the multiple stakeholders who work with learning constructs (teachers, learners, learning and assessment designers, learning scientists, educational technologist, informal learning facilitators, etc.), and provide an ongoing ""living sandbox"" where this knowledge is modeled and used to support teaching and learning across learning environments, technologies, and experiences.",2018-05-01,[],,45602,,2018-06-20T20:00:43.152Z,['jgroff@media.mit.edu'],True,learning-genome,2016-10-20,"['data', 'learning-teaching']",The LearningGraph,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-10-18T15:43:01.389Z,"In the United States, there are an estimated 1.7 million people living with amputation, with that number expected to double by 2050. Complications of prosthetic leg use in persons with lower extremity amputation (LEA) include delayed wound healing, recurrent skin ulcerations, and pressure damage to soft tissues. This can result in limited mobility, which further contributes to conditions such as obesity, musculoskeletal pathologies (e.g., osteoarthritis, osteopenia, and osteoporosis), as well as cardiovascular disease. Traditionally, fabrication of prosthetic sockets remains a fundamentally artisanal process with limited input of quantitative data. Even with advances in computer-aided design and manufacturing (CAD/CAM), prosthetists often modify sockets using non-quantitative craft processes requiring substantial human hours and financial cost. The goal of this research is to develop and validate musculoskeletal ultrasound imaging techniques for creating predictive biomechanical models of residual limbs that will reduce the barrier for and cost of computer aided design (CAD)-driven prosthetic socket design in the US and in low-and middle-income countries.",,['biomechatronics'],,45611,,2019-04-17T19:24:01.508Z,"['hherr@media.mit.edu', 'branger@media.mit.edu', 'kmoerman@media.mit.edu']",True,ultrasound-prosthetic-interface-design,2016-10-18,"['bioengineering', 'health']",Ultrasound imaging for transtibial prosthetic interface design,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-10-18T21:21:48.945Z,"We show that using thin slices (< 1 minute) of facial expression and body language data, we can train a deep neural network to predict whether two people in a conversation will bond with each other. Bonding is measured using the Bonding subscale of the Working Alliance Inventory. We show that participants who experience bonding perceive their conversational partner as interesting, charming, and friendly, and do not perceive them as distant or annoying. The data are collected from a user study of naturalistic conversations, in which participants were asked to interact for 20 minutes, and were recorded using cameras, microphones, and Microsoft Kinects. To ensure participants did not become self-conscious of their non-verbal cues, they were told the purpose of the study was to train machine learning algorithms to read lips. We show that not only can we accurately predict bonding from participants' personality, disposition, and traits, but that we can predict whether the participant will experience bonding up to 20 minutes later, using only one-minute thin slices of facial expression and body language data. This ability could be extremely useful to an intelligent virtual agent, because if it could detect at one-minute intervals whether it was bonding with its user, it could make course corrections to promote enjoyment and foster bonding. We provide an analysis of the facial expression and body language cues associated with higher bonding, and show how this information could be used by an agent to synthesize the appropriate non-verbal cues during conversation.",2019-07-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2016/10/18/heatMapNoLabels.png,45653,,2022-05-12T17:31:01.290Z,"['jaquesn@media.mit.edu', 'picard@media.mit.edu']",True,predicting-bonding,2015-10-01,"['human-machine-interaction', 'perception', 'machine-learning']",Predicting Bonding in Conversations,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-18T23:16:57.791Z,Social robots take an active role in the inference process by producing social cues to elicit nonverbal responses from children to better understand their cognitive state.,,['personal-robots'],,45673,,2017-06-05T15:58:26.432Z,[],False,interactive-inference-of-mental-states,2016-10-18,"['robotics', 'kids']",Interactive Inference of Mental States,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-19T14:17:13.002Z,"This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.This project will support social scientists, philosophers, and policy and legal scholars who undertake research that aims to impact how artificial intelligence technologies are designed, implemented, understood, and held accountable. It will also provide a platform to create, convene, and support a diverse and powerful network of people and institutions who are working to steer AI in ethically conscious directions, both in fields of specialized AI as well as general AI.  The project will investigate the social implications of the maturation and proliferation of AI. It will help catalyze and support research that advances AI in the public interest and fund engineers who want to help define public interest in AI through the code they write and machines they build. 	The initiative also organized a high-level symposium at the Media Lab on the topic that took place in April 2016 between the academic community and industry leaders working on AI.",2019-12-01,"['ethics', 'scalable-cooperation']",,45767,,2020-02-28T17:44:03.187Z,"['irahwan@media.mit.edu', 'tenzin@media.mit.edu']",True,ai-ethics-and-governance,2016-04-01,"['artificial-intelligence', 'extended-intelligence', 'social-science']",AI Ethics and Governance,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-10-19T14:20:15.432Z,"This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.The initiative, working with George Church, professor of genetics at Harvard Medical School, and Kevin Esvelt, head of the Sculpting Evolution Group  at the Media Lab, will explore the broader ethical dimensions for developing tools that involve CRISPR and the expansion of gene editing technologies. This work aims to create a dialog between scientists and citizens, and will include spiritual leaders, religious leaders, and community leaders in a wider conversation about the ethical implications and potential repercussions of the introduction and deployment of these emerging technological interventions. This project is exploring novel ways to inform citizens about science and how they can affect policy based on news scientific developments.
                    
                ",2019-12-01,['ethics'],,45775,,2020-02-28T17:45:04.803Z,"['esvelt@media.mit.edu', 'tenzin@media.mit.edu']",True,gene-editing-and-biomedical-ethics,2017-05-01,"['genetics', 'ethics']",Gene Editing and Biomedical Ethics,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-10-19T14:21:38.225Z,"This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.This initiative is designing and disseminating new tools and pedagogies for introducing “non-prescriptive” ways of learning about ethical frameworks and values in educational and governmental organizations, including law enforcement agencies, around the world.  The program is collaborating with high schools and middle schools to develop new curricula and resources  for teachers that integrate critical thinking methodologies with hands-on workshops.  The current project has involved designing apps and games and is working with a variety of collaborators including Education Arcade, Learning Games Network, MasterCard Foundation, Hope Lab, WGBH, and Televisa. Programs have already been deployed through organizations in the US, Mexico, Colombia, India, and parts of Africa.
                    
                ",2019-12-01,['ethics'],https://dam-prod2.media.mit.edu/x/2017/07/21/spark.jpg,45783,,2020-02-28T17:45:32.774Z,['tenzin@media.mit.edu'],True,ethics-learning-and-education,2017-05-01,[],Ethics and Empathy Learning,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-10-19T14:22:47.449Z,"This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.As part of its broader work around learning, this project is exploring both individualized and community-based models for promoting empathy by designing training methods and developing complementary  technologies. The initiative launched 20 Day Stranger app with Playful Systems . The initiative is also working with the Opera of the Future group on a Vocal Vibrations/ Finding Your Voice interactive exhibition targeted towards awareness, empathy, and empowerment. The exhibition debuted in Paris and Cambridge with plans for Mexico City in 2017.",2019-12-01,['ethics'],,45791,,2020-02-28T17:45:53.589Z,"['slavin@media.mit.edu', 'tod@media.mit.edu', 'tenzin@media.mit.edu']",True,learning-empathy,2017-05-01,[],"Strangers, Voices, and Society",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-10-19T16:14:18.217Z,"The use of data-driven methods to examine dynamic spaces, relationships, and mechanisms within an urban environment frames the city as a complex system. This framing in turn casts residents as contributors and actors rather than passive subjects. This project uses narrative interactive visualization to integrate public data with urban planning concepts for communication to and by residents of a city. 
                    
                ",2017-05-01,['social-computing'],https://dam-prod2.media.mit.edu/x/2016/10/19/Draw your city test basemap8_0hMiana.jpg,45808,,2022-06-10T14:40:02.141Z,['zhangjia@media.mit.edu'],True,narrative-visualization-for,2016-09-01,"['design', 'data', 'storytelling', 'urban-planning', 'interfaces']",Narrative Visualization for Distributed Urban Interventions,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-10-19T18:38:19.419Z,"Unlike traditional randomized controlled trials that generalize relationships in large groups of people, single-case experiments seek to quantify an individual's reaction to an intervention by measuring an independent variable's effect on a dependent variable (i.e., an intervention's effect on an outcome behavior). These single-case experiments are then combined back together using Bayesian Statistics methods in order to learn more general patterns about a population. We are interested in single-case experiments that test the causal relationships between behaviors that have been observed to be correlated with higher wellbeing.Thus, instead of using an RCT to find what works for the imaginary ""average"" person, we can learn what works for each individual and then carefully combine data to generalize the results to other real individuals.To our knowledge, single-case experiments have not been implemented in a smartphone app format. We believe that a successful app will allow researchers to dramatically scale the number of participants in these studies.Code available on GitHub! ",2017-06-01,"['advancing-wellbeing', 'affective-computing']",https://dam-prod2.media.mit.edu/x/2018/05/01/Pasted image at 2016_10_19 02_42 PM.png.1400x1400.png,46914,,2021-10-07T16:49:33.342Z,"['fergusoc@media.mit.edu', 'picard@media.mit.edu', 'akanes@media.mit.edu', 'sataylor@media.mit.edu', 'amohan@media.mit.edu']",True,quantifyme,2016-06-01,"['data', 'wearable-computing', 'behavioral-science', 'affective-computing', 'wellbeing']",QuantifyMe,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-19T19:01:56.945Z,"Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these ""osseointegrated"" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.
                    
                ",,['biomechatronics'],,46922,,2021-09-17T19:34:33.977Z,"['clites@media.mit.edu', 'mcarney@media.mit.edu', 'lfreed@media.mit.edu', 'hherr@media.mit.edu', 'syeon@media.mit.edu', 'tonyshu@media.mit.edu', 'thhsieh@media.mit.edu']",True,osseo,2016-10-19,"['robotics', 'design', 'bioengineering', 'consumer-electronics', 'biomechanics', 'mechanical-engineering', 'electrical-engineering', 'engineering', 'mechatronics', 'bionics']",An osseointegrated prosthesis with bi-directional neural communication,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-19T19:07:02.076Z,"Lower-extremity amputation surgery has not seen significant change since the Civil War. This research is focused on the development of novel amputation paradigms that leverage native biological end organs to interpret efferent motor commands and to provide meaningful neural feedback from an artificial limb. Surgical replication of natural agonist-antagonist muscle pairings within the residuum allow us to use biomimetic constructs to communicate joint state and torque from the prosthesis directly to the peripheral nervous system. We hypothesize that these architectures will facilitate control of advanced prosthetic systems to improve gait and reduce metabolic cost of transport.
                    
                ",,['biomechatronics'],,46930,,2021-09-17T19:33:57.237Z,"['clites@media.mit.edu', 'shriyas@media.mit.edu', 'hherr@media.mit.edu', 'lfreed@media.mit.edu']",True,amputation,2016-08-15,"['bioengineering', 'health', 'neurobiology']",Revolutionizing amputation surgery for the restoration of natural neural sensation and mobility,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2016-10-19T22:30:14.577Z,"Room-scale virtual reality opens up exciting new possibilities for exploratory learning. Phenomena that otherwise cannot be experienced directly (e.g. subjects that are microscopic, remote, or dangerous) can be transformed into environments that are immersive, interactive and social. Electrostatic Playground is a VR physics lab where multiple users can explore and discover principles of electrostatics through experimentation. It also concretizes abstract notions of electrostatics in the form of tangible, interactive objects. Users can learn by directly manipulating physics objects while receiving real-time feedback from the environment. We've incorporated the ability to record these interactions in order to provide a means of authoring content, reviewing one's notes, and teaching others. Electrostatic Playground is a multi-user lab where users can explore and discover principles in electrostatics.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/04/30/mr-electrostatic.png,46971,,2019-04-17T20:09:56.460Z,"['swgreen@media.mit.edu', 'wjlc@media.mit.edu', 'hbedri@media.mit.edu', 'erinhong@media.mit.edu']",True,vr-physics-lab,2016-06-01,[],Electrostatic Playground: A multi-user virtual reality physics learning experience,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-19T22:36:53.600Z,"Visit DataUSADataUSA is the most comprehensive site visualizing public data for the United States. Through interactive profiles, DataUSA makes available data from a variety of public sources, including the American Community Survey, the Bureau of Economic Analysis, the Bureau of Labor and Statistics, the Department of Education (IPEDS), and the county health records from the University of Wisconsin in Madison. ",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/uuid/ff01792a-caec-4aa0-b7e1-8b2bbd1cb872,46995,,2020-08-24T17:28:16.301Z,['hidalgo@media.mit.edu'],True,datausa,2014-10-01,"['design', 'data', 'data-visualization']",DataUSA,PUBLIC,http://datausa.io,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2016-10-19T22:52:19.724Z,"What if we could look at the entire history of humanity at once? Pantheon aims to create a data-driven view of history by collecting, visualizing, and analyzing data on the biographies of historical characters. We are particularly interested in what this data can teach us about the production of knowledge across history, the relation between accomplishment and fame, and information wanes from history.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2016/10/20/Pantheon_1.jpeg,47019,,2020-08-28T19:02:06.673Z,"['kzh@media.mit.edu', 'amy_yu@media.mit.edu', 'crisjf@media.mit.edu', 'hidalgo@media.mit.edu']",True,pantheon-new,2016-10-19,"['art', 'communications', 'data', 'entertainment', 'music', 'technology', 'social-change', 'history']",Pantheon,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0
False,2016-10-19T23:03:39.001Z,Can I borrow your network? Shout! is a marketplace for retweets that allows people to exchange micro-contracts for future retweets. Shout! facilitates the coordination of social media diffusion efforts by groups.,2016-06-01,['collective-learning'],https://dam-prod2.media.mit.edu/x/2016/10/20/Shout.jpg,47027,,2018-05-07T18:22:08.055Z,['hidalgo@media.mit.edu'],True,shout,2015-09-01,['social-media'],Shout!,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-10-19T23:05:25.962Z,"Streetscore is a computer vision algorithm that estimates people's perception of urban environments. We have used Streetscore to create high resolution maps of urban perception with the goal of studying the social impact of urban perception, and also, to study urban change.",2016-10-22,[],,47035,,2016-12-05T00:16:57.478Z,['naik@media.mit.edu'],True,streetscore-new,,[],Streetscore,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-10-19T23:06:22.512Z,"DataViva made available data for the entire economy of Brazil, including exports and imports for each municipality and product, and occupation data for every municipality, industry, and occupation.You can experience dataviva at legacy.dataviva.info",2017-04-06,['collective-learning'],https://dam-prod2.media.mit.edu/x/2016/10/20/DataViva.jpeg,47043,,2020-08-24T17:33:39.235Z,['hidalgo@media.mit.edu'],True,dataviva-new,,[],DataViva,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2016-10-19T23:07:17.723Z,"The current interface of emails is designed around time and messages, pushing people to focus on what is more recent rather than important. Immersion is a design experiment that centers the email interface on people and the networks that people form.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2017/03/14/Screen Shot 2017-03-14 at 10.37.57 AM.png,47051,,2020-08-24T17:25:57.250Z,"['hidalgo@media.mit.edu', 'smilkov@media.mit.edu', 'djagdish@media.mit.edu']",True,immersion-new,2016-10-01,['data'],Immersion,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-10-19T23:13:07.001Z,Participie was a design experiment on direct participation for constrained choices (like budgets).,2018-07-01,['collective-learning'],,47059,,2020-08-24T21:11:38.001Z,['sanjayg@media.mit.edu'],True,participie-new,,[],Participie,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2016-10-19T23:13:57.837Z,"The Observatory of Economic Complexity (OEC) is the world's leading data visualization tool for international trade data. The OEC makes more than 50 years of international trade data available through dozens of millions of interactive visualizations.Visit the OEC at: https://oec.world/ (This is an external site, not affiliated with the Media Lab.)",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2017/11/02/Screen Shot 2017-11-02 at 11.55.26 AM.png,47067,,2020-08-19T19:55:58.160Z,['hidalgo@media.mit.edu'],True,oec-new,2011-09-12,[],The Observatory of Economic Complexity (OEC),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2016-10-19T23:14:38.401Z,"Place Pulse is a crowdsourcing effort to map urban perception. By asking users to select images from a pair, Place Pulse collects the data needed to evaluate people's perceptions of urban environments. This data is also the data used to train Streetscore. Place Pulse was developed by Phil Salesses as part of his requirement to complete his master's thesis. The present version of Place Pulse was re-engineered by Daniel Smilkov and Deepak Jagdish.",2014-08-01,[],,47075,,2018-05-07T18:06:16.218Z,"['salesses@media.mit.edu', 'smilkov@media.mit.edu', 'djagdish@media.mit.edu']",True,place-pulse-new,2010-08-01,[],Place Pulse,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-10-20T00:11:38.557Z,"This collection of Tod Machover’s music focuses on chamber and orchestral music composed during the last decade, both with and without electronic enhancement. Machover’s music is a fascinating blend of expressive and lyrical melody combined with a sophisticated ear for textural complexity. The resulting music is always a treat for the ears—colorful, vibrant, and rhythmically propulsive. The largest composition on this disc is the piano concerto Jeux Deux, scored for large orchestra (the work was commissioned and first performed by the Boston Symphony Orchestra), with the soloist performing on a “hyperpiano”—a concert grand piano which interacts with sensors and computer programs in order to expand its technical possibilities. Machover produces cutting-edge music with a heart!",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2016/10/20/Screen-Shot-2016-03-03-at-9.44.23-AM-8860-768x772.png,47083,,2019-04-17T19:56:25.695Z,['tod@media.mit.edu'],True,but-not-simpler,2011-11-08,"['robotics', 'music', 'performance']",…but not simpler…,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-20T01:40:46.267Z,"
                    Optogenetic techniques have recently been applied to peripheral nerves as a scientific tool with the translatable goal of alleviating a variety of disorders, including chronic pain, muscle fatigue, glucose-related pathologies, and others.  When compared to the electrical stimulation of peripheral nerves, there are numerous advantages: the ability to target molecularly defined subtypes, access to opsins engendering neural inhibition, and optical recruitment of motor axons in a fashion that mimics natural recruitment, which eliminates the fatigue roadblock inherent to functional electrical stimulation. The ability to control peripheral nerves situated under deep tissue structures with transdermal, optical signals would be of enormous benefit, integrating all of the advantages conferred by optogenetics while averting the drawbacks associated with implantable devices, such as mechanical failure, device tissue heating, and a chronic foreign body response. We work to develop novel molecular and optical methods in an effort to enable this transdermal optogenetic peripheral nerve control. A further example of a potential clinical application involves optogenetically targeting the vagus nerve, a peripheral cranial nerve implicated in numerous ailments, including epilepsy, migraines, obesity, hypertension, fibromyalgia, Crohn’s disease, asthma, depression, and obsessive-compulsive disorder.  An efficient method of stimulating the vagus nerve with minimal side-effects and high target specificity, such as described here, may have profound implications to the study of various illnesses and disabilities.",,['biomechatronics'],,47091,,2021-09-17T19:35:13.765Z,"['bmaimon@media.mit.edu', 'shriyas@media.mit.edu', 'hherr@media.mit.edu', 'esb@media.mit.edu', 'lfreed@media.mit.edu']",True,transdermal-optogenetic-peripheral-nerve-stimulation,2015-01-01,"['robotics', 'health', 'human-machine-interaction', 'neurobiology', 'prosthetics', 'neural-interfacing-and-control']",Transdermal Optogenetic Peripheral Nerve Stimulation,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-10-20T01:58:44.272Z,Unconventional mixing of research fields introduces a new method to study human behavior using social robots.,2019-09-01,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/19/cover.png,47107,,2019-10-09T14:05:35.275Z,"['cynthiab@media.mit.edu', 'jinjoo@media.mit.edu']",True,human-robot-trust,2012-11-03,"['artificial-intelligence', 'data', 'human-machine-interaction', 'social-science', 'machine-learning', 'social-robotics', 'nonverbal-behavior', 'affective-computing', 'trust']",Human-Robot Trust,PUBLIC,http://www.jinjoolee.com,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-10-20T06:59:20.095Z,"A computational model predicts the degree of trust a person has toward their novel partner, and advances our scientific understanding about interpersonal trust.
                    
                ",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2016/10/20/cover.png,47115,,2017-06-05T15:54:07.885Z,"['jinjoo@media.mit.edu', 'cynthiab@media.mit.edu']",True,computational-trust,2013-12-04,"['artificial-intelligence', 'machine-learning', 'nonverbal-behavior', 'affective-computing']",Computational Trust,PUBLIC,http://www.jinjoolee.com,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-10-20T14:34:05.340Z,"As more powerful and spatially aware Augmented Reality devices become available, we can leverage the user’s context to embed reality with audio-visual content that enables learning in the wild. Second-language learners can explore their environment to acquire new vocabulary relevant to their current location. Items are identified, ""labeled"" and spoken out loud, allowing users to make meaningful connections between objects and words. As time goes on, word groups and sentences can be customized to the user's current level of competence. When desired, a remote expert can join in real-time for a more interactive ""tag-along"" learning experience.",2018-05-31,"['ml-learning', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2017/09/29/WordSense.jpg,47131,,2018-08-20T16:21:23.964Z,"['pattie@media.mit.edu', 'cdvm@media.mit.edu']",True,wordsense-learning-language-in-the-wild,2016-09-15,"['virtual-reality', 'augmented-reality', 'learning-teaching', 'interfaces', 'language-learning']",WordSense,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-20T15:45:17.248Z,"Inspired by the simplicity and aesthetics of traditional Montessori education, these materials live within and extend this pedagogy to address new proficiencies and emerging fields, such as computational thinking. The foundation of computer science is not included in many early childhood and elementary curricula. When it is, however, the emphasis tends to be on the interface, rather than the concepts behind it. These materials break down the fundamentals of computation into a set of discrete and tangible concepts that are expressed in hands-on, tactile ways. ",2017-05-01,['social-computing'],https://dam-prod2.media.mit.edu/x/2016/10/20/materials - 38.jpg,47139,,2022-06-10T14:40:02.161Z,"['sdkamvar@media.mit.edu', 'smithkim@media.mit.edu']",True,new-learning-materials-for-computational-thinking,,['learning-teaching'],New Learning Materials for Computational Thinking,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-10-20T15:55:46.915Z,"Who should decide whether, when, and how to alter the environment? These are hard questions, especially when the decisions will impact people in many different communities or nations. Daisy drive systems may help by empowering local communities to make decisions concerning their local environments without imposing them on anyone else.The problem with current CRISPR-based gene drive systems is that they can spread indefinitely—potentially affecting every population of the target species throughout the world. It's unclear how such ""global"" drives can be safely tested, much less whether nations will ever agree to use them. To return power to the hands of local communities, we devised a new form of drive system called a ""daisy drive"" that can only affect local environments. The trick was to teach DNA to count and limit gene drive spreading to a pre-programmed number of generations. We hope that daisy drives will simplify decision-making and promote responsible use by allowing local communities to decide how to solve their own ecological problems.",,['sculpting-evolution'],https://dam-prod2.media.mit.edu/x/2019/04/02/daisy_chains.png,47147,,2019-04-03T16:07:53.609Z,"['buchthal@media.mit.edu', 'esvelt@media.mit.edu']",True,daisydrives,2016-06-06,"['bioengineering', 'genetics', 'synthetic-biology', 'biology']",Daisy Drives,PUBLIC,http://www.sculptingevolution.org/daisydrives,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-10-20T16:08:13.031Z,"These wall drawings are generated by a series of predesigned instructions that dictate their form. The instructions give both constraint and flexibility, so that the piece that unfolds has a clear structure and at the same time expresses the individual aesthetic preferences of the participants who contribute. Since each step depends on the previous steps, the result is a dynamic, collaborative piece, authored collectively by the artist and the exhibit visitors. ",2017-05-01,['social-computing'],https://dam-prod2.media.mit.edu/x/2016/10/20/SK_KS_04.jpg,47156,,2022-06-10T14:40:02.185Z,"['sdkamvar@media.mit.edu', 'smithkim@media.mit.edu']",True,boundaries-drawings,,['art'],Boundaries Drawings,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-10-20T16:32:35.536Z,"Foundations (2015 - 2016): The Year 1 MIT/Google collaboration program has Pilot Labs in NYC and Oakland. In the Labs, we introduce a cohort of 8th and 9th graders to the rudiments of Coding and Maker Technologies. Our pedagogy emphasizes Making and Doing and is greatly emphasized by Seymour Papert’s theory of Constructivism and Mel King’s work with youth at the South End Technology Center. For the program, we design and author Curricula and Learning Activities for the Coaches/Instructors, the students, and the parents.The Domains include Coding, Design, Fabrication, Games, Digital Music and Interactive Media. The Toolbox includes Scratch, Formit, Tinkercad, Libre Office, YouiDraw, LittleBits, Laser Cutters, 3D printers, Codebender/Arduino, Touchboard, and MaKey MaKey.  In addition, the students receive Leadership Training, Mathematics, and College Preparation.",,['code-next'],https://dam-prod2.media.mit.edu/x/2016/10/20/160210B-619.jpg,47164,,2017-04-04T21:48:15.281Z,"['tcarew@media.mit.edu', 'bdunning@media.mit.edu', 'lholt@media.mit.edu']",True,code-next,,"['kids', 'learning-teaching', 'computer-science']","Foundations : Design of Codified Curricula in Coding, Design, Fabrication, Games, Music, Media, etc.",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-10-20T17:01:57.176Z,"For centuries, across geographies, religions, and cultures, people try to innovate ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity. This challenge is especially important in a time when we wonder what the limits of Artificial Intelligence are: Can machines learn to scare us? Towards this goal, we present you Haunted Faces and Haunted Places: computer generated scary imagery powered by deep learning algorithms!
                    
                ",2016-11-01,[],https://dam-prod2.media.mit.edu/x/2016/10/20/1000_haunted_5768013969_839183995c_b.png,47172,,2020-07-23T18:49:42.817Z,"['irahwan@media.mit.edu', 'nobradov@media.mit.edu', 'cebrian@media.mit.edu', 'pinary@media.mit.edu']",True,nightmare-machine,2016-10-20,"['artificial-intelligence', 'crowdsourcing', 'social-media', 'storytelling', 'machine-learning']",Nightmare Machine,PUBLIC,http://nightmare.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-10-20T18:58:08.291Z,"Since the killing of Michael Brown, the Black Lives Matter movement has organized on social media to draw attention to the deaths of unarmed black people killed by US police. Have news organizations responded to this demand, and have we seen a significant change over time in reporting about those deaths?In this analysis of deaths from January 2013 through June 2016, we show that an unarmed black person killed by US police received 10.5x the incidence rate of news articles after Michael Brown’s death than those killed before, but that the predicted number of articles is no longer significantly different from 2013 levels.",,[],https://dam-prod2.media.mit.edu/x/2016/10/20/slides_mapping_police_violence.001.png,47345,,2016-12-13T16:23:36.317Z,"['jnmatias@media.mit.edu', 'rahulb@media.mit.edu', 'ethanz@media.mit.edu', 'ngyenes@media.mit.edu']",True,whose-lives-matter-in-the-news,2015-07-01,['civic-media'],Whose Lives Matter in the News?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1
False,2016-10-20T19:36:41.877Z,"The Mediated Atmosphere project envisions a smart office that is capable of dynamically transforming itself to enhance occupants' work experience.In the knowledge economy, worker satisfaction is paramount to retention and productivity. Recent studies have identified a decline in workplace satisfaction. Our research demonstrates how Mediated Atmosphere address this growing need. We created a workspace prototype equipped with a modular real-time control infrastructure, integrating biosignal sensors, controllable lighting, projection, and sound.",,['responsive-environments'],,47520,,2020-05-19T19:22:34.018Z,"['nanzhao@media.mit.edu', 'azaria@media.mit.edu', 'joep@media.mit.edu', 'richer@media.mit.edu']",True,mediated-atmospheres,2015-09-01,"['human-computer-interaction', 'architecture', 'health', 'learning-teaching', 'music', 'wearable-computing', 'sensors', 'machine-learning', 'community', 'technology', 'covid19']",Mediated Atmosphere,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-10-20T20:44:04.299Z,"What if our mobile devices could sense and then adapt to the spatial, temporal, and social context of their local environments? Imagine if your smartphone was smart enough to know that it should not be ringing loudly when you are in an important meeting, or that it should not be in silent mode when you are trying to find where you have misplaced it at home. We have created an inexpensive secure system that delivers this goal by embedding contextual information into the environment rather than the phone.  In that way, all mobile devices at a given location can detect the broadcasted contextual information using Wi-Fi and change their behavior accordingly,  without requiring any handshake or internet connection. By leveraging the latest and most inexpensive Wi-Fi modules on the market, and by building our own embedded firmware, server-side software, and mobile app, we are able to deploy this system in a secure and massively scalable way. ",2018-06-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2016/10/21/P1080994ok.JPG,47986,,2020-06-10T16:26:43.247Z,"['alims@media.mit.edu', 'vmb@media.mit.edu']",True,quiesense,2016-10-21,[],QuieSense:  Distributed context-awareness system for Wi-Fi enabled mobile devices,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-10-20T20:45:31.436Z,Terahertz time-gated spectral imaging for content extraction through layered structures.,,['camera-culture'],https://dam-prod2.media.mit.edu/x/2016/10/20/read-closed-book.png,47994,,2017-10-10T19:20:00.582Z,['barmak@media.mit.edu'],True,reading-through-a-closed-book,2016-09-01,['imaging'],Reading Through a Closed Book,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-10-20T20:59:37.828Z,"We are currently developing novel DNA editing technologies to broaden the scope of genome engineering. Our strategy is based on identifying and engineering endonucleases from diverse living systems, along with targeting with synthetic molecules. Together these components confer greater stability, minimize off-target DNA cleavage, and eliminate sequence restrictions for precision genetic manipulations within cells.",,['molecular-machines'],https://dam-prod2.media.mit.edu/x/2016/10/21/tdON 0825.jpg,48003,,2017-09-23T06:18:07.043Z,"['jacobson@media.mit.edu', 'njakimo@media.mit.edu', 'pranam@media.mit.edu']",True,synthetic-genome-engineering,2016-06-06,"['bioengineering', 'genetics', 'synthetic-biology']",Synthetic Genome Engineering,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-10-20T21:18:34.320Z,"SpeechBlocks is a medium that allows children (4-5 years old) to engage in open-ended play with writing. They can build arbitrary compositions out of words and associated images, which can become cards, signs, stories, and ""books."" We hypothesize that such creative, self-expressive play can foster development of basic literacy skills, like phonological awareness. However, because users of SpeechBlocks are not yet in command of writing, it is necessary for the system to scaffold and guide them. We study a variety of ways to accomplish this.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2020/06/01/SpeechBlocks-on-2-Tabs_5HT45jf-cropped2.png,48011,,2020-06-01T13:11:55.634Z,"['snehapm@media.mit.edu', 'isysoev@media.mit.edu', 'jhgray@media.mit.edu']",True,speech-blocks,2015-03-01,"['design', 'construction', 'kids', 'learning-teaching', 'storytelling', 'language-learning', 'computer-science', 'creativity']",SpeechBlocks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-20T21:31:49.850Z,"StoryBlocks aims to promote creative expression, literacy development, and social-emotional development through storytelling for children ages six to ten. In this app, children create personally generated, comic-style stories by inserting characters, setting emotions, typing dialogue, using words to insert images that customize scenes, and recording their voices to narrate their unique stories. With StoryBlocks, we can collect a corpus of children’s stories in order to build analysis tools that can document children’s narrative development over time, and support coaches in providing personalized scaffolding for children’s narratives. ",,[],https://dam-prod2.media.mit.edu/x/2019/04/11/IMG_2422_gMB5dHi.JPG,48019,,2020-06-09T14:45:23.012Z,"['anneli@media.mit.edu', 'jnazare@media.mit.edu', 'exposito@media.mit.edu', 'sballing@media.mit.edu', 'snehapm@media.mit.edu', 'jhgray@media.mit.edu', 'micshen@media.mit.edu']",True,storyblocks,2016-07-01,"['design', 'kids', 'learning-teaching', 'storytelling']",StoryBlocks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-20T21:35:55.921Z,"Speech synthesis in tutor mode. Using phones for literacy learning is an empowering application of mobile technology, but there are elements of the human tutor that have yet to be replicated in current apps. Namely, when reading a story, a tutor is likely to be more expressive and colorful in tone. When encountering a new word, a tutor might emphasize the vowel phoneme or stress a consonant pair the child has yet to master. By modeling speech with deep neural networks, our speech synthesizer will be able to interpolate between speaking styles, switching from 'normal' mode to 'tutor' mode as needed.
                    
                ",,['social-machines'],https://dam-prod2.media.mit.edu/x/2016/10/20/chatterbox.png,48027,,2016-12-05T00:17:02.173Z,['echu@media.mit.edu'],True,chatterbox,2016-04-01,[],ChatterBox,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-20T21:38:55.749Z,"Analyzing detailed data from SpeechBlocks to understand how kids engage with constructionist literacy learning technologies, with the goal of empowering caregivers (e.g. parents, older siblings, tutors) with these insights.",,[],https://dam-prod2.media.mit.edu/x/2016/10/20/Screen Shot 2016-10-20 at 7.35.14 PM.png,48035,,2021-01-06T15:10:31.512Z,"['jnazare@media.mit.edu', 'minasg@media.mit.edu', 'isysoev@media.mit.edu', 'anneli@media.mit.edu', 'echu@media.mit.edu', 'snehapm@media.mit.edu']",True,play-analytics,2016-02-01,"['design', 'kids', 'learning-teaching', 'storytelling']",Play Analytics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-20T21:41:40.919Z,"ShapeBlocks is a play analytics observatory that tracks, remembers, and aids players in building traditional LEGO-style structures. As players build a structure using these blocks, an underlying geometry engine analyzes the players' moves and suggests next steps (if a target structure is provided). The players can see real-time updates of what they are building in 3D. Instead of only suggesting, the AI learns from the players' moves and corrects itself through reinforcement learning. This essentially gives an opportunity for children and machines to learn shapes and geometry together.Other use cases include urban design, and interactive strategy games and/or storytelling experiences that fuse the physical and virtual world together.This is a work in progress. The hardware is complete, and the AI tool and games are currently being built.",,['social-machines'],,48043,,2016-12-05T00:17:27.676Z,"['saquib@media.mit.edu', 'dkroy@media.mit.edu']",True,shapeblocks,2016-08-01,"['artificial-intelligence', 'human-machine-interaction', 'kids', 'learning-teaching', 'machine-learning']",ShapeBlocks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-21T01:03:28.825Z,"We explore an art form where machines take on an essential role in the aesthetics and processes of the creation. Our main theme can be summarized as ""body, hybrid, and evolve,"" as we study an artistic medium that incorporates mechanical machines that institutes a hybrid creation process as well as an expressive capacity beyond body limits.Flying Pantograph transposes human-scale drawing acts to a physically remote output canvas in different scales and aesthetics. A drone becomes an ""expression agent,"" modified to carry a pen and be controlled by human motions, then carries out the actual process of drawing on a vertical wall. Not only mechanically extending a human artist, the drone plays a crucial part of the expression as its own motion dynamics and software intelligence add new visual language to the art. This agency forms a strong link between a human artist and the canvas; however, at the same time, it is a deliberate programmatic disconnect that offers space for exploiting machine aesthetics as a core expression medium.This seemingly straightforward technical realization is in fact a combination of non-trivial mechanical and algorithmic solutions. The drone, a floating machine, is relying on a slim chance of stabilization acquired by battling the vortex of air, the pressure and friction on the canvas surface, and the capricious mind of the human artist. This suspense, the vulnerability to instability, and the aftermath of crashing, poses a contrast with the optimistic idea of technologically evolved capability of a human artist.At this critical point of balance, we embody an instance of evolution in form of an artistic medium. The interaction between people and our installation itself is one message, where the outcome drawing of the interaction offers another. This pushes forth the idea of collective and technological evolution across scale.",2019-06-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/uuid/51779851-1cee-4a5b-80ef-c5310ec8da41,48083,,2023-10-12T22:12:28.456Z,"['harshit@media.mit.edu', 'sangwon@media.mit.edu', 'pattie@media.mit.edu']",True,flying-pantograph,2016-10-20,['art'],A Flying Pantograph,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-10-21T15:08:35.875Z,"DIVE is a web-based data exploration system that lets non-technical users create stories from their data without writing code. DIVE combines semantic data ingestion, recommendation-based visualization and analysis, and dynamic story sharing into a unified workflow. LinksFront-end repository: github.com/MacroConnections/dive-frontendBack-end repository: github.com/MacroConnections/dive-backend",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2018/06/20/dive-visualization_WYrJVYE.gif,48121,,2020-08-24T17:30:49.981Z,"['kzh@media.mit.edu', 'hidalgo@media.mit.edu']",True,dive,2015-01-01,"['data', 'human-machine-interaction', 'interfaces', 'data-visualization']",DIVE,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-10-22T14:28:54.223Z,"The Digital Construction Platform (DCP) is an experimental enabling technology for large-scale digital manufacturing. In contrast to the typical gantry-based approach to digital construction, robotic arm systems offer the promise of greater task flexibility, dynamically expandable workspaces, rapid setup times, and easier implementation with existing construction techniques. Potential applications for this system include fabrication of non-standard architectural forms; incorporation of data gathered on-site in real time into fabrication processes; improvements in construction efficiency, quality, and safety; and exploration of autonomous construction systems for use in disaster relief, hazardous environments, and extraterrestrial exploration.",2021-05-01,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2016/10/22/SJK_3263_iingLZH.jpg,48183,,2022-06-10T14:04:08.471Z,"['jleland@media.mit.edu', 'cail@media.mit.edu', 'stevenk@media.mit.edu', 'neri@media.mit.edu']",True,digital-construction-platform-v-2,2015-08-01,"['robotics', 'architecture', 'construction']",Digital Construction Platform,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-10-22T14:56:29.704Z,"The Digital Construction Environment is the first architectural-scale structure fabricated with the Digital Construction Platform (DCP).  Using the Mediated Matter group’s Print-In-Place construction technique, an open-domed structure with a diameter of 14.6 m and a height of 3.7 m was manufactured over a print time of 13.5 hours.",2018-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2016/10/22/dome_thesis_sk 101.jpg,48191,,2021-05-21T14:17:40.439Z,"['stevenk@media.mit.edu', 'jleland@media.mit.edu', 'cail@media.mit.edu', 'neri@media.mit.edu', 'darweesh@media.mit.edu']",True,3d-printed-hemi-ellipsoidal-dome,2016-07-17,"['architecture', 'construction', 'storytelling', '3d-printing', 'engineering']",DCP: Digital Construction Environment,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-10-23T00:34:13.903Z,"When we analyzed self-reported relationship surveys from several experiments around the world (from human subjects, not hobbits!), we found that while most people assume friendships to be two-way, only about half of friendships are indeed reciprocal. In itself this may seem like an interesting but minor finding, but this large proportion of asymmetric friendships translates to a major effect on the ability of individuals to persuade others to cooperate or change their behavior.For example, when we examined the properties of friendship networks and how the directionality of ties can impact the level of influence that individuals exert on one another (based on analysis of a fitness and physical activity intervention where information about physical activity was collected passively by smartphones), we found that the program was more effective when a unilateral friendship tie existed from the buddy (the person applying peer pressure) to the subject (the person receiving the pressure) than when the friendship tie was from the subject to the buddy. In this example, reciprocal friendships are best, but having a buddy who thinks of the subject as a friend is the next best relationship. We attribute the difference to our peer-to-peer incentive mechanism—as buddies were rewarded based on the progress of the subject, there are likely to be differences in communication when the buddy believes the subject to be their friend versus when they do not.The findings of this work have significant consequences for designing interventions that seek to harness social influence:Intervention designers, whether with fitness programs, smoking cessation programs, or any other attempt to change a subject’s behavior, can't rely on how the subject perceives the relationship with the buddy to create effectiveness.Also, we shouldn’t assume people with a high number of social ties are “influencers.” Such people are no better and often are worse than average at exerting social influence. Our results suggest that this is because many of those ties either are not reciprocal or go in the wrong direction, and therefore won’t lead to effective persuasion.We demonstrate that an assumption common in previous studies of social influence, namely that friendships are created equal or reciprocal by default, is erroneous, which may have significantly biased the research results.We hope that by understanding the factors and network properties that impact the level of social influence individuals exert on one another, we can be more effective at promoting behavioral change, disseminating new ideas, and even promoting products.",2016-08-18,['human-dynamics'],,48199,,2016-12-05T00:17:27.755Z,"['sandy@media.mit.edu', 'amaatouq@media.mit.edu']",True,friendship-reciprocity-behavioral-change,2015-05-01,"['health', 'networks', 'wearable-computing', 'social-science', 'behavioral-science', 'computer-science']",Friendship Reciprocity and Behavioral Change,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-10-23T01:46:30.089Z,"Earlier studies proved that behavior is highly shaped and constrained by one's social networks, and demonstrated ways in which individuals can manipulate these networks to achieve specific goals. A great example is the much-studied ""strength of weak ties"" hypothesis, which states that the strength of a tie between A and B increases with the overlap of their friendship circles, resulting in an important role for weak ties in connecting communities. Mark Granovetter first proposed this idea in a study that emphasized the nature of the tie between job changers in a Boston suburb and the contacts who provided the necessary information for them to obtain new employment.  Basically, although people with whom the job seekers had strong ties were more motivated to provide information, the structural position of weak ties played a more important role. The implication is that those to whom one is weakly tied are more likely to move in different circles, and will thus have access to different information than the people to whom you are tied more strongly. Much of our knowledge about how mobility, social networks, communication, and education affect the economic status of individuals and cities has been obtained through complex and costly surveys, with an update rate ranging from fortnights to decades. However, recent studies have shown the value of mobile phone data as an enabling methodology for demographic modeling and measurement.Many of our daily routines are driven by activities either afforded by our economic status or related to maintaining or improving it, from our movements around the city, to our daily schedules, to our communication with others. As such, we expect to be able to measure passive patterns and behavioral indicators, using mobile phone data, that could describe local unemployment rates. To investigate this question, we examined anonymized mobile phone metadata combined with beneficiaries' records from an unemployment benefit program. We found that aggregated activity, social, and mobility patterns strongly correlate with unemployment. Furthermore, we constructed a simple model to produce accurate reconstructions of district-level unemployment from mobile communication patterns alone.Our results suggest that reliable and cost-effective indicators of economic activity could be built based on passively collected and anonymized mobile phone data. With similar data being collected every day by telecommunication services across the world, survey-based methods of measuring community socioeconomic status could potentially be augmented or replaced by such passive sensing methods.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2016/10/22/unemployment.png,48222,,2019-06-05T19:17:28.176Z,"['sandy@media.mit.edu', 'amaatouq@media.mit.edu']",True,social-physics-of-unemployment,2014-07-23,"['data', 'economy', 'politics', 'wearable-computing', 'social-science', 'behavioral-science', 'computer-science']",Social Physics of Unemployment,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2016-10-23T02:53:54.278Z,"A transhumanist project towards co-evolution, rooted in desireI want to become a cephalopod is a trans-humanist proposal for using mostly the octopus, but also cuttlefish and squid, as a model species for the future of the human. This research began as a response to the ideology dominating the promises of genetic modification: the scouring of earthly genotypes in order to augment and ""improve"" human capabilities, or solve human ""problems."" This positions other species and their DNA as yet another resource to be mined, extracted and manipulated, a sort of ""shopping"" or ""collage"" project for biology. In our work, we ask ""what if?"": What if the future of biology was rather an encounter with a species, as-is, and perhaps on its own terms? What if the term ""model species"" meant not that the species was for humans of high utilitarian use, easy to produce, maintain, practice science upon—what if ""model"" meant something more like the ""mode"" or ""role model""? What if I wanted to abandon the oncoming future of becoming a computer-aided AI robot, and become a cephalopod instead? What if the role of desire and fantasy in the discourse and practice of innovation was made explicit, even foregrounded? And what if the epic project of transhumanism focused first on training as a technology, rather than immediately moving to material, product-based intervention? What if we were to start Training Transhumanism (informed by the practices of the communities studied in  ¡ONWARDS + UNDER!)?To limit our thinking of the future of the human to its current form is limiting, and rooted in traditionalism. The first stages of becoming a cephalopod focuses on three main attributes: Camouflage. To practice camouflage is to be a shape-shifter, constantly aware of one’s hyper-local environment, and ready in response. It is a practice of fluidity, flexibility, impermanent identity. I will create a training program for myself in order to develop these sensitivities and capabilities for the human, pulling from somatic practices, attention training, flexibility training (for both body and mind), ego relinquishment (the consistent internal representation of who we are). This will involve as few props as possible. A demonstration, and research interviews towards developing the practice, will be filmed.The Decentralized Brain. The octopus has 3/5 of its neurons in her arms, and her arms can make decisions independent of the main brain (sometimes she has to watch her different arms to know what they are up to), while sharing a single intention “set” by the main brain. But how can a human decentralize her brain? She is a very centralized organism—head, on top of spinal cord, two appendages on either side, and so on… Perhaps it takes more than one person to become a cephalopod. Perhaps the question is not how does one human decentralize her brain, but how do two humans become a single organism (at least for some period of time)? How do we push beyond negotiation, beyond collaboration, to a place where two humans can share a single intention, and work as two arms in achieving it? And how might one train humans to be more sensitive and better adept in such relations? A training regimen is under development.Embodied knowledge, and relation to the world. Cephalopods are basically giant tongues. They relate to the world through touch, chemically sensing what they encounter. Although octopuses in particular have excellent eyesight, what if the future of the human relied more on physical contact and chemical sensing (through our nose and mouth) in order to interact, understand, and act upon the world? How do we develop and extend our abilities to physically, tactically, and chemically sense the world?",2018-08-31,['responsive-environments'],https://dam-prod2.media.mit.edu/uuid/fa139585-b37d-45e2-85bd-7cda9233ff10,48230,,2018-10-23T15:18:45.248Z,['simun@media.mit.edu'],True,i-want-to-become-a-cephalopod,2016-10-04,[],I want to become a cephalopod,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-10-23T20:16:22.659Z,"Reframe is a creative prompt tool that juxtaposes language from a designer’s notes in surprising ways to provoke new associations between concepts in their project. Inspired by field research at IDEO, it was originally developed as a card deck to help designers integrate some ‘structured serendipity’ into their design process during early concept brainstorming sessions. Reframe specifically tries to help introduce new variables into a designer’s creative habits while also integrating language that is important to their projects: after uploading relevant text, documents and websites, users can quickly generate randomized prompts or change individual words within the prompt until they find one they prefer.If you are interested in learning more about this tool or would like to try it yourself or in your organization, Pip is continuing to develop her PhD research outside of the MIT Media Lab and would love to hear from you: https://lookingsideways.design/",2019-10-30,['object-based-media'],https://dam-prod2.media.mit.edu/uuid/087414b6-7404-4b0f-a733-eebe873a231f,48254,,2020-05-27T22:24:25.929Z,['pip@media.mit.edu'],True,reframe,2016-07-18,['design'],Reframe,PUBLIC,http://www.pipmothersill.com/,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-24T05:06:36.751Z,"The 8K Data Manipulator is a Unity game engine application which harnesses the gesture recognition and limb tracking available through Microsoft Kinect and combines it with the multitouch capability of our 8K display. Using either gesture at a distance or close-up touch, a user can rotate, zoom, and slice a very large graphical dataset. The 8K Data Manipulator is currently being used to visualize seismic data; however, it is capable of loading an arbitrary point cloud or volume, supporting medical, terrestrial, biochemical, social network, design, and other applications.",2019-09-01,['object-based-media'],,48284,,2020-06-10T16:35:30.044Z,['pe25171@media.mit.edu'],True,the-8k-data-manipulator,2016-09-15,[],The 8K Data Manipulator,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-10-24T14:30:16.157Z,"The well-known ""small-world"" phenomenon indicates that an individual can be connected with any other in the world through a limited number of personal acquaintances. Furthermore, Nicholas and Fowler show that not only are we connected to each other, but we could also shape the behavior of our friends' friends. In this project, we are interested in understanding how social influence propagates and triggers behavioral change in social networks. Specifically, we analyze a large-scale, one-month international event held in the European country of Andorra using country-wide mobile phone data, and investigate the change in the likelihood of attending the event for people that have been influenced by and are of different social distances from the attendees. Our results suggest that social influence exhibits the ripple effect, decaying across social distances from the source but persisting up to six degrees of separation. We further show that influence decays as communication delay increases and intensity decreases. Such ripple effect in social communication can lead to important policy implications in applications where it is critical to trigger behavior change in the population.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2016/10/25/pic_jWWNvOI.png,48299,,2019-04-19T14:55:02.488Z,"['emoro@media.mit.edu', 'sandy@media.mit.edu', 'xdong@media.mit.edu', 'yleng@media.mit.edu']",True,the-ripple-effect-your-are-more-influential-than-you-think,2016-08-01,"['data', 'social-science', 'machine-learning', 'behavioral-science']",The Ripple Effect: You are more influential than you think,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0
False,2016-10-24T20:12:42.941Z,"Geeking, workshoplogy, and freak science on the microcolonization of estrogen biomolecules Estrofem! Lab is dedicated to the development of a mobile estrogen lab: a set of tools, protocols, and wetware for low-cost, participatory biohacking necessitated by its genesis project, Open Source Estrogen. Regarded sometimes as hobo science, freak science, and public amateurism, the Estrofem Lab and its workshopologies aim to detect and extract estrogen from bodies and environmental sources, providing the contextual framework for why we hack estrogen, and why we perform science as citizens and hacktivists. This ongoing artistic investigation has led to creation of yeast estrogen sensors (YES-HER yeast) containing human estrogen receptor for detection, vacuum pump solid phase extraction (SPE) using cigarette filters, and DIY column chromatography using broken glass bottles, smashed silica gel, and methanol.",2010-05-01,['design-fiction'],https://dam-prod2.media.mit.edu/x/2016/10/24/IMG_9872 (1)_zmP8SCi.jpg,48354,,2022-06-10T13:44:47.899Z,['maggic@media.mit.edu'],True,estrofem-lab,2015-09-07,[],Estrofem! Lab,PUBLIC,http://maggic.ooo/Estrofem-Lab-2016,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2016-10-24T21:12:18.246Z,"I am a traveling saleswoman, a nomadic cuntress performing with sex hormones. Exhibited at Raumschiff Gallery during Ars Electronica in Linz, Hormone Microperformance is an installation of hormonal shrines situated next to the ""freak-science"" experimentation process from which it originates. Hormones, when isolated from the body, act as pheromones that can influence the mind and behavior through chemical signaling. Using urine samples given by the other artists of the show, sex hormones were extracted and connected to oxygen masks for the audience to inhale and experience. What they experience is a microcolonization of the mind that is both ancient and evolutionary, but imperceptible to the naked eye.",2017-08-31,['design-fiction'],https://dam-prod2.media.mit.edu/x/2016/10/24/IMG_0010-2.jpg,48362,,2017-10-11T20:29:21.605Z,['maggic@media.mit.edu'],True,microperformativity-sex-hormones,2016-09-07,[],Hormone Microperformance,PUBLIC,http://maggic.ooo,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-10-24T22:41:23.446Z,Abstract data visualizations for enhancing social interactions through clothing and augmented reality.,2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2016/10/25/JordiBackground.jpg,58811,,2022-06-10T14:01:01.976Z,"['geek@media.mit.edu', 'afuste@media.mit.edu']",True,artextiles,2016-09-01,"['augmented-reality', 'data', 'fashion', 'social-science', 'data-visualization']",ARTextiles: Promoting Social Interactions Around Personal Interests,PUBLIC,http://annafuste.com,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-25T06:10:16.311Z,"Fluxa is a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his or her limbs, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people.",,[],https://dam-prod2.media.mit.edu/x/2016/10/25/Fluxa-jewlery.jpg,61310,,2017-01-18T19:06:58.973Z,"['pattie@media.mit.edu', 'xxxxxxin@media.mit.edu']",True,fluxa-body-movements-as-a-social-display,,['wearable-computing'],Fluxa: Body Movements as a Social Display,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-10-25T13:23:43.216Z,"www.ajl.orgAn unseen force is rising—helping to determine who is hired, granted a loan, or even how long someone spends in prison. This force is called the coded gaze. However, many people are unaware of the growing impact of the coded gaze and the rising need for fairness, accountability, and transparency in coded systems. Without knowing discriminatory practices are at play, citizens are unable to affirm their rights or identify violations. Learn more at the official website: www.ajl.org",,[],https://dam-prod2.media.mit.edu/x/2016/10/25/poetofcode_uxBhdBY.gif,61334,,2020-08-19T00:11:37.917Z,"['ethanz@media.mit.edu', 'joyab@media.mit.edu']",True,algorithmic-justice-league,2016-10-14,"['computer-vision', 'artificial-intelligence', 'civic-media', 'civic-technology', 'government', 'neurobiology', 'imaging', 'social-science', 'ethics', 'computer-science', 'biology', 'technology', 'biotechnology', 'racial-justice']",Algorithmic Justice League,PUBLIC,http://www.poetofcode.com,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-10-25T16:41:57.752Z,"The CivilServant project supports online communities to run their own experiments on the effects of moderation practices on antisocial behavior, harassment, discrimination, and community well-being online. All results are published to an open repository of collective knowledge on practices that contribute to fair, flourishing social life online.The first experiment, in a 13.2 million subscriber community, showed that posting rules at the top of conversations prevents problems and increases engagement.",2017-06-01,"['civic-media', 'ethics-and-governance']",,61367,,2021-07-13T18:34:52.277Z,"['ethanz@media.mit.edu', 'jnmatias@media.mit.edu']",True,civilservant,2016-07-01,"['artificial-intelligence', 'civic-media', 'civic-technology', 'data', 'social-media', 'social-science']",CivilServant: User-led randomized trials online,PUBLIC,http://civilservant.io,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-11-02T15:25:12.955Z,"Our group develops technologies that can see through walls and perform motion capture through occlusions. To do so, we rely on wireless signals, like WiFi. These signals traverse walls and reflect off humans behind the wall before returning to a wireless receiver. We design and develop new algorithms and software-hardware systems that can extract these signals and analyze them to capture human motion from behind a wall.
                    
                ",,['signal-kinetics'],https://dam-prod2.media.mit.edu/uuid/e20639f3-6cde-4538-93ac-1b010e93e70d,67055,,2023-06-26T14:48:42.814Z,['fadel@media.mit.edu'],True,seeing-through-walls-computer-vision,2016-11-01,"['sensors', 'rfid']",Seeing Through Walls,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-11-02T15:27:11.402Z,"Wireless networks—consisting of WiFi, LTE, RFIDs, and millimeter-wave devices—have become integral parts of our everyday lives. Our research explores how we can make these networks faster, more robust, and seamlessly mobile. It also explores how we can use these networks for purposes other than communication, such as localization, sensing, and control.
                    
                ",,['signal-kinetics'],,67063,,2019-08-29T00:58:11.371Z,['fadel@media.mit.edu'],True,programming-wireless-networks-computer-networks,2016-11-01,[],Programming Wireless Networks (Computer Networks),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-11-02T15:30:44.483Z,"Today’s health sensors (which monitor breathing, heartbeats, steps, etc.) require their users to wear them on their bodies. In contrast, our technologies can monitor human health without requiring the user to wear any device on his/her body. To do so, we capture and analyze wireless signals reflected off the human body; we then use these reflected signals to extract breathing and heartbeats without any physical contact with the human body. We are currently exploring techniques to remotely sense additional health metrics like blood pressure, oxygen saturation, and glucose levels. Monitoring these health metrics can render ICU (intensive care unit) vital sign monitors completely noninvasive and enable continuous monitoring of diabetes patients.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2017/08/10/baby-monitor.png,67071,,2019-04-17T18:06:52.094Z,['fadel@media.mit.edu'],True,health-sensing-human-computer-interaction,2016-11-01,"['virtual-reality', 'artificial-intelligence', 'augmented-reality', 'bioengineering', 'environment', 'health', 'sensors', 'imaging', 'technology', 'ocean', 'climate-change']",Health sensing using wireless signals,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-11-02T15:32:08.536Z,"In the age of ubiquitous connectivity and the Internet of Things, our security and privacy have taken on new dimensions. For example, how can we ensure that our locations are not being tracked from our cellphones? And, how can we prevent an unauthorized user from hacking into our smart home systems? Our research aims at developing primitives that can address these challenges. To do so, we explore intrinsically new security mechanisms that operate across all computing stacks to secure not only the bits but also the integrity of the sensed signals, and to protect the privacy of the sensed environments.
                    
                ",,['signal-kinetics'],,67079,,2018-09-05T01:10:16.706Z,['fadel@media.mit.edu'],True,cyber-physical-security-and-privacy-system-network-security,2016-11-01,[],Cyber-Physical Security and Privacy (System & Network Security),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-11-02T15:36:27.431Z,"Can drones find missing items? Every year, companies lose billions of dollars due to misplaced items and faulty inventory records in their warehouses. Consider that the smallest Walmart warehouse is larger than 17 football fields, making it impossible to keep track of all items in the warehouse. To overcome this challenge, we introduce RFly, a drone-based wireless system that can scan and locate items in warehouses. The system leverages cheap, battery-free RFID (Radio Frequency Identifier) stickers, which are attached to every item in the warehouse similar to barcodes. These RFIDs power up and respond with a unique identifier when commanded by a wireless device called a reader. To scan a warehouse, a drone operator dispatches a small, inexpensive, and safe drone which flies throughout a warehouse, cataloging and localizing all the RFIDs in a warehouse. The video below shows how the system operates.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2017/08/24/RFly-photo.png,67087,,2018-08-22T16:00:28.691Z,"['fadel@media.mit.edu', 'yunfeima@media.mit.edu', 'nselby@media.mit.edu']",True,wireless-sensing-for-drones-agile-robots-robotics,2016-11-01,"['systems', 'networking', 'autonomous-vehicles']",RFly: Drones that find missing objects using battery-free RFIDs,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-11-08T19:01:17.868Z,"We demonstrate a smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is inherently low-cost and the power consumption is minimal making it portable to carry out a range of studies in the field. All essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing of dimensions 88 mm × 37 mm × 22 mm and the entire device weighs 48 g. The resolution of the spectrometer is 15 nm, delivering accurate and repeatable measurements. The device has a dedicated app interface on the smartphone to communicate, receive, plot and analyze spectral data. The performance of the smartphone spectrometer is comparable to existing bench-top spectrometers in terms of stability and wavelength resolution. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruit samples. Ultra-Violet (UV) fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. A satisfactory agreement was observed between ripeness and fluorescence signals. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.",,[],https://dam-prod2.media.mit.edu/x/2016/11/08/Apples.jpg,67113,,2016-11-08T19:18:03.587Z,[],False,asdf,,[],Food sensing on smartphones,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-11-08T19:19:57.172Z,"We demonstrate a smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is inherently low-cost and the power consumption is minimal making it portable to carry out a range of studies in the field. All essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing of dimensions 88 mm × 37 mm × 22 mm and the entire device weighs 48 g. The resolution of the spectrometer is 15 nm, delivering accurate and repeatable measurements. The device has a dedicated app interface on the smartphone to communicate, receive, plot and analyze spectral data. The performance of the smartphone spectrometer is comparable to existing bench-top spectrometers in terms of stability and wavelength resolution. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruit samples. Ultra-Violet (UV) fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. A satisfactory agreement was observed between ripeness and fluorescence signals. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2016/11/08/ezgif.com-gif-maker.gif,67136,,2018-10-22T20:30:10.303Z,['ajdas@media.mit.edu'],True,food-sensing-on-smartphones,2016-11-08,[],Food sensing on smartphones,PUBLIC,http://www.mit.edu/~ajdas,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-11-08T19:25:09.416Z,Computation photography applied to imaging the middle ear for infections.,,['camera-culture'],https://dam-prod2.media.mit.edu/x/2016/11/08/IMG_4945.JPG,67144,,2018-10-22T20:26:44.372Z,['ajdas@media.mit.edu'],True,computation-ear-imaging,,[],Computation Ear Imaging,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2016-11-22T20:30:01.175Z,"Oasis won Silver at the Edison Awards 2017.Oasis received the Best Paper Award at VRST 2016.Oasis is a novel system for automatically generating immersive and interactive virtual reality environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas to enable real-walking in the generated virtual environment. Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our system allows a casual user to easily create and experience VR in any indoor space of arbitrary size and shape without requiring specialized equipment or training.Oasis can be used, for example, to create storyspaces where friends and family can remotely participate in a session of storytelling around the campfire. The freedom to move around and interact with the virtual world allows for a new form of storytelling when combined with  traditional  narration techniques like vocalization, movement, and gestures. We call this human-in-the-loop storytelling, distinguishing it from current VR storytelling experiences where the software system is the storyteller.",2017-08-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2016/11/22/oasis.png,67209,,2018-08-20T16:24:31.868Z,"['pattie@media.mit.edu', 'sra@media.mit.edu']",True,oasis,2016-02-01,"['virtual-reality', 'augmented-reality']",Oasis,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-11-22T21:06:59.308Z,"Multiplayer virtual reality games introduce the problem of variations in the physical size and shape of each user’s space for mapping into a shared virtual space. We designed an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this design through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience.
                    
                ",,[],https://dam-prod2.media.mit.edu/x/2016/11/22/snowball.png,67226,,2017-04-20T08:59:00.349Z,"['sra@media.mit.edu', 'andresc@media.mit.edu']",True,snowballvr,2016-01-01,[],SnowballVR,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-11-23T21:48:17.738Z,"Since the release of Scratch in 2007, young people around the world have programmed and shared more than 15 million Scratch projects . The first generation of Scratch was an application that kids downloaded to local machines. With Scratch 2.0, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.",2020-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/12/12/c37c09da-1854-11e6-8dc7-9a298f2b1f01.jpg,67354,,2022-09-22T01:27:41.643Z,"['ascii@media.mit.edu', 'bowman@media.mit.edu', 'cwillisf@media.mit.edu', 'ericr@media.mit.edu', 'rschamp@media.mit.edu', 'tmickel@media.mit.edu', 'kaschm@media.mit.edu']",True,scratch-3-0,2016-01-01,"['design', 'gaming', 'kids', 'learning-teaching', 'computer-science', 'technology']",Scratch 3.0,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-11-28T17:31:24.661Z,"Team Edge (2016 - 2017): The Year 2 MIT/Google Collaboration. For it, we design and author Curricula and Learning Activities in the following Domains: Coding, Computational Design, the Design Process, Fabrication, Game Design and Engineering. The Toolbox includes Processing, Arduino (Firmata), Fritzing, Autodesk Circuits, Laser Cutters, and 3D Printers. Similar to Year 1 (Foundations), the students will receive Leadership Training, Mathematics, and College Preparation.",,['code-next'],https://dam-prod2.media.mit.edu/x/2016/11/28/160203-017.jpg,67377,,2017-04-04T21:05:00.355Z,"['tcarew@media.mit.edu', 'bdunning@media.mit.edu']",True,team-edge,,"['kids', 'learning-teaching', 'computer-science']","Team Edge: Design of Codified Curricula in Coding, Design, Fabrication, Games, Engineering, etc.",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2016-11-28T17:34:09.203Z,"Our Personal Research Projects group is dedicated to the closure of the digital and educational divides and gaps as they effect the underserved populations of America. We will broaden the conversation about the right and the value of their inclusion in the innovation and invention space. We will research, develop and create technology-driven innovative solutions to problems in those communities. We will challenge “old school” and rusted out educational models. And, we will explore and design 21st century models as a deterrent to unemployment, poverty, and discrimination. The full participation of underserved communities is critical to long term American productivity and competitiveness. We Will Fab the Dream.",,[],https://dam-prod2.media.mit.edu/x/2016/11/28/IMG_3192-2_2odUlRK.JPG,67385,,2017-04-04T20:59:05.979Z,"['tcarew@media.mit.edu', 'bdunning@media.mit.edu']",True,techquity-research,,"['learning-teaching', 'computer-science']",Personal Research Projects,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-12-05T17:39:32.255Z,"Traditional writing instruments have the potential to enable new forms of interactions and collaboration though digital enhancement. This work specifically enables the user to utilize pen and paper as input mechanisms for content to be displayed on a shared interactive whiteboard. We designed a pen cap with an infrared led, an actuator and a switch. Pointing the pen cap at the whiteboard allows users to select and position a ""canvas"" on the whiteboard to display handwritten text while the actuator enables resizing the canvas and the text. It is conceivable that anything one can write on paper anywhere, could be displayed on an interactive whiteboard.
                    
                ",2011-12-01,[],https://dam-prod2.media.mit.edu/x/2016/12/05/pointshare.png,67428,,2016-12-05T17:41:40.122Z,[],True,point-share,2011-09-01,[],Point & Share,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-12-05T18:06:20.269Z,"We classify characters in Tamil, a south Indian language, using convolutional neural networks (ConvNets) into 35 different classes. ConvNets are biologically inspired neural networks. Unlike other vision learning approaches where features are hand designed, ConvNets can automatically learn a unique set of features in a hierarchical manner. We augment the ConvNetJS library for learning features by using stochastic pooling, probabilistic weighted pooling, and local contrast normalization to establish a new state-of-the art of 94.4% accuracy on the IWFHR-10 dataset.",2014-12-15,[],https://dam-prod2.media.mit.edu/x/2016/12/05/similarity.jpg,67437,,2016-12-05T18:10:30.101Z,[],True,tamil-handwriting-recognition,2014-11-01,[],Tamil Handwriting Recognition,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-12-08T16:55:20.553Z,"Rhythm is a collection of open-source tools to make it easier for researchers to examine, analyze, and augment human interaction. Rhythm includes hardware to measure face to face interaction, software platforms to quantify social dynamics from online videoconferencing, and analysis and visualization tools to craft interventions that affect social behavior. For more information, visit  rhythm.mit.edu, or our main github repository.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2016/12/08/DSC_8468.jpg,69630,,2019-01-21T17:59:43.190Z,"['amohan@media.mit.edu', 'dcalacci@media.mit.edu', 'orenled@media.mit.edu']",True,rhythm,2016-09-01,"['communications', 'sensors', 'social-science', 'collective-intelligence', 'behavioral-science', 'nonverbal-behavior', 'data-science']",Rhythm: Open measurement and feedback tools for human interaction,PUBLIC,http://rhythm.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-12-08T20:49:13.474Z,"City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.All current CityScope development, tools, and software are open source here.      ",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2017/10/19/Oct-19-2017 10-12-07.gif,69894,,2023-09-29T15:04:27.261Z,"['kll@media.mit.edu', 'noyman@media.mit.edu', 'alonsolp@media.mit.edu', 'ryanz@media.mit.edu', 'agrignar@media.mit.edu', 'doorleyr@media.mit.edu', 'yasushis@media.mit.edu', 'gbabio@media.mit.edu', 'markuse@media.mit.edu']",True,cityscope,2017-08-01,"['design', 'computer-vision', 'human-computer-interaction', 'architecture', 'artificial-intelligence', 'augmented-reality', 'bioengineering', 'civic-technology', 'data', 'energy', 'environment', 'human-machine-interaction', 'open-source', 'real-estate', 'systems', 'transportation', 'urban-planning', 'machine-learning', 'collective-intelligence', 'computer-science', 'mapping', 'data-visualization', 'data-science']",Theme | CityScope,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-12-08T21:17:27.829Z,"Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat.   Going beyond the robotization and electrification of cars,   we conceptualize, prototype and pilot mobility interventions  through the following five research activities:",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2020/02/24/pev.gif,69904,,2023-09-29T14:57:49.078Z,"['mcllin@media.mit.edu', 'kll@media.mit.edu', 'ptinn@media.mit.edu', 'lukeji@media.mit.edu', 'cq_zhang@media.mit.edu', 'taiyu@media.mit.edu', 'jerryao@media.mit.edu', 'yagol@media.mit.edu', 'cassiano@media.mit.edu', 'abhia@media.mit.edu']",True,mod,2017-08-01,"['robotics', 'design', 'artificial-intelligence', 'civic-technology', 'cryptocurrency', 'energy', 'environment', 'politics', 'transportation', 'urban-planning', 'interfaces', 'machine-learning', 'mapping', 'internet-things']",Theme | Mobility On-Demand,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-12-08T21:22:45.126Z,"Changing Places researchers  are developing scalable strategies for understanding and creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2017/10/13/replace-ikea-kitchen.gif,69912,,2023-09-29T15:21:42.886Z,"['kll@media.mit.edu', 'mkh@media.mit.edu', 'aizpurua@media.mit.edu', 'hlarrea@media.mit.edu', 'oarino@media.mit.edu', 'alonsolp@media.mit.edu', 'maitanei@media.mit.edu']",True,places,2017-08-01,[],Theme | Changing Places,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-12-10T15:53:11.646Z,"Computation is a powerful artistic medium. The introduction of computers as a tool for making art has established new forms of art which are dynamic: able to actively change in response to an artist’s actions. Tools for dynamic art, like programming languages, offer artists new creative capabilities, but can often be difficult to learn and use in expressive ways.  In my dissertation work I'm developing two systems for supporting Active Dynamic Drawing:  Para and Dynamic Brushes. Para is a direct-manipulation parametric tool that supports accessible but expressive procedural graphic art through a direct-manipulation interface. Dynamic Brushes is a system for enabling artists to create their own dynamic drawing tools. Dynamic Brushes builds on lessons gained through evaluating Para to support the combination of drawing by hand with procedural manipulation and augmentation. The development of Para and Dynamic Brushes is informed through in-depth interviews with professional artists, and evaluated through a series of open-ended studies where professional artists create their own artwork with the tools.  These studies demonstrate  how dynamic mediums can extend manual art practice by supporting exploration, enabling gradual learning, and allowing manual artists to leverage existing skills.",2017-08-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/04/04/para_figure.png,70348,,2017-10-10T22:34:46.711Z,['jacobsj@media.mit.edu'],True,active-drawing,2014-09-01,[],Active Drawing,PUBLIC,http://web.media.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-12-11T15:07:27.974Z,"Launched in 2007, the Scratch Online Community enables children, primarily between the ages of 8 and 16, to share interactive media such as games, stories, and animations created with the Scratch programming environment. As of 2023, more than 100 million people have joined the Scratch Online Community. They have shared more than 130 million projects and posted more than 800 million comments.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/03/02/OnlineCommunity_pJZ7jbr.png,70356,,2023-06-05T19:46:42.602Z,"['eschill@media.mit.edu', 'christan@media.mit.edu', 'otts@media.mit.edu', 'nrusk@media.mit.edu', 'mres@media.mit.edu']",True,scratch-online-community,2013-01-30,['computer-science'],Scratch Online Community,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2016-12-11T15:09:19.315Z,"As children tinker with materials in the world, they are constantly putting things together and taking them apart. They are learning through play—trying out new ideas, exploring alternate paths, making adjustments, imagining new possibilities, expressing themselves creatively. In the process, they learn about the creative process and develop as creative thinkers.As digital technologies enter the lives of children, there is risk that they will crowd out tinkering, with children spending more time watching screens than tinkering with materials. Yet, in our work, we have seen how digital technologies can also be used to open up new opportunities for tinkering.Working in collaboration with the Tinkering Studio at the Exploratorium, Fondazione Reggio Children and the LEGO Foundation, we are developing a new generation of tools, activities, and spaces to support playful investigation and experimentation, integrating digital and physical materials. The new activities will enable children to engage in new types of inquiry into light, sound, motion, and storytelling. In the initial set of activities, called ""light play,"" children can program colored lights and moving objects to make dynamic patterns of shadows.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/12/13/30103406613_aaa188064a_o_AC86YuW.jpg,70364,,2018-03-27T10:44:38.668Z,"['tarmelop@media.mit.edu', 'nrusk@media.mit.edu', 'mres@media.mit.edu']",True,computational-tinkering,2016-09-01,[],Computational Tinkering,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-12-11T15:11:34.621Z,"The MIT Scratch Team is exploring ways to make it easier for newcomers to get started creating with coding. We are designing ""microworlds""— customized versions of the Scratch editor that contain a small set of blocks for making projects based on a theme. Microworlds offer a more creative entry point to coding. While many introductory coding experiences focus on engaging children in puzzles with one right answer, microworlds provide an open-ended experience, enabling children to explore, experiment, and create, while still providing a more simplified and scaffolded entry point into coding.Each microworld includes subset of the Scratch programming blocks that are most relevant and useful for the particular interest area, along with specialized graphical assets related to the interest area. In addition to aligning with a particular interest area, each microworld highlights how coding can enable young people to create projects and express ideas with code. For example, by tinkering with the music microworld, young people can see how they can use code to make musical melodies and beats; by tinkering with the soccer microworld, young people can see how they can use coding to make objects move and start building their own game. The project is part of the Coding for All project. The Coding for All project brings together an interdisciplinary research team from the MIT Media Lab, the Digital Media and Learning Hub at University of California Irvine, and Harvard University’s Berkman Center for Internet and Society to develop new online tools and activities to engage more young people in developing computational fluency, particularly youth from groups currently underrepresented in computing. ",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/12/15/screencapture-localhost-mw-editor-html-1481816025263.png,70372,,2022-09-22T01:15:59.530Z,"['morant@media.mit.edu', 'otts@media.mit.edu', 'nrusk@media.mit.edu', 'mres@media.mit.edu']",True,microworlds,2015-09-01,"['kids', 'learning-teaching', 'storytelling', 'computer-science']",Microworlds,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2016-12-11T15:14:14.841Z,"ScratchBit is an effort to enable children to create more seamlessly in both the physical and digital world by creating a dedicated physical interface for the Scratch programming language and environment. Designed to be rugged, low cost, and highly composable, the ScratchBit allows children to take the materials around them—such as cardboard, clothes, skateboards, and trees—and  transform them into inputs to their digital creations on Scratch. Unlike the Makey Makey which was designed to make these connections electronically, the ScratchBit is designed to create these connections through motion and mechanism.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2018/05/07/DSC03302 (1).JPG,70380,,2022-09-22T00:59:20.471Z,"['ascii@media.mit.edu', 'ericr@media.mit.edu', 'jieqi@media.mit.edu', 'khanning@media.mit.edu', 'nrusk@media.mit.edu', 'mres@media.mit.edu']",True,scratch-pad,2016-09-01,"['design', 'kids', 'learning-teaching']",ScratchBit,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-12-11T15:16:56.926Z,"Every day, young people around the world use the Scratch programming language to create and share thousands of interactive projects on the Scratch website. Yet many students aren’t sure how to get started coding their own projects.  To address this, we have launched a new set of free resources to help students learn to create with code. The Things to Try page offers a variety of project ideas, such as creating an animated story, making a pong game, or designing a virtual pet. For each theme, students can use step-by-step tutorials or printable activity cards. In addition, the site offers educator guides you can use to organize a class or workshop based on the theme.The Scratch Activity Cards is a collection of more than 80 colorful cards with 11 project themes. The front of each card illustrates an activity students can do with Scratch, such as animating a character or keeping score in a game. The back of the card shows how to snap together blocks of code to make their projects come to life.  These resources are designed to let students learn at their own pace and personalize their projects. Students can work individually or pair up to make projects together.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/02/11/getting started.001.jpeg,70388,,2022-09-22T01:21:56.204Z,"['nrusk@media.mit.edu', 'shrutid@media.mit.edu']",True,getting-started-with-scratch,2015-09-01,"['gaming', 'kids', 'learning-teaching']",Getting Started with Scratch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-12-11T15:22:05.416Z,"Paper circuitry blends conductive craft materials with electronics components to engage learners in circuit building and programming through making arts and crafts.  Learners can take advantage of the expressive richness of paper to create artifacts that are technically functional, aesthetically unique and personally meaningful. Chibitronics circuit stickers are a toolkit designed for paper circuits that transforms flexible circuit boards into interactive stickers for crafting circuits.",2020-05-31,['lifelong-kindergarten'],,70396,,2022-09-22T01:26:36.271Z,['jieqi@media.mit.edu'],True,paper-circuits,2014-01-01,[],Paper Circuits,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-12-12T16:13:26.552Z,"Novel technologies for additive manufacturing are enabling design and production at nature’s scale. We can seamlessly vary the physical properties of materials at the resolution of a sperm cell, a muscle cell, or a nerve cell. Stiffness, color, hygroscopy, transparency, conductivity, even scent, can be individually tuned for each three-dimensional pixel within a physical object. The generation of products is therefore no longer limited to assemblages of discrete parts with homogeneous properties. Rather like organs, objects can be computationally ""grown"" and 3D printed to form materially heterogeneous and multi-functional products.",2019-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2016/12/13/33_Mask_5_close_notext.jpg,70937,,2021-05-21T14:16:33.851Z,"['neri@media.mit.edu', 'limulus@media.mit.edu', 'ssunanda@media.mit.edu', 'rssmith@media.mit.edu', 'kolb@media.mit.edu', 'bader_ch@media.mit.edu']",True,vespers,2016-12-12,"['design', 'art', '3d-printing', 'biology', 'materials', 'covid19']",Vespers II,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2016-12-12T19:56:03.932Z,"We propose that fundamentally new strategies must be found for creating the places where people live and work, and the mobility systems that connect these places, in order to meet the profound challenges of the future.Building on current work at the Media Lab, City Science researchers will initially focus on the following project themes. Additional project themes will be added in response to the priorities of corporate members, MIT researchers, and the City Science advisory board. These six initial themes represent a cross section of the interdisciplinary research that will be undertaken to address the major challenges associated with global urbanization.The world is experiencing a period of extreme urbanization. In China alone, 300 million rural inhabitants will move to urban areas over the next 15 years. This will require building an infrastructure equivalent to the one housing the entire population of the United States in a matter of a few decades.In the future, cities will account for nearly 90% of global population growth, 80% of wealth creation, and 60% of total energy consumption. Developing better strategies for the creation of new cities, is therefore, a global imperative.Our need to improve our understanding of cities, however, is pressed not only by the social relevance of urban environments, but also by the availability of new strategies for city-scale interventions that are enabled by emerging technologies. Leveraging advances in data analysis, sensor technologies, and urban experiments, City Science will provide new insights into creating a data-driven approach to urban design and planning. To build the cities that the world needs, we need a scientific understanding of cities that considers our built environments and the people who inhabit them. Our future cities will desperately need such understanding.",,['city-science'],https://dam-prod2.media.mit.edu/uuid/4bb9447a-c520-474c-803f-39e02e2303c5,73478,,2019-06-15T22:54:53.557Z,['kll@media.mit.edu'],False,cities-network-unpublished,2017-08-01,[],Cities Network (UNPUBLISHED),PUBLIC,http://cities.media.mit.edu/,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2016-12-12T20:51:45.707Z,"The project investigates how to make origami structure with inflatables with various materials. We introduce a universal bending mechanism that creates programmable shape-changing behaviors with paper, plastics, and fabrics. We developed a software tool that generates this bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. A custom heat-sealing head that can be mounted on usual three-axis CNC machines to precisely fabricate the designed transforming material is presented. We envision this technology could be used for designing interactive wearables and toys, and for the packaging industry.
                    
                Visit http://tangible.media.mit.edu/project/aeromorph/.Honorable Mention Paper Award, UIST 2016",,[],https://dam-prod2.media.mit.edu/x/2016/12/12/2016-12-06-19_10_09.gif,74219,,2020-05-28T17:46:28.720Z,"['ishii@media.mit.edu', 'heibeck@media.mit.edu', 'jifei@media.mit.edu', 'chinyich@media.mit.edu', 'nv2247@media.mit.edu']",True,aeromorph,2016-11-01,[],aeroMorph,PUBLIC,http://tangible.media.mit.edu/project/aeromorph/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2016-12-13T19:02:48.249Z,"Between 1989 and 2015, the World Wide Web transformed from an esoteric system for publishing technical notes to a basic infrastructure of commerce, learning and social interaction. In the process, the Web has centralized around a few key points of control, owned by large, for-profit, publicly traded companies which have enormous influence on our online interactions. And because so many of our interactions - commercial, interpersonal and civic - are mediated online, we have inadvertently given these companies a great deal of control over our political lives and civic discourse. In collaboration with the Center for Civic Media, we will identify and evaluate the status of structurally decentralized projects in the fields of online publishing, online social networks, and discovery of online content (directory and search). From this work we will launch an experiment in building a structurally decentralized publication system designed to solve a real and relevant problem within academic computing, but more broadly, to offer a proof of concept for one approach to building decentralized social networks and publishing systems.",2020-10-01,['digital-currency-initiative-dci'],,78612,,2021-10-04T18:18:42.223Z,"['cbarabas@media.mit.edu', 'narula@media.mit.edu']",True,decentralized-web,,['blockchain'],Decentralized Web,PUBLIC,https://dci.mit.edu/decentralizedweb,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2016-12-13T19:29:35.193Z,"Since the Bank of England first raised the concept of a central bank digital currency (CBDC) in its 2015 research agenda, the subject has gained considerable traction in the intervening five years. The challenge facing central banks considering CBDC is how to use the insights gained from a decade of cryptocurrencies and what features are beneficial or harmful. CBDC is a form of central bank money and as such security is of the utmost importance. Our current work is focused on how to bring the benefits of programmability to CBDC without compromising security. We have engaged with a number of central banks in an effort to understand their considerations and address them in our research when possible. Central banks regularly undertake research papers with external academics in a particular field, this work is in that tradition and, as with all academic research, does not imply endorsement of any policy position. The central banks are: Bank of Canada, European Central Bank, Federal Reserve, Bank of Japan, De Nederlandsche Bank and Monetary Authority of Singapore.",2022-12-31,['digital-currency-initiative-dci'],https://dam-prod2.media.mit.edu/x/2020/06/15/aleksi-raisa-M4DMyeU9Etc-unsplash_ZBIOMQ6.jpg,78620,,2023-03-30T19:29:09.878Z,"['robleh@media.mit.edu', 'narula@media.mit.edu']",True,central-banks-and-digital-currency,2017-03-28,"['banking-and-finance', 'economy', 'blockchain']",Central Bank Digital Currency (CBDC),PUBLIC,https://dci.mit.edu/cbdc-central-bank-digital-currency,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2016-12-14T13:16:15.237Z,"The main purpose of this project is to develop a hands-on learning activity around tinkering with microbiological bricks to be play-tested with kids and educators. I am exploring how microorganisms can create generative art and music and  how we could transform any affordable webcam into a  microscope that can connect directly to the computer. I am also looking at how this live stream from microcosmos world could  be visualized in many different ways by applying multiple filters and image transformations such that it provides us with many new perspective about the world around us. This live connection to living organisms enables us also to imagine new ways of interacting and tinkering with them.  Some initial explorations lead to the creation of a bio-kaleidoscope that is alive (literally). I hope this project will  explore new ways in which kids could  play with the micro-cosmos around us, explore the magic at the intersection of digital, physical and bio tinkering and discover how using familiar things, such as webcams, to do unfamiliar actions can boost our creative confidence. ",2017-09-30,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2016/12/14/unnamed.png,79424,,2017-06-05T21:44:02.634Z,['sdruga@media.mit.edu'],False,biobricks,2016-11-01,[],BioBricks,PUBLIC,http://drugastefania.com,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2016-12-14T18:00:41.765Z,"I am currently creating perfumes that capture the smell of individuals who have emotional significance to me.  An exploration in the use of science for emotional ends, I have successfully bottled the scent profiles of three people.  In the obsessively hygienic and reason-driven laboratory where I distill these smells, I often reflect on the constant negotiation between the animal and the cultured human within ourselves. This project is currently conducted in tandem with research in neuroscience, correlating olfactory stimuli with behavioral responses.  ",2017-08-31,['design-fiction'],https://dam-prod2.media.mit.edu/x/2016/12/14/IMG_7294-1.JPG,79446,,2017-10-11T20:28:56.930Z,['wonder@media.mit.edu'],True,human-perfume,2016-09-01,"['design', 'art', 'bioengineering']",Human Perfume,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2016-12-14T22:33:18.246Z,"EngageME: Personalized machine learning and humanoid robots for measuring affect and engagement of children with autismEngageME is a project aimed at building a new technology to enable automatic monitoring of affect and engagement of children with ASC (Autism Spectrum Conditions) in communication-centered activities.This work has been published in Science Robotics, June 2018.",2019-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/06/21/1820x550_hands.jpg,79467,,2022-05-12T17:29:26.445Z,"['picard@media.mit.edu', 'orudovic@media.mit.edu']",True,engageme,2016-10-01,"['robotics', 'human-computer-interaction', 'health', 'human-machine-interaction', 'wearable-computing', 'sensors', 'social-science', 'machine-learning', 'social-robotics', 'affective-computing', 'autism-research', 'human-augmentation']",Personalized Machine Learning for Autism Therapy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2016-12-15T20:08:35.034Z,"Visualizing the social network of twitter conversations, Presidential Elections 2016. Try it!",,[],https://dam-prod2.media.mit.edu/x/2020/10/22/electome.png,79522,,2021-01-25T17:09:59.460Z,['mmv@media.mit.edu'],True,datavrse,2016-10-01,[],DataVRse,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2016-12-15T22:22:57.345Z,"Prolonged space travel plays a severe toll on the human body: microgravity impairs muscle and bone growth, and high doses of radiation cause irreversible mutations. As we seriously consider the human species becoming space-faring, a big question stands: even if we do break free from Earth’s orbit, can we adapt to the extreme environments of space? Lisa Nip examines our odds.
                    
                ",,[],,79531,,2016-12-16T20:27:34.595Z,[],False,limitations-and-approaches-to-solving-the-human-body-in-space,,[],Limitations and approaches to solving the human body in space,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-12-15T22:27:18.893Z,"Prolonged space travel plays a severe toll on the human body: microgravity impairs muscle and bone growth, and high doses of radiation cause irreversible mutations. As we seriously consider the human species becoming space-faring, a big question stands: even if we do break free from Earth’s orbit, can we adapt to the extreme environments of space?",,[],https://dam-prod2.media.mit.edu/x/2016/12/15/Screenshot 2016-12-15 14.31.40.png,79539,,2016-12-15T22:42:49.457Z,['lisanip@media.mit.edu'],True,limitations-of-and-approaches-to-augmenting-the-human-body-in-space,,[],Limitations of and approaches to augmenting the human body in space,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2016-12-29T21:34:08.542Z,"The mission of the Media Lab's new Center for Terrestrial Sensing is to explore unconventional ways to sense and visualize inaccessible natural environments where it is impossible for humans to go physically such as underground, undersea oil fields and in the atmosphere. How people connect with, navigate, and interact with large amounts of geoscience information is an area with world-changing potential and deep challenges. The Center for Terrestrial Sensing aims to connect People to the Planet.",2016-01-01,['center-for-terrestrial-sensing'],,84611,,2023-09-06T20:59:15.385Z,[],True,center-for-terrestrial-sensing,2014-09-01,[],Center for Terrestrial Sensing,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-01-10T20:10:35.142Z,"Cultural Lens is a speculative design project that reverses traditional gender responsibilities around clothing and personal appearance, interrogating the status of prevalent historical arguments around modesty and etiquette. Every society, culture, and religion has implicit or explicit expectations for women's clothing and public appearance, which has recently led to debates around issues for human rights, freedom, and self-expression.Cultural Lens asks society-at-large to share the burden of enforcing public appearance for women. Instead of forcing women to wear clothing styles that are deemed acceptable by the public, Cultural Lens allows members of the public that might be offended–by, for example, perceived immodesty or improper etiquette–to have the freedom to selectively filter the appearance of the women they see in public to conform  literally to their view of how women should look. The system uses a Microsoft HoloLens to implement an Augmented Reality visual field for the user.  The system can identify people and faces in the view, classify gender, and apply visual filters to their appearance according to the user's preference. For example, Cultural Lens can add digital veil to the faces of all women the user observes. ",,[],https://dam-prod2.media.mit.edu/x/2017/01/10/3.jpg,89381,,2017-03-29T20:15:11.938Z,"['djfitz@media.mit.edu', 'nv2247@media.mit.edu', 'manisham@media.mit.edu']",True,cultural-lens,2017-01-03,[],Cultural Lens,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2017-01-10T23:37:01.241Z,"ATENÇÃO: Saiu o resultado do Desafio Aprendizagem Criativa Brasil 2017! Clique aqui para conhecer os fellows e projetos selecionados!---- Desafio Aprendizagem Criativa Brasil é uma iniciativa da Fundação Lemann e do MIT Media Lab que visa fomentar a implementação de soluções inovadoras – novas tecnologias, produtos e serviços – que ajudem a tornar a educação brasileira mais mão na massa, significativa, colaborativa e lúdica.O Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de Aprendizagem Criativa, especialmente no que se refere a projetos mão na massa envolvendo programação e construção no mundo físico, em escolas públicas (de Educação Infantil ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.Os representantes dos projetos selecionados ganharão uma Creative Learning Fellowship  para ajudar a implementar seu trabalho. As inscrições vão até o dia 5 de fevereiro de 2017 e devem ser feitas única e exclusivamente através do formulário abaixo.Clique aqui para a chamada de projetos completa.Clique aqui para o formulário de inscrição.Clique aqui para respostas às perguntas mais frequentes.Atenção:  esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no fórum da Rede Brasileira de Aprendizagem Criativa.",2017-12-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/01/11/22531183118_2a2f2332c3_o.jpg,89391,,2018-01-22T15:34:04.822Z,"['mres@media.mit.edu', 'leob@media.mit.edu']",True,desafio-aprendizagem-criativa,2017-01-13,[],Desafio Aprendizagem Criativa Brasil 2017,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-01-10T23:38:35.450Z,"The Festival of Invention and Creativity (FIC) is a great celebration of the inventive, collaborative, and hands-on  spirit  of Brazilian education. In it, children, young people, their families and educators have  opportunity to explore high and low tech tools and materials, participate in interactive workshops and learn in a stimulating and relaxed way.The Festival aims to disseminate, inspire and facilitate the implementation of creative learning activities  in schools and non-formal educational environments. In 2018, the Brazilian Creative Learning Network, with support from the MIT Media Lab and the Lemann Foundation, facilitated the organization of more than 15 regional Festivals throughout the country.----O Festival de Invenção e Criatividade (FIC) é uma grande celebração do espírito inventivo, colaborativo e mão na massa da educação brasileira. Nele, crianças, jovens, seus familiares e educadores tem a oportunidade de explorar materiais e tecnologias high e low tech, participar de atividades interativas, e aprender de forma estimulante e descontraída.O Festival tem como objetivos divulgar, inspirar e facilitar a implementação de atividades de aprendizagem mão na massa em escolas e ambientes de educação não formal.Só em 2018, a Rede Brasileira de Aprendizagem Criativa, contando com o apoio do MIT Media Lab e da Fundação Lemann, facilitou a organização de mais de 15 Festivais regionais por todo o país.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/01/31/23001146159_49b835863d_o.jpg,89399,,2023-09-12T15:52:54.062Z,['leob@media.mit.edu'],True,festival-de-invencao-e-criatividade,2017-01-10,[],Festival of Invention and Creativity,PUBLIC,http://www.ficmaker.org.br,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0
False,2017-01-10T23:40:04.324Z,"Somos uma rede de educadores, artistas, pesquisadores, empreendedores, alunos e outros interessados na implementação de ambientes educacionais mais mão-na-massa, criativos e interessantes nas escolas, universidades, espaços não-formais de aprendizagem e residências de todo o Brasil.A Rede Brasileira de Aprendizagem Criativa surgiu em 2015 a partir de uma parceria entre o Programaê (uma colaboração da Fundação Lemann com a Fundação Telefonica Vivo) e o Lifelong Kindergarten Group do MIT Media Lab. Atualmente, contamos com centenas de participantes de todo o Brasil.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/01/11/22521481805_e688606e4d_o.jpg,89407,,2017-04-19T17:55:54.327Z,['leob@media.mit.edu'],True,rede-brasileira-de-aprendizagem-criativa,2015-10-12,[],Rede Brasileira de Aprendizagem Criativa,PUBLIC,http://aprendizagemcriativa.org/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-01-10T23:46:45.144Z,"(In Portuguese here)The Lemann-MIT Creative Learning Challenge aims to support the design and implementation of innovative technologies, products, services and initiatives that help make Brazilian education more meaningful, hands-on, collaborative and playful.The Challenge also aims to identify, support, and connect Brazilian individuals -- artists, researchers, educators, technology developers, entrepreneurs and decision makers -- who might play a key role in fostering creative learning practices, especially coding and making, in Brazilian public K12 schools and non-formal educational environments. Representatives from the selected projects will receive a Creative Learning Fellowship to support the implementation of their work. Among other things, the Fellows will:Receive technical, educational and entrepreneurial support for the implementation of their projects during the Fellowship period;Get more familiarized with Creative Learning and the work of the MIT Media Lab's Lifelong Kindergarten Group (LLK);Help LLK researchers (and the broader Media Lab community) learn more about challenges and opportunities inherent to Brazilian education;Join an active community of other Creative Learning fellows and supporters.By joining a network of practitioners, decision makers and researchers who share the same ideals, we hope the Fellows will learn from each others’ experiences and, together, help catalyze a large and sustainable transformation of Brazilian education and contribute to the development of a more participatory and creative society for all.",,['lifelong-kindergarten'],,89415,,2018-01-22T14:42:54.607Z,[],False,creative-learning-challenge,2017-01-12,[],Lemann-MIT Creative Learning Challenge,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-01-16T19:54:27.696Z,"NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Early experiments conducted with a prototype of NeverMind suggest that the long-term memory recall accuracy of sequences of items is nearly tripled compared to paper-based memorization tasks. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/09/21/nevermind header.jpg,91433,,2019-04-18T17:07:28.479Z,"['rosello@media.mit.edu', 'exposito@media.mit.edu', 'pattie@media.mit.edu']",True,nevermind,2016-09-01,"['augmented-reality', 'extended-intelligence', 'health', 'learning-teaching', 'technology']",NeverMind: Using AR for memorization,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-01-21T18:30:53.356Z,"
                    Can a magician and a robot collaborate on stage to create a believable, evocative performance?We are studying how the quality of robot movement, perceived robot agency, and blended static/dynamic interactivity between a robot and human performer might influence an audience’s emotional state and belief in the validity of a robot character during a performance. Agency and BelievabilityRobotic animation techniques for live performance typically rely on backstage human puppeteering or playback of pre-rendered animation sequences. However, these methods are insufficient for high-speed, close human-robot proximity and coordination, especially when the human performer’s position and timing are unpredictable (ex. rapid passing of objects between human-hands and robot-grippers). Furthermore, simple playback of animation can detract from the believability of the performance if an audience is not convinced that the robot has agency (i.e. its ability to act on its own).Static / Dynamic InteractionWe are developing tools that allow us to compose a human-robot performance that blends pre-rendered choreography with key moments of dynamic interactivity to enhance the realism of the character. If the performance successfully modulates the degree to which the robot responds to the human in a pre-defined manner versus behavior that is completely reactive to the dynamic performer, then the audience might still perceive the robot as having complete agency. For example, as the robot is playing back a choreographed series of poses, it might also track the face of the performer to maintain eye contact. By blurring lines in this interaction, the audience might be more willing to believe the robot is animate.Additional Research Questions•What is the affect space for a human-magician performance?Can we improve on current robot animation techniques by including computational choreography and aesthetics-influenced motion planning in ways that lead to desired emotional reactions in observed human-robot collaboration?What are challenges and opportunities when designing human-robot performances? • Can we generate a new class of tools and approaches that facilitate artistic and functional robot programming by non-experts?",2015-12-31,[],,93481,,2017-11-25T16:09:32.395Z,"['tempest@media.mit.edu', 'dnunez@media.mit.edu']",True,magicrobot,2015-03-01,"['human-machine-interaction', 'perception', 'social-robotics', 'performance']",Magic Robot Interaction,PUBLIC,http://magiclab.nyc,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-01-23T20:59:30.896Z,"Body Quest is a room-scale virtual reality playground for learning about biology and chemistry. Learning about how complex microscopic 3D structures interact is hard on paper, and only slightly easier with videos or passive 3D simulations. Interactive, room-scale VR environments open up new possibilities for building an intuitive and visual understanding of these subjects—and it can even be fun!Our video was submitted to the DOE EdSim Challenge. It is a fully functional prototype—the mixed reality video representing the physical and virtual experiences were composited in real-time by calibrating physical and virtual cameras, while filming in front of a green screen. The prototype is built around one particular biochemical interaction, whereby a viral protein cleaves sugar off the end of a mucus chain. We hope to develop future learning interactions around a general simulation backend, which will host both structured and unstructured learning experiences, including games.",2017-08-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/04/28/Screen-Shot-2018-01-21-at-3.36.07-PM.png,94271,,2018-08-20T16:43:43.257Z,['swgreen@media.mit.edu'],True,body-quest,2016-12-15,[],Body Quest: A Room-Scale VR Playground for Biology and Chemistry,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-01-31T00:27:00.761Z,"Amoeba Wall: A context aware wall system: Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable coworking space that accommodates a wide range of activities in a small area. Award- A' Design Award 2017 - Gold Prize- Honorable Mention - Tomorrow Workplace Competition by METROPOLISPublication-H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9",2016-12-31,[],https://dam-prod2.media.mit.edu/x/2019/02/10/00MAIN_qqDLgh7.jpg,97126,,2019-02-11T03:26:16.506Z,"['honghaod@media.mit.edu', 'alonsolp@media.mit.edu', 'oi7@media.mit.edu']",True,amoeba-wall,2016-01-04,"['robotics', 'architecture']",Amoeba Wall: A context-aware wall system,PUBLIC,http://rnd.studio/project/amoeba-wall,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2017-02-05T21:42:50.589Z,The setup involves adding one controller/tracker per foot and one at the base of the back along with two hand-held controllers and the HMD. I'm using the FinalIK asset from the Unity Asset Store.,2018-07-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/04/20/fbbik.PNG,98867,,2018-08-20T16:21:46.064Z,['sra@media.mit.edu'],True,full-body-tracking,2017-01-09,[],VR Full Body Tracking,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-02-10T20:56:14.462Z,"This is a grassroots challenge to get friends to participate in democracy by making calls to congresspeople in all 50 states. Live phone calls are the best way to directly express your opinion on an issue to your elected officials. Your mission is to pass message this along to friends who will make calls and also pass the message/link along to others who will do the same. It's a social chain letter and a call to action for a better participatory democracy.  We help you make your call and you pass on an invitation for your friends to do the same. Your invite can stress your opinion on a given issue. The winners are the first ten chains to reach 50 states and accumulate the most challenge points. You get 250 points for making a call, 125 points for a call that your friend makes, 65 points for the call their friend makes, on and on. Everyone on the chain earns points. Points count for your first call to each of your two senators and your representative. You get a bonus for a ""grand slam""—a network that reaches all 435 representatives and 100 senators.There is a leaderboard and a network view so you can track how you are doing. You can also see how much of the country your chain is covering.",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2017/02/10/map.png,100895,,2019-06-04T20:46:21.258Z,"['lip@media.mit.edu', 'lemeb@media.mit.edu', 'jasrub@media.mit.edu', 'trich@media.mit.edu', 'pewebb@media.mit.edu', 'britneyj@media.mit.edu']",True,fiftynifty,2017-02-13,"['communications', 'data', 'government', 'social-media', 'social-science']",FiftyNifty,PUBLIC,https://fiftynifty.org,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-02-23T14:22:48.353Z,"The Storytelling project uses machine-based analytics to identify the qualities of engaging and marketable media. By developing models with the ability to “read” emotional arcs and semantic narrative video content, our researchers aim to map video story structure across many story types and formats.To complement this content-based analysis, our researchers are also developing methods to analyze how emotional and semantic narratives affect viewer engagement with these stories. By tracking “referrals” of video URLs on social media networks, our researchers hope to identify how stories of different types and genres diffuse across networks, who influences this spread, and how video story distribution might be optimized. Given this project’s two-pronged strategy, our hope is to develop a robust story learning machine that uniquely maps the relationship between story structure and engagement across networks.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2017/12/14/SVGZ-AI-storytelling-machines-ex1.jpg,105046,,2018-06-12T19:49:38.670Z,"['dkroy@media.mit.edu', 'pralav@media.mit.edu', 'soroush@media.mit.edu', 'russell5@media.mit.edu', 'echu@media.mit.edu']",True,story-learning-machine,2017-02-23,"['computer-vision', 'artificial-intelligence', 'entertainment', 'social-media', 'storytelling', 'machine-learning']",The Story Learning Machine,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-02-27T19:51:05.003Z,"We propose a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications: 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.",,['affective-computing'],,107003,,2017-07-11T15:22:59.660Z,['jaquesn@media.mit.edu'],True,sequence-tutor,2016-09-01,[],Sequence Tutor,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2017-03-03T15:46:05.653Z,"We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.",,"['living-mobile', 'responsive-environments']",https://dam-prod2.media.mit.edu/x/2017/03/03/rovables-main.jpg,108209,,2017-03-03T18:38:10.500Z,"['geek@media.mit.edu', 'cindykao@media.mit.edu', 'dmajilo@media.mit.edu', 'artemd@media.mit.edu', 'joep@media.mit.edu']",True,rovables,2016-09-01,"['robotics', 'wearable-computing', 'sensors', 'interfaces']",Rovables,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-03-08T20:44:03.606Z,"A large portion of the chemical and biological processes underlying our everyday experience remains imperceptible to us. Be it the contents of rain, the ocean, or human tears, chemical codes mediate interactions between organic systems from the environment to our bodies and food. As humans, we understand information mediated by our senses—through textures, symbols, odors, and tastes. In order to design for a wider array of sensory modalities in representing fluid-based information and enable user interaction with these systems, we have developed Organic Primitives. It is a new medium for transforming objects into information displays. Chemical input is converted into human senses through a set of color-, odor-, and form-changing materials. ",2018-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2017/08/18/viirj-toolset.jpg,109797,,2021-05-21T13:30:37.694Z,"['viirj@media.mit.edu', 'kakehi@media.mit.edu']",True,organic-primitives,2014-11-23,"['human-machine-interaction', 'interfaces']",Organic Primitives,PUBLIC,http://www.OrganicPrimitives.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-03-15T02:14:49.246Z,"The sense of agency (SoA) describes the feeling of being the author and in control of one's movements. It is closely linked to automated aspects of sensorimotor control and understood to depend on one's ability to monitor the details of one's movements. As such SoA has been argued to be a critical component of self-awareness in general and contribute to presence in virtual reality environments in particular. A common approach to investigating SoA is to ask participants to perform goal-directed movements and introducing spatial or temporal visuomotor mismatches in the feedback. Feedback movements are traditionally either switched with someone else's movements using a 2D video-feed or modified by providing abstracted feedback about one's actions on a computer screen. The aim of the current study was to quantify conscious monitoring and the SoA for ecologically valid, three dimensional feedback of the participants' actual limb and movements. This was achieved by displaying an Infra-Red (IR) feed of the participants' upper limbs in an augmented virtuality environment (AVE) using a head-mounted display (HMD). Movements could be fed back in real-time (46ms system delay) or with an experimental delay of up to 570ms. As hypothesized, participant's SoA decreased with increasing temporal visuomotor mismatches (p<;.001), replicating previous findings and extending them to AVEs. In-line with this literature, we report temporal limits of 222±60ms (50% psychometric threshold) in N=28 participants. Our results demonstrate the validity of the experimental platform by replicating studies in SoA both qualitatively and quantitatively. We discuss our findings in relation to the use of virtual and mixed reality in research and implications for neurorehabilitation therapies.",2016-09-02,[],,112182,,2017-04-07T12:16:14.654Z,[],True,the-temporal-limits-of-agency-for-reaching-movements-in-augmented-virtuality,2016-08-01,[],The Temporal Limits of Agency for Reaching Movements in Augmented Virtuality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-03-17T16:23:31.489Z,"Tree is a virtual experience that transforms you into a rainforest tree. With your arms as branches and body as the trunk, you experience the tree’s growth from a seedling into its fullest form and witness its fate firsthand. Collaborating with director Milica Zec and Winslow Porter, we designed and constructed the entire tactile experience throughout the film. With precisely controlled physical elements including vibration, heat, fan and body haptics, the team created a fully immersive virtual reality storytelling to, where the audience no longer watches but is transformed into a new identity, a giant tree in the peruvian rainforest.Tree debuted at Sundance Film Festival 2017 New Frontier and also had its presentation in Tribeca Film Festival 2017.  The project is part of our research about body ownership illusion in virtual reality (early project: TreeSense). The tactile experience is crucial for establishing a body ownership illusion instead of restricting the experience to the visual world. We aim to have the audience not just see, but feel and believe ""being"" a tree.",2017-06-30,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/08/08/Tree_featured image_Credit_Jakob Kudsk Steensen-s.jpg,112608,,2018-10-20T22:01:11.764Z,"['pattie@media.mit.edu', 'xxxxxxin@media.mit.edu', 'yedan@media.mit.edu']",True,tree,2017-01-18,"['virtual-reality', 'augmented-reality', 'environment']",Tree,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-03-20T17:20:56.986Z,"The boom in Chinese housing prices in recent years has given rise to intensive concern about  the economic fundamentals of housing prices. This project mainly focuses on giving deep insight into housing prices from the perspective of industry composition, especially by checking the characteristics (agglomeration, innovation, diversity, and so on) of a city's position in the industry space. ",2017-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2017/03/28/pic.png,113797,,2017-04-03T00:02:27.435Z,"['lduan@media.mit.edu', 'hidalgo@media.mit.edu', 'sunlijun@media.mit.edu']",True,industry-space-and-housing-price,2017-02-10,[],Industry space and housing prices,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2017-03-23T06:48:28.700Z,"Nutritional assessment is an extremely important problem for every American. Studies suggest that as many as 90% of Americans fall short of Vitamins D&E as a result of their regular dietary habits, and up to 50% of Americans can’t get enough Vitamin A & Calcium.This problem is even more prominent in less wealthy communities, where not only food budget is more limited, but education in basic nutritional facts also more lacking. Even if full records of daily food intake are available, knowledge about nutrients in foods is needed to reflect on recent food consumption and subsequently act to nutritionally complement recent habits.Therefore, there are two major obstacles stopping many ordinary Americans from healthily managing their diets. The first is recording dietary intake, and the second is interpreting nutritional profiles from the foods you’re eating. It’s after these two steps that insight into nutritional intake can be inferred and insights into dietary balance can be made. We set out to drastically lower the efforts involved in both steps by utilizing machine learning technologies. Image recognition technologies will be utilized to allow easy recording of dietary intake via photos, and nutrition data will be subsequently inferred based on USDA’s nutritional database, which also serves as the basis for nutritional evaluations from dietary records by nutritionists.The result is a machine agent that can assist users to keep track of their dietary habits, and feedback nutritional deficiencies and suggestions to improve current diet to the user when needed.",2017-06-09,['social-machines'],,114361,,2019-02-14T16:55:54.219Z,['lukeglw@media.mit.edu'],True,olive-ai-nutrition-management,2016-11-01,[],Olive - AI Nutrition Management,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-03-23T20:37:09.203Z,"In Mexico City, a cacophony of sounds will come to your window from dusk till dawn. Merolicos—distinctive chants, recorded songs, and particular chimes—announce the commodities of roaming merchants or ambulantes. Locals recognize the whistle of the knife-sharpener, the triangle signaling gas delivery, the pre-recorded drone broadcasting tamales, and the terrify shriek of steam indicating that sweet plantains are nearby.These vendors carry their wares in retrofitted tricycles and ad hoc push-carts—weaving through traffic, lining sidewalks, and turning parks into impromptu marketplaces. They blanket the city with a vast, informal, mobile market that crisscrosses Mexico City’s sharply segregated communities. One of the city’s biggest ailments, marginalization and segregation, are exemplified through neighborhoods “far away so close”—sitting just across the avenue from each other, yet living in drastically different worlds.Exquisite Triciclo infiltrates these divided communities, doing so by appropriating the idiosyncratic cargo tricycle and hacking into the ambulantes’ roaming sales network. Rather than selling tamales or drinking water, it turns this tool of commerce into a mobile stage, a public easel. Inspired by the surrealist parlor game of creating “exquisite corpses” the Triciclo endorses co-creation as a way to know thy neighbor.Besides its analog output, Exquisite Triciclo is outfitted with GPS, a Go-Pro camera and a “Smart Pen,” registering contrasting aesthetic and idiomatic differences pertaining the residents of each neighborhood.  These devices keep a navigable digital record, as well as a tangible physical one, to be exhibited. The Triciclo’s added technology is powered through its pedaling thanks to a generator installed on the rear wheel.Methods of fabrication are meant to serve as an easily replicable model for the region—the tricycles of Mexico City are personal machines, pimped and decorated by their owners to attract attention.  Exquisite Triciclo reflects its spirit in its making: gathering the knowledge and input of a diverse group of designer, fabricators, and community members to create a ludic instrument, from creation through use.",2018-06-01,[],https://dam-prod2.media.mit.edu/x/2017/03/23/closed-cropped_wbsZ4H9.jpg,114416,,2020-06-10T15:52:05.014Z,['edwinapn@media.mit.edu'],True,triciclo,,[],TRICICLO,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2017-03-23T22:56:34.149Z,"Presenting RFind, a new technology that allows us to locate almost any object with extreme accuracy by transforming low-cost, battery-free wireless stickers into powerful radars. At a high level, our technology operates by measuring the time it takes the signal to travel from the wireless sticker to an access point. By taking into account the speed of propagation of light, we can then map the time to an exact location (with sub-centimeter precision) in 3D space.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2017/11/03/Screenshot 2017-11-03 12.15.47.png,114813,,2018-10-19T14:06:36.714Z,"['fadel@media.mit.edu', 'yunfeima@media.mit.edu', 'nselby@media.mit.edu']",True,rfid-localization,2016-12-04,"['artificial-intelligence', 'augmented-reality', 'human-machine-interaction', 'networks', 'security', 'systems', 'sensors', 'interfaces', 'networking']",RFind: Extreme localization for billions of items,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-03-27T14:09:07.775Z,"Segregation, or income inequality, is one of the major problems of our society. Residential segregation has long been of research interest in fields such as sociology, economics, and psychology. But our behaviors are also segregated. In this project we want to visualize how human behaviors—like conversations in Twitter, mobility around the cities, or purchasing—show patterns of segregation. Our objective is to allow people to understand the segregation of behaviors in their cities to increase the awareness of that problem, but also to show how we can address potential solutions by using different layers of big datasets.",2017-06-30,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2017/03/27/segregation.jpg,118159,,2022-05-24T19:04:09.785Z,"['alotaibi@media.mit.edu', 'eaman@media.mit.edu', 'xdong@media.mit.edu', 'alfredom@media.mit.edu', 'sandy@media.mit.edu']",True,visualizing-patterns-of-segregation,2017-03-27,[],Visualizing patterns of segregation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-03-27T16:32:53.219Z,"We use terahertz transmission through sedimentary rock samples to assess the macro and micro porosity. We exploit the notable water absorption in terahertz spectrum to interact with the pores that are two orders of magnitude smaller than the terahertz wavelength. Terahertz water sensitivity provides us with the dehydration profile of the rock samples. The results show that there is a correlation between such a profile and distribution of micro to macro porosity of the rock. The study further estimates the absolute value of total porosity based on diffusion theory. We compare our results with mercury injection capillary pressure as a benchmark to confirm our analytic framework. This porosimetry method can set a foundation for a more affordable, less invasive porosimetry that can be used in geological studies and in other industries without the need for hazardous mercury or ionizing radiation. 
                    
                ",2017-04-06,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/03/27/figure1awith no label_f3IdEwN.png,118175,,2017-04-03T20:57:41.280Z,['barmak@media.mit.edu'],True,rock-porosity-measurement-using-optical-scattering,2016-07-06,['sensors'],Rock porosity measurement using optical scattering,PUBLIC,http://web.media.mit.edu/~barmak/,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-03-27T16:41:32.799Z,This is the 3rd concept exhibition of imaginarium of technology which explores the future with advancement of batteries and how battery technology may be improved.,2017-04-28,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/03/27/iMT 2nd (5) Electric_Jump_Jet_6ZQ0JTQ.jpg,118191,,2017-04-03T20:58:19.940Z,['barmak@media.mit.edu'],True,future-of-batteries-and-future-with-better-batteries,2016-09-09,['energy'],Future of batteries and future with better batteries,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2017-03-27T17:02:57.010Z,This exciting concept exhibition which is the 4th concept exhibition of the imaginarium of technology series will explore the practical possibilities for the future of artificial intelligence.  Check it out at E14-274,2017-05-31,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/03/27/AI cover art.png,118199,,2017-04-03T20:58:05.877Z,['barmak@media.mit.edu'],True,a-practical-look-at-the-future-of-artificial-intelligence-concept-exhibition,2017-04-05,['artificial-intelligence'],A practical look at the future of Artificial Intelligence- iMT 4th Concept exhibition,PUBLIC,http://imtspace.com/index.php,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2017-03-27T18:39:25.735Z,"Lensless imaging with compressive ultrafast sensingTraditional cameras require a lens and a mega-pixel sensor to capture images. The lens focuses light from the scene onto the sensor. We demonstrate a new imaging method that is lensless and requires only a single pixel for imaging. Compared to previous single pixel cameras our system allows significantly faster and more efficient acquisition. This is achieved by using ultrafast time-resolved measurement with compressive sensing. The time-resolved sensing adds information to the measurement, thus fewer measurements are needed and the acquisition is faster. Lensless and single pixel imaging computationally resolves major constraints in imaging systems design. Notable applications include imaging in challenging parts of the spectrum (like infrared and THz), and in challenging environments where using a lens is problematic.
                    
                ",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/03/29/header_GfRAVzG.JPG,118210,,2019-04-19T17:50:37.233Z,"['raskar@media.mit.edu', 'guysatat@media.mit.edu']",True,imaging-without-a-lens-and-only-a-few-pixels,2015-01-01,"['sensors', 'imaging', 'computer-science']",Efficient lensless imaging with a femto-pixel,PUBLIC,http://web.media.mit.edu/~guysatat/,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-03-27T22:20:30.631Z,"How do regions acquire the knowledge they need to diversify their economic activities? How does the migration of workers among firms and industries contribute to the diffusion of that knowledge? Here we measure the industry-, occupation-, and location-specific knowledge carried by workers from one establishment to the next, using a dataset summarizing the individual work history for an entire country. We study pioneer firms—firms operating in an industry that was not present in a region—because the success of pioneers is the basic unit of regional economic diversification. We find that the growth and survival of pioneers increase significantly when their first hires are workers with experience in a related industry and with work experience in the same location, but not with past experience in a related occupation. We compare these results with new firms that are not pioneers and find that industry-specific knowledge is significantly more important for pioneer than for non-pioneer firms. To address endogeneity we use Bartik instruments, which leverage national fluctuations in the demand for an activity as shocks for local labor supply. The instrumental variable estimates support the finding that industry-specific knowledge is a predictor of the survival and growth of pioneer firms. These findings expand our understanding of the micromechanisms underlying regional economic diversification.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2018/08/01/TheFlowOfKnowledge.png,118635,,2020-08-28T19:04:29.038Z,"['crisjf@media.mit.edu', 'bjun@media.mit.edu', 'hidalgo@media.mit.edu']",True,industry-knowledge,2017-03-01,"['data', 'economy']","The role of industry, occupation, and location-specific knowledge in the survival of new firms",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2017-03-27T22:33:10.334Z,"In 1990 Germany began the reunification of two separate research systems. Yet, the institutional unification of these system does not necessarily imply their actual unification. Here we study the evolution of the network of co-authorships between East and West German scholars between 1974 and 2014 to identify the fields that integrated more successfully, and also, the factors predicting re-unification success. We find that the unification of the German research network was fast during the 1990s, but then stagnated at an intermediate level of integration. Next, we study the integration of the twenty largest academic fields (by number of publications prior to reunification) and find an inverted U-shaped between a field's East or West ``dominance'' (a measure of the concentration of the scholarly output of a field in East or West Germany prior to 1990) and the field's subsequent level of integration. We check for the robustness of these results by running Monte Carlo simulations, and a differences-in-difference analysis. Both methods confirm that fields that were dominated by either West or East Germany prior to the reunification integrated less than those whose output was balanced among East and West. Finally, we explore the origins of this inverted U-shape relationship by comparing the mixing patterns, and show that this inverted U-shaped relationship can be explained as a consequence of a tendency of scholars from the most productive regions to collaborate preferentially with scholars from other top regions. These results shed light on the mechanisms governing the reintegration of networks in the content of scholarly communities that were separated by institutions.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2017/10/11/Figures3.jpg,118643,,2020-08-24T17:05:02.571Z,"['flaviopp@media.mit.edu', 'hidalgo@media.mit.edu', 'bjun@media.mit.edu']",True,meet-me-in-the-middle-the-reunification-of-the-german-research-and-innovation-system,2016-04-01,[],Meet me in the middle: The reunification of the German research and innovation system,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2017-03-28T05:00:56.408Z,"We present 'Smell Camera' innovative way of capturing memories in the form of smell which can be recorded, stored and played in future.System Design: The device consists of a hand-held pneumatic pump which is controlled by the user's phone. The user can record memories through this arrangement. The smells are encapsulated in a gelatin capsule which can be preserved in an air-tight personalized accessory.  Whenever the user wants to experience the same environment, feeling or evoke the same emotions, the user can play the smell. ",2017-03-31,['living-mobile'],https://dam-prod2.media.mit.edu/x/2017/03/28/IMG_20141220_131144068.jpg,118663,,2017-03-28T05:24:52.754Z,[],True,smell-camera,2016-03-30,[],Smell Camera,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-03-28T05:39:07.491Z,"We present Smell Camera: an innovative way of capturing memories in the form of smell that can be recorded, stored, and played in the future.The device consists of a hand-held pneumatic pump which is controlled by the user's phone, through which the user can record memories. The smells are encapsulated in a gelatin capsule which can be preserved in an air-tight personalized accessory. Whenever the user wants to experience the same environment or feeling, or evoke the same emotions, the user can play the smell and relive the  moments. ",,[],https://dam-prod2.media.mit.edu/x/2017/03/28/IMG_20141220_131144068_ZKa7bpO.jpg,118679,,2018-03-24T23:06:36.139Z,['manisham@media.mit.edu'],True,smell-camera2,2016-03-30,[],"Smell Camera: Record, Play, Rewind",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-03-28T14:34:57.338Z,"Leveraging the power of platforms, big data, and advanced analytics for species protection and the public good in a privacy-preserving, scalable, and sustainable mannerModern tracking technology enables new ways of mining data in the wild. It allows wildlife monitoring centers to permanently collect geospatial data in a non-intrusive manner and in real time. Unfortunately, such sensible data is exposed to fraud and misuse and there is already a first reported case of ""cyber-poaching."" Based on stolen geospatial data, poachers can easily track and kill animals. Meanwhile, cautious monitoring centers limited data access for research and public use. We propose a novel privacy-preserving system to allow these monitoring centers to securely answer questions from the research community and the public while the raw data is protected against unauthorized third parties. Based on the core system, several new applications are conceivable, such as a mobile app for preventing conflicts between human and wildlife or for engaging people in wildlife donation. Besides providing a solution and working on specific use cases, the intention of this project is to start a discussion about the need for data protection in the animal world. 
",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2017/03/28/background4_VeU5JKD.jpg,118687,,2017-05-24T15:32:00.205Z,"['sandy@media.mit.edu', 'rfrey@media.mit.edu']",True,secure-sharing-of-wildlife-data,2017-02-01,"['ecology', 'networks', 'privacy', 'sensors', 'blockchain', 'trust', 'computer-science', 'gis']",Secure Sharing of Wildlife Data,PUBLIC,http://www.WildlifeData.org,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-03-29T03:31:01.541Z,"There is a deep fear that human jobs will be replaced by AI. Rather than racing against the machines, our aim is to show that a human-AI combination will perform better than humans and AI working alone. Although no man is better than a machine for some tasks, ""no machine is better than a man with a machine"" (Paul Tudor Jones) . Thus, by building ""bots"" that are compatible with human behavior, and specifically leverage the manner in which humans use social information, we have been able to build bots that extend human intelligence capabilities. In a large-scale financial trading experiment, we have shown that groups of humans and ""socially compatible"" AI bots can successfully incorporate human intuition into their decisions and consequently not only do better than humans alone, but also do better than similar AI bots that use only objective information.
                    
                ",,['human-dynamics'],,119106,,2020-07-05T21:34:36.278Z,"['dhaval@media.mit.edu', 'sandy@media.mit.edu', 'emoro@media.mit.edu', 'pkrafft@media.mit.edu']",True,social-ai-and-extended-intelligence,2016-07-01,['artificial-intelligence'],Social AI and Extended Intelligence,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-03-29T03:35:43.475Z,"We build recommender bots that use machine learning and network analytics to create personalized recommendations for users on various social and financial platforms. We show that bots that work not just on the raw user data, but instead build on human intuition, do far better. We are in the process of live testing these bots on various platforms. ",,['human-dynamics'],,119114,,2020-07-05T21:34:22.928Z,"['dhaval@media.mit.edu', 'emoro@media.mit.edu', 'sandy@media.mit.edu', 'pkrafft@media.mit.edu']",True,evolution-strategies-applied-to-collective-intelligence,2017-03-10,[],Social Learning Recommender Bots,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-03-29T06:10:42.484Z,"Combining the art of two worlds–fashion and biology–into one. Conforming Materials is working towards designing fully recyclable clothing. Through the development of a biomaterial that changes from a crystallized powder into a solid and back, at just the right temperatures, we have harnessed a breakthrough technology that could change fashion as we know it. This biomaterial can be manipulated in a multitude of ways due to its hydrophilic and hydrophobic properties. For example, imagine having a dress that can be tailored to encapsulate smell, eliminating the need for perfume. Imagine wanting a new wardrobe and being able to 100% recycle the biomaterial due to it being made solely of biological components. Imagine going for a run and your clothes trap the odor of your sweat and release a pleasant scent in its place. The possibilities of this material to be used in clothing design are endless. Electronics can be integrated into the design process due to the ability of the biomaterial to be conductive and overlaid with conductive ink. This component would enable analysis on biometrics and human data, with a wide variety of applications including sports clothing.",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2017/03/29/DSC00243.JPG,119122,,2022-06-10T14:01:01.995Z,['manisham@media.mit.edu'],True,conforming-materials,2014-12-15,[],Conforming Materials,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-03-29T17:57:32.538Z,"An interface for smashing filter bubbles, Panorama is built to allow open, transparent, and collaborative exploration of news from all across the political map. It presents different perspectives and encourages serendipity in news exploration, versus getting all of our news from one single source. Panorama is a human-in-the-loop interface. The computer processes more than 10,000 news stories each day, both broadcast and written, and it uses machine learning algorithms to decide  what topics each story is talking about and if the stories are positive, subjective, or trending. The machine learning process pours over massive datasets and learns to generalize in smart ways, but not in the same smart ways that humans generalize. As a result, it can be brilliant and also get very confused. With Panorama, some of the training data was a large open set of movie reviews, and while this is a great dataset to start with, it is not mapped so well to news stories. As humans interact with Panorama, they are encouraged to give better labels to stories; those labels are fed back into the algorithm to make it better.Having a lot of information about each news story and all stories together allows us to create an open-box news aggregator. With most aggregators we use today (like the Facebook News feed), the user has no idea what are the algorithms and filters that decide what s/he will see. Panorama is open: the user can decide to view everything, or filter only to specific things that he s/he is interested in, by playing with the sliders and seeing in real time how the news feed changes accordingly. For example, you could easily get all stories about animals, from the right side of the political map, that are also positive and objective. Panorama also exposes interesting patterns, such as the topics that different news sources focus on every day, and what sources had many objective versus subjective stories.",2018-06-06,['viral-communications'],https://dam-prod2.media.mit.edu/x/2017/03/30/Screen Shot 2017-03-29 at 4.30.47 PM.png,119132,,2018-10-17T23:55:32.429Z,"['lip@media.mit.edu', 'jasrub@media.mit.edu']",True,panorama,2016-10-01,"['design', 'crowdsourcing', 'data', 'human-machine-interaction', 'interfaces', 'machine-learning', 'collective-intelligence']",Panorama,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-03-29T20:37:04.907Z,"This research track focuses on the use of computational (and experimental) techniques to understand the biomechanical behavior of human tissue as well as the musculoskeletal system.  This knowledge feeds into novel methods for computational modeling based design of biomechatronic devices which in turn aim to restore or improve the human body. These devices include prosthetic and orthotic devices, and exoskeletons. ",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2017/03/29/socketLattice3.png,119148,,2021-09-17T19:38:35.895Z,"['branger@media.mit.edu', 'hherr@media.mit.edu', 'danask@media.mit.edu', 'kmoerman@media.mit.edu', 'vsam@media.mit.edu', 'xingbang@media.mit.edu']",True,mechanical-interfaces,2015-11-01,"['bioengineering', '3d-printing', 'biomechanics', 'prosthetics']",Computational Biomechanics,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-03-30T22:23:30.422Z,"Industrial development is the process by which economies learn how to produce new products and services. But how do economies learn? And who do they learn from?  The literature on economic geography and economic development has emphasized two learning channels: inter-industry learning, which involves learning from related industries; and inter-regional learning, which involves learning from neighboring regions. Here we use 25 years of data describing the evolution of China's economy between 1990 and 2015--a period when China multiplied its GDP per capita by a factor of ten--to explore how Chinese provinces diversified their economies. First, we show that the probability that a province will develop a new industry increases with the number of related industries that are already present in that province, a fact that is suggestive of inter-industry learning. Also, we show that the probability that a province will develop an industry increases with the number of neighboring provinces that are developed in that industry, a fact suggestive of inter-regional learning. Moreover, we find that the combination of these two channels exhibit diminishing returns, meaning that the contribution of either of these learning channels is redundant when the other one is present. Further, we address endogeneity concerns by using the introduction of high-speed rail as an instrument to isolate the effects of inter-regional learning. Our differences-in-differences (DID) analysis reveals that the introduction of high speed-rail increased the industrial similarity of pairs of provinces connected by high-speed rail. Also, industries in provinces that were connected by rail increased their productivity when they were connected by rail to other provinces where that industry was already present. These findings suggest that inter-regional and inter-industry learning played a role in China's great economic expansion.",2017-09-01,[],https://dam-prod2.media.mit.edu/x/2017/03/30/space_mEs6t4c.png,119564,,2017-03-31T00:31:45.552Z,"['hidalgo@media.mit.edu', 'gaojian@media.mit.edu', 'bjun@media.mit.edu']",True,collective-learning,2016-09-01,"['developing-countries', 'economy', 'learning-teaching', 'networks', 'collective-intelligence']",Collective Learning  in China's Regional Economic Development,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2017-03-31T16:10:55.085Z,"Scratch Memories is a web-based visualization tool that empowers children to celebrate and reflect on their creative journey with Scratch. The system dynamically generates personalized visualizations in the form of a video, highlighting a user’s key moments, diverse creations, and collaborative experiences in the online community. Existing tools for visualizing children’s progress in computational learning are primarily designed for educators, and often focus exclusively on evaluating predefined concepts in individual projects. The goal of Scratch Memories is to present a new approach towards designing positive reflective experiences that value the full range of children’s contributions as members of a creative community.The tool engages young people to reflect on their personal growth over time—starting from their first experiments with code to seeing the increasing diversity and complexity of their projects over time; and from their initial interactions in the community to seeing how their projects have inspired others around the world. Such reflective experiences can not only help young creators feel proud about how far they have come, but also to feel inspired by their own trajectories to continue exploring new possibilities.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/03/31/123_ZO8AGid.png,119597,,2018-11-07T15:55:47.871Z,['shrutid@media.mit.edu'],True,scratch-memories,2016-12-20,"['kids', 'learning-teaching', 'data-visualization']",Scratch Memories,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-03-31T20:50:11.759Z,"The gram-positive cocci Streptococcus pneumoniae causes pneumonia, otitis media, meningitis, and bacteremia in pediatric, elderly, and immunocompromised populations. Pneumococcal infection is the leading cause of pneumonia in children worldwide. Pneumococcal infections also occur frequently in at-risk populations including individuals with diabetes, asthma, chronic obstructive pulmonary disease, cardiovascular disease, human immunodeficiency virus (HIV), and sickle cell disease. In developed countries, pneumococcal infection is responsible for approximately 30% of all adult pneumonia cases and has a mortality rate of 11% to 40%. Due to this organism's impact on both morbidity and mortality in adults and children, healthcare efforts have relied on vaccines to reduce the rate of pneumococcal disease over the past 30 years. Vaccine research has focused on using immunogenic proteins and carbohydrates found on the pneumococcal surface as antigens.Previous efforts to use protein vaccines were not successful as they only stimulated the human immune system.  New research from Dr. Pratik Shah reports the discovery of a new protein molecule to immunize children, currently utilized by government agencies in Brazil and China, and by the Gates foundation, to develop affordable vaccines for prevention of pneumococcal diseases.  Shah's approach cripples the bacterial nutrient acquisition and virulence pathways  in addition to promoting effective recognition by the host immune system .Project 1:  Discovery ofnovel protein vaccine antigens protective against Streptococcus pneumoniae  pneumonia and invasive infectionsDiscovered a bacterial ABC transporter that results in significant protective immunity in mice against carriage, pneumonia and bacteremiaProtein antigen-PotD used by government of Brazil and China in vaccine development and awarded Raymond Sarber National Award for Discovery in Microbiology by American Society of Microbiology.Project 2: Elucidate the role of host and bacterial polyamine metabolism in bacterial infectionsDiscovered polyamine biosynthesis and transport mechanisms are required for pneumococcal infection and are targets for prophylactic and therapeutic interventions",,['health-0-0'],https://dam-prod2.media.mit.edu/x/2017/04/03/Slide1_Ltcsa9f.png,119640,,2018-05-02T18:40:23.029Z,['pratiks@media.mit.edu'],True,next-generation-low-cost-protein-vaccines-against-infectious-diseases,2010-01-01,"['bioengineering', 'health', 'pharmaceuticals', 'synthetic-biology']",Next-Generation Protein Vaccines Against Infectious Diseases,PUBLIC,http://bit.ly/2nJawNG,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-03-31T20:59:57.151Z,"""Without urgent, coordinated action by many stakeholders, the world is headed for a post-antibiotic era, in which common infections and minor injuries which have been treatable for decades can once again kill,"" says Dr. Keiji Fukuda, WHO’s assistant director-general for health security. ""Effective antibiotics have been one of the pillars allowing us to live longer, live healthier, and benefit from modern medicine. Unless we take significant actions to improve efforts to prevent infections and also change how we produce, prescribe, and use antibiotics, the world will lose more and more of these global public health goods and the implications will be devastating.""The WHO's ""Antimicrobial resistance: global report on surveillance 2014 report"" notes that resistance is occurring across many different infectious agents, but the report focuses on antibiotic resistance in seven different bacteria responsible for common, serious diseases such as bloodstream infections (sepsis), diarrhea, pneumonia, urinary tract infections, and gonorrhea. The results are cause for high concern, documenting resistance to antibiotics, especially ""last resort"" antibiotics, in all regions of the world. New research by Dr. Pratik Shah at Harvard Medical School identified a cost-effective way to treat bacterial infections without antibiotics. Dr. Shah described a molecular switch, controlled by bacterial diets, that toggles microbial infectivity in humans. Exploiting bacterial diets to train them to be good residents of our bodies shows that unorthodox ways to combat and treat antibiotic resistance may lead to the next generation of antimicrobials.",,['health-0-0'],,119648,,2018-10-22T17:55:29.309Z,['pratiks@media.mit.edu'],True,antimicrobials-that-treat-infectious-diseases-without-leading-to-resistant-bacteria,2014-11-03,"['bioengineering', 'food', 'health', 'pharmaceuticals', 'synthetic-biology']",Antimicrobials to Treat Infectious Diseases without Leading to Resistant Bacteria,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-03-31T21:01:42.876Z,"ImmerSound is a virtual reality experience wherein one can compose music by drawing in 3D. The resulting composition is a sculpted soundscape to be experienced both visually and in 3D audio. The user starts by choosing an instrument in the system and testing the sound that this instrument would produce at different locations. Then the user can ""paint"" a melody in space, where the elevation of the ""sound brush"" defines the pitch of the instrument, and the speed of the hand corresponds to the tempo of the melody created.  A wide range of instruments enables the creation of rich compositions with percussion, bass, classical instruments, and ambient sounds. This project associates sounds and space in a new way by offering an intuitive and natural way to interact with music. One can also imagine the same type of visual compositional space used as a neutral zone for collaboration between two or more people in different geographical locations and from different cultural backgrounds, using the universal language of music to connect in less-biased ways. This system is a first example of the potential of virtual reality for music and experiences of connection.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/03/31/logoNotes.png,119656,,2017-04-05T18:49:37.476Z,['rebklein@media.mit.edu'],True,immersound-vr,2016-09-15,[],ImmerSound VR,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-03-31T22:38:11.005Z,"LeakyPhones is a public/private headset that was designed to encourage face-to-face interactions, curiosity, and healthier social skills by letting users ""peek"" into each other's music just by looking at one another. Gaze is an important social signal in human interaction. Though its interpretation may vary across cultures, it is generally agreed that eye contact indicates interest and the point of attention in a conversation. Despite this, many common personal computing technologies, such as our smartphones and headphones, require significant visual and auditory attention thereby inhibiting our ability to interact with others. LeakyPhones offers a new approach for addressing this challenge. ",2018-05-30,"['tangible-media', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/uuid/6358be19-feb7-48d8-8472-19fdd2f67853,119665,,2020-06-09T17:29:41.500Z,"['amosg@media.mit.edu', 'ishii@media.mit.edu', 'achituv@media.mit.edu']",True,leakyphones,2017-03-16,"['design', 'communications', 'music', 'technology', 'social-change', 'wellbeing']",LeakyPhones,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-03T14:17:29.421Z,"Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control. This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm l x 4mm h x 4mm w ) that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.  For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous  target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful  neuromas.The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2017/11/09/Screen Shot 2017-11-09 at 9.51.55 AM_oZlEGRT.png,119723,,2021-09-17T19:35:51.344Z,"['bmaimon@media.mit.edu', 'syeon@media.mit.edu', 'shriyas@media.mit.edu', 'crtaylor@media.mit.edu', 'clites@media.mit.edu', 'rriso@media.mit.edu', 'lfreed@media.mit.edu', 'hherr@media.mit.edu']",True,electrical-interfaces,2016-01-04,"['robotics', 'health', 'human-machine-interaction', 'prosthetics', 'neural-interfacing-and-control']",Neural Interfaces,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-04-03T14:28:14.983Z,"Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. ",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2018/05/03/mechatronics.jpg,119731,,2021-09-17T19:36:07.586Z,"['kuan525@media.mit.edu', 'kenpasch@media.mit.edu', 'mcarney@media.mit.edu', 'mbweber@media.mit.edu', 'tonyshu@media.mit.edu', 'romka@media.mit.edu', 'syeon@media.mit.edu', 'emrogers@media.mit.edu', 'lucydu@media.mit.edu', 'handford@media.mit.edu']",True,dynamic-interfaces,2016-01-01,"['robotics', 'computer-science', 'biomechanics', 'mechanical-engineering', 'engineering', 'mechatronics', 'prosthetics']",Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2017-04-03T15:45:24.839Z,An interactive atlas of census data for direct consumption by individual citizens.,2020-08-01,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2018/03/14/powers_headerimage_IJAMwu0.jpg,119758,,2022-06-10T13:29:45.130Z,['zhangjia@media.mit.edu'],True,the-constant-atlas,2017-01-01,"['civic-media', 'data', 'mapping', 'gis']",The Constant Atlas,PUBLIC,https://constantatlas.github.io/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0
False,2017-04-03T17:42:20.912Z,"kinetiX is a transformable material featuring a design that resembles a cellular structure. It consists of rigid plates or rods and elastic hinges. These modular elements can be combined in a wide variety of ways and assembled into multifarious forms.This project describes a group of auxetic-inspired material structures that can transform into various shapes upon compression. While the majority of the studies of auxetic materials focus on their mechanical properties and topological variations, our work proposes a parametric design approach that gives auxetic structures the ability to deform beyond shrinking or expanding. To do so, we see the auxetic structure as a parametric four-bar linkage. We developed four cellular-based material structure units composed of rigid plates and elastic/rotary hinges. Different compositions of these units lead to a variety of tunable shape-changing possibilities, such as uniform scaling, shearing, bending and rotating. By tessellating those transformations together, we can create various higher level transformations for design. The simulation is validated by the 3D printed structures.  We hope this work will inspire research in metamaterials design, shape-changing materials, and transformable architecture.",2018-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/87f34c2d-9a38-4546-a766-d7a16d383b2c,119792,,2020-06-09T17:33:40.569Z,"['ishii@media.mit.edu', 'jifei@media.mit.edu', 'jannik@media.mit.edu', 'nv2247@media.mit.edu']",True,kinetix,2017-03-01,"['design', 'art', 'manufacturing', 'engineering', 'fabrication', 'metamaterials']",kinetiX,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2017-04-03T18:00:48.388Z,"Building on the understanding of music and architecture as creators of spatial experience, this project aims to create a novel way of unfolding music’s spatial qualities in the physical world.​ ​The objective is to create a new type of architectural typology that morphs responsively with a musical piece.​ ​Presenting spatial and musical composition as one synchronous entity. The goal is to create a multisensory environment where music’s perpetually changing characteristics reconfigure the spatial organization of a​ ​​space. This space - a hanging cube - will perform a​​ ​spatial choreography of sound, movement, light and color​​;​ p​resenting a dynamic​ ​room that is alive and in constant flux​.​ ​This performance will construct​ an ​aesthetic ​experience that challenges models of thinking, presenting a post-humanistic phenomenological encounter of the world​ ​to​​ ​stretching our cognition and malleable forms.​ ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/04/03/Nicole's Project_YBZczlz.jpg,119802,,2018-05-07T22:54:15.752Z,[],False,spaces-that-perform-themselves,2017-04-03,[],Spaces That Perform Themselves,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2017-04-03T18:46:15.449Z,"As we generally experience on earth, there is no space without sound and there is no sound without space. Building on the understanding of music and architecture as creators of spatial experience, this project presents a novel way of unfolding music’s spatial qualities in the physical world. Spaces That Perform Themselves exposes an innovative response to the current relationship between sound and space: where we build static spaces to contain dynamic sounds. What if we change the static parameter of the spaces and start building dynamic spaces to contain dynamic sounds? A multi-sensory kinetic architectural system is built in order to augment our sonic perception through a cross-modal spatial choreography that combines sound, movement, light, color, and vibration. By breaking down boundaries between music and architecture, possibilities of a new typology that morphs responsively with a musical piece can be explored. As a result, spatial and musical composition can exist as one synchronous entity. These spatial choreographies build up the scenario to study the possible relationships between a human body and a robotic architectural body, throughout a dance of perception and matter. This project seeks to contribute a novel perspective on leveraging technology, art, science, and design to provide a setting to enrich and augment the way we relate to the built environment. The objective is to enhance our perception and challenge models of thinking by presenting a post-humanistic phenomenological encounter of the world.Update: A trigger system has been implemented to enable users to play the cube in real time and explore interactive ways to compose spaces with a palette of movements, sounds, light, color, and vibration.",,[],https://dam-prod2.media.mit.edu/x/2018/05/04/IMG-1934.JPG,119810,,2019-01-30T16:50:51.633Z,['nicolelh@media.mit.edu'],True,spaces-perform-themselves,2017-04-02,"['design', 'architecture', 'music', 'performance']",Spaces that Perform Themselves,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-03T19:16:17.444Z,"The control of living systems as part of design interfaces is of interest to both the scientific and design communities due to the ability of living organisms to sense and respond to their environments.  They may, for example, detect and break down harmful environmental agents, or create beneficial products when environmental levels dropped below a certain threshold.  However, it is also important for these systems to be reversible, so that the biological components are only active when their functionality is necessary, and the system can remain dormant otherwise. The Living Material Library is an exploration of tunable hybrid systems. Our work in this area demonstrates the means through which intrinsic material properties may be functionally changed through environmental factors and, in turn, serve as dynamic substrates for living systems. Nearly all organisms have highly developed sensing capabilities, and have been shown to behaviorally respond to changes in substrate properties. By creating a tunable and reversible material system, we explore how cell behavior such as adhesion, patterning, and differentiation may be influenced via an active interface.
                    
                In this iteration, we propose a reversible material system that allows for control of living interactions (much like a light switch). We are particularly interested in fluid material systems (such as electrorheological fluids) that transition from a liquid-like to a solid-like state when exposed to electric fields and currents. This endeavor brings to light the complex relationship between dynamic materials and living systems. While other methods of cell intervention often rely on light, chemicals, or temperature, here we explore substrate material properties as inputs for organisms.  Our library may allow for more directed inquiry into processes such as collective cell durotaxis, general mechanotaxis, and active sensing. This marks an initial foray into establishing candidate design methods for responsive applications.",2020-06-01,"['object-based-media', 'mediated-matter']",https://dam-prod2.media.mit.edu/x/2019/06/26/DSC_0252.JPG,119818,,2020-06-10T16:20:38.305Z,"['neri@media.mit.edu', 'vmb@media.mit.edu', 'ssunanda@media.mit.edu', 'bdatta@media.mit.edu']",True,living-materials-library,2017-03-01,['materials'],Living Materials Library,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2017-04-03T21:01:56.241Z,"Public libraries are one of most trusted public institutions in the U.S. and increasingly provide a broad range of education services, ranging from early learning programs, to makerspaces, to adult training. Libraries are not storage places for books, but community hubs for social change and innovation. The Public Library Innovation Exchange (a.k.a. PLIX) develops STEAM learning experiences that are based on MIT Media Lab research and designed to support learners to become:Designers—rather than users—of technologyScientists, who are answering their own—rather than other people's—questionsCreators—rather than consumers—of art and knowledgePLIX is a community of library professionals and MIT Media Lab researchers who co-create learning experiences, develop and share facilitation practices, and strive to integrate creative learning into public library programming.Visit our project website to learn more: plix.media.mit.edu",,"['ml-learning', 'lifelong-kindergarten', 'digital-learning-studio', 'media-lab-research-theme-decentralized-society', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2020/06/11/image4_BRMmyZy.jpg,119831,,2023-09-07T23:19:46.090Z,"['kamcco@media.mit.edu', 'ps1@media.mit.edu', 'hbailey@media.mit.edu', 'lguterma@media.mit.edu', 'cohenm@media.mit.edu', 'hlubinka@media.mit.edu', 'ave@media.mit.edu', 'pelire@media.mit.edu', 'adodd@media.mit.edu']",True,public-library-innovation-exchange,2017-03-15,"['robotics', 'design', 'human-computer-interaction', 'art', 'construction', 'data', 'food', 'health', 'learning-teaching', 'perception', 'makers', 'space', 'data-visualization', 'alumni']",Public Library Innovation Exchange,PUBLIC,https://plix.media.mit.edu/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-04-04T00:44:21.028Z,"The reproductive organs of the female body have long been a site of contention, where opposing ideologies in religion, politics, and cultural differences often play out. Of all the questions, that of reproductive rights strikes a particularly sensitive nerve.US President Donald Trump recently signed an executive order which cut off all US funding to international NGOs whose work includes abortion services or advocacy. Images of this executive order being signed by Donald Trump flanked by a cabinet of men have circulated widely, begging the question: why do these men feel they have a right to determine women's reproductive choices?",2017-08-31,['design-fiction'],https://dam-prod2.media.mit.edu/x/2017/04/03/black shirt black sperm look to right smaller_U84RKNY.png,119843,,2017-10-11T20:28:30.946Z,['wonder@media.mit.edu'],True,woman-of-STEAM-grabs-back,2017-01-09,"['design', 'art', 'bioengineering', 'genetics']",Brain-Controlled Interface for  the Motile Control of Spermatozoa,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-04-04T03:51:55.451Z,"8K Brain Tour is a visualization system for terabyte-scale, three-dimensional (3D) microscopy images of brains.  High resolution (8K or 7680 x 4320 pixels), large format (85” or 188 cm x 106 cm), and touch-sensitive interactive rendering allows the viewers to dive into massive datasets capturing a large number of neurons and to investigate nanoscale and macroscale structures of the neurons simultaneously.",2020-07-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/2022/06/07/8kbt.jpg,119851,,2022-07-15T15:03:40.802Z,"['esb@media.mit.edu', 'shoh@media.mit.edu', 'bandy@media.mit.edu', 'itot@media.mit.edu', 'kanaya@media.mit.edu']",True,8k-brain-tour,2017-02-23,"['data', 'neurobiology', 'imaging']",8K Brain Tour: Interactive 3D visualization of terabyte-sized nanoscale brain images at 8K resolution,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2017-04-04T14:26:45.952Z,"Inertia is a platform for exploring physical interactions between real active agents and virtual elements in an augmented reality environment. The user is able to interact with a tangible active element that reacts to collisions and forces from virtual objects displayed in augmented reality. Physical forces and collisions in real-time can be better understood when applied to a tangible object, raising possibilities for learning and gaming.",2018-09-01,['living-mobile'],https://dam-prod2.media.mit.edu/x/2018/04/18/Pool2.png,119860,,2022-06-10T14:01:01.961Z,['afuste@media.mit.edu'],True,inertia,2017-01-01,"['augmented-reality', 'learning-teaching']",Inertia,PUBLIC,http://annafuste.com,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-04T15:05:19.897Z,"Most digital fabrication workflows impose a strong separation between design and fabrication. Designs are first modeled in computer aided design software, and when completed, converted to tool-paths which are uploaded to control software and autonomously executed by the fabrication machine. While there are advantages to this highly structured workflow, it restricts the ability for improvisation and revision. In addition, it eliminates the opportunity for embodied forms of expression, and direct engagement with the material during the fabrication process.I explored ways of supporting exploratory, intuitive, and immediate design practices in digital fabrication by creating a system for interactive control of a three-axis Computer Numerical Control (CNC) machine. I circumvented the traditional control interface of a large-format ShopBot machine to enable direct control by a human operator. I developed a tablet-based interface where people could draw designs with a pressure sensitive stylus. Each stroke a person drew was executed by the machine as it was completed. I also developed a custom drawing tool that fit into the ShopBot spindle and enabled the designer to switch between two different colors of acrylic paint as they drew. The tool mechanism was wireless, and was driven by two servomotors controlled through a bluetooth-enabled microcontroller.The complete system was made available to the general public during a four-day installation which enabled people to interact with the machine. In the process, I observed how the drawing-based interface lowered barriers to entry for digital fabrication and enabled people to execute organic and gestural forms and patterns with the machine. This work is part of ongoing research to explore ways of modifying existing digital fabrication machines to support embodied and intuitive forms of design and making.",2017-08-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/04/04/cnc_drawing.jpg,119873,,2018-01-03T21:38:05.112Z,[],True,sketch-based-cnc,2016-08-01,[],Sketch-based CNC,PUBLIC,http://media.mit.edu/~jacobs,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-04T16:18:44.910Z,"Imagine if you want to travel to a destination and once you are there hear only car horns, or see only blue, or feel a single spectral color of the city. The project Is about experiencing a city in a different way and to discovery distinct paths to travel. This is by including artificial soundscapes and visuals from other contexts. Part of this is knowing what sounds are around us and what are we looking at when we are walking in the city. The new experiences is an extra layer of sensory stimuli in the city. The study includes experiencing a city with colors and sounds from another location that we haven’t been to. This ""transfer"" process transfers colors and sounds to another location - while still being in the same city. One potentially could transfer emotional content from one city to another one. Overall, the immersive and multi-sensory representation of a map is crucial for allowing participants to fully feel that place. In addition, the goal is to offer a helpful, immersive, subjective - rather than a detached, observational- experience.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/04/04/Screen Shot 2017-04-04 at 12.22.42 PM.png,119889,,2017-04-04T16:24:50.150Z,['thomassl@media.mit.edu'],True,panoptic-journey,,[],Panoptic Journey,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-04T18:57:10.975Z,"Can we sonfiy calming breathing and passively influence a state of calm?Deep breathing has been scientifically proven to affect the heart, brain, digestive system, and the immune system. We believe designing a technology to promote deep breathing can facilitate transition into a calm state. Nowadays, many people are spending a significant amount of time listening to music while working or studying. This makes music a good means for providing auditory breathing cues. While it has been shown that liminal auditory cues can be effective in encouraging a healthy breathing pattern, we are examining the use of subliminal encouragement of  breathing modulation using music. Auditory ambient feedback has long been studied and is proved to be effective. It has been explored in concert settings, interactive installations, and smartphone applications. However, our aim is to design an intervention that is unobtrusive and doesn't keep people from doing their primary work. In order to find the best auditory feedback design, we have designed a controlled study comparing an interactive rhythmic ambient music track that responds to a user's current breathing patterns to a fixed rate music track whose speed of playback is pegged to a rate slightly below the user's natural resting breathing rate. A control condition with no music is also included. We will compare the resulting breathing patterns, heart rate, EEG signals, and self-reported measures to determine if the ambient music feedback has any effect on the user's state of mind and body. If successful, a musical system to subliminally encourage calming breathing patterns may be integrated into workplace environments, hospitals, and other places where it is necessary to promote less stressful and healthy environments.Our preliminary results significant shift in multiple physiological measures that indicate a state of calmness.",2018-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/2017/04/05/merged-music-slow.gif,119898,,2022-05-12T17:42:41.287Z,"['picard@media.mit.edu', 'gleslie@media.mit.edu', 'asma_gh@media.mit.edu']",True,brainbeat,2016-12-01,"['human-machine-interaction', 'music', 'affective-computing']",BrainBeat: Breath-based music therapy,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-04-04T19:04:14.018Z,"“The Talking Drums” is a sound installation where we created a modular system for sending images through rhythmic patterns. It is an encrypted language to empower a musical community by sending secret messages, avoiding surveillance and listening to each other. It’s inspired by the communication method and instrument used by African communities to send messages across the continent. They did this by drumming, and by a rhythmic language that only could be understood by the community.For the installation we collected different objects from MIT’s dump, especially obsolete technologies, such as hard drives and old screens. ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/04/04/Screen Shot 2017-04-04 at 3.05.54 PM.png,119906,,2018-10-19T19:49:21.579Z,"['yasushis@media.mit.edu', 'nicolelh@media.mit.edu', 'thomassl@media.mit.edu']",True,talking-drums,,[],Talking Drums,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-04-04T19:10:10.488Z,"Giving voice and information to objects and spaces around us.Objects in our lives are usually either digital or not; mostly a wall is just a wall. The Sonic Murals project explores what happens when we blur those lines. Implementing touch capacitance and conductive pigments in an innovative way, any surface can become a sensor, a tool for data collection, or a musical instrument, as exhibited in this project. When interacting with touch or proximity sensors on a sonic mural, one can experience spacial exploration and sound creation on a more omnisensory level.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/10/13/Screen Shot 2017-10-13 at 9.25.20 PM_wckcyhq.png,119914,,2020-01-17T10:20:36.839Z,['arieger@media.mit.edu'],True,sonic-murals,2017-01-01,"['music', 'sensors']",Sonic Murals,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-04T19:18:29.894Z,"Chronosonogy was born through extending the neuroscience research of Teki Et Al, which reveals ""Distinct Neural Substrates of Duration-Based and Beat-Based Auditory Timing” and Fassnidge Et Al’s work examining ""Visual Interference of Auditory Signal Detection."" Our perception of time is impacted by combining factors of visual-auditory override and imaginary notes sensations. Chronosonogy is both an experience and a newly discovered time-shifting phenomenon that activates a neurological quirk situated in fronto-temporal-parietal regions of our brains.  ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/04/04/Screen Shot 2017-04-04 at 3.20.13 PM_JjYOHJI.png,119922,,2019-04-17T19:57:53.118Z,['arieger@media.mit.edu'],True,chronosonogy-sonic-sensory-time-shifting,2017-04-04,[],Chronosonogy: Sonic sensory time shifting,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-04T19:33:14.515Z,"MM-RT is a tabletop tangible musical interface that employs electromagnetic actuators and small permanent magnets to physically induce sounds on objects. When, for example, a box with permanent magnets inside is placed on top of a pad, an electromagnet installed below the pad actuates the permanent magnets, causing them to bounce and hit the walls of a box. Timbre generation on each box is physically and digitally constrained: Each object comprises different materials and size, and a granular synthesis technique (a digital form of time domain additive synthesis) is used to create the sound producing mechanism.
                    
                ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/04/04/16195938_10102226734153818_1163939158972074616_n.jpg,119930,,2022-03-28T15:25:09.247Z,['akito@media.mit.edu'],True,mm-rt,,"['art', 'music', 'creativity', 'technology', 'alumni']",MM-RT,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-04T19:50:09.588Z,"iCenters: We work with a select group of Historically Black Colleges and Universities (HBCUs) to encourage and support the development of iCenters (Innovation Centers). The model for the iCenters is greatly influenced by the South End Technology Center (SETC). SETC was the first community located Maker Space to spin out of the Media Lab’s Bits and Atoms Lab. It is helmed by MIT Professor Emeritus Mel King. iCenters emphasize experiential learning by making and doing. By learning to code and learning to make and do with fabrication tools, we capture and retain interest in technology and facilitate closure of the Technology gap.",,['code-next'],,119938,,2017-04-04T21:48:44.601Z,"['tcarew@media.mit.edu', 'bdunning@media.mit.edu']",True,icenters-the-design-of-a-methodology-to-encourage-a-new-generation-of-computer-scientists-inventors-and-innovators,2014-01-01,[],"iCenters : The Design of a Methodology to Encourage  a New Generation of Computer Scientists, Inventors, and Innovators",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-04-04T20:01:13.984Z,"L2D:D2L: Learn to Develop:Develop to Learn. Much of our work is with young people from underserved communities. To better understand the age group as end users, we added a youth cohort to our Content Development staff. Their responsibility is to learn the nuances of Content Development, recommend Best Practices, create Prototypes, and develop Peer Appropriate Learning Activities.",,['code-next'],,119946,,2017-04-04T21:27:36.398Z,"['tcarew@media.mit.edu', 'bdunning@media.mit.edu']",True,l2d-d2l-high-school-students-develop-coding-and-fabrication-learning-activities-for-their-peers,2016-10-17,[],L2D:D2L : High School Students Develop Coding and Fabrication Learning Activities For Their Peers,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-04-04T20:11:25.577Z,"Spelman: Spelman College is a distinguished Black Women’s college in Atlanta. Nationally, twenty-three percent (23%) of the Black Women in STEM graduate from Spelman College. With their Innovation Center at Spelman, we piloted a two-way Long Distance Learning course in coding, microcontrollers (MaKey MaKey and Arduino), and digital fabrication. Students received credit for the course. For the course, we authored and designed  Experiential Learning Tutorials which we first taught to our Spelman teaching counterpart, Dr. Jerry Volcy. He heads Spelman’s Innovation Center. The final products of the course were fabricated interactive lamps, and  fabricated digital musical instruments.The course allowed us to test our Pilot design for a “Television Studio in a Suitcase” (TSS) broadcast system. TSS is an affordable, miniaturized and portable interactive broadcast system. It enables us to teach technology online from anywhere. It works. In the next series at Spelman, we will teach Robotics and Mechatronics.",,['code-next'],https://dam-prod2.media.mit.edu/x/2017/04/04/Spelman_Header_Image_cNhXSAu.JPG,119954,,2017-10-11T17:49:30.834Z,"['tcarew@media.mit.edu', 'bdunning@media.mit.edu']",True,spelman-the-design-of-a-two-way-television-studio-in-a-suitcase-broadcast-system-to-teach-fabrication-and-prototyping-remotely,2016-09-15,[],Spelman: The Design of a two-way ‘Television Studio in a Suitcase’ Broadcast System to Teach Fabrication and Prototyping Remotely,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-04-04T21:21:39.781Z,"Aether Muse proposes an extension for live internet streams of musical audio/video performance.  We explore how a two-way data connection between distributed audience listening can enhance the connection between a musician and their audience. By generating synchronized visualization for client watching the performance in the browser, musicians gain a new means to communicate with their fans and grow their audiences. This project also showcases ""tcchh,"" a custom digital audio effect for warping and stretching a live audio signal.",2022-06-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/04/04/charles project.png,119962,,2023-10-12T22:19:13.186Z,['holbrow@media.mit.edu'],True,aether-muse,,[],Aether Muse,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-04-04T21:30:07.288Z,"NCCU: music is a strong and universally accepted aspect of black culture. It has given birth to the genres of blues, jazz, gospel, rap, house, disco, funk, soul, trap, ragga, ska, dub, grime, reggae, calypso, hip hop, r&b, dubstep, soul, and neo-soul. We believe that there are new instruments and ideas to be discovered. We are beginning this paradigmatic journey at an innovation center we launched at North Carolina Central University. ",,['code-next'],https://dam-prod2.media.mit.edu/x/2017/04/04/NCCU 1_2ZN5Ih2.png,119970,,2018-05-08T12:02:11.010Z,"['tcarew@media.mit.edu', 'bdunning@media.mit.edu', 'fonuoha@media.mit.edu']",True,nccu-the-creation-of-new-black-musical-instruments-and-new-black-musical-idioms,2015-03-01,[],NCCU: The Design and Fabrication of New Black Musical Instruments,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-04-04T21:53:35.818Z,RoomScope is an immersive real-time architectural sensor visualization using Microsoft HoloLens.,2017-10-31,['changing-places'],https://dam-prod2.media.mit.edu/x/2017/04/04/RoomScope.jpg,119978,,2017-04-05T18:40:23.388Z,['honghaod@media.mit.edu'],True,roomscope,2017-02-05,"['augmented-reality', 'internet-things']",RoomScope,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2017-04-05T14:04:05.623Z,"In collaboration with Massachusetts General Hospital, we are conducting a clinical trial exploring objective methods for assessing depression and its severity. We are challenging the assessment methods that were created decades ago and which rely mostly on self-reported measures. We are including information from wearable sensors and regular sensors in mobile phones to collect information about sleep, social interaction, and location changes to find behavioral patterns that are associated with depressive symptoms. 
                    
                This research is supported by National Institute of Health, MIT-MGH Grand Challenge, and MIT J-Clinic.",2021-04-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/03/07/predicted_vs_actual_users_UPDATED-1.jpg,119988,,2022-05-12T17:31:42.895Z,"['asma_gh@media.mit.edu', 'sfedor@media.mit.edu', 'picard@media.mit.edu']",True,behavioral-indications-of-depression-severity,2015-01-01,"['computer-vision', 'artificial-intelligence', 'extended-intelligence', 'health', 'human-machine-interaction', 'wearable-computing', 'machine-learning', 'affective-computing', 'natural-language-processing', 'wellbeing']",Behavioral Indications of Depression Severity,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-04-07T11:57:11.818Z,"Virtual and augmented reality headsets are unique as they have access to our facial area, an area that presents an excellent opportunity for always-available input and insight into the user's state. Their position on the face makes it possible to capture bio-signals as well as facial expressions.  The  PhysioHMD platform introduces a software and hardware modular interface built for collecting affect and physiological data from users wearing a head-mounted display. The platform enables researchers and developers to aggregate and interpret signals in real-time and use them to develop novel, personalized interactions, as well as evaluate virtual experiences. Our design offers seamless integration with standard HMDs, requiring minimal setup effort for developers and those with less experience using game engines. The PhysioHMD platform is a flexible architecture that offers an interface that is not only easy to extend but also complemented by a suite of tools for testing and analysis. We hope that PhysioHMD can become a universal, publicly available testbed for VR and AR researchers.To create a seamless experience, we have integrated several bio-signal sensors into the faceplate of an HTC VIVE VR headset and utilized the Shimmer3 sensor for emotion-sensing. For the collection of Galvanic Skin Response, dry electrodes were positioned on the forehead area due to the fact that it is one of the areas most dense with sweat glands. GSR data reflects emotional arousal, but in order to identify how arousal, valence, motivation, and cognition interact in response to physical or psychological stimuli, it becomes necessary to complement GSR with other biosensors. For the heart rate, a PPG (photoplethysmogram) sensor, which senses the rate of blood flow by utilizing light to monitor the heart’s pumping action, was placed in the temple region of the user. This is done to get insights into the respondent's physical state, anxiety and stress levels (arousal), and to determine how changes in their physiological state relate to their actions and decisions.",2019-09-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/02/IMG_20180428_130707Web.jpg,120076,,2022-05-31T21:15:12.205Z,"['gbernal@media.mit.edu', 'pattie@media.mit.edu', 'yangtao@media.mit.edu', 'abyjain@media.mit.edu']",True,physiohmd,2017-02-01,"['virtual-reality', 'bioengineering', 'human-machine-interaction', 'sensors', 'interfaces']",PhysioHMD,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-04-07T12:03:26.190Z,"With advances in virtual reality (VR) and physiological sensing technology, even more immersive computer-mediated communication through life-like characteristics is possible. As a solution for the current lack of culture, expression, and emotions in VR avatars, we propose a two-fold solution. First, integrate bio-signal sensors into the head-mounted display (HMD) and implement techniques to detect aspects of the emotional state of the user. Second, connect the data collected to an expressive avatar: Emotional Beasts. The creation of Emotional Beasts allowed us to experiment with the manipulation of a user's self-expression in VR space and as well as the perception of others in it, with the goal of pulling the avatar design away from the uncanny valley and making it more expressive, more relatable to our own mannerisms. Based on this we have implemented a prototype system in which VR, human motion, and physiological signals are integrated to allow avatars to become more expressive in virtual environments in real time.",2022-05-07,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/10/18/EmotionalBeasts.jpg,120084,,2022-05-31T21:14:04.101Z,['gbernal@media.mit.edu'],True,emotionalbeasts-1,2016-08-01,"['virtual-reality', 'art', 'sensors', 'interfaces', 'affective-computing']",EmotionalBeasts,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-04-26T20:23:50.177Z,"Two-dimensional radiographs, while commonly used for evaluating sub-surface hard structures of teeth, have low sensitivity for early caries lesions particularly those on tooth occlusal surfaces. Radiographs are also frequently refused by patients over safety concerns. Translucency of teeth in the nearinfrared (NIR) range offers a non-ionizing and safe approach to detect dental caries. We report construction of a NIR (850 nm) LED imaging system, comprised of a NIR source and an intraoral camera for rapid dental evaluations. The NIR system was used to image teeth and successfully detected secondary, amalgam–occluded and early caries lesions without supplementary image processing. The camera-wand system was also capable of revealing demineralized areas, deep and superficial cracks, and other clinical features of teeth usually visualized by X-rays. The NIR system’s clinical utility, simplistic design, low cost and user friendliness makes it an effective dental caries screening technology in conjunction or in place of radiographs.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/04/26/Fig1.jpg,125345,,2018-05-03T17:47:20.946Z,[],False,near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth-1,2017-04-12,"['computer-vision', 'bioengineering', 'health', 'imaging']",Near-Infrared Imaging for Detecting Caries and Structural Deformities in Teeth,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-05-05T17:09:15.240Z,"Can tattoos embrace technology in order to make the skin interactive?The DermalAbyss project was the result of a collaboration between MIT researchers Katia Vega, Xin Liu, Viirj Kan, and Nick Barry, and Harvard Medical School researchers Ali Yetisen and Nan Jiang. DermalAbyss was a proof-of-concept that presented a novel approach to bio-interfaces in which the body surface would be rendered as an interactive display. By replacing traditional tattoo inks  with biosensors whose colors change in response to variations in the interstitial fluid, it blended advances in biotechnology with traditional methods in tattoo artistry. This was a research project, which has ended. There are no plans to develop DermalAbyss as a product or to pursue clinical trials.",2017-05-31,"['responsive-environments', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2017/05/05/Profile-image.jpg,128173,,2020-01-09T16:10:10.051Z,"['nbarry@media.mit.edu', 'viirj@media.mit.edu', 'pattie@media.mit.edu', 'joep@media.mit.edu', 'xxxxxxin@media.mit.edu', 'katiav@media.mit.edu']",True,d-Abyss,2016-06-01,"['health', 'wearable-computing']",DermalAbyss: Possibilities of Biosensors as a Tattooed Interface,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2017-05-05T18:25:27.218Z,"Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. The user begins with specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled contact iron that seals layers of thermoplastic fabric. In this project, we showcase the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.Project Website",2017-12-31,[],https://dam-prod2.media.mit.edu/uuid/ce9206db-4144-4f6c-ac05-0768e9743c23,128182,,2020-06-09T17:23:53.992Z,"['udayan@media.mit.edu', 'sareen@media.mit.edu', 'jifei@media.mit.edu', 'kakehi@media.mit.edu', 'pattie@media.mit.edu', 'ishii@media.mit.edu']",True,inflatables,2017-05-10,"['robotics', 'human-machine-interaction', 'manufacturing']","Printflatables: Printing human-scale, functional, and dynamic inflatable objects",PUBLIC,http://tangible.media.mit.edu/project/printflatables/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-05-07T07:13:51.255Z,"We developed a concept of transformative appetite, where edible 2D films made of common food materials (protein, cellulose or starch) can transform into 3D food during cooking. This transformation process is triggered by water adsorption, and it is strongly compatible with the ‘flat packaging’ concept for substantially reducing shipping costs and storage space. To develop these transformable foods, we performed material-based design, established a hybrid fabrication strategy, and conducted performance simulation. Users can customize food shape transformations through a pre-defined simulation platform, and then fabricate these designed patterns using additive manufacturing. Three application techniques are provided:  2D-to-3D folding, hydration-induced wrapping, and temperature-induced self-fragmentation, to present the shape, texture, and interaction with food materials. Based on this concept, several dishes were created in the kitchen, to demonstrate the futuristic dining experience through materials-based interaction design.",,[],,128971,,2019-02-14T19:43:51.271Z,"['wwen@media.mit.edu', 'liningy@media.mit.edu', 'dvlevine@media.mit.edu', 'ishii@media.mit.edu']",True,transformative-appetite,2017-05-07,"['design', 'food']",Transformative Appetite,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-05-17T23:22:52.382Z,"We introduce a family of fragile electronic musical instruments designed to be “played” through the act of destruction. Each Fragile Instrument consists of an analog synthesizing circuit with embedded sensors that detect the destruction of an outer shell, which is destroyed and replaced for each performance. Destruction plays an integral role in both the spectacle and the generated sounds.",2017-12-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2017/05/17/Screen Shot 2017-01-30 at 9.37.17 PM.png,158267,,2017-05-17T23:43:56.207Z,"['x_x@media.mit.edu', 'ddh@media.mit.edu', 'tod@media.mit.edu', 'joep@media.mit.edu']",True,fragile-instruments,2016-06-01,['music'],Fragile Instruments,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2017-05-18T00:09:28.082Z,"SoundFORMS creates a new method for composers of electronic music to interact with their compositions. Through the use of a pin-based shape-shifting display, synthesized waveforms are projected in three dimensions in real time affording the ability to hear, visualize, and interact with the timbre of the notes. Two types of music composition are explored: generation of oscillator tones, and triggering of pre-recorded audio samples. The synthesized oscillating tones have three timbres: sine, sawtooth and square wave. The pre-recordedAudio samples are drum tracks. Through the use of a gestural vocabulary, the user can directly touch and modify synthesized waveforms.",2016-06-01,[],https://dam-prod2.media.mit.edu/x/2017/05/17/gif1.gif,158278,,2023-04-04T15:19:41.675Z,"['ishii@media.mit.edu', 'tice@media.mit.edu', 'ddh@media.mit.edu']",True,soundform,2016-02-01,[],SoundFORMS: Manipulating Sound Through Touch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-05-18T00:36:39.887Z,"This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.",2016-08-01,[],,158309,,2017-05-18T01:07:33.691Z,"['x_x@media.mit.edu', 'ddh@media.mit.edu', 'thomassl@media.mit.edu', 'rebklein@media.mit.edu', 'akito@media.mit.edu', 'pewebb@media.mit.edu', 'tod@media.mit.edu', 'joep@media.mit.edu', 'ishii@media.mit.edu']",True,kinephone,2016-05-01,['music'],Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-05-18T18:19:08.737Z,"We support computer scientists and engineers from the Middle East to create technologies for refugee learners. The Refugee Learning Accelerator provides training, mentorship, and funding to teams of computer scientists, engineers, and designers. Who can apply? University students and recent graduates with backgrounds in computer science, design, and electrical engineering coming from some of the places most impacted by the current refugee crisis: Jordan, Lebanon, Syria, Iraq, and Palestine.More information available here. ",2019-09-30,['ml-learning'],https://dam-prod2.media.mit.edu/x/2017/07/31/RF2102136_20170305_tertiary-ed_50_TAR.jpg,158321,,2019-11-21T20:24:21.067Z,"['barrons@media.mit.edu', 'ps1@media.mit.edu']",True,refugee-learning-accelerator,2017-05-01,['learning-teaching'],Refugee Learning Accelerator,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-05-22T14:48:54.076Z,"Have you ever wondered what a friend would do if she was in your decision-making situation? Or thought about where a family member might go if he was visiting a travel destination with you? In many cases, you can only guess what a person would do if they were in your shoes. But now you may be able to securely ""borrow their  identity"" and ask a question with the confidence of receiving a relevant and  valuable answer. Can software agents become our digital heirs? Can a head of state, a scientist, or a business owner leverage machine intelligence to complement succession planning?  What if you could select the digital identity of a deceased person from a social network and activate it as a pluggable ontology into your iPhone’s Siri and ask a question?Our digital identity has become so rich and intrinsic that without it, it may feel like a part of us is missing. The number of sensors we carry daily and the digital footprints we leave behind have given us enough granular patterns and data clusters that we can now use them for prediction and reasoning on behalf of an individual. We believe that by enabling our digital identity to perpetuate, we can significantly contribute to global expertise and enable a new form of an intergenerational collective intelligence.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2018/02/09/flybits_icons_alpha.0048.png,159915,,2021-03-18T18:38:57.324Z,['rahnama@media.mit.edu'],True,augmented-eternity,2017-05-22,"['artificial-intelligence', 'data', 'social-science', 'machine-learning', 'collective-intelligence', 'behavioral-science']",Augmented Eternity and Swappable Identities,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-05-23T16:05:34.584Z,The Reality Editor is a web browser for the physical world: Point your phone or tablet at a physical object and an interface pops up with information about that object as well as services related to that object. The Reality Editor platform is open and entirely based on web standards making it easy for anyone to create Reality Editor enabled objects as well as Reality Editor applications that integrate the physical and digital world in one experience.Reality Editor version 2.0 Reality Editor  version 2.0 is now available for download and adds the following features:World Wide Web conform content creation.Spatial Search - Instantly browse through relevant information in the physical world around you. you to browse reality.Bi-Directional AR - A real-time interactions system.Private and Decentralized infrastructure for connecting the IoT objects.Logic Crafting - A visual programming language designed for Augmented Reality.The  Reality Editor works on iOS and you can get it here. Try it out with our Starter App and some Philips Hue Lights or the Lego WeDo 2.0. Learn more about Logic Crafting in our User Interface 101.,2017-12-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/05/26/RE05.jpg,160336,,2018-10-11T18:46:45.692Z,"['pattie@media.mit.edu', 'heun@media.mit.edu', 'benolds@media.mit.edu']",True,reality-editor-20,2017-05-22,"['augmented-reality', 'interfaces', 'internet-things']",Reality Editor 2.0,PUBLIC,http://www.valentinheun.com,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-05-30T22:35:25.911Z,"Tourism is Cuba’s third largest source of foreign currency, behind the two dominant industries of sugar and tobacco. The number of visitors so far in 2016 jumped 13.5 percent over the year to 1.5 million tourists. However, the hunt for novelty in Cuba doesn’t conceal its disquieting poverty and struggles: lack of regulation on sex work, ungovernable black markets and creaky infrastructure. Everyday in Havana, we inevitably enjoyed the material comforts like all the other tourists while investigating the power relations of the nation as the artists.This project calls attention to the actions of each individual, using the nation’s subsidized, soft, sweet, round daily buns as the vehicle. Made of imported flour, sugar, dry yeast and water, the daily bun tastes nothing but pale flour. Collaborating with IFF (International Flavor and Fragrance), we designed the bread with a complete flavor that balances the nuances of gasoline, sweat and white ginger flower (the national flower of Cuba).",2017-05-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/05/30/Bread-8.jpg,163109,,2017-11-30T15:56:33.024Z,['xxxxxxin@media.mit.edu'],True,havana-bread,2017-03-01,[],Havana Bread,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2017-06-01T17:17:08.068Z,"How can we add the missing ""T"" and ""E"" in preschool STEAM education?",2018-12-31,['personal-robots'],https://dam-prod2.media.mit.edu/x/2017/06/04/IMG_20170424_145024096.jpg,163549,,2020-04-03T17:51:41.838Z,"['randiw12@media.mit.edu', 'cynthiab@media.mit.edu', 'haewon@media.mit.edu']",True,pop-kit,2016-10-01,"['human-computer-interaction', 'artificial-intelligence', 'human-machine-interaction', 'kids', 'learning-teaching', 'interfaces', 'social-robotics', 'ethics', 'technology', 'covid19']",PopBots: An early childhood AI curriculum,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2017-06-04T02:50:29.719Z,"The process by which children learn native languages is markedly different from the process of learning a second, or non-native, language. Children are typically immersed in their native languages. They receive input from the adults and other children surrounding them, based on immediate need and interaction, during every waking hour.  Second language learners are exposed to input from the new language in very different ways, most commonly in a classroom setting. The second language learner relies heavily on memory skills with sparse interaction, in contrast to the first language learner that can rely on environmental reinforcement and social interaction to learn words. Social robots have the potential to drastically improve on this paradigm, making the second-language learning experience more like the experience of learning a native language by engaging the child in a rich, interactive exposure to the target language, especially aspects not typically covered by traditional technological solutions, such as prosody, fundamental phonetics, common linguistic structures, etc.Our project explores how to design child-robot  interactions that encourage child-driven language learning, that adapt and personalize each child’s learning experience. We incorporate game design and machine learning into the child-robot interaction design. The child and robot play through a suite of educational games together. Using real-time sensor data and gameplay features, the robot constructs a model of each child's learning and emotional trajectory, then uses these models to inform its own decision making during the game. Thus, the robot's behaviors become personalized to individual children based on their learning style, personality and knowledge/emotional states during gameplay. ",,['personal-robots'],,164381,,2018-05-03T20:56:50.720Z,"['cynthiab@media.mit.edu', 'haewon@media.mit.edu', 'samuelsp@media.mit.edu', 'hchen25@media.mit.edu']",True,personalized-interaction-for-language-learning,2017-01-01,"['interfaces', 'social-science', 'social-robotics']",Personalized  Interaction for Language Learning,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-06-04T13:24:06.595Z,"Creating long-term interpersonal interaction and shared experiences with social robots Many of our current projects explore the use of social robots as a technology to support young children's early language development. In this project, instead of focusing on how to make social robots effective as an educational tools, we ask why they are effective. Based on our prior work, we hypothesize that a key aspect of why social robots can benefit children's learning is their nature as a relational technology—that is, a technology that can build long-term, social-emotional relationships with users. Thus, in this project, our goals are twofold. First, we aim to understand how children conceptualize social robots as relational agents in learning contexts, and how children relate to these robots through time. Second, we explore the core nature of autonomous relational technologies, that is, relational AI. We will examine how adding features of relational AI to a social robot impacts longitudinal child-robot learning interactions, including children's learning, engagement, and relationships.As part of this project, we are taking a second look at work we have done so far, this time through the lens of children's relationships. We are creating assessments for measuring young children's relationships. We are developing a computational relational AI model, which we will test during a longitudinal study with a social robot.Read more about children's relationships with robots here!",2019-09-01,"['ml-learning', 'personal-robots']",https://dam-prod2.media.mit.edu/x/2017/12/20/session8-p004b.jpg,164394,,2019-10-09T14:07:18.323Z,"['picard@media.mit.edu', 'cynthiab@media.mit.edu', 'jakory@media.mit.edu']",True,relational-ai,2016-09-01,"['robotics', 'artificial-intelligence', 'cognition', 'kids', 'learning-teaching', 'social-robotics', 'long-term-interaction', 'language-learning']",Relational AI,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-06-06T08:35:05.298Z,"Should workers worry about automation and AI?Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/06/06/luddite_NdfTYdV.jpeg,165196,,2020-07-23T20:02:38.536Z,"['mrfrank@media.mit.edu', 'irahwan@media.mit.edu', 'emoro@media.mit.edu', 'cebrian@media.mit.edu', 'hyoun@media.mit.edu', 'groh@media.mit.edu', 'alexis@media.mit.edu']",True,future-of-work-ai-automation-labor,2017-06-06,"['artificial-intelligence', 'data', 'economy', 'human-machine-interaction', 'urban-planning', 'social-science']","AI, Automation, Labor, and Cities: How to map the future of work",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2017-06-06T13:12:49.236Z,Climate change is going to alter the environments that we depend on in myriad ways. We're using data to identify and quantify these potential human impacts. ,2020-07-01,[],https://dam-prod2.media.mit.edu/x/2017/06/06/drought.jpg,165211,,2020-07-23T18:47:11.939Z,"['nobradov@media.mit.edu', 'pinary@media.mit.edu', 'felbo@media.mit.edu', 'irahwan@media.mit.edu', 'emoro@media.mit.edu', 'cebrian@media.mit.edu']",True,identifying-the-human-impacts-of-climate-change,2016-07-01,"['environment', 'government', 'health', 'social-media', 'social-science', 'public-health', 'wellbeing', 'climate-change']",Identifying the human impacts of climate change,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2017-06-07T02:07:18.023Z,"Recent rapid advances in Artificial Intelligence (AI) and Machine Learning have raised many questions about the regulatory and governance mechanisms for autonomous machines. This is not about individual gadgets, but about complex, networked systems of humans and algorithms making decisions in business, government, and the media. We need conceptual frameworks for designing new governance architectures for these human-machine social systems. In doing so, it is helpful to learn lessons about human cooperation and governance from political philosophy and cultural anthropology. Read more here.",2020-07-01,[],https://dam-prod2.media.mit.edu/x/2017/06/06/leviathan.jpg,165623,,2020-07-23T18:41:09.916Z,['irahwan@media.mit.edu'],True,governance-artificial-intelligence-social-media,2017-05-01,"['artificial-intelligence', 'government', 'ethics']",Society-in-the-Loop,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-06-13T18:15:43.495Z,"Every 98 seconds, a person in the United States is sexually abused. Every 16 hours, a woman in the United States is murdered by her romantic partner or ex-partner. Sexual abuse, assault, and harassment are regarded as some of the most common human rights violations in the world by the United Nations. Our work examines methods to prevent sexual assault, from pre-historic times to latest technologies, to inform contemporary designs. Intrepid investigates multiple methods to detect initial signs of assault and develop methods for communication and prevention of assault. We also explore olfactory stimuli as a potential means to prevent sexual assault in real-time.",2017-06-30,['living-mobile'],https://dam-prod2.media.mit.edu/x/2017/06/13/IMG_5574.jpg,168118,,2018-04-20T17:21:38.623Z,"['manisham@media.mit.edu', 'ethanz@media.mit.edu', 'geek@media.mit.edu']",True,intrepid,2016-01-04,"['consumer-electronics', 'developing-countries', 'human-machine-interaction', 'security', 'wearable-computing', 'sensors', 'interfaces', 'gesture-interface']",Intrepid,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-06-18T17:49:42.145Z,"Opinion aggregation on social media uses various mechanisms,  such as ""Likes"" or thumbs-up/-down, which handle a single item at a time. In many domains (e.g., political discussion), we need to consider the relationships between different claims, and how they rebut one another through complex webs of arguments and counter-arguments. We study methods for aggregating opinions about such complex argument networks, the quality of the outcomes of different methods of opinion aggregation, and whether strategic agents can manipulate those outcomes.
                    
                ",2020-07-01,['scalable-cooperation'],,169378,,2020-07-23T18:48:33.674Z,"['irahwan@media.mit.edu', 'awad@media.mit.edu']",True,opinion-aggregation,2008-01-01,"['artificial-intelligence', 'collective-intelligence']",Opinion Aggregation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2017-06-19T14:23:25.642Z,"The Open Leadership Camp (OLC) is a new type of professional development program for senior leaders of nonprofit and public sector organizations. It aims to  apply the principles of open source, open innovation, and the decentralized nature of the web to the way some of our most crucial social sector organizations work. This project is a collaboration between Mozilla and the ML Learning Initiative, and is hosted by MIT Media Lab director Joi Ito and Mitchell Baker, co-founder and executive chairwoman of the Mozilla Corporation. In March 2017, we brought together our first cohort of 14 participants, including the CEO of Consumer Reports, the CIO of the City of Detroit, and the CEO of WGBH.  Our second cohort, from November 2017, included the director of the United Nations Office for Outer Space Affairs (UNOOSA), the executive director of Conservation International, and the director of the Cleveland Public Library.",2017-12-31,"['ml-learning', 'digital-learning-studio']",https://dam-prod2.media.mit.edu/x/2017/06/19/IMG_4966.JPG,169773,,2020-12-02T01:11:02.471Z,"['ps1@media.mit.edu', 'kamcco@media.mit.edu', 'yumikom@media.mit.edu']",True,open-leadership-camp,2017-01-01,"['learning-teaching', 'open-source', 'open-access']",Open Leadership Camp,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-06-20T19:47:44.995Z,"We present AnimaStage: a hands-on animated craft platform based on an actuated stage. Utilizing a pin-based shape display, users can animate their crafts made from various materials. Through this system, we intend to lower the barrier for artists and designers to create actuated objects and to contribute to interaction design using shape-changing interfaces for inter-material interactions.We introduce a three-phase design process for AnimaStage with examples of animated crafts. We implemented the system with several control modalities that allow users to manipulate the motion of the crafts so that they could easily explore their desired motion through an iterative process. Dynamic landscapes can also be rendered to complement the animated crafts. We conducted a user study to observe the subject and process by which people make crafts using AnimaStage. We invited participants with different backgrounds to design and create crafts using multiple materials and craft techniques. A variety of outcomes and application spaces were found in this study.Project Page",2017-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2017/06/20/AnimaStage2.jpg,170211,,2020-06-09T18:30:04.910Z,"['ken_n@media.mit.edu', 'udayan@media.mit.edu', 'daniell@media.mit.edu', 'ishii@media.mit.edu']",True,animastage,2017-06-12,[],AnimaStage,PUBLIC,http://tangible.media.mit.edu/project/animastage,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-07-07T09:21:05.193Z,"Social Mirror is a web application that helps Twitter users interactively explore the politically active parts of their social network.  Worsening political polarization over the past several years has exacerbated ideological echo chambers, which in turn have further fueled polarization by widening knowledge and empathy gaps between disparate groups.  We hope digital tools like Social Mirror can help inspire self-reflection, and ultimately, intellectual humility by providing people with a new view of their social media ecosystems and helping them form new network connections.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2017/07/07/Screen Shot 2017-07-07 at 5.25.39 AM.png,175745,,2021-10-07T17:55:29.321Z,"['annyuan@media.mit.edu', 'ngillani@media.mit.edu', 'dkroy@media.mit.edu']",True,social-media-mirror,2017-06-01,"['social-networks', 'politics', 'social-media']",Social Mirror,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-07-07T19:02:32.005Z,"We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with an $\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/07/07/output_SZIGSG8.png,175776,,2019-04-19T17:47:51.865Z,['otkrist@media.mit.edu'],True,architecture-selection-for-deep-neural-networks,2016-05-01,[],Architecture selection for deep neural networks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2017-07-07T21:14:12.751Z,"Given the cross-border impact of AI and related technologies, what are appropriate and workable governance mechanisms that can operate at a global scale? Leveraging insights from research into innovative governance models in the Internet realm, this project examines the application of governance models to AI systems through a series of case studies and working meetings in the US, Europe, and Asia. Outputs include a policy report, recommendations, and an international working group.",2019-08-31,['ethics-and-governance'],,175801,,2021-07-13T18:35:33.657Z,[],True,test-project-friday-pm,2017-07-07,"['artificial-intelligence', 'government', 'ethics']",AI and Global Governance,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-07-08T00:29:41.213Z,"Responsive Science is a way of conducting research that invites openness and community involvement from the earliest stages of each project. Real-time interaction between scientists, citizens, and broader communities allows questions and concerns to be identified before experiments are performed, fosters open discussion, and encourages research studies and new technologies to be redesigned in response to societal feedback.",,['sculpting-evolution'],https://dam-prod2.media.mit.edu/x/2017/09/06/Screen Shot 2017-09-06 at 8.08.35 PM.png,175850,,2017-09-07T00:11:35.521Z,"['devora@media.mit.edu', 'ave@media.mit.edu']",True,responsive-science,,"['communications', 'ecology', 'environment', 'genetics', 'health', 'learning-teaching', 'synthetic-biology']",Responsive Science,PUBLIC,https://www.responsivescience.org/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-07-10T14:15:23.819Z,This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.The  Humanizing AI in Law (HAL) project aims to build the technical and legal foundations necessary to establish a due-process framework for auditing and improving decisions made by artificial intelligence systems as they evolve over time. This work is directed at the concerning software that has been deployed within the criminal justice system to aid judges in the sentencing of criminal defendants.,2019-12-01,"['ethics', 'ethics-and-governance']",https://dam-prod2.media.mit.edu/x/2018/02/08/5646757752_1a652c731a_o.jpg,176632,,2020-02-28T17:43:03.922Z,"['madars@media.mit.edu', 'cbarabas@media.mit.edu', 'kdinakar@media.mit.edu', 'nsaltiel@media.mit.edu', 'joi@media.mit.edu', 'zittrain@media.mit.edu']",True,HAL,2017-06-01,"['artificial-intelligence', 'government', 'social-science', 'ethics', 'racial-justice']",Humanizing AI in Law (HAL),PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-07-10T14:45:03.090Z,"Through a series of expert workshops and advanced horizon scanning and foresight methods, this foundational project aims at developing an overall mapping of key ethical and governance challenges in the AI space for purposes of enhancing our collaborative network and identifying areas ripe for impact. Outputs include translational tools, such as a visual navigation aid mapping people/institutions, research, and existing efforts related to the Ethics and Governance of AI Initiative  that bridge key technology and policy concepts.This project will be led by  Ryan Budish, Amar Ashar, and Elena Goldstein from the Berkman Klein Center.",2020-06-02,['ethics-and-governance'],,176640,,2022-06-10T13:49:58.781Z,[],False,ai-challenge-mapping,2017-06-01,[],AI Challenge Mapping,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2017-07-12T18:30:20.968Z,"Object Classification through Scattering Media with Deep LearningA method for classifying objects hidden behind a scattering layer with a neural network. Training on synthetic data with variations in calibration parameters allows the network to learn a model that doesn't require calibration during lab experiments.Traditional techniques to see through scattering media rely on a physical model that needs to be precisely calibrated. Computationally overcoming the scattering relies heavily on accurately calibrated physical models. Thus, such systems are extremely sensitive to a precise and lengthy calibration process. In this work we overcome this bottleneck by utilizing neural networks and their ability to learn models that are invariant to data transformation. In our case, the transformations are variations in the imaging system calibration parameters. To that end, we create a synthetic dataset that contains variations in all calibration parameters (we use a Monte Carlo forward model to render the measurements). The system is then tested on actual lab experiments without specific calibration or tuning.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2017/07/12/Untitled_xd75sGu.jpg,176699,,2018-10-20T00:40:19.319Z,"['raskar@media.mit.edu', 'barmak@media.mit.edu', 'otkrist@media.mit.edu', 'guysatat@media.mit.edu']",True,calibration-invariant-i,2016-09-01,"['human-machine-interaction', 'machine-learning']",Calibration Invariant Imaging,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-07-13T18:03:28.086Z,"BayesDB is open-source AI software that lets any programmer answer data analysis questions in seconds or minutes with a level of rigor that otherwise requires hours, days, or weeks of work by someone with good statistical judgment. This project empowers domain experts to solve problems themselves, and allows users to easily check results quality by comparing inferred relationships to common-sense knowledge and comparing model simulations to reality and expert opinion.  More information at the BayesDB project page. ",2019-08-31,['ethics-and-governance'],,176713,,2021-07-13T18:37:24.273Z,[],True,bayesdb,2017-07-01,"['artificial-intelligence', 'data', 'machine-learning', 'ethics']",BayesDB,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-07-13T18:11:18.127Z,"Bringing together a diverse group of stakeholders, this pilot examines the intersection between AI and inclusion, and explores the ways in which AI systems can be designed and deployed to support diversity and inclusiveness in society. While promoting learning opportunities through engagement in events, learning calls, and joint research activities, the pilot is making educational resources and other outputs accessible via a variety of platforms. Thematically, an emphasis has been placed on the impact of AI on underserved groups – whether in terms of age, ethnicity, race, gender and sexual identity, religion, national origin, location, skill and educational level, and/or socioeconomic status – and on how these communities think about AI systems.
                    
                ",2019-08-31,['ethics-and-governance'],,176721,,2021-07-13T18:38:28.356Z,[],True,ai-and-inclusion,2017-07-01,[],AI and Inclusion,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-07-21T15:30:05.003Z,"This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.Despite gazillions of leadership programs across university campuses and organizations, we increasingly live in an era of leadership deficit. Transformative Leadership helps participants explore how to align professional values with their deep personal values and develop reflective thinking skills for ethics and values-based leadership. The program is tailored for various demographics including leadership in business organizations, governments and law enforcement agencies. This program is equipped with longitudinal assessment protocols to study in-depth transformational abilities of individuals and how they leverage it for organizations.Transformative Leadership is currently offered in the US, Mexico, Colombia, India  and Canada.  At MIT, Transformative Leadership modules are offered through MIT Sloan and Open Leadership at MIT Media Lab.",2019-12-01,['ethics'],https://dam-prod2.media.mit.edu/x/2017/07/21/IMG_0008.JPG,178730,,2020-02-28T17:44:27.743Z,['tenzin@media.mit.edu'],True,transformative-leadership,2017-02-01,"['learning-teaching', 'ethics']",Transformative Leadership,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-07-27T20:09:24.096Z,"Emotional content is an important part of language. There are many use cases now showing that natural language processing is becoming an increasingly important part of consumer products. We are attempting to learn more about human emotions.In his 2006 book The Emotion Machine, legendary computer scientist Marvin Minsky (co-founder of the field of Artificial Intelligence and one of the founding faculty members of the MIT Media Lab) wrote about the central role of emotions in reasoning—reminding us that AI will only be capable of true commonsense reasoning once it has understood emotions. To Minsky, emotions are not the opposite of rational reason, something to be weeded out before we can think clearly; rather, emotions are just a different way of thinking.TRY DEEPMOJI             HELP TEACH OUR AI ABOUT EMOTIONSBut this is hardly helpful to a computer scientist trying to construct an emotional machine by programming a concrete set of rules. If you ask two people to explain what makes a particular sentence happy, sad, serious, or sarcastic, you will likely get at least two different opinions. Much of what determines emotional content is context-specific, culturally constructed, and difficult to describe in an explicit set of rules.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/08/11/emoji-2074153 (1)_go0aoJF.png,181112,,2020-07-23T18:48:53.031Z,"['irahwan@media.mit.edu', 'felbo@media.mit.edu', 'nobradov@media.mit.edu']",True,deepmoji,2017-08-02,"['communications', 'data', 'social-media', 'social-science', 'machine-learning']",DeepMoji,PUBLIC,http://deepmoji.mit.edu,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-08-04T13:17:35.594Z,"IdeaA lab is where unique ideas are generated and early prototypes are synthesized. A factory is where designs are mass produced and quality is validated. They seem so far away from each other, that we usually separate research from reality, prototype from production. How can we demolish this separation for a future where design, technology and production are tightly coupled together? In this project, we are exploring the idea of ""researcher in residency"" in the context of manufacturing. As the city of manufacturing, Shenzhen has the most concentrated factories across industry in the world. By bringing researchers to the factory floor for 4 weeks, we are experimenting methods to fuse the lab and factory as a future venue for innovation.Projects OutcomeTowards the end of the workshop, 8 projects were generated among 10 researchers. Three projects involved collaborations between 2 or 3 people, and the rest were individual projects. They range from art to sensors, from textiles to robotics.Jie Qi, Donald Derek - Sound SpiralsThis project explored blending natural materials and flexible printed circuit boards to create working speaker elements.  We experimented with a number of unusual PCB composite materials including knitted textile, paper and pressed leaves.  We created speaker coils by etching spiral shapes into flexible PCB, following Perner-Wilson's example handcrafted speakers, and laminated these directly onto the experimental materials using the industrial heat press. Artem Dementyev - Circuit RoboticsThe main idea of this project was to create actuators and sensors using only flexible circuit board technology, rather than separating the electrical components from the mechanical and structural components. Using the coverlay lamination process we were able to embed channels between the copper layer and the coverlay. The channels were used as air pockets to create pneumatic actuators. With a different channel geometry, we were able to embed shape memory alloy wires into the flexible printed circuit board to create an alternative actuator.Laya Anasu - Self-Disassembly KnitIn a knitting process, waste can be produced by the automatically generated supporting roles of knit. This project asks a question: if we knit many small pieces, how can we reduce the waste? The student utilizes a yarn that dissolves in water to stitch all small pieces together into one big knit. The result is a large knitted fabric that can disassemble into many designed, smaller pieces.Ani Liu - Psyche in the Age of Mechanical ReproductionThis is an art project that explores the interplay of factory workers, knitted fabric and data visualization. The student took the EEG data from a factory worker, translated the measured stress level to the program parameter, which controls the tension of a knit. The result is the individual knitted carpet that reflects the emotional signature of a factory worker.Amos Golan - Knitting with Unusual YarnsUsually a digital knitting machine takes soft yarn below 1400 denier. This project developed an auxiliary device that can be attached to the machine to control the pulling and feeding force of a yarn. By doing so, we can now knit much harder materials such as copper wire, shape memory alloy, etc. The student produced material samples that smoothly inlays a shape memory alloy inside, to create shape-change textile. Miguel Perez - Structural TextileThe project explores the possibility of integrating plastic filament into the knitting process to create fabric that has tunable stiffness. The result is a new process that involves knitting, heat press and forming. The student also created material samples that are produced with this process. Guillermo Bernal - NeuroknitThis is a design exploration of how to create a three dimensional VR helmet with knitting process. The helmet contains also small pocket inside to hold a electro circuit that was developed in the flexible PCB factory. The result is a interplay between designing the shape of the Flex PCB and fine tuning the shape of the knitted helmet.Jifei Ou, Daniel Oran, and Donald Derek - Sensitive TextilesThis project explores the design space of making textile-based sensor at the stitch level. The factory has created hundreds of knitting patterns that are mostly used for visual aesthetics. The students looked at their microstructure and developed stretch sensor and potentiometer by incorporating conductive yarn. The result is a series of production-ready textile sensors. Those sensors are connected to a control board that was developed in the flexible PCB factory.ParticipantsJifei Ou | Instructor | Tangible MediaArtem Dementyev | Instructor | Responsive EnvironmentsJie Qi | Instructor | Lifelong KindergartenAmos Golan | Student | Tangible MediaAni Liu | Student | Design FictionDaniel Oran | Student | Synthetic NeurobiologyDonald Derek | Student | Responsive EnvironmentsGuillermo Bernal | Student | Fluid InterfacesLaya Anasu | Student | City ScienceMiguel Perez | Student | Playful Systems",2017-09-01,[],https://dam-prod2.media.mit.edu/x/2017/10/09/OPTION1s.jpg,183920,,2023-04-29T19:48:43.289Z,"['jifei@media.mit.edu', 'jieqi@media.mit.edu', 'artemd@media.mit.edu', 'amosg@media.mit.edu', 'wonder@media.mit.edu', 'ddh@media.mit.edu', 'gbernal@media.mit.edu', 'layanasu@media.mit.edu', 'danoran@media.mit.edu', 'mperez4@media.mit.edu']",True,hacking-manufacturing-2017,2017-06-01,[],Hacking Manufacturing 2017,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2017-08-10T20:43:39.926Z,"EEEeb Spring 2019:  Urban OceansMarch 24, April 7 and 21, May 19, June 2 To register, please visit this link. Sponsored and run by members of the MIT Media Lab and the Empowered Brain Institute, Ecology, Evolution, and Engineering for Empowered Brains is an eight-week, sensory-friendly series of related educational workshops for neurodiverse individuals (ages 8 - 14) which aims to hone skills in understanding, interpreting, and protecting the natural environment. Through creative, hands-on teaching exercises and field visits, participants become comfortable with basic ecological principles, as well as emerging technologies used to sculpt ecological and evolutionary processes. We discuss contemporary issues related to conservation and highlight engineering strategies with which to address these obstacles. Through project-based learning, students will have the opportunity to develop understanding by experimentation—or play—and workshops will emphasize immersion, rather than memorization.  Wholly, we seek to foster a safe and creative learning space in which students are able to develop the necessary technical literacy to become future leaders in the myriad realms of environmental science. For questions, please contact Avery Normandin (ave@media.mit.edu).",,['sculpting-evolution'],https://dam-prod2.media.mit.edu/x/2017/09/06/Screen Shot 2017-09-06 at 4.11.17 PM.png,186300,,2019-01-10T16:02:03.396Z,"['ave@media.mit.edu', 'devora@media.mit.edu', 'jvanzak@media.mit.edu', 'pcuellar@media.mit.edu']",True,empowered-brains,2019-01-10,"['ecology', 'environment', 'kids', 'learning-teaching', 'storytelling', 'urban-planning', 'engineering', 'climate-change']","Ecology, Evolution, and Engineering for Empowered Brains",PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-08-15T16:36:23.172Z,"BubbleSynth translates motion tracking and blob detection data collected from floating soap bubbles into sound, by using the size, position, age, and velocity of the bubbles to adjust the parameters of a harmonic sine wave via Open Sound Control (OSC). Audio samples, which are provided by SuperCollider, may be triggered in real time either by rule sets that link bubble tracking data to specific sounds, or by physical interaction with the bubbles. The system has been designed to provide a framework that allows any motion tracking data to be ported in, while the SuperCollider backend can be changed merely by adding new SynthDefs.",2022-08-01,[],https://dam-prod2.media.mit.edu/x/2017/08/15/bubbleSynth2.jpg,187881,,2022-08-09T00:56:43.626Z,['novysan@media.mit.edu'],True,bubblesynth-soap-to-sound,2012-09-01,"['art', 'music', 'creativity', 'technology', 'alumni']",BubbleSynth: Soap to Sound,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-08-15T19:09:03.868Z,"""I solemnly swear that I am up to no good.""The Marauder's Map from the Harry Potter universe uses UltraWide Band transmitters and receiver base stations to triangulate position and tag identity through time of arrival and angle of arrival. The application is written in C++ in openFrameWorks, taking advantage of the ofxTCPClient class to connect to the server and retrieve the TCP/IP stream of tag positions and speed. The TCP/IP stream operates at 10Hz, while the UWB tags operate at up to 200Hz.",2020-03-19,[],https://dam-prod2.media.mit.edu/x/2017/08/15/maraudersMap01.jpg,187892,,2020-03-19T18:49:02.527Z,['novysan@media.mit.edu'],True,marauders-map,,"['entertainment', 'gaming', 'human-machine-interaction', 'interfaces']",Marauder's Map,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-08-15T19:25:11.906Z,"“I feel nearer, more a part of the painting, since this way I can walk round it, work from the four sides and literally be in the painting” — Jackson PollockPollock Patterns is a fusion of exploratory data visualizations based on realtime location-aware computing and the aesthetics of Abstract Expressionism. The application was developed in C++ using openFrameworks, OpenCV, and liberal use of OpenGL’s Framebuffer Object system. The program reads data directly from a TCP stream and converts it into X,Y coordinates for UWB tags. Each located instance becomes a paint emitter source as it traverses both the physical space and the simultaneously indexed world of the virtual canvas. A physics-based fluid dynamics system determines the rate of flow, viscosity, and surface tension of each paint emitter, resulting in thin, stringy lines as a brush moves quickly, or pools and spatters as a brush slows or changes direction suddenly. Filter operations on the framebuffer, such as blurs and other convolutions, add to the verisimilitude of liquid paint being drizzled across the canvas. Pixel operations are calculated in the GPU, allowing large numbers of tags and effects to be rendered at a reasonable frame rate.",2014-01-13,['object-based-media'],https://dam-prod2.media.mit.edu/x/2017/08/15/pollock01_h44HI7K.jpg,187900,,2020-01-13T20:01:06.233Z,['novysan@media.mit.edu'],True,pollock-patterns,,"['entertainment', 'gaming', 'human-machine-interaction']",Pollock Patterns,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-08-15T19:41:56.789Z,"SYNTHetic Biology — A Tribute to Greg Bear’s Blood MusicA musical ecosystem  driven by and reacting to the movement of living biological organisms. The bio-SYNTH can take input from a realtime USB microscope or utilize captured video (as in this case).Bacteria are tracked and elements such as size, age, velocity, and positions relative to the screen and each other are fed as seed values into a granular synthesizer. Waves are altered and samples are triggered according to an editable rule set.Samples from the Mercury and Apollo space program, Sputnik, and science fiction films were inspired by Greg Bear’s 1985 Nebula and Hugo award winning novel Blood Music, and suggest the movements of the quickly evolving ""noocytes"" as they begin to explore their god-like host being Vergil Ulam.The installation premiered at the Peabody Essex Museum in Salem, MA on September 18, 2014 and part of the After Hours PEM PM Series.",2020-03-19,[],https://dam-prod2.media.mit.edu/x/2017/08/15/bacteriaSynth.jpg,187908,,2022-03-27T16:26:43.135Z,['novysan@media.mit.edu'],True,synthbacteria,2014-08-15,[],SYNTHBacteria,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2017-08-20T00:26:48.016Z,"Cloud-Based Urban Data PlatformcityIO (input/output) is a cloud- and database-driven platform which allows remote participation, database augmentation, and high-end complex visualization. cityIO operates anywhere, on multiple platforms and devices, using client-side apps or web-based interfaces. The cityIO platform is built for scale and to serve large volumes of end-users in real time, in order to augment multi-participant discussions and decision-making processes. Utilizing the mass adaptation of mobile and hand-held devices, cityIO promotes an equal and decentralized discussion for multiparty stakeholders. cityIO offers a suite of augmented reality data-visualization tools that utilize server-side data and analysis. cityIO allows client-side interactions in multiple forms:AUGMENTED REALITY URBAN SIMULATIONcityIO is intended to reduce complexity in design and planning tools and to support  data-driven environments for planners, designers, and decision makers. cityIO uses modern simulation tools and employs cutting-edge AR applications in order to offer an immersive user experience for both planning professionals and the general public. These simulations can augment indoor and outdoor environments, physical models, and technical drawings. REMOTE AND DECENTRALIZED PUBLIC PARTICIPATIONUsing self-explanatory web and mobile apps with high-end visualization and user interfaces, cityIO offers cities, municipalities, and planning authorities the ability to better communicate complex planning processes and to aggregate the public's opinion in real time. cityIO's scalable server side allows multiple users to collaborate, participate, and voice their opinions on design and planning initiatives.  CITYI/O HAMBURGcityIO Hamburg augments cityMatrix table. This deployment allows design in the urban context of the Rothenburgsort neighborhood.",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/08/19/Augmented reality of Gold BRT_SM.jpg,189909,,2018-10-20T17:04:48.161Z,['noyman@media.mit.edu'],True,cityio,2016-05-01,"['data', 'urban-planning']",cityIO,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-08-21T00:18:35.911Z,"Aalto University, Finland, and the MIT Media Lab’s City Science group are co-developing a version of the MIT CityScope platform for urban analysis, efficient resource utilization, and spatial programming for campus development, using Otaniemi as a testbed. Aalto joins a network of City Science collaborators which includes Tongji University (Shanghai), Taipei Tech (Taiwan), HafenCity University (Hamburg), and ActuaTech (Andorra).",2021-01-01,['city-science'],https://dam-prod2.media.mit.edu/uuid/401105a8-2485-4000-b750-f7e9ece2ec16,190303,,2023-01-10T20:52:56.323Z,"['kll@media.mit.edu', 'agrignar@media.mit.edu', 'alonsolp@media.mit.edu', 'noyman@media.mit.edu']",True,city-science-lab-aalto,2017-05-01,"['architecture', 'data', 'transportation', 'urban-planning', 'mapping']",City Science Lab @ Aalto,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2017-08-28T01:52:09.883Z,"Affinity is a high-level machine learning API (Application Programming Interface)  dedicated exclusively to molecular geometry. Affinity is written in TensorFlow, some small proportion of high-performance code is  in low-level C++.  Depending on the application it can be configured as multi-CPU, multi-CPU single GPU, or multi-GPU system. Affinity has  it's own web page at affinity.mit.edu",,['molecular-machines'],,193373,,2017-08-28T16:12:07.806Z,"['mkkr@media.mit.edu', 'jacobson@media.mit.edu', 'kfirs@media.mit.edu']",True,affinity,,['machine-learning'],Affinity: Deep learning API for molecular geometry,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-08-28T16:08:28.890Z,"Affinity is a high-level machine learning API (Application Programming Interface) dedicated exclusively to molecular geometry. Affinity is written in TensorFlow; a small proportion of high-performance code is in low-level C++.  Depending on the application it can be configured as multi-CPU, multi-CPU single GPU, or multi-GPU system. Affinity has  its own web page at affinity.mit.edu ",,['molecular-machines'],https://dam-prod2.media.mit.edu/x/2017/08/28/affinity_logo_3.png,193399,,2019-04-17T19:38:38.863Z,"['mkkr@media.mit.edu', 'kfirs@media.mit.edu', 'jacobson@media.mit.edu']",True,affinity-tensorflow,2017-08-01,[],Affinity: Deep Learning API for Molecular Geometry,PUBLIC,https://affinity.mit.edu,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-08-28T17:08:48.369Z,"Protein-protein interactions (PPIs) are an essential part of many biological pathways in living organisms. With use cases such as regulation of gene expression, enzymatic catalyzation, and muscle contraction, understanding PPIs is a critical step toward a better understanding of life itself. Moreover, aberrant human PPIs may lead to multiple diseases, such as Alzheimer's, Creutzfeldt–Jakob, and cancer. Despite the undisputed importance of PPIs, only a small portion of the human interactome is known. The PPI mapping problem is composed of two subproblems: the Interaction Problem—identifying the two or more proteins involved in a particular interaction; and the Position Problem—recognizing the residues within the interacting proteins that are crucial for the interaction (also known as hot spots or interacting residues). Current experimental techniques for PPI mapping, like Yeast 2 Hybrid or Alanine scans, are limited in scale, tedious, and expensive, therefore establishing the need for a fast, efficient, and accurate computational system.DeepPPI is a Deep Learning algorithm that uses known PPIs to identify reoccurring patterns in the human interactome. These underlying patterns can be used, in turn, to predict both the existence of a new interaction and the interacting residues within the relevant proteins. Through this project, we hope to answer a fundamental biological question: How does nature, via evolution, create new protein-protein interactions? Additionally, we believe that DeepPPI will serve as a large-scale computational alternative to Alanine Scans and other experimental methods, contributing to the study of diseases and development of new therapeutics.",,['molecular-machines'],https://dam-prod2.media.mit.edu/x/2017/08/28/Screen Shot 2017-08-28 at 6.01.56 PM_Af0ylt9.png,193415,,2017-09-05T00:43:44.403Z,['kfirs@media.mit.edu'],True,deepppi,2017-08-01,[],DeepPPI,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-09-07T14:56:22.646Z,"Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.Learn more below about City Science Andorra research below.",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/09/09/Andorra.jpg,197374,,2023-09-07T13:51:22.004Z,"['alonsolp@media.mit.edu', 'kll@media.mit.edu', 'mcllin@media.mit.edu', 'ptinn@media.mit.edu', 'agrignar@media.mit.edu', 'csmuts@media.mit.edu', 'devisj@media.mit.edu', 'noyman@media.mit.edu', 'doorleyr@media.mit.edu', 'ryanz@media.mit.edu', 'maitanei@media.mit.edu']",True,city-science-andorra,2014-05-01,"['design', 'architecture', 'data', 'developing-countries', 'energy', 'environment', 'government', 'kids', 'learning-teaching', 'open-source', 'transportation', 'urban-planning', 'sensors', 'machine-learning', 'behavioral-science', 'data-visualization', 'public-health', 'data-science']",City Science Lab @ Andorra,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-09-07T18:39:13.312Z,"View the main City Science Andorra project profile.With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending. Current ProjectsEvent AnalysisSocial NetworkLocation Recommendation system EVENT ANALYSISBased on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra. After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/02/24/Screen Shot 2019-02-24 at 18.32.00.png,197403,,2019-02-25T15:33:28.936Z,"['agrignar@media.mit.edu', 'noyman@media.mit.edu', 'alonsolp@media.mit.edu', 'kll@media.mit.edu', 'jiw@media.mit.edu', 'devisj@media.mit.edu', 'doorleyr@media.mit.edu', 'nlutz@media.mit.edu']",True,andorra-tourism,2015-08-01,[],Andorra | Tourism,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-09-08T17:32:22.846Z,"Most countries are projected to see the number of people ages 65 and older surpass the population under the age of 15 by 2050. The limitations of current solutions to assisting older adults, the increased social and emotional toll on caregivers, and the inability of institutions to create structural solutions in a timely manner calls for a paradigm shift in the way we approach aging.As these new meanings of age, aged, and aging are re-negotiated at a personal and collective level, the main goal of this research initiative is to study aging adults’ daily living assistance, social and emotional needs, and intergenerational connection while exploring the optimized modalities for embodied agents to successfully deliver these interactions. We see embodied agents as a method to enable older adults to age-in-place, supporting them in ways such as promoting social connectedness, tracking vitals, coaching in emotional wellness, and assisting with medical adherence.Our work is rooted in partnering with the community through co-design and participatory design methods to inform robot design by empowering older adults to engage in our research. We prioritize developing robot interactions that can be tested long-term in older adults’ homes to better inform how social robots can shape aging-in-place.Currently, we are running a long-term codesign study with older adults. Over the course of the year, older adults will engage in interviews, interactive artwork, living with a robot, prototyping on a robot, and design guideline generation. If you are 70 years of age or older and interested in participating in future study opportunities, please contact Anastasia Ostrowski (akostrow@media.mit.edu).",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/05/02/Screen Shot 2018-05-02 at 11.23.34 AM.png,197826,,2019-05-10T19:51:29.310Z,"['cynthiab@media.mit.edu', 'haewon@media.mit.edu', 'nikhita@media.mit.edu', 'akostrow@media.mit.edu']",True,designing-social-robots-for-older-adults,2017-06-01,"['robotics', 'design', 'human-machine-interaction', 'social-science', 'social-robotics', 'long-term-interaction', 'technology', 'social-change']",Designing social robots for older adults,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-09-11T17:45:38.037Z,"View the main City Science Andorra project profile.The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&D from the companies in order to build together an Andorran living lab, an ""innovation district"" where national and international companies can test and deploy their products and ideas and cultivate human capital.Current ProjectsAndorra Innovation SpaceAndorra Cultural HeritageDrones patterns and flows, collaboration living labYoung Future",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/09/11/AndorraInnovationspacetest_1.gif,199014,,2018-07-09T18:49:41.844Z,"['kll@media.mit.edu', 'alonsolp@media.mit.edu', 'devisj@media.mit.edu', 'agrignar@media.mit.edu', 'noyman@media.mit.edu', 'csmuts@media.mit.edu', 'doorleyr@media.mit.edu']",True,andorra-innovation,2016-09-01,"['data', 'kids', 'learning-teaching', 'perception', 'urban-planning', 'data-visualization']",Andorra | Innovation,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-09-11T19:15:01.693Z,"View the main City Science Andorra project profileWith no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.",,[],https://dam-prod2.media.mit.edu/x/2017/09/12/Bannermobility_3.gif,199035,,2017-10-25T05:56:26.309Z,"['kll@media.mit.edu', 'mcllin@media.mit.edu', 'ptinn@media.mit.edu', 'devisj@media.mit.edu', 'alonsolp@media.mit.edu', 'agrignar@media.mit.edu', 'doorleyr@media.mit.edu', 'ryanz@media.mit.edu']",True,andorra-mobility,2016-09-01,[],Andorra | Mobility,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-09-11T19:19:52.564Z,"View the main City Science Andorra project profile.Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/12/pasted-image-35412.png,199043,,2021-03-01T19:32:53.898Z,"['kll@media.mit.edu', 'alonsolp@media.mit.edu', 'devisj@media.mit.edu', 'agrignar@media.mit.edu', 'csmuts@media.mit.edu', 'jiw@media.mit.edu', 'yleng@media.mit.edu', 'doorleyr@media.mit.edu', 'ryanz@media.mit.edu', 'noyman@media.mit.edu']",True,andorra-dynamic-urban-planning,2016-09-01,"['computer-vision', 'augmented-reality', 'data', 'open-source', 'urban-planning', 'mapping', 'data-visualization']",Andorra | Dynamic Urban Planning,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2017-09-20T19:38:31.220Z,"Storytelling is a fundamental way in which human beings understand the world. Imagine watching a movie telling the story of your life, how would you respond to it and how would it change your perception of your own memories? Personalized animated movies are generated from Unity, customized to each user's mood and behavior date collected through self-reports. Our study shows that personalized animations can elicit strong emotional responses from participants and lengthier writing of self-reflection compared to a non-personalized control. Moving forward, we're looking at using personalized animation to encourage cognitive reappraisal and positive thinking.",2018-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/2017/09/22/system_overview_small-01 copy.png,202766,,2022-05-12T17:34:03.628Z,"['fpeng@media.mit.edu', 'picard@media.mit.edu']",True,personlized-animation,2016-10-01,"['art', 'human-machine-interaction', 'behavioral-science', 'affective-computing']",Personalized Animated Movies,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-09-21T22:24:12.801Z,"With families living further apart, it has become increasingly difficult for people to stay connected—particularly in the case of grandparents and children. The challenge lies in deciding when, how and what to engage on. Can technology act as a proactive facilitator of human-human connection?Social robots are uniquely positioned to act as active facilitators of human-human connection. However, in order to do so, they require the ability to be proactive. Proactivity demands that an agent not only respond to it's environment, but also exhibit goal-directed behavior by taking the initiative. In this work, an ecosystem for connected social robots to utilize the surfaces of the home as a canvas for expression in order to engage grandparents and grandkids in human-human interaction is proposed. This work models proactivity in an agent as a function of understanding the context, proposing a goal, and taking initiative through an interaction. Human studies will be conducted in order to understand and draw inspiration from human behavior to drive how a robot gets an individual's attention. Further interaction studies will serve to design and evaluate the form of expression (robot, environment, or both) most relevant for given contexts.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2017/09/26/jibo-cup-empty-looking-e1421803351123-1940x1090.jpg,203220,,2018-04-25T03:28:31.115Z,['nikhita@media.mit.edu'],True,proactive-connected-spaces,2017-08-01,[],"Proactive, Connected Spaces",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-09-25T20:56:04.527Z,"The Tree Computer is a robotic greenhouse built to explore plant stress, health, longevity, and productivity across a wide range of crops. ",2020-04-30,['open-agriculture-openag'],https://dam-prod2.media.mit.edu/x/2019/04/12/20180914-tfoe-mit-food-computer-1381 (1).jpg,204423,,2020-05-06T18:58:55.664Z,"['zandera@media.mit.edu', 'calebh@media.mit.edu', 'poitrast@media.mit.edu', 'tsavas@media.mit.edu', 'hildreth@media.mit.edu', 'rebekahj@media.mit.edu', 'snowak@media.mit.edu']",True,tree-computer,2017-10-10,['agriculture'],Tree Computer,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2017-09-26T14:42:53.136Z,"We present VisualSoundtrack, a system designed  as a tool for soundtrack composers to experiment with original musical  content in differing musical “styles."" The system allows a user to  rapidly prototype musical ideas with respect to the target media (such  as a film or podcast) by having him/ her input original musical motifs,  capitalizing on a corpus of existing soundtrack samples to source  various styles, and allowing the user to identify the most appropriate  style sources for the target media by visually architecting a path  through a highly abstracted feature space.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2017/09/26/VS_interface_Oi6yssb.png,204431,,2018-10-19T20:45:43.275Z,"['ishwarya@media.mit.edu', 'joep@media.mit.edu']",True,visualsoundtrack,2016-12-01,"['human-machine-interaction', 'music']",VisualSoundtrack,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-09-27T17:50:17.894Z,"In ""We saw a game!"" we expose the different perspectives in TV sports and news in order to build broadcasting systems that unify rather than divide. We use the galvanizing impact of sports and live events as a forum, and then we add production and viewing opportunities to distinguish fact from opinion and to challenge the basis of those opinions.In 1951, when the Dartmouth football team played against Princeton, there was deep disagreement between the two schools as to what had happened during the game. In ""They Saw a Game: A Case Study,"" the psychologists Albert Hastorf and Hadley Cantril found that when the same motion picture of the game was shown to a sample of undergraduates at each school,  each individual perceived a different game, and their versions of the game was just as ""real"" as other versions were to other people. However,  little is known about whether and how broadcasting media are adding fuel to the fire. In order to study the relationship between storytelling/perspectives and opinion formation, we built the following two applications: ""Let's see a game!"" and ""Let's watch news!""In the first step, we built an interactive application that exposes different perspectives in sports broadcasting. The application plays two broadcasts of the same game, created for each team's home audience. The user can tune into an audio channel by moving the slider. Additional buttons allow the user to take other actions.",2017-12-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/17/Videocomparison2019.png,204866,,2021-02-10T13:00:56.528Z,"['mhjiang@media.mit.edu', 'hbedri@media.mit.edu', 'lip@media.mit.edu']",True,let-s-see-a-game,2017-09-11,[],We saw a game!,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-09-28T05:46:43.576Z," News reporting today suffers from sensationalism. News agencies are constantly fighting for attention and clicks, leading to headlines and photos that exaggerate a single perspective. What if you could get a full perspective on certain news topics by exploring the news in AR? The spatial nature of AR allows a user to gain a more complete perspective on a story.  Having a constant holographic widget on your desk also allows you to follow developing stories, such as a foreign conflict. In addition, the interactive nature of AR means that users can explore the news in a delightful way.",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2017/09/28/giphy (4).gif,204952,,2018-10-22T16:52:05.905Z,"['hbedri@media.mit.edu', 'lip@media.mit.edu']",True,holonews,2017-09-28,[],Holonews,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-09-28T15:18:36.652Z,"Increasingly in the US, people have to take responsibility for their health information.  Simultaneously, medical providers must make patient data available. MedRec fully decentralizes access rights via an Ethereum blockchain, thereby giving patients control over record distribution. Our model is the World Wide Web:  MedRec is a network. Patients and providers operate nodes that authorize others to retrieve data. It is a basis for a generally useful permissioning system.There is no website or central repository of permissions. Instead, patients and medical records originators establish a relationship and based on that, the patient creates smart contracts that other members of the network can use to authorize access to a record database. The parameters of contracts are kept in a blockchain that is maintained by all member providers/originators who at the same time use those contracts to provide access to their database. The patient/user contracts themselves are held by the patients in a wallet that resides on their device[s] as an app. This app is secure and recoverable in case the physical device is lost or damaged.For a full overview, technical documentation, and updates, visit the project's website.",2019-09-15,['viral-communications'],https://dam-prod2.media.mit.edu/x/2017/11/22/medrec-system-concept.png,204962,,2020-10-23T17:39:35.777Z,"['kalli@media.mit.edu', 'agnescam@media.mit.edu', 'nchinda2@media.mit.edu']",True,medrec,2018-01-01,[],MedRec,PUBLIC,https://medrec.media.mit.edu,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-09-29T14:36:56.794Z,"The onset of Mixed Reality as a platform offers the opportunity to create new, playful paradigms for building and fostering creativity. The Holobits application leverages the tried and tested features of physical block building platforms like LEGO and introduces the benefits of building in mixed environments to support making and storytelling. The proposed system combines the hand tracking capabilities of the Leap Motion with the spatial mapping of Hololens to enable hands-on building experiences with virtual blocks, denoted as “bits.” These blocks have different attributes and characteristics that determine how they look and behave within the mixed reality building space. The platform also allows users to share their creative building process in a frame-by-frame fashion that enables remixing and reflection on every play session. Holobits allows users to record their interactions with their creations to make animated environments with ease and support storytelling. Another way to enable collaboration is to let kids share models (or download someone else's creation in your space), allowing multiple users to build in a shared physical space or over distances, where a player can “hop” into the physical space of the Hololens user using virtual reality. Last but not least, we intend to integrate the Scratch visual programming toolkit into the Holobits platform to allow users to orchestrate their virtual creations and create the ultimate interactive stories.",2018-05-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/09/29/Movies & TV 9_29_2017 11_01_55 AM (2).png,205398,,2018-11-30T17:21:40.954Z,"['cdvm@media.mit.edu', 'pattie@media.mit.edu', 'afuste@media.mit.edu', 'minakhan@media.mit.edu', 'pe25171@media.mit.edu']",True,holobits,2017-02-13,[],Holobits: Creativity and fun in Mixed Reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-09-29T15:28:40.403Z,"Embodied theories of language propose that the way we communicate verbally is grounded in our bodies. Nevertheless, the way a second language is conventionally taught does not capitalize on kinesthetic modalities. The tracking capabilities of room-scale virtual reality systems afford a way to incorporate kinesthetic learning in language education.  Words in Motion is a virtual reality language learning system that reinforces associations between word-action pairs by recognizing a student’s movements and presenting the corresponding name of the performed action in the target language. Experiments with Words in Motion suggest that the kinesthetic approach in virtual reality has less immediate learning gain in comparison to a text-only condition. However, virtual kinesthetic learners showed significantly higher retention rates after a week of exposure. Positive correlation between the times a word-action pair was executed and the times a word was remembered by the subjects, supports the premise that virtual reality can impact language learning by leveraging kinesthetic elements.",2018-05-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/07/WhatsApp Image 2018-05-07 at 11.34.02 AM.jpeg,205408,,2018-08-20T16:26:03.600Z,"['pattie@media.mit.edu', 'cdvm@media.mit.edu']",True,words-in-motion,2017-02-13,"['virtual-reality', 'language-learning']",Words in Motion,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-10-02T02:51:03.152Z,"Scalable Urban Infrastructure for Human-Machine CohabitationNew infrastructure to help sustain public-sector participation and operation, and maximize public interest and safety.Advancements in autonomous technology have led automobile makers and tech companies to focus on reinventing the automobile—increasing computational capability and enhancing sensor systems. But due to strict road-safety regulations, this vehicle-centric, inside-out approach may take years to materialize, and when it does, restricting “autonomy” to selected vehicles will limit autonomy’s impact on street safety and accessibility.To address current issues, The City Science group focuses on ways to offload often-heavy computational requirements from the vehicle through affordable interventions to street infrastructure by creating human-machine readable traffic signs and urban markers.With the support of a new genre of smart urban infrastructure, we believe this “autonomy-lite” approach will soon allow lightweight autonomous vehicles to be widely deployed and navigate smoothly in most urban environments. ",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/02/IMG_20170826_070741.jpg,206187,,2018-03-28T22:18:15.132Z,"['mcllin@media.mit.edu', 'ptinn@media.mit.edu', 'taiyu@media.mit.edu']",True,smart-infra,2017-05-01,[],Urban Tattoo,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2017-10-02T20:19:05.093Z,"Facilitating coexistence, trust-building, and collaboration among people and machines.New modes of 21st century urban transportation are becoming more lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.  We strive to enable new forms of human-machine co-existence, trust, and collaboration.This work focuses on enabling:Intuitive and effective two-way communication between vehicles and pedestrians;Street safety and traffic-yielding mechanisms; andBehavior change related to the adoption of active mobility mode, or electric assist.",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/02/Narindra_Nick_HMI_ZfoLt7m.jpg,206583,,2020-03-16T03:45:48.542Z,"['mcllin@media.mit.edu', 'ptinn@media.mit.edu', 'lukeji@media.mit.edu', 'cq_zhang@media.mit.edu', 'jerryao@media.mit.edu', 'yagol@media.mit.edu', 'cassiano@media.mit.edu']",True,hmi,2016-11-01,[],Human-Machine Interaction (HMI) in Social & Public Environments,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-10-03T18:43:52.546Z,"The goal of this project is providing machines with the ability of understanding what a person is experiencing from her frame of reference, taking into account the scene context: where is this person, what is this person doing, how does this person look, etc. You can find more information about this project on this website.",,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/10/18/context_sea2.png,206612,,2018-10-20T16:59:10.142Z,"['agata@media.mit.edu', 'picard@media.mit.edu']",True,emotion-recognition-in-context,2017-01-02,[],Emotion Recognition in Scene Context,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-10-03T18:48:21.444Z,Emotionally Intelligent Music Playback opens possibilities to various emotional trajectories through a piece of music. The listener can navigate through emotional territories via a touchscreen interface. The system transitions seamlessly to corresponding emotional interpretations extracted from various existing renditions of the same composition.,2020-07-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/10/13/emotionally_intelligent_cover.png,206620,,2022-09-14T20:13:31.487Z,"['hanelee@media.mit.edu', 'davidsu@media.mit.edu']",True,emotionally-intelligent-playback,2017-09-01,[],Emotionally Intelligent Playback,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-03T20:22:23.760Z,"Exploring contextual multimodal cues as memory aids Objective We are exploring the potential of proximity-triggered contextual audio and visual cues to help early-stage Alzheimer’s patients recall familiar people and places. In particular, we are using proximity beacons to determine when the user is physically close to another person, such as a loved one. The beacons will then trigger cues in the form of:audio conveying contextual information such as name, relationship, time/place/details of last interaction;images and video (using AR) showing previous interactions along with text displaying contextual information; andmusic in the form of specific songs associated with specific individuals.Research QuestionsWe’re interested in tackling the following questions:Which cue modalities are the most effective in improving recognition in early-stage Alzheimer’s patients?What advantages and challenges are afforded by each of the different modalities?",2019-07-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/10/17/Cue.jpeg,207029,,2022-09-14T20:17:16.686Z,"['vparth@media.mit.edu', 'davidsu@media.mit.edu', 'tomasero@media.mit.edu', 'pattie@media.mit.edu']",True,cue,2017-09-22,[],Cue,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-10-03T21:00:34.193Z,"Discreet Teeth Gestures for Mobile Device InteractionByte.it is a miniaturized, discreet interface that uses teeth gestures for hands-free input for wearable computing. As humans, we are constantly seeking to communicate and consume information, and mobile devices give us  access the world wide web, our digital selves, and all our digital assets with the touch of our fingers. Context can temporarily reduce our abilities to perform certain activities, preventing us from having a fluid interaction with mobile computing. Hands are not always available, and sustained visual attention is often required for successful task performance and social norms. Current screen-based interfaces are not designed to be used by a person engaged in another attention demanding activity such as walking , talking, or driving, leading to ineffective interactions and even dangerous situations.Audio interfaces are a potential solution as they can provide a high-bandwidth communication channel without requiring visual attention. Speech has been the predominant interaction modality for audio interfaces, but it can be ineffective in situations with loud environmental noise, or inappropriate in certain social or dynamic on-the-go contexts. Recent work has explored teeth gestures as a solution for interaction in these contexts, but these attempts are limited by the number of gestural primitives recognized (bandwidth) and the discreteness of the interfaces used to detect these gestures.Byte.it expands on this work by exploring the use of a smaller and more unobtrusively positioned sensor (accelerometer and gyroscope) for detecting tooth clicks of different groups of teeth and bite slides for everyday human-computer interaction. Initial results show that an unobtrusive position on the lower mastoid close to the mandibular condyle can be used to classify teeth tapping of four different teeth groups (front, back, left, and right teeth click) with an accuracy of 88 percent, or an accuracy of 84 percent for seven different teeth clicking and bite sliding gestures (front, left, and right click, and front, back, left, and right slide).The applications currently being explored are centered around dynamic, on-the-go, hands-and-eyes-free contexts. For example, (1) controlling the different commands of a media player, such as play/pause, volume, and current time of a song, podcast, or audiobook. Productivity-wise, being able to subtly (2) start and stop audio recordings of conversations or meetings, and tag relevant events that might be worth reviewing later. Teeth gestures could also allow for a discrete and rapid way to (3) accept or reject incoming  alerts, notifications, and reminders, while minimizing task-switch time. A minimal set of teeth gestures could also enable the seamless (4) access of information streams such as  messages, emails, news, or relevant notes about the person, place, and/or time of interest that could enhance the current interaction.This research aims to investigate the following:1) Understand how people could use teeth gestures to perform specific interaction commands in order to establish a standardized teeth gesture language.2) Identify the optimal position of the sensor to achieve the highest gesture classification accuracy possible while ensuring a discreet form factor.3) Measure the performance of our classification algorithm in the wild, while sitting, standing, walking, running, and cycling.4) Assess the usability of the interface + applications in the wild.Publication: Byte.it: Discreet Teeth Gestures for Mobile Device InteractionTomás Vega Gálvez, Shardul Sapkota, Alexandru Dancu, Pattie Maes, CHI Conference on Human Factors in Computing Systems Extended Abstracts (CHI’19 Extended Abstracts), May 4–9, 2019, Glasgow, Scotland UK, ACM, New York, NY, USA, 6 pages, https://doi.org/10.1145/3290607.3312925",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/09/04/cover_weLcp4l.JPG,207037,,2020-05-21T07:32:59.655Z,"['tomasero@media.mit.edu', 'pattie@media.mit.edu']",True,byte_it,2017-09-07,"['design', 'human-computer-interaction', 'consumer-electronics', 'interfaces', 'human-augmentation']",Byte.it,PUBLIC,http://www.tomasvega.com,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-10-04T12:27:04.033Z,"Local changes in the volume, shape, and mechanical properties of the residual limb can be caused by adjacent joint motion, muscle activation, hydration, atrophy, and more. These changes affect socket fit quality and might cause inefficient load distribution, discomfort, and dermatological problems. Analyzing these effects is an important step in considering their influence on socket fit, and in accounting for their contribution within the socket design process. In this study, a 360° 3D digital image correlation (3D-DIC) system was developed for the full-field deformation measurements of the residuum. A multi-camera rig was designed for capturing synchronized image sets as well as force measurements from a hand-held indenter. Custom camera calibration and data-processing procedures were specifically designed to transform image data into 3D point clouds, and automatically merge data obtained from multiple views into continuous surfaces. Moreover, a specially developed data-analysis procedure was applied for correlating pairs of largely deformed images of speckled surfaces, from which displacements, deformation gradients, and strains were calculated. Characterization of the full-field deformations using 3D-DIC provides insight into the patterns and sources of the phenomena. In addition, local and subject-specific soft tissue mechanical properties were obtained by analyzing surface deformation and force measurement during indentation using inverse FE analysis. These data can be used to accurately describe the residuum’s biomechanical behavior. Consequently, prosthetic socket designs that take into account these effects can be considered.",,['biomechatronics'],https://dam-prod2.media.mit.edu/uuid/9267ef25-76bd-4586-b627-22db13eb0b35,207055,,2022-04-07T22:07:03.182Z,"['danask@media.mit.edu', 'kmoerman@media.mit.edu', 'hherr@media.mit.edu', 'xingbang@media.mit.edu', 'drclee@media.mit.edu', 'cimeyer@media.mit.edu', 'friccioa@media.mit.edu', 'amjaeger@media.mit.edu']",True,analysis-of-residual-limb-changes-using-digital-image-correlation-and-finite-element-modelling,2017-05-01,[],Analysis of residual limb changes using digital image correlation and finite element modeling,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-10-05T12:27:38.600Z,"Conversational agents and intelligent toys are present in children's homes and influence their development. The introduction of these devices in the home raises questions about how children's development and their interactions with their peers and family may be impacted. Most of the current intelligent agents (connected toys, conversational devices, robots, computers) can only be placed along an animate-inanimate continuum (Keil 1991; Van Duuren & Scaife, 1995). As these devices become more human-like in form or function, they are being attributed more social and moral characteristics (Kahn 2012). This raises the question of family engagement and interventions in children's interaction with connected toys and intelligent agents (Druga 2017, Mcreynolds 2017). In order to further explore this I am designing a platform where children could program and customize embodied intelligent agents together with their family, and especially with their grand-parents. The agents will play an active role in prompting and guiding the users through the learning activities. This novel paradigm of interaction aims to explore new potential learning pathways and outcomes from having a reflective conversation with an artifact in the making.",2018-08-31,['personal-robots'],,207462,,2018-10-24T01:05:35.065Z,[],True,cognimate,2017-08-01,[],Cognimates: collaborative creative learning with embodied intelligent agents,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-05T14:22:58.189Z,"Forecasting the supply of fleets to meet emerging travel demands and service needs in citiesThe availability of vehicles is a critical factor behind successful shared-use mobility services. Proper management of supply-demand dynamics is paramount for achieving viability in a new mobility service, as achieving scale often requires a large capital investment. Under-supplying the fleet would result in low service availability and user dissatisfaction; over-supplying results in inefficient use of resources. In addition, as a new shared mobility platform diversifies its service across both passenger and freight delivery, its required scale of operation and investment becomes more difficult to estimate.  In this service deployment simulation research, the City Science group aims to create an accessible simulation tool to enable cities to forecast the size of deployment of new shared mobility services using the Persuasive Electric Vehicle (PEV) delivering passengers and packages as an initial test case. The simulation tool also allows for testing various system design features, such as strategies for rebalancing fleets and for distributing charging stations.Web Demo",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/05/SimApp_Thumbnail.png,207470,,2019-12-23T15:36:41.826Z,"['ptinn@media.mit.edu', 'mcllin@media.mit.edu']",True,fleet-simulation,2016-04-01,[],"Service deployment simulation for light-weight, mixed-use autonomous fleets",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-10-05T14:34:24.540Z,"Open-Source Autonomous Platform for Educational and Service Design ApplicationsHow can new technologies respond to society’s diverse industrial, socio-economic, and educational needs?Despite AI and robotics being widely trumpeted as keys to the new Industrial Revolution, access to their development remains largely restricted to companies and institutions that are rich in capital and/or data, potentially further deepening the socio-economic disparity observed across continents. As a likely result, these new technologies generate limited positive externalities. For instance, are automobiles really the most critical area in need of self-driving technology? Where else might AI and robotics be applied to lead to increased urban livability, socioeconomic equity, and the vibrancy of local businesses?Building upon the architecture of MIT’s open-source race car platform, the City Science group introduces a new open-ended and heavy-duty self-driving platform.  Torque is intended to be used by educators and makers and is ideal for hackathons and classroom instruction. Torque will soon allow rapid prototyping of usage scenarios and services for various contexts and needs. ",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/05/Torque+RaceCar.png,207478,,2018-05-04T15:23:33.802Z,"['mcllin@media.mit.edu', 'ptinn@media.mit.edu', 'cq_zhang@media.mit.edu', 'lukeji@media.mit.edu']",True,torque,2017-07-01,[],Torque,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-10-05T20:42:42.950Z,"InspirationSleep is a forgotten country of the mind. A vast majority of our technologies are built for our waking state, even though a third of our lives are spent asleep. Current technological interfaces miss an opportunity to access the unique, imaginative, elastic cognition ongoing during dreams and semi-lucid states. In turn, each of us misses an opportunity to use interfaces to influence our own processes of memory consolidation, creative insight generation, gist extraction, and emotion regulation that are so deeply sleep-dependent. In this project, we explore ways to augment human creativity by extending, influencing, and capturing dreams in Stage 1 sleep. It is currently challenging to force ourselves to be creative because so much creative idea association occurs in the absence of executive control and directed attention. Sleep offers an opportunity for prompting creative thought in the absence of directed attention, especially if dreams can be guided.Scientific BackgroundDuring sleep onset, a window of opportunity arises in the form of hypnagogia, a semi-lucid sleep state where we all begin dreaming before we fall fully unconscious. Hypnagogia is characterized by phenomenological unpredictability, distorted perception of space and time, and spontaneous, fluid idea association. Edison, Tesla, Poe, and Dalí each accessed this state by napping with a steel ball in hand; when the ball dropped to the floor below just as they fell asleep, they awoke to capture the creative ideas generated in their hypnagogic dreams.Engineering & ExperimentationIn this project, we modernize the steel ball technique using a custom sleep onset tracker and auditory feedback which we together call Dormio. With the Dormio system, we are able to reliably influence hypnagogic dreams and collect dream content. We found that active use of the ""targeted  dream incubation"" protocol during hypnagogia can augment human creativity. This Dormio system enables future research into sleep, an underutilized and understudied state of mind vital for memory, learning, and creativity. Dormio has been published at CHI and in Consciousness and Cognition. It has been used for several studies, both at the MIT Media Lab and in independent labs outside of MIT. ContributorsThis work has been highly collaborative. The following contributors, in alphabetical order by first name, have all made it possible: Abhinandan Jain, Adam Haar Horowitz, Christina Chen, Eyal Perry, Ishaan Grover, Kathleen Esfahany, Matthew Ha, Oscar Rosello, Pattie Maes, Pedro Reynolds-Cuéllar, Robert Stickgold, and Tomás Vega. Learn MoreTo learn more, see the FAQ below and explore the publications related to this project. Scroll down to the bottom of the page to see more photos of the Dormio system.",,"['personal-robots', 'fluid-interfaces', 'civic-media']",https://dam-prod2.media.mit.edu/x/2018/04/17/MTT_N11JLLB.jpeg,207510,,2023-10-11T13:41:33.304Z,"['pcuellar@media.mit.edu', 'igrover@media.mit.edu', 'adamjhh@media.mit.edu', 'rosello@media.mit.edu', 'tomasero@media.mit.edu', 'abyjain@media.mit.edu', 'eyalp@media.mit.edu', 'kaes@media.mit.edu']",True,sleep-creativity,2017-09-01,"['human-computer-interaction', 'art', 'artificial-intelligence', 'augmented-reality', 'cognition', 'consumer-electronics', 'health', 'human-machine-interaction', 'learning-teaching', 'neurobiology', 'open-source', 'storytelling', 'wearable-computing', 'interfaces', '3d-printing', 'behavioral-science', 'ethics', 'creativity', 'technology', 'cognitive-science', 'industry', 'wellbeing']",Dormio: Interfacing with Dreams,PUBLIC,http://www.adamjhh.com/,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-05T22:00:50.298Z,"DropletIO proposes aqueous droplets as a programmable material for biology, art, and design. The DropletIO system can actuate and sense macro-scale droplets (nano-liter to micro-liter) on planar surfaces. The system can precisely move, merge, split, oscillate, and change the shape of droplets. We built custom printed circuit boards that integrate actuation and sensing, which act as building blocks for droplet control on devices of various form factors. We show how DropletIO boards can be integrated into a range of tools for biology, everyday objects as ubiquitous information displays and as an interaction medium for art and entertainment.  Droplets in Biology  Droplet-based microfluidics is extensively used in biology and chemistry. With DropletIO as the core technology, we are building a desktop machine to automate small volume liquid handling. The programmable system is capable of manipulating tiny droplets of biological samples/reagents with precise volume control. Our desktop machine will reduce lab equipment cost, eliminate human errors, and allow for the scaling of complex biological experiments from lab to production with ease. With the machine we want to bring down the cost of running assays from $10,000 to $10 to bring healthcare to billions of people. Our solution replaces current liquid handling built on leaky tubes and unreliable pumping mechanisms with a solid-state device. Our digital device is entirely electronic and compact, a system that inexpensively scales to address complex experiments with small volume liquids. Due to its digital nature, a biologist using our system could define biological protocols by programming, executing, and sharing them. Thus, operations would scale digitally from lab to production. The system furthers cost savings by producing significantly less disposable waste such as pipette tips.",2017-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2017/10/05/vlcsnap-2017-10-05-19h44m50s646 copy.png,207518,,2021-10-20T18:05:06.289Z,"['udayan@media.mit.edu', 'ishii@media.mit.edu']",True,droplets,2017-01-01,"['art', 'bioengineering', 'genetics', 'human-machine-interaction', 'manufacturing', 'pharmaceuticals', 'synthetic-biology', 'biotechnology']",DropletIO,PUBLIC,http://udayan-u.com,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2017-10-09T16:52:10.812Z,"The Blank Canvas directs immersion inwards using virtual reality, augmenting awareness of the microscopic worlds inside each of us and the science that is changing them today. It has been shown at Cannes Film Festival, Vision Summit, VR Sci Fest, and the World Economic Forum.This is the first episode of The Blank Canvas, a VR platform that showcases the future of science and scientific communication. So many of the brilliant contemporary innovations in science are lost to the general public because they happen at scales so small we can barely comprehend them. The Blank Canvas leverages the power of immersive technologies  to make these ideas come to life in macro planetary scale, explaining themes like DNA editing, hacked viruses, and CRISPR. We build collaborations between scientists and engineers for accurate, inspirational science storytelling that turns textbooks into experience. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/10/18/Blank Canvas Cover Shot.png,208712,,2019-10-03T19:01:31.524Z,['adamjhh@media.mit.edu'],True,the-blank-canvas,2017-01-01,['virtual-reality'],The Blank Canvas,PUBLIC,http://www.adamjhh.com/,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-10-10T00:06:53.143Z,"Light, color, texture, geometry and other architectural design elements have been shown to produce predictable and measurable effects on our minds, brains, and bodies. This suggests spaces that can mirror or transform feelings or serve specific purposes like improving learning or enhancing wellbeing can be designed. With Auris, we take a first step towards the design of such spaces in virtual reality by attempting to automatically generate affective virtual environments that can affect our emotions. The input to Auris is a song (audio and lyrics) and the output is a VR world that encapsulates the mood and content of the song.",2018-07-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/10/09/apple_XJteC0I.png,209107,,2018-08-20T16:22:53.032Z,"['sra@media.mit.edu', 'pralav@media.mit.edu', 'pattie@media.mit.edu']",True,auris-creating-affective-virtual-spaces-from-music,2017-02-01,[],Auris: Creating Affective Virtual Spaces from Music,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-10T00:14:03.825Z,"GVS or galvanic vestibular stimulation is a technology that directly affects a user's vestibular system by altering their sense of balance and direction. It works through electrical stimulation via electrodes placed on the mastoid bones behind each ear. In standing users, GVS evokes a prolonged ""galvanic body sway."" In walking users, it affects balance and causes users to stagger in the anodal direction. However, in walking users, with their head pitched forward, it causes them to turn smoothly from their planned trajectory in the anodal direction. Dark Room is a cooperative asymmetrical ""escape the room"" style game played by a VR and a PC user, inspired by the single-player mobile game Dark Echo. The PC user controls the walking direction of the VR user to guide them around virtual or physical obstacles. The VR player uses echolocation to detect obstacles. Video and paper available here: http://web.media.mit.edu/~sra/gvs.html",2018-07-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/10/09/virtual_obstacles.png,209115,,2019-04-19T17:08:16.864Z,"['sra@media.mit.edu', 'xuhaixu@media.mit.edu', 'pattie@media.mit.edu']",True,galvr-a-novel-collaboration-interface-using-gvs,2017-08-15,['technology'],A VR Collaboration Interface Using Galvanic Vestibular Stimulation,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-10T03:37:31.025Z,"The problem of ethical decision making presents  a grand challenge for modern AI research. Arguably the main obstacle to automating ethical decisions is the lack of a formal specification of ground-truth ethical principles, which have been the subject of debate for centuries among philosophers (e.g., trolley problem). We present an algorithm to automate ethical decisions; using machine learning and computational social choice (new theory of swap-dominance efficient voting rules), we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million voters through the Moral Machine website. Our proof of concept shows that the decision the system takes is likely to be the same as if we could go to each of the 1.3 million voters, ask for their opinions, and then aggregate their opinions into a choice that satisfies mathematical notions of social justice.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/10/12/Senado_Romano.jpg,209123,,2020-07-23T20:01:32.310Z,"['gaikwad@media.mit.edu', 'irahwan@media.mit.edu', 'awad@media.mit.edu', 'dsouza@media.mit.edu']",True,a-voting-based-system-for-ethical-decision-making,2017-01-20,"['artificial-intelligence', 'machine-learning', 'ethics']",A voting-based system for ethical decision making,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2017-10-10T15:54:58.343Z,"Developed by Ira Winder with the MIT Centre for Transportation and Logistics, the model seeks to use real population data and create a simulation to optimize delivery cost and coverage. This could be modified and applied to many disciplines, industries, and population types. The platform has the user place stores on a Tactile Matrix, a type of tangible interface, and displays the output of their potential delivery coverage and cost. This optimization game of sorts is a whole new approach to maximizing delivery potential. The interactive interface and layers of finely granulated and detailed data allow the user to make meaningful interventions and see the intertwining of many rich data sets. Photos by James Li. Video by Nina Lutz. ",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/10/lastmile2.jpg,209133,,2017-10-16T15:43:32.494Z,"['kll@media.mit.edu', 'jiw@media.mit.edu']",True,last-mile-logistics,2016-01-04,"['computer-vision', 'architecture', 'augmented-reality', 'data', 'gaming', 'human-machine-interaction', 'open-source', 'systems', 'interfaces', 'mapping']",Last Mile Logistics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2017-10-10T16:28:28.700Z,"This project is the first of two projects in collaboration with GSK. We are developing a computational simulation that allows a human user (or AI) to test drug manufacturing investment scenarios for an entire portfolio over multiple years. We aspire to help decision-makers understand the possible impact of new techniques such as CBM on selected key performance metrics. This game like simulation allows various stakeholders to come together and make collaborative decisions regarding the entire supply chain. The software works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in an approachable, tangible way. Screenshots courtesy of Ira Winder. Photos by Nina Lutz.",2018-06-01,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/10/IMG_7178.jpg,209144,,2018-08-20T20:42:11.340Z,"['kll@media.mit.edu', 'jiw@media.mit.edu', 'nlutz@media.mit.edu']",True,gsk-manufacturing-initiative,2017-01-09,"['computer-vision', 'augmented-reality', 'open-source', 'mapping']",GSK Manufacturing Initiative,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2017-10-10T16:44:10.796Z,"This project is part of a parallel research endeavor with GSK Manufacturing. By simulating how scientists at the Upper Providence site interact with one another and the space around them, we hope to help assist future renovations in a range of GSK locations. This was motivated by GSK’s drive to improve spatial configuration within their organization, including various open office environments and even smart labs. We hope this project will serve as both a packaged decision support system and a framework for GSK scientists and stakeholders to reconfigure with their own spatial inquiries and case studies. Our goals can be enumerated as below. 
Design a decision support tool that allows R&D to understand how changes to physical environments and adjacencies may have an impact on key workplace indicators. Encourage data-driven demonstration and discussion of decisions related to spatial changes. 
Employ spatial mathematical models to calculate key workplace indicators, including

Space utilization
Time accessibility between amenities
Synergy, defined as potential for interaction among researchers

Deploy the decision support tool as an evolving platform that exists in two forms: 

Tangible user interface “Tactile Matrix”
Traditional executable application that can be used on personal machines with mouse and keyboard interface. 
Provide transparent source code and open source, off the shelf technologies that allows for adaptation and customization for various case studies and applications. 

Screenshots courtesy of Nina Lutz. ",2018-06-01,['city-science'],https://dam-prod2.media.mit.edu/x/2018/05/03/Screen Shot 2018-05-03 at 9.26.31 AM_3hH3y20.png,209153,,2018-08-20T20:43:34.774Z,"['jiw@media.mit.edu', 'kll@media.mit.edu', 'nlutz@media.mit.edu']",True,gsk-places-initiative,2017-01-09,"['computer-vision', 'augmented-reality', 'open-source', 'mapping']",GSK Places Initiative,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2017-10-10T16:45:31.978Z,"This is an open source geospatial exploration tool. Using various public APIs including Open Street Map and the United States Census, we can make dynamic, flexible models of how people are moving through the city. These models include accessibility in cities, multimodal transportation networks, and diversity. Overall this allows anyone with or without an urban planning background to build strong models with geospatial and urban data. This system works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in a tangible way.Photos by Nina Lutz. ",2018-06-01,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/10/geo4.JPG,209161,,2018-09-04T19:47:57.544Z,"['kll@media.mit.edu', 'jiw@media.mit.edu', 'nlutz@media.mit.edu']",True,geobits,2016-07-11,"['computer-vision', 'augmented-reality', 'open-source', 'mapping']",GeoBits,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-10-10T17:19:36.126Z,"Mathematical experiences are intrinsic to our everyday lives, yet mathematics education is mostly confined to textbooks. Seymour Papert used the term ""Mathland"" to propose a world where one would learn mathematics as naturally as one learns French while growing up in France. We built a mixed reality application that augments the physical world with interactive mathematical concepts and annotations to create a real-life Mathland. Using Mathland, people can collaboratively explore, experience, and experiment with mathematical phenomena in their real, physical environments using tangible objects. Mathland opens up new opportunities for mathematical learning using Papert's constructionist principles in an immersive environment that affords situated learning, embodied interaction and playful constructionism.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/10/18/Screen Shot 2018-10-18 at 4.58.45 AM.png,209173,,2019-04-17T20:11:43.769Z,"['minakhan@media.mit.edu', 'ashris@media.mit.edu', 'trujano@media.mit.edu', 'jsirera@media.mit.edu']",True,mathland,2017-03-08,"['augmented-reality', 'learning-teaching']",Mathland: Play with math in mixed reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2017-10-10T18:31:07.765Z,This project focused on pedestrian accessibility in collaboration with Singapore Centre for Liveable Cities. Researchers and planners came together to design an interface that would allow both citizens and planners to interact with a model regarding pedestrian accessibility. The tangible interface allows users to come together to have conversations and make interventions to make the case study area more accessible for pedestrians. ,2018-06-01,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/10/IMG_8476.jpg,209191,,2018-08-22T03:18:47.497Z,"['jiw@media.mit.edu', 'kll@media.mit.edu', 'nlutz@media.mit.edu']",True,singapore-pedestrian-accessibility,2016-06-06,"['computer-vision', 'augmented-reality', 'open-source', 'mapping']",Singapore Pedestrian Accessibility,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-10-10T20:50:59.097Z,"Improvements in ingestible electronics with the capacity to sense physiological and pathophysiological states have transformed the standard of care for patients. Yet, despite advances in device development, significant risks associated with solid, non-flexible gastrointestinal transiting systems remain. Here, we report the design and use of an ingestible, flexible piezoelectric device that senses mechanical deformation within the gastric cavity. We demonstrate the capabilities of the sensor in both in vitro and ex vivo simulated gastric models, quantify its key behaviours in the gastrointestinal tract using computational modelling and validate its functionality in awake and ambulating swine. Our proof-of-concept device may lead to the development of ingestible piezoelectric devices that might safely sense mechanical variations and harvest mechanical energy inside the gastrointestinal tract for the diagnosis and treatment of motility disorders, as well as for monitoring ingestion in bariatric applications.",,"['conformable-decoders', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2017/10/10/news14.jpg,209224,,2023-09-07T23:32:08.294Z,"['canand@media.mit.edu', 'zijunw@media.mit.edu']",True,lead-zirconate-titanate-gastrointestinal-sensor-pzt-gi-s,2017-10-10,"['bioengineering', 'health', 'sensors']",Flexible piezoelectric devices for gastrointestinal motility sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2017-10-10T23:53:19.810Z,"The mismatch between the supply of graduates’ skills and the needs of the labor market has become increasingly obvious and problematic over the past years. This has prompt policymakers, educators, employers and applicants to reevaluate the role of higher education system. But how do each of these actors perceive higher education? How similar are, according to them, the different degree programs and institutions?In this project, we use a data-driven approach to unveil the structure of similarities between degree programs as perceived from the candidates. To that end, we use applicants’ preferences to higher education in Chile and Portugal between the years of 2007 and 2014 as a proxy to measure the similarity between each pair of degree programs. We find that:The two structures share the same topological features, despite coming from two different political and social-economical contexts;We quantify the mismatch between the current state of the art classification used by educators and policymakers and the structure identifyWe find the existence of strong spatial patterns in the assortment of gender, application scores, demand and unemployment levels; andWe find that structure of similarities encapsulates non-trivial information about the nature of each degree program, allowing us to predict with high accuracy the level of unemployment by just taking into account the relative position of a degree program in the higher education option set. Currently, we are preparing a manuscript to present our findings.",2017-12-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2017/10/11/edu-highered.jpg,209628,,2017-10-17T18:44:40.535Z,"['ccandiav@media.mit.edu', 'hidalgo@media.mit.edu', 'flaviopp@media.mit.edu']",True,mapping-higher-education-network,2016-06-01,"['data', 'networks']",Mapping Higher Education Option Space,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2017-10-11T00:17:19.947Z,"It is well established that countries, regions and institutions tend to develop towards related activities. This implies that, for instance, countries are more likely to enter a new activity that is closer/related with the activities it has already developed. An empirical fact that results from the overlapping of the necessary knowledge of each activity. In this context, the product space—a network relating countries economic activities—has been instrumental in capturing the role of relatedness in the economic development of countries. But, although relatedness seems to be a major driver for the diversification of countries exports and research activities, there are many instances when countries deviate from this norm, but to what extent do they benefit from such actions? Is it possible to pinpoint a particular stage of development of a country in which these exceptions are more likely to occur or are they purely at random?Using 50 years of trade date we have analyzed how countries diversify their products portfolios in the context of the Economic Complexity and Product Space10. We have shown that 1) there is an intermediate and non-trivial stage of economic development at which countries are more likely to develop towards unrelated activities; that 2) countries that do so achieve a faster economic growth; and 3) that low and high developed economies are the ones that are more likely to diversify towards related varieties.These results have significant implications in the literature of regional development. For instance, recently the European Union presented a regional plan of development, coined as Smart Specialization, which advocates for a one rule that fits all: regions should develop the most related and highest reward activities. Our results suggest more caution. Indeed, our findings point out that the development stage of a country, or a region, plays a determinant role in devising a development strategy. For instance, while low and highly developed regions should look forward to developing related activities, regions at an intermediate level of development should be incentivized to pursue the development of unrelated activities and diversify. These results build up to the conclusions of the previous project (2.1), in the sense that economies should adopt dynamical diversification strategies in which the big challenge is to identify the narrow window for unrelated diversification.",2017-12-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2017/10/10/Untitled.png,209637,,2017-10-11T00:23:45.415Z,"['hartmado@media.mit.edu', 'aamena@media.mit.edu', 'hidalgo@media.mit.edu', 'flaviopp@media.mit.edu']",True,a-new-project,2016-06-01,"['networks', 'social-science']",Do countries benefit from jumping into unrelated varieties?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1
False,2017-10-11T01:56:14.551Z,"We study textiles and nets as a base form for growing infrastructure and allowing reconfigurable sensing on low gravity bodies (for instance, roving spectrometers). We also study mechanisms for adhering a space web to a low gravity body. For example, can a modified snap bracelet be used to land infrastructure on an asteroid? It is notoriously difficult to stick a landing on a low gravity body, particularly if locomotion across the body is desired. We have been studying the use of arrays of bistable pinching elements for grappling onto the unpredictable contours of asteroids and other distant low gravity bodies. Each pinching element is mechanically actuated via an impact force, much like a snap bracelet. By coupling together arrays of such elements, we seek to demonstrate that the chain  can conform with added precision to the topological structure of the body, as well as grapple more effectively. This mechanism can ultimately be used to land large-scale structures like nets and tethers across the body, which then serve as infrastructure for crawling distributed sensors or sensory membranes, among other possibilities. Additionally, we completed a study on a candidate low cost spectral imager payload for determining iron content in rock samples. A concept paper on the broader work was published by the 31st Annual AIAA/USU Conference on Small Satellites.  A prototype was deployed on a microgravity flight and results summarized at AIAA Scitech 2019. There may also be compelling uses for the technology on Earth for adhering sensors to  terrain that is erratic and difficult to access, like the roof of a cave, or structures at the bottom of the sea floor. Two prototypes—one equipped with sensors—were tested on a microgravity flight by throwing them at a rocky target object. Data from the flight will be used to characterize the behavior of  chains of one vs three bistable elements in order to inform future design decisions.",2019-12-31,"['responsive-environments', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2017/11/20/IMG_0746.JPG,209645,,2023-09-06T22:33:01.945Z,"['cherston@media.mit.edu', 'paul_str@media.mit.edu']",True,Grappler,2017-09-01,"['design', 'architecture', 'art', 'environment', 'manufacturing', 'sensors', 'space', 'materials', 'fabrication', 'zero-gravity']",Space Webs as Infrastructure for  distributed sensor networks and other infrastructure on low gravity bodies,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-10-11T02:45:26.976Z,"We demonstrate a method for augmenting existing visual interfaces, including 3D and conventional displays, with haptic feedback capabilities, by utilizing a large number of closely spaced vortex-ring generators mounted along the periphery of the display. We present our first prototype of a multimodal interactive interface platform with 16 independently-controlled air-vortex ring  generators with one angular degree of freedom each. Our system has applications as an interactive interface, as a research tool, as an automotive control interface, and as a platform for creative expressions.",2019-09-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2018/04/30/AirTap.jpg,209654,,2020-06-10T15:37:56.576Z,"['alims@media.mit.edu', 'vmb@media.mit.edu']",True,airtap,2017-02-02,[],AirTap,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-11T04:11:35.011Z,"During the last few decades two important intellectual contributions have reshaped our understanding of international trade. On the one hand, work emphasizing trade frictions and extended gravity models has shown that countries trade more with those with whom they share a language, colonial past, or ethnic social relationships. This is interpreted as evidence of trade not being only about differences in factor endowments and transportation costs, but the result of complex social processes where information frictions and social networks play a key role. On the other hand, work emphasizing knowledge diffusion has shown that the probability that a country starts exporting a product increases with the number of related products it already exports. Yet, despite the importance of these two recent findings, little is known about their intersection: does knowledge on how to export to a destination also diffuses among related products? Here, we use bilateral trade data from 2000 to 2015, disaggregated into 1,242 product categories, to create an extended gravity model of bilateral trade that reproduces previous findings (effects of language, distance, colonial past, etc.) and shows that, in addition to these, countries are more likely to increase their exports of a product to a destination when: (i) they export related products to it, (ii) already export that product to some of its neighbors, and (iii) have neighbors who also export the same product to that destination. We interpret these findings as evidence of knowledge diffusion among related products and among geographic neighbors, both in the context of exporters and importers. Then, we explore the magnitude of these effects for new, nascent, and experienced exporters, and also, for groups of products classified according to Lall's technological classification of exports. We find that the effects of product and geographic relatedness are stronger for new exporters, and also, that the effect of product relatedness increases with the technological sophistication of products. These findings support the idea that international trade is shaped by knowledge and information frictions that are partially reduced in the presence of product relatedness.",2019-08-31,[],https://dam-prod2.media.mit.edu/x/2017/10/11/Figures2.jpg,209662,,2020-08-24T16:43:48.767Z,"['gaojian@media.mit.edu', 'hidalgo@media.mit.edu', 'bjun@media.mit.edu', 'aamena@media.mit.edu']",True,relatedness-knowledge-diffusion-and-the-evolution-of-bilateral-trade,2017-03-01,[],"Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0
False,2017-10-11T17:58:54.012Z,"“Power Props” is a sensor and actuator platform for Live Action Role Playing (LARP), immersive theater, theme park, and other transmedia experiences.The system is an open framework allowing any kind of sensor or actuator to be easily programed and reprogrammed on the fly, allowing novice LARP masters to add magic props and special visual effects to their game.",2022-08-01,[],https://dam-prod2.media.mit.edu/x/2017/10/11/yamabushi.jpg,209672,,2022-08-09T00:54:25.029Z,['novysan@media.mit.edu'],True,power-props,2012-06-01,"['entertainment', 'gaming', 'human-machine-interaction', 'interfaces', 'performance']",Power Props,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2017-10-11T18:17:28.669Z,"Collective memory and attention are sustained by two channels: oral communication (communicative memory) and the physical recording of information (cultural memory). Here, we use data on the citation of academic articles and patents, and on the online attention received by songs, movies, and biographies, to describe the temporal decay of the attention received by cultural products. We show that, once we isolate the temporal dimension of the decay, the attention received by cultural products decays following a universal biexponential function. We explain this universality by proposing a mathematical model based on communicative and cultural memory, which fits the data better than previously proposed log-normal and exponential models. Our results reveal that biographies remain in our communicative memory the longest (20–30 years) and music the shortest (about 5.6 years). These findings show that the average attention received by cultural products decays following a universal biexponential function.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2018/12/11/annie-spratt-49879-unsplash.jpg,209680,,2020-08-28T19:03:56.601Z,"['ccandiav@media.mit.edu', 'hidalgo@media.mit.edu', 'crisjf@media.mit.edu']",True,temporal-scales-in-human-collective-forgetting,2017-02-01,"['art', 'communications', 'entertainment', 'music', 'technology', 'social-change', 'history', 'data-science']",The universal decay of collective memory and attention,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1
False,2017-10-11T19:04:10.959Z,"Note: On Oct. 25, 2021, the PLOS journal reporting the research associated with this project retracted the publication.Flavor, in addition to making our food delicious, is one way of sensing biochemical richness. A highly flavorful plant generally contains a greater quantity and diversity of molecules—often with useful functional roles in our own metabolism—than a bland-tasting plant. Flavor is a built-in reward for eating plants that has fueled our drive to domesticate and breed a massive biodiversity of vegetation over the last 10,000 years. OpenAg is going deep into the biochemical machinery, evolution, and ecology of plants to make growing food for the optimization of specific chemical profiles (flavor, pharmaceutical properties, nutrition) a reality.Plants rely on rich and diverse chemistry for self-defense and stress adaptation. OpenAg is working to induce a plant to synthesize these molecules by adding specific stresses to the plant’s environment and measuring chemical shifts.These specialized metabolite molecules can manifest as flavor, pharmaceutical compounds, and rich nutritional profiles. Flavor itself is frequently tied to additional healthful bio-activities for humans—such as vitamins, antioxidants, stimulants, and nutrients.",2020-04-30,['open-agriculture-openag'],https://dam-prod2.media.mit.edu/x/2019/04/22/flavor_compounds.png,209688,,2021-10-29T14:07:53.797Z,['rebekahj@media.mit.edu'],True,openag-flavor-ecology,2017-10-02,"['agriculture', 'food', 'biology', 'technology']","Optimizing plants for flavor, nutrition, and pharmaceutical content",PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-10-11T19:58:43.910Z,"Lower extremity amputation leads to limitations of biological function of individuals, which leads to challenges remaining physically active and participating in athletic activities. Physical activity is very important for cardiovascular health, weight management, and mental health. Designing devices aimed at increasing accessibility to sports will encourage individuals with amputations to continue or begin participating in athletic pursuits such as rock climbing. After an initial design in 2015, the group is revisiting this design problem, aiming to create a less massive, more robust and versatile design. The major design changes include a lightweight 2-degree-of-freedom actuator, modular feet for the approach of the climb, as well as slot, edge, and ice climbing, and changing the control to a less invasive system. The position control is predicted to be determined by the user via the Tongue-Computer Interface being designed in parallel.The initial research presents the design and evaluation of a 2-degree-of-freedom powered ankle-foot prosthesis for rock climbing. The aim of this device is to restore function of the ankle and subtalar joints for trans-tibial amputees during rock climbing, providing the user with myoelectric position control of the foot. Precise positional control of the foot is especially important while climbing, as the climber’s ability to successfully scale a route requires them to reliably reorient the foot to various shapes and orientations of holds. Passive prostheses do not allow the user to reposition the foot, and current powered prostheses are too bulky and heavy to provide benefit during rock climbing. The design requirements for this device are that it must be lightweight (< 1.5 kg), low profile, robust, with 2 degrees of freedom of electromyographically controlled movement. The custom designed device consists of 2 non-backdrivable linear actuators in a differential pair, allowing for powered motion in plantarflexion/dorsiflexion and inversion/eversion. Load cells aligned axially with each actuator are used to provide force feedback to the device, allowing for position control during free-space motion, and powering off the actuators when the device is loaded, relying on the non-backdrivable transmission to maintain ankle and foot position while loaded. This control scheme reduces the power requirements of the device, allowing for lighter batteries as well as smaller motors and transmission.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2018/05/04/IMG_2576.jpg,209696,,2023-03-11T20:16:43.466Z,"['emrogers@media.mit.edu', 'hherr@media.mit.edu', 'mcarney@media.mit.edu', 'syeon@media.mit.edu', 'enates49@media.mit.edu']",True,ankle-foot-prosthesis-for-rock-climbing,2017-09-15,"['biomechanics', 'mechanical-engineering', 'mechatronics', 'prosthetics']",Design of a 2-Degree-of-Freedom Powered Ankle-Foot Prosthesis for Rock Climbing,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-10-11T20:27:23.537Z,"YOUR URGE TO BREATHE IS A LIE (a direct quote from my free-diving instructor) is a progressive approach towards innovation, human response to ecological change, and the transhumanist project.  Embrace change: don't build walls to hold the future back; adapt and dive right in;Re-model the model species: to truly evolve, model the future of the human body-mind after the cephalopod;Embrace the knowledge and abilities embedded in ancient rituals and global traditions of living in, on, under the water;Train the human to biological limits of capacity and capability, and then add on technology; andEmbrace desire.YOUR URGE TO BREATHE IS A LIE is a manifesto, a call to action, an invitation to become the future of the human, streaming in new ways towards each other and the sea.",2018-08-31,['responsive-environments'],https://dam-prod2.media.mit.edu/uuid/8d9d4627-f5f9-4a83-9bae-c456dab84f5b,209705,,2018-10-23T15:18:15.909Z,['simun@media.mit.edu'],True,your-urge-to-breathe-is-a-lie,2017-10-11,[],YOUR URGE TO BREATHE IS A LIE,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-10-11T20:34:03.716Z,"¡ONWARDS + UNDER! proposes that as sea levels rise, particularly in highly populated coastal cities, the current plan to keep nature at bay and build a wall against the sea is untenable (and that walls against perceived threats not only do not work, but are dangerous metaphors to deploy).  ¡ONWARDS + UNDER! proposes that we embrace a changing climate and a rising sea level, and respond by adapting our bodies, abilities and lifestyles to live more intimately in, on and under the sea. To understand what this might look like, I dove into cultures and histories of humans living intimately with oceans, including the physical and ecological practices diving women of Japan (Ama) and Korea (Haenyo); the pearl divers of the Arabian/Iranian Gulf (and tin particular the music they use to organize their oceanic labors); the Moken people (sometimes called “sea-gypsies” or “water-people”) a buddhist Austronesian people that are trying to maintain a semi-nomadic hunter-gatherer lifestyle based almost exclusively on the sea amidst changing maritime and immigration regulations; the “Aquatic Ape” hypothesis which first became popularized as a feminist critique of dominant evolutionary theory and proposes that a crucial step in the evolution of homo sapiens involved a period of semi-aquatic lifestyle in what is now South Africa (the ape first stood up in water); and the recently invented sport/meditation culture of free-diving (which borrows techniques developed by the US military). This research is a mix of first and secondary sources: historical, interviews, site visits, and a personal embodied physical practice (“your urge to breathe is a lie” is a direct quotation from my free-diving instructor) - all research has been filmed and/or audio recorded. What can we learn from the knowledge and practices developed by indigenous groups, extreme sports sub-cultures, and alt-evolutionary theorists in order to imagine a human future amidst ecological crisis?",2018-08-31,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2017/10/11/G0010036.JPG,209713,,2018-10-23T15:18:31.439Z,['simun@media.mit.edu'],True,onwards,2017-10-11,[],¡Onwards+Under!,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-10-11T21:12:18.344Z,"As we generally experience on earth, there is no space without sound and there is no sound without space. Building on the understanding of music and architecture as creators of spatial experience, this project presents a novel way of unfolding music’s spatial qualities in the physical world. Spaces that Perform Themselves exposes an innovative response to the current relationship between sound and space: where we build static spaces to contain dynamic sounds. What if we change the static parameter of the spaces and start building dynamic spaces to contain dynamic sounds? A multi-sensory kinetic architectural system is built in order to augment our sonic perception through a cross-modal spatial choreography that combines sound, movement, light, color, and vibration. By breaking down boundaries between music and architecture, possibilities of a new typology that morphs responsively with a musical piece can be explored. As a result, spatial and musical composition can exist as one synchronous entity. These spatial choreographies build up the scenario to study the possible relationships between a human body and a robotic architectural body, throughout a dance of perception and matter. This project seeks to contribute a novel perspective on leveraging technology, art, science, and design to provide a setting to enrich and augment the way we relate to the built environment. The objective is to enhance our perception and challenge models of thinking by presenting a post-humanistic phenomenological encounter of the world.",,['opera-of-the-future'],,209721,,2019-04-17T20:06:12.678Z,['nicolelh@media.mit.edu'],True,spaces-that-perform-themselves-1,2016-09-01,[],Spaces that Perform Themselves,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-10-12T14:16:39.075Z,"Your social media. Your rules.Gobo is an experiment, not a startup. We’re building it to change the conversation on social media and imagine a better version of it. This is a technology-to-think-with—a tool we want you to play with and push against. Gobo is being built by a small team at MIT Media Lab's Center for Civic Media, where we work on technologies for social change.For questions, feedback, and musings, you can reach the Gobo team at gobo@media.mit.edu.Control your own feedSocial media companies use algorithms to control what we see on our feeds, but we don’t know how these algorithms work. As a result, we’re often unaware why certain posts show up in our feed while others don’t. Gobo allows you to control the algorithms, or a set of “rules,” so you can decide what gets shown on your feed and know why.Connect multiple platformsWe believe that multiple social media platforms should exist to serve different purposes. However, it’s not easy to keep up with all these platforms, especially when your data can’t be easily shared between them. Gobo allows you to connect up to three platforms, so you can view all of your feeds in one place. See what gets hiddenWe believe that transparency can help you better understand what you see on social media and keep platforms accountable for algorithmic bias. Gobo tells you why certain posts are hidden based on the rules you set. It also shows you how many posts are hidden, so you can understand the overall impact of the rules you set.Expand your perspectiveSocial media companies make assumptions about what we want to see based on what we read and click on. They tend to show us content we’re already engaging with, reinforcing our echo chambers. Instead of assuming what you want to see, Gobo allows you to add unfamiliar perspectives into your feed, so you can better understand the range of opinions that are shared online.",2020-08-01,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2019/04/09/gobo-white-bg.png,210115,,2022-06-10T13:29:45.142Z,"['jasrub@media.mit.edu', 'ahope@media.mit.edu', 'ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'dsjen@media.mit.edu', 'gaikwad@media.mit.edu', 'belen@media.mit.edu']",True,gobo,2017-09-01,"['artificial-intelligence', 'civic-media', 'civic-technology', 'social-media', 'machine-learning', 'community', 'technology']",Gobo,PUBLIC,https://gobo.social,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-10-12T20:43:19.785Z,"Nebula is a voice-controlled interactive software app that allow users to conduct a choir of diverse vocal sounds by using only their voice as input. The system is based on the Constellation project by Akito van Troyer that takes sonic material and organizes it visually to let anyone compose creative soundscapes. Nebula uses hundreds of vocal samples that are represented as individual stars and organized by perceptual and spectral audio features. The samples get triggered and activated when the user sings or produces any sound with the voice. The voice is analyzed in real time, and this analysis is then used to to trigger and mix a cascade of sounds with similar features. The voice becomes a kind of conductor's baton that creates a dialogue without words between the individual and the community. And once a participant uses Nebula, their own voice, first used as a controller, is then transformed into a new sample adding an additional star to the experience for all subsequent participants. The result - a final cosmos of voices—provides material that might be used by composer Tod Machover for the final Philadelphia Voices City Symphony. ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/10/12/overlay.png,210145,,2018-10-19T19:38:07.641Z,['rebklein@media.mit.edu'],True,nebula,2017-08-01,[],Nebula,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2017-10-12T21:00:08.658Z,"SNaSI (Social Navigation through Subtle Interactions) is a wearable system to help blind people in face-to-face interactions. Assistive systems for the blind have been an important area of research in the wearable community for many decades. Most of those systems focus on spatial navigation issues. In the last few years, we start noticing a move toward technologies to assist the blind with social navigation. Most of those systems treat social navigation the same way as spatial navigation, focusing mainly on utilitarian aspects of human interaction (what is needed to obtain information, what information is exchanged, etc.). In this project we argue that when moving into this space we need to think about social accessibility and respect of human connection first because, most of the time, face-to-face interaction aims primarily to create and reinforce human connection rather than exchanging information. With this in mind, we defend the importance of designing with subtlety in regards of framing, reasoning and design challenges. To this end, we present SNaSI—Social Navigation through Subtle Interactions—a wearable garment designed to help blind and visually impaired people in face-to-face interaction with sighted peers and we describe how our design criteria were guided by subtlety and social acceptability. Designed in collaboration with Microsoft Research UK, HXD group, Morrison Cecily, Huburn Josh, Grayson Martin.",,[],https://dam-prod2.media.mit.edu/x/2017/10/12/image3.jpg,210162,,2017-10-13T20:15:18.696Z,['rebklein@media.mit.edu'],True,snasi,2016-06-01,[],SNaSI: Wearable device to help the blind with social navigation,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-10-13T00:50:03.832Z,"EcoFlux provides a new way for people to experience the inner working of ecosystems by using augmented or virtual reality to explore unseen ecological processes. This project used a human-centered design and development process to create a proof-of-concept technology that visualizes models of ecosystem processes using real-time, sensor-based data. EcoFlux builds on the existing MIT Media Lab project DoppelMarsh, is a virtual environment that changes in response to real-time environmental conditions captured by the distributed sensor network at the Tidmarsh wetland site. EcoFlux is the first of its kind to visualize models of molecular motion and carbon flow in 3D, within the context of the physical site, and driven by real-time data. Whether experienced remotely or integrated on site, EcoFlux can be used to inspire curiosity for visitors, enhance scientific understanding for researchers, and promote community development by demonstrating the value of ecological restoration. As environmental sensing becomes more ubiquitous in our daily lives, this thesis provides a foundation for making meaning from this wealth of information and harnessing human sensory systems to encourage new insights.",2017-06-01,[],,210581,,2017-10-13T01:51:36.482Z,[],True,ecoflux,2016-06-01,[],"EcoFlux: Ecosystem Visualizations Driven by Real-Time, Sensor-Based Data",PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2017-10-13T14:49:56.463Z,"We are transforming a classic music jewelry box into a digital memory box and Skype portal that enable those not familiar with technology to stay in touch with their family and friends. The box has three different modes. To switch mode the user only has to turn the small crank in the back, like they would do with a regular music box. The crank is linked to a rotary encoder. The back of the box is covered with a two-way mirror covering a small LCD screen; when the screen is turned off, it looks like a regular mirror but when the screen is on, it looks like a display. In the first mode, the box plays the favorite music of the user with the screen off. In the second mode, the display shows photographs of family and friends. By turning the crank or by clicking on the characters in the photographs, the box goes into mode 3, which is a Skype portal enabling the user to instantly call a family member face-to-face. This device is mainly imagined for elderly parents with dementia or memory loss.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/04/26/MMB1 copy.png,210597,,2018-04-27T15:51:14.777Z,['rebklein@media.mit.edu'],True,memory-music-box,2017-10-01,[],Memory Music Box,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-10-13T16:10:34.085Z,"Philadelphia Voices is the latest in the series of City Symphonies projects that Tod Machover and the Opera of the Future group	have created since 2012. Previous City Symphonies have centered on Toronto, Edinburgh, Perth, Lucerne, and	Detroit.Each project paints a musical portrait of a city—using “traditional” musical elements as well as real sounds recorded by residents—to portray the essence of their city's history and future. Everyone living in that city is invited to collaborate to create the symphony,	resulting in an unprecedented creative collaboration around music, sound, and storytelling. Philadelphia Voices has been in progress	since spring 2017 and will culminate in performances in Philadelphia (Kimmel Center) and New York (Carnegie Hall) in April	2018. A special mobile app has been developed to allow anyone with a smartphone to collect sounds and video and to upload those files to a communal database for listening and morphing. Opera of the Future researchers have created new software that enables anyone to contribute their voice to a specially-designed sonic landscape from Philadelphia. Workshops and special activities have been organized	with local singers from every age and background, and Tod Machover has chosen several hundred of them to sing in the final performances with The Philadelphia Orchestra under the baton of its music director, Yannick Nézet-Séguin. Since Philadelphia is considered the birthplace of 	American democracy, Philadelphia Voices will investigate the current state of democracy from a Philly	perspective. The project will also consider the society in which we want to live, and what we are willing to do to achieve that ideal.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/10/13/James Madison & Tod.png,210605,,2018-10-19T19:39:16.909Z,['tod@media.mit.edu'],True,philadelphia-voices,2017-10-13,"['art', 'crowdsourcing', 'music', 'performance']",Philadelphia Voices,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-10-13T19:47:13.913Z,"Cognitarium 2.0  is a planetarium for the mind. Enter a world of 40 Hz sounds, multicolored lights and mind-entraining frequencies. Cognitarium 2.0 combines early groundbreaking research in gamma frequencies, multi sensory music and cognition research, to leap into the future of cognitive health and cross-modal experiences. This edition of the Cognitarium is a collaboration combining creative cognition research by Alexandra Rieger, lighting physics and control from Ben Bloomberg, the gamma drones of David Su, and 40 Hz sculpted sound as defined by Tod Machover.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/05/01/Cognitarium 2.0.jpg,210625,,2018-05-02T16:58:17.658Z,['arieger@media.mit.edu'],True,cognitarium,2017-06-10,[],Cognitarium 2.0,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-13T21:05:24.536Z," Boxlab Visualizer is a tool for replaying and annotating multi-modal datasets synchronized to audio/video recordings for ground truth labeling. Many common data formats are supported, and visualizations are automatically populated with the most appropriate representation, such as time series, histogram, heat map, or cartesian space. BoxLab Visualizer is intended to facilitate free sharing of datasets and collaborative annotation.",2011-12-31,[],https://dam-prod2.media.mit.edu/x/2017/10/13/BoxLabVisualizerCropped.png,210642,,2017-10-26T15:53:15.273Z,[],True,boxlab-visualizer,2008-09-01,[],BoxLab Visualizer,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2017-10-13T21:31:06.225Z,"The web enables massive realtime communication and collaboration, but most media on the web does not take advantage of these features. Media on the internet typically uses the web only as a distribution medium.If we are going to make next-generation internet media, we need to think about how to integrate the unique properties of the web into the media itself. This involves rethinking the role and design of web servers so they facilitate realtime interaction instead of serving requests.Models for internet-enabled interaction and collaboration like forums, chatroom, live documents, metrics and A/B testing, are not designed with interactive media in mind. This project is our very first exploration using custom web server technology and a new interaction model to facilitate online collaboration.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/10/16/Art at the Lab.png,210650,,2018-05-01T13:57:18.927Z,['holbrow@media.mit.edu'],True,eternal,2017-10-01,[],Eternal,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2017-10-13T23:40:33.966Z,"The Wanderers were unveiled as part of the exhibition: ‘The  Sixth Element: Exploring the Natural Beauty of 3D Printing' on display  at EuroMold, 25-28 November, Frankfurt, Germany. This work was done in collaboration with Christoph Bader and Dominik Kolb.  The wearables were 3D-printed with Stratasys multi-material 3D printing  technology. Members of the Mediated Matter group led by Will Patrick and Sunanda Sharma  are currently working on embedding living matter in the form of  engineered bacteria within the 3D structures in order to augment the  environment.",2017-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/uuid/7d91171c-8ca5-4c05-86a6-d99a02f89e07,211043,,2022-03-11T15:59:26.773Z,"['bader_ch@media.mit.edu', 'kolb@media.mit.edu', 'neri@media.mit.edu']",True,wanderers,2014-07-01,"['design', 'architecture', 'art', 'synthetic-biology', 'space', 'biology', 'materials']",Wanderers,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-14T03:43:08.192Z,"An Urban Decision-Support System Augmented by Artificial IntelligenceThe decision-making process in urban design and urban planning is outdated. Currently, urban decision-making is mostly a top-down process, with community participation only in its late stages. Furthermore, many design decisions are subjective, rather than based on quantifiable performance and data. Current tools for urban planning do not allow both expert and non-expert stakeholders to explore a range of complex scenarios rapidly with real-time feedback. CityMatrix was an effort towards evidence-based, democratic decision-making. Its contributions lie in the application of Machine Learning as a versatile, quick, accurate, and low-cost approach to enable real-time feedback of complex urban simulations and the implementation of the optimization searching algorithms to provide open-ended decision-making suggestions. The goals of CityMatrix were: Designing an intuitive Tangible User Interface (TUI) to improve the accessibility of the decision-making process for non-experts. Creating real-time feedback on multi-objective urban performances to help users evaluate their decisions, thus to enable rapid, collaborative decision-making. Constructing a suggestion-making system that frees stakeholders from excessive, quantitative considerations and allows them to focus on the qualitative aspects of the city, thus helping them define and achieve their goals more efficiently.CityMatrix was augmented by Artificial Intelligence (AI) techniques including Machine Learning simulation predictions and optimization search algorithms. The hypothesis explored in this work was that the decision quality could be improved by the organic combination of both strengths of human intelligence and machine intelligence.The system was pilot-tested and evaluated by comparing the problem-solving results of volunteers, with or without AI suggestions. Both quantitative and qualitative analytic results showed that CityMatrix is a promising tool that helps both professional and non-professional users understand the city better to make more collaborative and better-informed decisions. ",,['city-science'],https://dam-prod2.media.mit.edu/uuid/a9c05f13-d750-4832-90e4-f8ae2ccc9682,211071,,2018-10-17T18:10:46.064Z,"['kll@media.mit.edu', 'agrignar@media.mit.edu', 'alonsolp@media.mit.edu', 'ryanz@media.mit.edu']",True,citymatrix,2016-02-26,"['architecture', 'artificial-intelligence', 'civic-technology', 'data', 'human-machine-interaction', 'real-estate', 'systems', 'interfaces', 'machine-learning', 'collective-intelligence', 'computer-science']",CityMatrix,PUBLIC,https://media.mit.edu/people/ryanz,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-10-14T15:11:53.472Z,"This brief excerpt video shows a glimpse of some of Tod Machover’s innovative, unusual opera realized at—and with the collaboration of—the MIT Media Lab over the past 30 years.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2017/10/16/Screen Shot 2017-10-14 at 9.18.12 AM.png,211080,,2017-10-17T13:37:38.335Z,['tod@media.mit.edu'],True,tod-machover-operas-1987-2014,,"['artificial-intelligence', 'music', 'storytelling', 'performance', 'creativity', 'voice']",Tod Machover: Operas 1987-2014,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2017-10-14T21:46:32.687Z,"We introduce SkinBot: a lightweight robot that moves over the skin's surface with a two-legged suction-based locomotion mechanism and captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin, such as SkinBot, will enable a more systematic study of the human body and offer great opportunities to advance our knowledge in many areas such as telemedicine, human-computer interfaces, body care, and fashion.",,"['affective-computing', 'responsive-environments']",https://dam-prod2.media.mit.edu/x/2018/09/27/front_robot.jpg,211089,,2019-04-19T14:35:05.713Z,"['joep@media.mit.edu', 'javierhr@media.mit.edu', 'artemd@media.mit.edu', 'sfollmer@media.mit.edu']",True,skinbot-a-wearable-skin-climbing-robot,2016-11-01,"['robotics', 'health', 'human-machine-interaction', 'wearable-computing', 'wellbeing', 'physiology']","SkinBot: A wearable, skin-climbing robot",PUBLIC,http://www.artemdementyev.com,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-10-15T17:59:28.998Z,A social robot modifies its behavior to change what you think about it!,2019-09-01,['personal-robots'],https://dam-prod2.media.mit.edu/x/2017/10/15/cover.png,211504,,2019-10-09T14:04:10.940Z,"['cynthiab@media.mit.edu', 'jinjoo@media.mit.edu']",True,artificial-listener-with-social-intelligence,2017-05-01,"['artificial-intelligence', 'data', 'human-machine-interaction', 'kids', 'storytelling', 'social-science', 'machine-learning', 'social-robotics', 'nonverbal-behavior', 'affective-computing']",Artificial listener with social intelligence,PUBLIC,http://www.jinjoolee.com,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2017-10-15T18:14:23.794Z,"Realtime detection of social cues in children’s voicesIn everyday conversation, people use what are known as backchannels to signal to someone that they are still listening, paying attention, and engaged. As listeners, we smile, nod, and say “uh-huh” to convey attentiveness, and we do this naturally with little thought. We give this feedback not randomly but at certain moments in the conversation because speakers give off social cues that signal upcoming backchanneling opportunities.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2017/10/15/cover_HTXnlk8.png,211512,,2018-05-07T17:34:51.716Z,"['cynthiab@media.mit.edu', 'haewon@media.mit.edu', 'gelso@media.mit.edu', 'jinjoo@media.mit.edu']",True,realtime-detection-of-social-cues,2017-03-01,"['artificial-intelligence', 'data', 'human-machine-interaction', 'kids', 'storytelling', 'social-science', 'machine-learning', 'social-robotics', 'nonverbal-behavior', 'affective-computing']",Realtime Detection of Social Cues,PUBLIC,http://www.haewonpark.com/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-10-16T01:59:19.984Z,"An 85-inch television screen combined with 8K resolution provides a tool through which multiple users can perform quick, highly detailed, visual analysis of a dataset to identify anomalies.  In order to test this, we developed a fully 3D application similar to the popular picture game Where’s Waldo.  In the Where’s Waldo game, users must find a person (Waldo) within a densely-populated picture.  In the developed application, users utilize touch and gestures to navigate a large 3D city in which a person of a varying size is placed randomly.  The application serves as a proof of concept that multiple users can interact with one large, high resolution dataset to visually find anomalies.",2019-09-01,['object-based-media'],,211929,,2020-06-10T16:36:02.416Z,['pe25171@media.mit.edu'],True,where-s-pedro,2017-01-07,[],Where's Pedro?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-16T02:24:06.727Z,"We designed, implemented, and tested a proof of concept, wrist-based wearable object identification system which allows users to “hover” their hands over objects of interest and gain access to contextual information that is tied to them, through an intelligent personal assistant. The system uses a fusion of sensors to be able to perform the identification of an object under a variety of conditions. Among these sensors, there is a camera (operating in the visible and infrared spectrum), a small solid-state radar, and multi-spectral light spectroscopy sensors. Users can interact with contextual information tied to an object through conversations with an intelligent assistant to permit a hands-free, non-obtrusive, and discreet experience. The system explores audio interfacing with augmented reality content without the hassle of phones or head-mounted devices.",2018-09-01,['object-based-media'],,211937,,2020-06-10T16:07:39.875Z,['pe25171@media.mit.edu'],True,hoverband,2017-09-01,[],Hover,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-10-16T02:47:57.638Z,"Machine learning model outperforms human judgementOur computational trust model is capable of predicting—above human accuracy—the degree of trust a person has toward a stranger by observing the nonverbal behaviors expressed in their social interaction. We used machine learning algorithms, specifically hidden Markov models (HMMs), to model the temporal relationship between specific nonverbal behaviors. By interpreting its resulting learned structure, we discovered that the sequence of low and high trusting behaviors a person emits provides further information of their trust orientation toward their partner. These discoveries shaped the feature engineering process that enabled a support vector machine (SVM) model to achieve a prediction performance more accurate than human judgment.",2019-09-01,['personal-robots'],https://dam-prod2.media.mit.edu/x/2017/10/15/cover_Om5oMmt.png,211945,,2019-10-09T14:06:42.177Z,"['picard@media.mit.edu', 'cynthiab@media.mit.edu', 'jinjoo@media.mit.edu']",True,predicting-trust-between-people,2013-12-01,"['artificial-intelligence', 'data', 'social-science', 'machine-learning', 'social-robotics', 'nonverbal-behavior', 'affective-computing', 'trust']",Predicting Trust Between People,PUBLIC,http://www.jinjoolee.com,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-10-16T18:57:58.510Z,"This project investigates a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN) using deep reinforcement learning (RL). Our method, which we call Sequence Tutor, allows models to improve sequence quality with RL, while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation for drug discovery. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.",2019-07-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2017/10/16/note_probs_x4.png,211978,,2022-05-12T17:34:50.033Z,['jaquesn@media.mit.edu'],True,improving-rnn-sequence-generation-with-rl,2016-06-01,['machine-learning'],Improving RNN Sequence Generation with RL,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-10-16T22:06:21.871Z,"In order to understand how exogenous shocks, like death, impact memorability by remembering, we use a data-set of biographies from Wikipedia for all individuals who have more than 15 different language editions. Here, we focus on different external shocks that are able to trigger remembering, such as Death, Nobel Prize, Academy Awards (Oscars), Ballon d'Or, Golden Globes, and Grammy's. All of these events show an exogenous-critical non-trivial herd behavior, as described by Crane and Sornette 2008.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2017/10/16/ALL.jpg,211990,,2020-08-28T19:04:10.252Z,"['ccandiav@media.mit.edu', 'crisjf@media.mit.edu', 'hidalgo@media.mit.edu']",True,the-laws-of-forgetting-ii,2017-10-16,['data'],The laws of forgetting II: How death and exogenous events shape our collective memory,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1
False,2017-10-17T03:52:05.801Z,"Social learning has shown that people are more likely to learn from those who are seen as prestigious, talented, or who share demographic attributes with learners. In order to demonstrate that, many experiments and data-based studies have been conducted in many different systems; however, classroom environments have been understudied, because of different complications in both designing experiments and collecting data.Combining both new technologies that are able to capture children's attention, e.g. video games, as well as experimental game theory, which provides us a formal framework to capture children's revealed preferences—a school classroom can provide an ideal environment for controlled social dilemma experiments, whose results can be contrasted against real-life indicators of school-life.The connection between cooperation inside a classroom and social relationships is central in our framework. Here, we navigate the social network structure by running a non-anonymous dyadic cooperative (video) game (Fig. 1), in 50 different public primary school classrooms, between grades 3-5, allowing us to map cooperation networks for each classroom.From the video game decisions, we build a weighted cooperation network for each classroom. The resulting network structure is able to capture different properties of the classroom, such as academic performance and social co-existence (Fig. 2). First, we find that positions in the social network have a significant power to identify, in an early stage, children who are susceptible to becoming  the victims of bullying, and children who have a high probability to be bullies (Fig. 2A). Second, we find a positive and statistically significant relationship between network centrality—measured as the sum of the outcome on the video game—and student’s academic performance (measured as GPA, even controlling for others socio-behavioral characteristics that are correlated with GPA (Fig. 2 B)).These results don't just help us to understand the elementary school environment, but also open new avenues for the role of networks in the education system, with a huge potential impact in education public policy. These results are useful inputs for decision makers and physiologists to prevent bullying and improve learning.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2017/10/17/Terreno.png,212013,,2020-08-24T16:38:44.980Z,"['ccandiav@media.mit.edu', 'hidalgo@media.mit.edu']",True,when-bullying-meets-video-game-theory,2017-10-17,[],When bullying meets (video) game theory: A novel framework to understand elementary school environments,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-10-17T04:24:28.068Z,"Project website: shelley.ai Human-AI collaborated stories: stories.shelley.ai Follow @shelley_ai to collaborate with Shelley! For centuries, across geographies, religions, and cultures, people have innovated ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity. This challenge is especially important at a time when we are exploring the limits of artificial intelligence: Can machines learn to scare us? In Halloween 2016 we presented the Nightmare Machine—computer-generated scary imagery powered by deep learning algorithms. This Halloween, we present Shelley: Human-AI Collaborated Horror Stories! Shelley is a deep-learning powered AI who was raised reading eerie stories coming from r/nosleep. Now, as an adult—and not unlike Mary Shelley, her Victorian idol—she takes a bit of inspiration in the form of a random seed, or a short snippet of text, and starts creating stories emanating from her creepy creative mind. But what Shelley truly enjoys is working collaboratively with humans, learning from their nightmarish ideas, creating the best scary tales ever. If you want to work with her, respond to the stories she'll start every hour on her Twitter account, and she will write with you the first AI-human horror anthology ever put together!",2017-11-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/10/20/shelley_header_name_rnuxsWI.jpg,212021,,2020-07-23T18:38:25.019Z,"['pinary@media.mit.edu', 'cebrian@media.mit.edu', 'irahwan@media.mit.edu']",True,shelley,2017-10-15,"['artificial-intelligence', 'crowdsourcing', 'data', 'social-media', 'storytelling', 'machine-learning']",Shelley: Human-AI Collaborated Horror Stories,PUBLIC,http://shelley.ai,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-18T16:34:52.963Z,"The esc-Pod  (or Escape Pod) is an exploratory platform for researchers investigating moments of refuge within our bustling work lives. The core of the esc-Pod consists of actuated work and rest surfaces. This allows for moments of productivity and relaxation to occur within a single space. The outer skin provides variable transparency, enabling a spectrum of visibility settings according to privacy requirements. The inner skin provides an infrastructure for the modulation of spatial experiences. Each panel is a pixel, connecting itself to the skin network, and can embody an array of senses.",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/18/escpod.gif,212431,,2019-04-08T17:01:14.555Z,"['kll@media.mit.edu', 'csmuts@media.mit.edu', 'kapeloni@media.mit.edu', 'cassiano@media.mit.edu', 'nlutz@media.mit.edu']",True,escape-pod-1,2016-08-01,"['architecture', 'human-machine-interaction', 'sensors']",Escape Pod,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-21T16:44:36.379Z,"CityScope Volpe is demonstrating most of the urban planning, analysis, and prediction features developed for the CityScope project. The site, a 14-acre parcel on the northern part of MIT/Kendall Square area of Cambridge, has been acquired and is  being developed by MIT. City Science researchers designed and built a CityScope urban performance tool that is aiming to predict the outcomes of multiple planning and development scenarios.",,['city-science'],https://dam-prod2.media.mit.edu/x/2017/10/21/Oct-19-2017 10-11-42.gif,213261,,2020-02-12T20:47:11.480Z,"['ryanz@media.mit.edu', 'alonsolp@media.mit.edu', 'noyman@media.mit.edu', 'agrignar@media.mit.edu', 'markuse@media.mit.edu']",True,cityscope-volpe,2016-11-01,"['human-computer-interaction', 'architecture', 'transportation', 'urban-planning', 'data-visualization']",CityScope Volpe,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2017-10-24T22:42:25.815Z,"Understanding social-emotional behaviors in storytelling interactions plays a critical role in the development of interactive and educational technologies for children. A challenge when designing for such interactions using technologies like social robots, virtual agents, and tablets is understanding the social-emotional behaviors pertinent to the storytelling context—especially when emulating a natural peer-to-peer relationship between the child and the technology.  We present P2PSTORY, a dataset of young children (5-6 years old) engaging in natural peer-to-peer storytelling interactions with fellow classmates. The dataset contains 58 recorded storytelling sessions along with a diverse set of behavioral annotations as well as developmental and demographic profiles of each child participant. The CHI 2018 paper presenting this dataset can be found here: Nikhita Singh, Jin Joo Lee, Ishaan Grover, and Cynthia Breazeal (2018). P2PSTORY: Dataset of Children Storytelling and Listening in Peer-to-Peer Interactions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.See below for instructions on how to access the dataset.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/04/29/p2pstory_cover.png,214838,,2019-09-20T13:40:21.991Z,"['cynthiab@media.mit.edu', 'nikhita@media.mit.edu', 'igrover@media.mit.edu', 'jinjoo@media.mit.edu']",True,p2pstory,2018-01-22,"['robotics', 'data', 'kids', 'learning-teaching', 'social-science', 'social-robotics', 'nonverbal-behavior', 'long-term-interaction']",P2PSTORY: Dataset of children as storytellers and listeners in peer-to-peer interactions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-10-26T21:52:08.494Z,"Recent developments in wearable robots and human augmentation open up new possibilities of designing computational interfaces integrated to the body. Particularly, supernumerary robot is a recently established field of research that investigates a radical idea of adding robotic limbs to users. Such augmentations, however, pose a limit in how much we can add to the body due to weight or interference with other body parts. To address that, we explore the use of soft robots as supernumerary robotic fingers. We present a pair of soft robotic fingers driven by cables and servomotors, and applications using the robotic fingers in various contexts. ",2018-08-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2017/10/26/Untitled-1.jpg,216058,,2018-08-20T16:45:50.186Z,"['sangwon@media.mit.edu', 'yuhanhu@media.mit.edu']",True,hand-development-kit,2016-08-01,[],Hand Development Kit,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-11-06T20:18:39.302Z,"Each new technology forces us to confront who we are as humans.  As technological innovations in deep space exploration keep advancing, and space travel becomes more accessible to the broader population, what might become of our memories of Earth? Speculating on a future where some of us might embark on a one-way trip into space, this project investigates the sensory modalities of memory beyond the digital.  In addition to the terabytes of data that we are sure to bring with us on this long journey, what other forms of communication and connection might we invent for an extraterrestrial future? Olfaction has been shown to have strong ties to emotion and memory. This project consists of a sensory token for astronauts that contains the unique scent of three memories of Earth: that of a loved one, that of a home, and that of a natural resource. Chemically, the fragrances are embedded in a special polymer designed to contain and release the scents over a long period of time. Through a dial, the user can choose to program and re-live one of three Earth experiences through an immersive olfactive experience. As an emotional time capsule, this project is akin to the Voyager Golden Record, but for precious smells. An exploration in the use of science for emotional ends, this project investigates alternative biological and perceptual modalities of communication and memory through olfaction.",2017-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/uuid/435a08bb-379f-4fe5-8947-aa5269cc8ca1,220022,,2023-09-06T22:32:12.039Z,['wonder@media.mit.edu'],True,memories-of-earth,2017-11-06,"['design', 'art', 'space', 'zero-gravity']",Smells for Space: Olfactory Timecapsule for Earthly Memories,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2017-11-07T16:11:07.077Z,"Today, the environments that humans occupy in space are designed for survival. Humans are carefully shuttled to and from space, and during their relatively short stays, they are provided with minimum supplies to remain alive and able to perform experiments. As we begin to plan less for short visits and more for life in space (such as a six to eight month trip to Mars and beyond) the question becomes: What does human culture look like in space?Nicole L'Huillier and Sands Fish decided to explore how design and creativity might evolve as we begin to do more than merely survive in space. The Telemetron is a unique mode of musical performance that takes advantage of the poetics of zero gravity, and opens a new field of musical creativity. The project attempts to expand expression beyond the limits of earth-based instruments and performers. Leveraging sensors, data transmission and capture (for performance after flight), as well as their experience as composers and performers, Sands and Nicole explore a new body language for music. The Telemetron was played for the first time during the inaugural Media Lab Space Exploration Initiative's Zero G flight. This instrument is a clear dodecahedron chamber that contains customized ""chimes"" containing gyroscopes. The chimes emit their telemetry as they spin and collide. Sensors record the position, direction, and spin of each chime. These elements create the composition. The performers play the instrument by moving it in space, shaking it, colliding it. The performance can be recorded to be experienced on earth or used as a live instrument during future space flights. The instrument can be played inside space craft or in the vacuum of space without the benefit of sound waves.Recorded as a beautiful audio-visual experience, this experiment opens the doors for new forms of creative expression, and brings the magic of space to musicians. We hope to reach beyond the utilitarian, and toward the inspiring.",2021-12-31,"['opera-of-the-future', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2017/11/22/ZG450_427-large.jpg,237377,,2023-09-06T22:31:44.621Z,"['nicolelh@media.mit.edu', 'sands@media.mit.edu', 'thomassl@media.mit.edu']",True,the-telemetron,2017-08-01,"['robotics', 'design', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",The Telemetron,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-11-08T09:15:24.095Z,"Can we enable social connectivity between astronauts and people on Earth through an embodied agent?Astronauts actively communicate with their families on Earth through several forms of digital and voice communication, including phone calls, video conferencing, and email. However, as astronaut Scott Kelly describes in the Time documentary A Year in Space, the experience can be incredibly isolating despite these affordances. Shortcomings of these modes of communication lie in their inability to translate emotion effectively, failure to facilitate shared experiences, lack of physical feedback, and the resulting perceived lack of control. The psychological effects of these limitations can become heightened over time, and peak during moments when the family on Earth is in need of support. As space becomes more accessible, it is important to consider how we design for social connectivity between people on Earth and in space.What if embodied social agents, besides being the astronaut's personal sidekick, could help to facilitate a more connected experience between space and Earth? From C3PO in Star Wars to Rosie the Maid in The Jetsons, the idea of robots in space has been well explored in fiction universe. On Earth, embodied social agents have been shown to be effective in providing companionship, relieving stress and anxiety, and fostering connection among people. In this project to send an embodied social agent into zero gravity, we explore several key themes relating to the potential for this technology to offer better connection and shared experience between astronauts and people on Earth.While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.",2018-12-31,"['personal-robots', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2017/11/20/image1_CRYrJvX.JPG,237774,,2017-12-05T19:04:57.085Z,"['nikhita@media.mit.edu', 'igrover@media.mit.edu', 'pcuellar@media.mit.edu', 'haewon@media.mit.edu']",True,social-robots-in-zero-gravity-scenarios,2017-10-01,"['human-machine-interaction', 'interfaces', 'social-robotics', 'space', 'zero-gravity']",Social Robots in Space: Initial Explorations,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-11-09T19:10:52.861Z,"This project seeks to examine the effects of altered gravity on an individual’s physiology during parabolic flight. Specifically, we will collect flight participants’ heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea. The results of this research will have both significant scientific and civilian value. To our knowledge, this experiment will be the first to investigate the new Multiple Arousal Theory in the context of motion sickness, as well as altered gravity. This theory was developed in the Affective Computing group at the MIT Media Lab and examines asymmetry in skin conductance signals from right and left wrists as differing metrics of emotional arousal and intensity. The parabolic flight configuration provides an inimitable circumstance to systematically analyze the evolution of these signals over the course of the repeated parabolic flight path. For example, we expect to see globally heightened stress and emotional arousal on the first pass, with maximal skin conductance peaks from both wrists just before the first moment of weightlessness. We expect these peaks to monotonically decrease over time with each pass, but to remain more elevated (relative to an individual’s baseline) for participants experiencing more self-reported nausea during flight. For individuals not experiencing extreme nausea, we expect to see a much higher skin conductance signal from their right wrists compared to their left (for right-handed participants) during the first few passes, with this difference decreasing steadily as the participant habituates to the flight pattern and sensations. Note that NASA and other researchers—including the Boston-local scientists at the Ashton Graybiel Spatial Orientation Lab at Brandeis University—have investigated spatial orientation and motion sickness, but they are just beginning to add the use of physiological sensors to their work. Not only does this demonstrate that the proposed experiment is at the forefront of scientific inquiry, but it also facilitates potential collaboration with world-renowned experts in the Boston area!In addition to sensor data, we intend to collect pre- and post-flight surveys recording participant reactions to different levels of gravity, including points at which they experienced nausea or discomfort. Pre-flight surveys will include nausea sensitivity metrics, designed to determine how likely a person is to feel nausea (i.e., separating those who feel carsick on a drive through town versus those who approach rollercoasters without hesitation). It will also ask about each participant’s feelings of anxiety, nausea, and excitement in anticipation of flying. Note that while these feelings may be experienced simultaneously, each one has a different effect on one’s physiology. After the flight, we will ask participants to rank which sections of the flight (e.g., beginning, middle, end) prompted the greatest sensations of anxiety, nausea, and excitement and to what degree. We will also annotate the flight video recordings to denote periods of high anxiety, nausea, or excitement.Then, we will use the survey, annotation, and sensor information to build a model that predicts when an individual might experience distress in altered gravity environments. This aspect of the study will leverage our research group’s unique expertise building machine learning algorithms for physiological data, but the results could have widespread impact. For example, such a system could be deployed to space travelers to help them monitor their physiology and anticipate or prevent feelings of discomfort during flight. As access to space travel becomes more pervasive, it is critical to understand the physiological effects of altered gravity on a population that does not solely include astronauts or specially trained individuals. Our models, along with the use of low-cost, commercially available sensors, would enable “space hacking” by tourists and other non-technical personnel, allowing them to measure and track their biosignals to achieve optimal wellness during space travel. ",2019-07-30,"['affective-computing', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2018/01/10/20171222_170402.jpg,238175,,2022-05-12T17:36:31.245Z,"['ktj@media.mit.edu', 'picard@media.mit.edu', 'sataylor@media.mit.edu', 'sfedor@media.mit.edu', 'jaquesn@media.mit.edu']",True,physio-freefall,2017-05-11,"['bioengineering', 'consumer-electronics', 'health', 'wearable-computing', 'sensors', 'space', 'wellbeing', 'zero-gravity']",Physio FreeFall,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2017-11-12T21:00:06.913Z,"Cognimates is  a platform where parents and children (7-10 years old) participate in creative programming activities in which they learn how to build games, program robots, and train their own AI models. Some of the activities are mediated by embodied intelligent agents which help learners scaffold learning and better collaborate. Learn more about our research, projects, and learning guides.Conversational agents and connected toys are becoming common in homes. Increasing exposure to ""intelligent"" technology raises important questions about the ways that children understand it and how they could learn with and from it. Embodied intelligent agents, such as social robots, afford longer-term engagement in the home for children and their families .  Building on the prior experience in the Personal Robots group of designing social robots for nurturing children's curiosity and learning, we built a platform where children and parents can learn to program with embodied intelligent agents which in turn become learning companions (Cognimates). The goal is to enable  learners  to interact with a social robot but also program it, train it to remember and learn things over time, and have  reflective conversations with their peers prompted by it. Why, how, and when can embodied intelligent agents support children and parents to learn via reflective teaching? What are the new intergenerational learning pathways that Cognimates could facilitate? How can these future learning companions be integrated into various learning applications and what are the generalizable design considerations?  In this research project we are addressing these questions by allowing children and parents to use a visual programming interface to control and customize an embodied intelligent agent. Demo video",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/04/21/website-banner.png,239352,,2019-03-27T16:43:04.722Z,"['sdruga@media.mit.edu', 'cynthiab@media.mit.edu']",True,cognimates,2017-11-12,"['robotics', 'human-computer-interaction', 'artificial-intelligence', 'cognition', 'kids', 'learning-teaching', 'open-source', 'computer-science', 'makers', 'technology']",Cognimates: Collaborative creative learning with embodied intelligent agents,PUBLIC,http://www.drugastefania.com/,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-11-13T19:01:09.206Z,"The future of human life outside of Earth will heavily depend on the ability to fabricate and manufacture things. Yet fabrication in space poses numerous difficulties. Some of these challenges include storage space in vehicles, availability of raw materials, lack of machines, and shortage of manpower.  Other challenges in fabricating objects in space are simply a result of  the  different  physical environment;  the lack of gravity introduces unexpected material  behaviour, as other forces aside from gravity become dominate. Surface tension, for example, becomes very dominant in determining the shape of liquid materials and adhesion between liquids and other materials also plays a more dominant role. Because of the reasons stated above, 3D printing in space was conceptually limited to fused deposition modeling (FDM) technologies, which are less susceptible to problems resulting from the harsh conditions. Liquid- or powder-based printing technologies are assumed to be very problematic for space fabrication because of liquid behavior in microgravity conditions. On the other hand, FDM technologies have a lot of limitations such as the inability to create transparent  structures or layerless shapes with defined smooth curvatures.In this experiment, we would like to harness surface tension's dominance in liquid behavior under zero gravity conditions  to create various controllable and accurate, layerless and transparent geometries using UV-curable resin. The resin will be hardened using a high-power UV light source.We will focus on rapid fabrication (in under 17 seconds) of the following shapes:Shapes that are hard to make on Earth without special machinery, e.g., perfect lenses.Shapes and materials that could be necessary in the space environment and are hard to make with existing methods available in space, such as ball bearings.",2018-05-30,"['tangible-media', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2017/11/19/AmosZG.jpg,239748,,2020-06-09T17:21:51.477Z,['amosg@media.mit.edu'],True,lift-liquid-resin-injection-fabrication-technology-in-micro-gravity,2017-09-01,"['manufacturing', 'space', 'zero-gravity']",Liquid resin fabrication in microgravity,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2017-11-14T12:03:52.008Z,"On Collective Debate, users take a test of their morality, then debate an artificial agent regarding a controversial claim: that differences in professional outcomes between men and women arise from bias as opposed to biology. Users indicate how much they agree with the claim, then they exchange arguments with the agent (who assumes the opposite position). After the debate, users are asked to re-evaluate their position. The artificial agent is trained to select arguments that nudge the user to become more moderate.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2018/02/13/hero_image.png,240141,,2018-02-21T18:31:02.278Z,['annyuan@media.mit.edu'],True,collectivedebate,2017-11-14,"['civic-media', 'collective-intelligence']",Collective Debate,PUBLIC,http://collectivedebate.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2017-11-14T18:32:30.771Z,The Scratch Team invited young people from around the world to create Scratch projects designed specifically to be played in zero gravity. Scratch members submitted over two hundred projects to this special initiative.  Eric Schilling from the Scratch Team deployed a diverse collection of these projects on the Space Exploration initiative's inaugural research flight in zero gravity. ,2020-05-31,"['lifelong-kindergarten', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2017/11/14/ScratchInSpace-CoverPhoto-01.png,240152,,2022-09-22T01:24:24.989Z,"['christan@media.mit.edu', 'eschill@media.mit.edu']",True,scratch-in-space,2017-10-01,"['computer-science', 'space', 'zero-gravity']",Scratch in Space,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2017-11-15T21:20:23.691Z,"The brain uses space to index, organize, and retrieve memories. However, our sense of space depends on our perception of gravity. We plan to test and understand the effect of altering gravity on human memory. Our experiment consists of a virtual reality experience that exposes the user to a sequence of small random mazes. We will compare the results of the experiment under different gravitational conditions.",2017-12-31,"['fluid-interfaces', 'space-exploration']",https://dam-prod2.media.mit.edu/uuid/03502fd7-afd7-4fb0-9f6f-3b04a295f564,240564,,2023-09-06T22:30:37.878Z,['mmv@media.mit.edu'],True,vr-maze-in-zero-gravity,2017-08-01,"['space', 'zero-gravity']",VR Maze in Zero Gravity,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-11-16T00:50:14.799Z,"The Orbit Weaver Suit for zero gravity was designed by Media Lab Director's Fellow Andrea Lauer in collaboration with Xin Liu. The design was inspired by a drawing Jordan Piantedosi made for Xin Liu, in which she is in a suit and casting strings out into space. The drawing is a reference to Orbit Weaver, a performance piece Xin created to test in a zero-gravity environment in November 2017.",2017-12-31,"['directors-fellows', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2017/11/29/ZG450_057.jpg,240592,,2023-09-06T22:34:16.409Z,"['alauer@media.mit.edu', 'xxxxxxin@media.mit.edu']",True,orbit-weaver-suit,2017-10-17,"['design', 'art', 'fashion', 'space', 'technology', 'zero-gravity']",Orbit Weaver Suit,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-11-16T15:30:48.081Z,"Expansion microscopy (ExM) is a new imaging modality developed by MIT Media Lab's Synthetic Neurobiology group. ExM allows biomolecules to be imaged at nanoscale resolution on conventional, high-speed, diffraction limited optics by synthesizing a swellable polymer network within the sample and thereby physically separating objects of interest isotropically. In the past, we were able to expand the brain tissue 4-20 times in the lateral dimension, achieving 20-90 nm resolution with affordable optics, including a webcam – achieving ~90nm resolution. Considering that existing optics needed for such high resolutions require bulky, expensive parts subject to misalignment and irreversible damage in a physically demanding condition like a spaceship, ExM has the potential to be the most affordable solution for nanoscale imaging of biomolecules in the reduced gravity environment. We aim to demonstrate its claimed feasibility through the project.",2017-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/uuid/196ba69c-fb5e-4454-af7f-02a89bc2c56b,240998,,2023-09-06T22:35:41.856Z,['jskang@media.mit.edu'],True,nanoscale-imaging-of-biomolecules-in-zero-g,2017-06-01,"['imaging', 'space', 'biotechnology', 'zero-gravity']",Nanoscale Imaging of Biomolecules in Zero G,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-11-17T00:56:32.504Z,"Political constitutions describe the fundamental principles by which nation-states are governed, the political and legal state institutions, the powers, procedures, and duties of those institutions, and the rights and responsibilities of individuals. How do these constitutions develop over long periods of time? What is the interplay between colonial history and global, time varying trends in determining the characteristics of a country's constitution? We explore these questions using new techniques of computational social science.",2020-07-07,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/12/11/magna_carta.jpg,241043,,2020-07-23T18:47:38.766Z,"['irahwan@media.mit.edu', 'cebrian@media.mit.edu']",True,evolution-of-the-social-contract,2016-07-01,"['politics', 'social-science', 'collective-intelligence']",Evolution of the Social Contract,PUBLIC,http://www.alexrutherford.org/constitutionology/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2017-11-17T13:43:01.380Z,"Structurally, zero gravity means that we do not have to contend with architecture's greatest arch-nemesis, gravity. This opens up a new world of possibilities where we can deploy structures that no longer have to counteract/resist gravitational force. We would like to explore new forms of rapid inflatable prototyping. Most importantly, this prototype explores surfaces utilizing materials that would normally fail on Earth, yet flourish in zero gravity.This year the MIT Media Lab's City Science group had an opportunity to think of architecture at the scale of the body that was literally out of this world. These are the results.",2019-12-31,"['space-exploration', 'city-science']",https://dam-prod2.media.mit.edu/x/2017/11/20/IMG_6159.JPG,241437,,2023-09-08T14:46:18.545Z,"['csmuts@media.mit.edu', 'kapeloni@media.mit.edu']",True,spatial-flux,2017-06-01,"['design', 'architecture', 'art', 'human-machine-interaction', 'sensors', 'space', 'zero-gravity']",Spatial Flux: Body and architecture in space,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2017-11-17T15:47:32.958Z,"Gravity anchors all existence on Earth.It pulls a chaotic world to one single point in every moment of life. Even though gravity is everywhere and unending, for most of the universe, vast empty space dominates, leaving us free from gravity’s tether. Is the weightless state a moment of true autonomy, or does the ungrounded body simply lose control?Orbit Weaver is a series of imaginative interactions with moving bodies in zero gravity. The project aims to understand, create, and share the beautiful and sentimental moments of being weightless and lost in outer space. ",2017-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/03/12/ZG450_278.jpg,241453,,2023-09-06T22:34:41.412Z,['xxxxxxin@media.mit.edu'],True,orbit-weaver,2017-09-01,"['design', 'agriculture', 'architecture', 'art', 'manufacturing', 'performance', 'space', 'technology', 'engineering', 'industry', 'zero-gravity']",Orbit Weaver,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-11-19T17:45:06.005Z,"Overview: How will we build the coming generations of Space Architecture—the modules, space ships, and space stations that will ensconce our space-faring species? Can we move beyond the 20th century paradigm of cylindrical tubes in orbit, to geodesic dome habitats,  to microgravity concert halls, to space cathedrals?The next generation of space architecture should delight, inspire, and protect humanity for our future in the near, and far, reaches of space. The future of human habitation in space lies in self-assembling, adaptive, and reconfigurable structures. Rather than transporting fixed, rigid habitation modules and risking astronaut Extravehicular Activities (EVAs) during construction, we can lower payload weight, reduce assembly complexity, and revolutionize space-structure modularity by relying on reconfigurable, self-assembly. We are currently undertaking a multi-year research effort to study, characterize, prototype and test ""TESSERAE"":  Tessellated Electromagnetic Space Structures for the Exploration of Reconfigurable, Adaptive Environments.  Each TESSERAE structure is made from a set of tiles. These tiles are tuned to autonomously self-assemble into a particular geometry—in our initial prototypes, we have focused on the buckminsterfullerene (20 hexagonal tiles, 12 pentagonal tiles). Each tile at minimum includes  a rigid outer shell, responsive sensing and control code for bonding diagnosis, electro-permanent magnets for dynamically controllable bonding actuation, and an on-board power harvesting and power management system. Habitat-scale TESSERAE tiles will also include clamping and sealing for pressurization. Tiles are released in microgravity testing environments to quasi-stochastically self assemble. The “TESSERAE” name and multi-tile structure hearken to the small, colored tiles used in Roman mosaics, where many standard pieces, or “tesserae,” interlock to form a larger creation. We make this reference to ancient history, when designing an artifact of our space exploration future, to tie architectural elements together across scales and across millennia.TESSERAE will function as multi-use, low-cost orbiting modules that supply a critical space infrastructure for the next generation of zero gravity habitats, science labs, staging areas for on-surface exploration, and more. TESSERAE should be thought of as flexible and reconfigurable modules to aid in both small-scale, agile mission operations and grand-scale, iteratively expanding space architecture. Our mission concept focuses on supporting LEO, Lunar and Mars operations, with dual-use orbit and surface capability: Tiles are packed flat and condensed for launchTiles are released after orbit insertion to quasi-stochastically self-assemble into the target geometry, while floating in microgravityOnce assembled, the structure can be reconfigured on demand (e.g., where a berthing port tile was needed yesterday, a cupola tile can be replaced tomorrow)Tiles can be disassembled entirely, packed flat again in an EDL (Entry, Descent and Landing) vehicle, and then deployed and ""snap-assembled"" with astronaut assists on the lunar or martian surfaceMultiple, interlocking TESSERAE can serve as a larger volume orbiting base (e.g.,  ""MOSAIC"": Mars Orbiting Self-Assembling Interlocking Chambers), in addition to supporting the coming waves of space tourists and space hotels in low Earth orbit.",,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2018/11/26/Space_Rendering_4K.jpg,242238,,2023-09-07T22:39:34.231Z,['aekblaw@media.mit.edu'],True,tesserae-self-assembling-space-architecture,2017-06-01,"['robotics', 'design', 'agriculture', 'architecture', 'art', 'manufacturing', 'sensors', 'computer-science', 'space', 'technology', 'alumni', 'engineering', 'industry', 'fabrication', 'zero-gravity']",TESSERAE: Self-Assembling Space Architecture,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2017-11-30T17:15:48.672Z,"What would your city look like after a disaster? Deep Empathy is a collaboration between the Scalable Cooperation group and the UNICEF Innovation Office to pursue a scalable way to increase empathy. The brutal, six-year-old Syrian war has affected more than 13.5 million people in Syria , including 80% of the country's children—8.4 million young lives shattered by violence and fear. Hundreds of thousands of people have been displaced and their homes destroyed. But people generate a response that statistics can't. And technologists—through tools like AI—have opportunities to help people see things differently. We wondered: ""Can AI increase empathy for victims of far-away disasters?"" This question led us to create a provocation for the research community to examine how AI can create narratives to tell the stories of some of the world's most intractable problems.",2018-05-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/12/04/boston.gif,246619,,2017-12-04T19:54:02.578Z,"['pinary@media.mit.edu', 'nobradov@media.mit.edu', 'cebrian@media.mit.edu', 'dubeya@media.mit.edu', 'irahwan@media.mit.edu']",True,deep-empathy,2017-12-01,"['artificial-intelligence', 'social-media', 'imaging', 'machine-learning']",Deep Empathy,PUBLIC,http://deepempathy.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2017-12-06T02:43:08.391Z,"The 2009  DARPA Network Challenge -- retrieving the locations of ten balloons placed at undisclosed positions in the US ---brought practical attention to the problems of social mobilization and information acquisition in a networked environment. The MIT Media Laboratory team won the challenge by acting as the root of a query incentive network that unfolded all over the world. However, rather than adopting a fixed-payment strategy, the team designed a novel incentive scheme based on 1/2-split contracts. Under such incentive scheme, a node u who does not possess the information can recruit a friend v through a contract stipulating that if the information is found in the subtree rooted at v, then v has to give half of her own reward back to u.Due to its empirical and theoretical superior performance, which can be proven mathematically, split contracts scheme has been used around the globe in a number of crowdsourcing competitions concerning information acquisition in a strategic networked environment. Here, we document the most prominent ones.",,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2017/12/05/Balloon10_OZd4jn1.jpg,248573,,2017-12-06T03:00:35.395Z,[],False,mobilise,,[],Time-Critical Social Mobilisation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0
False,2017-12-07T02:38:28.751Z,"Our eyes are our our mind's window to the external world. Vision is the primary way we sense our environment, and is a reliable indicator of our attention. As sensors, eyes evolved over millions of years to be fast, precise, and accurate, especially for tracking visual elements of interest (targets), like predators, prey, or baseballs. However, from an HCI perspective, these amazing abilities are also desirable actuators as well as sensors. What if the eyes can not only be a passive window of focus, but an active spatial cursor to indicate our target of focus? Also, what if we add lasers?",2015-12-16,['tangible-media'],https://dam-prod2.media.mit.edu/x/2017/12/06/laserface2.001.jpeg,248592,,2017-12-07T04:43:36.579Z,"['djfitz@media.mit.edu', 'novysan@media.mit.edu']",True,laser-face,2015-10-01,[],LaserVision,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-12-07T04:15:33.101Z,"We present a stiffness-changing interface based on a magneto-rheological (MR) fluid. The device consists of a material surface with electromagnetically induced visco-elasticity, which acts as a proxy for stiffness during tangible interaction with the material. We present several advantages of this enabling technology and outline potential applications and routes for future development.",2015-12-15,['tangible-media'],,248600,,2017-12-07T04:43:01.290Z,"['andresc@media.mit.edu', 'sareen@media.mit.edu', 'udayan@media.mit.edu', 'djfitz@media.mit.edu', 'ishii@media.mit.edu']",True,influx,2015-11-01,['interfaces'],inFlux,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-12-08T21:02:29.893Z,"Empathy is a core human skill. From early stages of our lives, being able to understand and behave with empathy is fundamental to our social experience. Research in the field of social robotics suggests that given a set of behaviors from a social robot, a child can perceive this agent as empathic. In this project, we explore a novel approach to modeling empathy in children using a social robot. Two social robots were programmed to have conversations containing interactions depicting empathic and non-empathic behaviors. Children were provided with opportunities to act on these interactions as well as to comment on the robot's behavior afterward.",2020-12-31,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/02/13/IMG_5442.JPG,248641,,2020-02-05T18:54:10.820Z,"['cynthiab@media.mit.edu', 'pcuellar@media.mit.edu']",True,children-s-perception-of-empathy-in-robot-robot-scenarios,2017-09-14,[],The role of social robots in fostering human empathy,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2017-12-11T20:01:35.440Z,"There are over one million registered charities in the United States alone, and many more worldwide. How do you choose among them? MyGoodness is a simple game that helps you understand how you give. In the game, you will make 10 giving decisions. Each decision is between two choices, and you tell us which you prefer.At the end of the game, we give you a summary of your ‘goodness’ and how it compares to others. You can share that feedback with whomever you would like.",2020-07-01,[],https://dam-prod2.media.mit.edu/x/2017/12/11/Pieter_il_Giovane_Bruegel_The_Seven_Acts_of_Charity.jpg,248654,,2020-07-23T18:39:10.118Z,"['irahwan@media.mit.edu', 'awad@media.mit.edu']",True,mygoodness,2017-12-11,"['artificial-intelligence', 'social-media']",MyGoodness,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2017-12-12T01:25:33.544Z,"Pain is a subjective experience commonly measured through patients' self reports. Unfortunately, self-report measures only work when the subject is sufficiently alert and cooperative, and hence they lack utility in multiple situations (such as during drowsiness) and patient populations (such as patients with dementia or paralysis). To circumvent the limitations of pain self-reports, in this project we are developing automatic methods for pain estimation based on physiological signals  and/or facial expressions.",2019-10-30,['affective-computing'],https://dam-prod2.media.mit.edu/uuid/58fe5028-7a8b-4c5c-bf60-a4bd57e4dee5,248663,,2022-05-12T17:32:36.884Z,"['dlmocdm@media.mit.edu', 'orudovic@media.mit.edu', 'picard@media.mit.edu']",True,pain-measurement,,['affective-computing'],Machine Learning for Pain Measurement,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-12-12T16:15:53.952Z,"Many drugs, such as monoclonal antibodies, are administrated using parenteral delivery devices via subcutaneous injections. Unfortunately, needle phobia, anxiety before and during needle insertion and pain during injections are key aspects that lead to poor therapeutic compliance and prevent wider applicability and acceptance of this technology across patient groups. Therefore, in order to improve patient experience, traditional pain scores using a visual analog scale (VAS) or other similar techniques have been used to compare and investigate different subcutaneous injection methodologies. However, they are subjective and it is difficult to power a clinical study to show significant differences in pain.In this study, we propose to use electrodermal activity (EDA), heart rate variability (HRV), and facial expression analysis as potential endpoints to determine quantitative pain scores during the injection process, together with other secondary endpoints such as wellness aspects of patients (e.g. sleep quality). Therefore, the objective of this study is to evaluate these endpoints in subcutaneous injections for different injection methodologies (consisting of different dose volumes, flow rates, needle gauges and injectate viscosity) in a clinical setting in humans. The data will be used to understand pain upon injection and see if there is any correlation between traditional pain scores (e.g. visual analog scale) and our proposed endpoints.",2019-10-30,['affective-computing'],https://dam-prod2.media.mit.edu/uuid/617abb5a-9328-4feb-a710-9b1663c7a522,248673,,2022-05-12T17:37:00.658Z,"['dlmocdm@media.mit.edu', 'picard@media.mit.edu']",True,injection-study,,[],Injection Study,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2017-12-13T00:18:19.350Z,"The explosion of mHealth in both abundant and resource-constrained countries is both a cause for celebration and for concern. While mHealth clearly has the potential to deliver information and diagnostic decision support to the poorly trained, it is not appropriate to simply translate the technologies which the trained clinician uses into the hands of non-experts. In particular, it is important that the explosion of access does not lead to a flooding of the medical system with low quality data and false negatives. Clearly for mHealth to expand, a paradigm shift in how data is analysed must occur. Data must be vetted at the front end, using automated algorithms, to provide robust filtering of low quality data.This project addresses the specific problem of vetting the quality of electrocardiograms (ECGs) collected by an untrained user in ambulatory scenarios using smartphone devices.",2019-10-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2017/12/12/Hypokalemia-1.3.png,248697,,2022-05-12T17:38:17.098Z,['dlmocdm@media.mit.edu'],True,ecg-ambulatory,2017-12-01,[],Electrocardiogram collection in noisy ambulatory environments with Android smartphone devices,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-12-13T05:36:35.080Z,"Often, we neglect to see the city as living, complex, and dynamic. However, shrouded by its masses of concrete and steel lie unique ecosystems awaiting exploration and inquiry. Now more than ever, as urban populations boom and city boundaries expand, there exists a pressing need to understand urban ecology, the environmental impact of cities and their development, and  the importance of designing in concert with nature, rather than against it. Yet, in spite of this, curricula for youth focused on ecology canonically instruct on topics which apply exclusively to natural, undeveloped systems—even in metropolitan schools where access to “nature” is difficult or a privilege. City as Classroom, City as Laboratory began as a series of educational workshops (2018) for youth in the Greater Boston area, ages 8 to 14, to simultaneously address the lack of nature-conscious design education, as well as our failure to leverage cities as invaluable resources for exploratory learning . Throughout the sessions, students explored Boston's ""urban wilds"" in order to become enveloped in the hybrid ecology of dynamic and ever-growing. The curriculum emphasizes low-cost, hands-on approaches for culturing ecological identity such that students are able to recognize and appreciate the complex ecological processes ongoing in urban contexts, and thus understand cities as novel ecosystems.The goal of this educational framework is to inspire urban youth to champion future endeavors related to the environmental and political spheres (in efforts related to conservation, wildlife protection, sustainability, infrastructure development) and to see the city as a forum for intervention.To learn more about the pilot curriculum and its deployment, check out this blogpost. Urban Ecology in Public Libraries Beginning in fall of 2019, the City as Classroom curriculum will be modified for deployment in public libraries as part of Media Lab's Public Library Innovation Exchange (PLIX) program. We are currently planning a pilot series which will focus on the development of landscape literacy over four sessions.For questions, please contact Avery Normandin (ave@media.mit.edu).",,['sculpting-evolution'],https://dam-prod2.media.mit.edu/x/2017/12/13/Screen Shot 2017-12-12 at 7.46.09 PM.png,248713,,2019-06-18T17:46:20.567Z,"['ave@media.mit.edu', 'devora@media.mit.edu']",True,city-as-classroom,2018-05-01,"['ecology', 'learning-teaching', 'storytelling', 'urban-planning', 'engineering', 'climate-change']","City as Classroom, City as Laboratory",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2017-12-21T17:55:02.394Z,"Time of Flight 3D cameras like the Microsoft Kinect are prevalent in computer vision and computer graphics. In such devices, the power of an integrated laser is amplitude modulated at megahertz (MHz) frequencies and demodulated using a specialized imaging sensor to obtain sub-cm range precision. To use a similar architecture and obtain micron range precision, this paper incorporates beat notes. To bring telecommunications ideas to correlation ToF imaging, we study a form of ""cascaded Time of Flight"" that uses a Hertz-scale intermediate frequency to encode high-frequency pathlength information. We show synthetically and experimentally that a bulk implementation of opto-electronic mixers offers: (a) robustness to environmental vibrations; (b) programmability; and (c) stability in frequency tones. A fiberoptic prototype is constructed, which demonstrates three micron range precision over a range of two meters. A key contribution of this paper is to study and evaluate the proposed architecture for use in machine vision.",,['camera-culture'],,265391,,2019-04-19T17:53:06.677Z,"['raskar@media.mit.edu', 'achoo@media.mit.edu']",True,high-frequency-lidar-using-beat-notes,2017-12-21,['imaging'],High-frequency LIDAR using beat notes,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2017-12-26T00:41:17.138Z,"We only perceive a tiny sliver of the world around us. We are constrained by what our senses can process. These senses evolved to react to what is immediately important for our survival. Our technological development has outstripped the pace at which our physical senses evolve. We do not have access to things such as the electromagnetic spectrum at 2.5 GHz, even though it is relevant to the day to day life of most of us. We are poor at perceiving things such as changes in the chemical composition of the air we breathe, even though it is critical for our long term survival as a species.  Can we augment a stroll through nature with sensory experiences usually outside the range of our perception? Haptic Footprints explore using vibrotactile rendering for this purpose.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2017/12/25/HapticFootprintB.jpg,266952,,2018-01-08T22:47:19.225Z,['paul_str@media.mit.edu'],True,haptic-footprint,2017-10-12,"['augmented-reality', 'perception', 'data-visualization']",Haptic Footprint,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-01-08T13:39:49.044Z,STARRING JADE PHOENIX & JADE RENEGADEIN COLLABORATION WITH ORGASMIC CREATIVE,2017-06-01,[],https://dam-prod2.media.mit.edu/x/2018/01/08/hmd-1.tif,272372,,2018-01-08T19:30:24.940Z,[],True,housewives-making-drugs,2016-09-01,[],Housewives Making Drugs,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1
False,2018-01-17T18:31:56.861Z,"Recent advances in medications for neurodegenerative disorders are expanding opportunities for improving the debilitating symptoms suffered by patients. Existing pharmacologic treatments, however, often rely on systemic drug administration, which result in broad drug distribution and consequent increased risk for toxicity. Given that many key neural circuitries have sub-cubic millimeter volumes and cell-specific characteristics, small-volume drug administration into affected brain areas with minimal diffusion and leakage is essential. We report the development of an implantable, remotely controllable, miniaturized neural drug delivery system permitting dynamic adjustment of therapy with pinpoint spatial accuracy. We demonstrate that this device can chemically modulate local neuronal activity in small-animal (rodent) and large-animal (nonhuman primate) models, while simultaneously allowing the recording of neural activity to enable feedback control. ",,"['conformable-decoders', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/uuid/0ea15bba-8c0d-442b-9e88-757a914dc498,276379,,2023-09-07T23:34:31.246Z,['canand@media.mit.edu'],True,miniaturized-neural-system-for-chronic-local-intracerebral-drug-delivery,2018-01-24,"['design', 'bioengineering', 'health', 'neurobiology', 'sensors', 'technology', 'biotechnology', 'engineering', 'wellbeing', 'microbiology', 'microfabrication']","Miniaturized Neural System for Chronic, Local Intracerebral Drug Delivery (MiNDS)",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-01-17T20:44:28.047Z,"​State-of-the art liquid handling systems are generally pump-driven systems connected with valves and tubes. These systems are manually assembled, expensive, and unreliable. With the growth of the genomic and drug industries, we are moving toward increasingly complex biological processes requiring very small volume liquid manipulation capability. Manually assembled mechanical systems do not scale to parallel manipulation of large amounts of small volume liquids. However, the electronics industry has demonstrated how to build robust integrated systems for information manipulation. With this as our motivation, we look toward electronics and integrated circuits to bring miniaturization, complexity, and integration to enable the next generation of biology.",2017-05-31,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/cabb668b-78c8-4b0d-ae3e-500e05624cfe,276389,,2020-06-09T17:35:12.604Z,"['udayan@media.mit.edu', 'ishii@media.mit.edu']",True,programmable-droplets,2016-01-01,"['design', 'human-computer-interaction', 'art', 'human-machine-interaction', 'biotechnology']",Programmable Droplets,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-01-18T22:29:10.537Z,"ATENÇÃO: Saiu o resultado do Desafio Aprendizagem Criativa Brasil 2018! Clique aqui para conhecer os fellows e os projetos selecionados!----O Desafio Aprendizagem Criativa Brasil é uma iniciativa da Fundação Lemann e do MIT Media Lab que visa fomentar a implementação de soluções inovadoras – novas tecnologias, produtos e serviços – que ajudem a tornar a educação brasileira mais mão na massa, significativa, colaborativa e lúdica.O Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de Aprendizagem Criativa, especialmente no que se refere a projetos mão na massa envolvendo programação e construção no mundo físico, em escolas públicas (de Educação Infantil ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.Os representantes dos projetos selecionados ganharão uma Creative Learning Fellowship para ajudar a implementar seu trabalho. As inscrições vão até o dia  9 de fevereiro e devem ser feitas única e exclusivamente através do formulário abaixo.Clique aqui para a chamada de projetos completa.Clique aqui para o formulário de inscrição.Clique aqui para respostas às perguntas mais frequentes.Atenção:  esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no  fórum da Rede Brasileira de Aprendizagem Criativa.",2018-12-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/01/11/22531183118_2a2f2332c3_o.jpg,276792,,2018-12-03T22:52:00.488Z,"['leob@media.mit.edu', 'mres@media.mit.edu']",True,desafio-aprendizagem-criativa-brasil-2018,2018-01-22,[],Desafio Aprendizagem Criativa Brasil 2018,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-01-22T16:05:58.084Z,"Cube Puzzles: A tangible platform for dynamic assessment of cognitive and psychomotor skillsCognitive and psychomotor assessments are a vital scientific tool; however, they are resource intensive, can be biased by examiner presence and interaction, and can disadvantage individuals with neurodevelopmental differences such as autism or ADHD. We present a new tangible user interface, Cube Puzzles, that dynamically tracks placement and orientation of colored wooden blocks. The system not only automates data acquisition and captures real-time learning progressions, but it also enables customizable, motivating feedback and synchronization with biosensors. We highlight the potential of the system with two user studies involving 22 participants. The first study (n=10) validates and visualizes the dynamic data capture of real-time user activity. The second study (n=12) explores Cube Puzzles as a gamified cognitive assessment platform by examining the effects of reinforcement on user performance, engagement, and physiological arousal. Our work has broad applications within educational and assessment domains, particularly for neurodiverse and elderly individuals.Part of the SPRING systemCube Puzzles is a new game-play module for SPRING. In this module, children move and orient colored blocks in order to match a displayed design. This module is inspired by the Block Design activity in the Wechsler Intelligence Scales (e.g., WAIS), quantifying visual-spatial orientation and motor skills. In free-play mode, Cube Puzzles also encourages open-ended design and play with geometric shapes and colors.",2021-08-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/01/22/IMG_6086.JPG,278107,,2022-05-12T17:43:06.348Z,['ktj@media.mit.edu'],True,cube-puzzles,2017-01-09,[],Cube Puzzles,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-01-22T20:42:30.701Z,"The Gender Shades project pilots an intersectional approach to inclusive product testing for AI.Algorithmic Bias PersistsGender Shades is a preliminary excavation of the inadvertent negligence that will cripple the age of automation and further exacerbate inequality if left to fester. The deeper we dig, the more remnants of bias we will find in our technology. We cannot afford to look away this time, because the stakes are simply too high.  We risk losing the gains made with the civil rights movement and women's movement under the false assumption of machine neutrality. Automated systems are not inherently neutral. They reflect the priorities, preferences, and prejudices—the coded gaze—of those who have the power to mold artificial intelligence.",2020-08-01,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2018/01/29/gendershades-01.png,278117,,2022-06-10T13:29:45.160Z,"['ethanz@media.mit.edu', 'joyab@media.mit.edu']",True,gender-shades,2017-01-01,"['design', 'computer-vision', 'art', 'artificial-intelligence', 'civic-media', 'civic-technology', 'food', 'government', 'neurobiology', 'wearable-computing', 'imaging', 'social-science', 'machine-learning', 'ethics', 'computer-science', 'biology', 'technology', 'biotechnology', 'racial-justice']",Gender Shades,PUBLIC,http://www.gendershades.org,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-01-23T02:27:32.010Z,Achieving a safe privacy-preserving information sharing environment for individualized care using blockchain-based technology in multiple use cases in the healthcare space.,,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2018/01/22/Blockchain-datastructure.png,278435,,2019-04-01T17:40:08.489Z,"['sandy@media.mit.edu', 'shada@media.mit.edu', 'ecstll@media.mit.edu']",False,healthy-blockchain,2017-09-01,"['health', 'networks', 'security', 'machine-learning', 'blockchain']",Healthy Blockchain,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-01-23T02:33:34.815Z,"Privacy-preserving mHealth application using Open Algorithm (OPAL) architecture to address urgent care challenges in Riyadh, Saudi Arabia.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2018/01/22/Heart_2QRcZnZ.png,278443,,2019-04-01T17:39:27.492Z,"['alotaibi@media.mit.edu', 'jobalbar@media.mit.edu', 'sandy@media.mit.edu', 'shada@media.mit.edu', 'hhao@media.mit.edu']",False,smart-2-opal,2017-11-01,"['health', 'networks', 'machine-learning', 'blockchain', 'trust']",SMART^2 OPAL,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2018-01-24T15:34:33.056Z,"The widespread synthesis of common organic building blocks in space could have biased life beyond Earth towards chemical similarities to life as we know it. Meteoritic exchange might also have produced shared ancestry, most plausible for Earth and Mars. We are building an instrument to target nucleic acids (DNA), called the Search for Extra-Terrestrial Genomes (SETG). Our approach integrates automated extraction and sequencing of DNA using the first commercially available nanopore device, the Oxford Nanopore Technologies MinION.A NASA team tested an earlier version of this technology during parabolic flight and on the international space station. Here we tested the latest iteration, with updated chemistry, flow cells, and software, in combination with high resolution acceleration and vibration measurement. Our goals are to 1) Quantify the impact of g-level and vibration on sequencing, and 2) perform sequencing during simulated Mars gravity for the first time.",2017-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/uuid/8d7e058d-8de8-443e-82fa-2a6f6034d952,278799,,2023-09-06T22:35:11.265Z,[],True,search-for-extra-terrestrial-genomes-setg,2017-08-01,"['genetics', 'space', 'zero-gravity']",Search for Extra-Terrestrial Genomes (SETG),PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-01-28T11:22:22.200Z,"Text-to-Motion generates a sequence of contingent robot animations to accompany the sentiment analyzed from an input sentence and its spoken audio. We trained a linear classifier to transfer learn our corpus of animated robot speech from DeepMoji network,  a long short-term memory (LSTM) network with an attention model trained on billion tweets. ",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/01/28/Screen Shot 2018-01-27 at 5.56.11 AM.png,280102,,2019-04-17T18:48:02.883Z,"['haewon@media.mit.edu', 'cynthiab@media.mit.edu']",True,text-to-motion-expressive-robot-motion-sequencing,2018-01-28,[],Text-to-Motion: Automatic sequencing of animative robot motions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-02-05T02:23:13.116Z,"The worlds of Design and Science are inextricably linked. Science is informed by Design. Design is a science. The interactions between the two are powerful, subtle, often unrecognized.The Journal of Design and Science (JoDS), a joint venture of the MIT Media Lab and the MIT Press, forges new connections between science and design, breaking down the barriers between traditional academic disciplines in the process. Targeting readers with open, curious minds, JoDS explores timely, controversial topics in science, design, and society with a particular focus on the nuanced interactions among them.Challenging both traditional academic silos and the established publishing practices associated with them, JoDS is hosted on a collaborative, open-access, open-review, rapid publication platform that actively invites community participation and supports rich, multimedia content. JoDS empowers authors and readers to engage in fruitful, ongoing and above all stimulating conversation and debate about their work and ideas.All JoDS articles are published under a CC-BY 4.0 License.",2020-07-01,['directors-office'],https://dam-prod2.media.mit.edu/x/2018/02/05/jods.jpg,282841,,2020-09-01T15:10:41.820Z,[],True,jods,2018-02-04,[],The Journal of Design and Science,PUBLIC,https://jods.mitpress.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-02-06T04:12:51.542Z,"We investigated the spread of all of the verified news stories–verified as either true or false–distributed on Twitter from 2006 to 2017.With a data set of roughly 126K stories tweeted by around 3M people over 4.5M times, we classified news as true or false using information from six independent fact-checking organizations that exhibited 95-98% agreement on the classifications. False information  spread significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political information than for false information about terrorism, natural disasters, science, urban legends, or financial information. We found that false information was more novel than true information, which suggests that people were more likely to share novel information. While false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust.Contrary to conventional wisdom, robots accelerated the spread of both true and false news at the same rate. This implies that false news spreads more than the truth because humans–not robots–are more likely to spread it.",2018-05-31,['social-machines'],https://dam-prod2.media.mit.edu/x/2018/03/03/spread-false-true-info-1.png,283179,,2018-06-08T00:37:43.310Z,"['soroush@media.mit.edu', 'dkroy@media.mit.edu']",True,the-spread-of-false-and-true-info-online,2016-01-01,['social-media'],The Spread of True and False Information Online,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-02-06T05:41:26.444Z,"In this project, we are motivated to address clinical trials issues using Blockchain technology. We propose, OPEN TrialChain, a privacy-preserving Blockchain-based data sharing infrastructure that uses open algorithms between stakeholder of the federation in the clinical trials ecosystem. OPEN TrialChain balances between the sharing of clinical data and the need for subject’s privacy protection by allowing queries on decentralized raw datasets from which it returns aggregated safe answers that are blinded (i.e. anonymized). Using Type II Diabetes as a case study, we study the adoption of OPEN TrialChain and how it can address clinical trials issues. Results show that OPEN TrialChain, first, encourages pharmaceutical companies to report their trial results with higher fidelity, as well as, federated ones to provide more detailed results in return for their peers’ detailed results. Furthermore, it allows for multiple studies to be queried for results from underrepresented demographics, producing greater insight from previously ignored minorities. Finally, analysis done using OPEN TrialChain should illustrate meaningful results without violating the privacy of individuals. Eventually, OPEN TrialChain is expected to optimize the clinical trial ecosystem by improving patient safety, saving lives, cutting drug development costs, encouraging transparent results, preserving patients privacy, and maintaining pharmaceuticals integrity.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2018/02/06/OPENTrialChain_RKQasbL.png,283187,,2019-04-01T17:38:57.336Z,"['sandy@media.mit.edu', 'shada@media.mit.edu']",False,open-trialchain,2018-01-01,"['health', 'privacy', 'security', 'machine-learning', 'trust', 'data-visualization']",OPEN TrialChain,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-02-08T21:06:55.314Z,"Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process. ",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/02/08/DSCF6037.JPG,283837,,2023-09-07T14:20:53.142Z,"['noyman@media.mit.edu', 'kll@media.mit.edu', 'ptinn@media.mit.edu', 'doorleyr@media.mit.edu', 'mcllin@media.mit.edu', 'yasushis@media.mit.edu', 'agrignar@media.mit.edu', 'alonsolp@media.mit.edu', 'mdchurch@media.mit.edu', 'ryanz@media.mit.edu']",True,city-science-lab-shanghai,2017-02-14,[],City Science Lab @ Shanghai,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-02-12T05:32:55.124Z,"Climate CubeSat Co-Building (C3) The Media Lab Space Exploration Initiative is driven by an effort to democratize the future of space exploration. As such, we are committed to the development of meaningful community engagement endeavors related to achieving an open and hackable ""New Space Age.""Rigorous investigation of Earth’s changing climate by satellite technology will benefit from more equitable access to currently hard-to-reach tools. Cube satellites (cubesats), originally developed for educational settings, have emerged as potent devices for remote sensing in the climate science sector. In general, these spacecrafts are assembled using cost-effective, commercially available off-the-shelf components (COTS), and therefore are more readily accessible than traditional satellite technology.The Space Exploration Initiative is driven to foster the development of accessible and hands-on learning opportunities for diverse populations in multiple formats, and in particular for space tinkerers, hackers, and enthusiasts. In line with this notion, and inspired by MIT's Vice President for Research, Dr. Maria T. Zuber, and  NASA’s 50 CubeSats, 50 States initiative, we developed the Climate CubeSat Co-Build (C3) program in partnership with MIT AeroAstro and Lincoln Laboratory: a multi-tiered outreach platform which aims to enable learning communities—classrooms, cities, and public libraries—nationally and internationally, to gain technical literacy in fundamental principles of climate science, small spacecraft engineering, and data analysis in a highly collaborative setting. As the flagship element of this program, we are working with a cohort of 20 Boston-area high school students, primarily from underrepresented minority communities, who will pioneer the development of a 3U cubesat with climate-sensing capabilities. Through the C3 curriculum, co-builders interface with local experts to manage and execute the build of an engineering cubesat model, including payload development, model testing, and subsequent analysis and iteration. The resulting “BeaverCube,” capable of gathering cloud top and ocean infrared data, will serve as a standardized model suitable for modification and deployment by high school classrooms across the globe. Our pilot curriculum, which drew students from J.D. O’Bryant School of Mathematics and Science (Roxbury) and the Clay Center for Science and Technology (Brookline), concluded in spring 2019, and resulted in the development of a 3U demosat capable of visible and infrared imaging, which we are currently preparing for testing on a high-altitude weather balloon. Space Explorations in Public Libraries To broaden the reach of C3 programming, we are working collaboratively with public librarians—as part of the ML Learning Initiative's project, the Public Library Innovation Exchange (PLIX)—to develop curricula for technical literacy, grounded in creative learning principles, for interested communities. We launched Space Explorations in the spring of 2019 with library patrons in Charlotte, North Carolina and Cambridge, Massachusetts, and are actively building a more robust global community of librarians seeking to bring elements of small spacecraft and mission design, space environments, and data literacy to their hometowns. Our use of an online community publishing platform (c3.pubpub.org) for these collaborations not only serves as a repository for lecture materials and workshop ideas, but also enables diverse learning communities to share data, experiences, successes, and failures in the learning process.LEGOSats for Playful Spacecraft PrototypingOur final mode of engagement involves the ongoing prototyping of “LEGOSat” kits—demo cubesat units which help facilitate the development of systems-level thinking, while leveraging the low-cost, modularity, and playful interface of LEGOs. Kits will consist of a standardized set of mostly COTS parts that can be customized by the builder (primarily the LEGO hardware shell). The 1U LEGOSat, for deployment on tethered mylar balloons, will be readily accessible for learning institutions with modest funding, and operate in a plug-and-play manner—making it more amenable for single-session workshops. The 3U version, in contrast, is likely to be built across multiple weekly sessions, is intended for older learners, and will require more advanced skills in hardware integration. Each LEGOSat will consist of a LEGO frame with an IR camera, a Raspberry Pi microcontroller, and a lightweight power source.As young students are empowered to build their own cubesats and collect and analyze their own data, they will both be participating in something larger than themselves and learning about the health and future of our planet.C3 collaborators: MIT Media Lab Learning Initiative & Public Library Innovation Exchange (PLIX)MIT Lincoln LaboratoryMIT AeroAstro STAR LabClay Center Amateur Radio ClubJ.D. O'Bryant School of Mathematics & Science",2020-10-31,"['ml-learning', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2018/02/12/26927531199_61569eb898_o.jpg,285087,,2023-09-07T15:41:53.714Z,"['ave@media.mit.edu', 'devora@media.mit.edu', 'aekblaw@media.mit.edu', 'kamcco@media.mit.edu']",True,cubesat,2018-01-01,['space'],Climate CubeSat Co-Building Outreach Program (C3),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-03-01T18:41:20.899Z,"Before automobiles were invented and widely adopted, animals like horses were the most common mode of transportation. While this change brought significant improvements in terms of reliability and efficiency, it also removed a core component: the emotional relationship that existed between the person and the animal.While largely ignored, the emotional states of drivers are quite important, as they influence not only driving behavior but also the safety of all road users. For instance, driving can be quite an emotionally stressful experience and, while certain amounts of stress help the driver to remain alert and attentive, too much or too little can negatively impact driving performance and safety. Furthermore, stress in large doses has been linked to a large array of adverse health conditions such as depression and various forms of cardiovascular disease.The Emotion Navigation  project aims to help people convert driving experiences into ones that help them not only be safe and healthy, but also ""feel better"" after the drive.  Feeling good at the end of a long day's commute home can have beneficial effects that go beyond the driver, enabling people to come home in a better mood for interacting with others who are important in their home life, facilitating relationships that are core to building resilience and wellbeing.  ",,"['affective-computing', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2021/09/29/Enav_photo.png,290483,,2023-09-07T22:58:52.557Z,"['javierhr@media.mit.edu', 'picard@media.mit.edu', 'fergusoc@media.mit.edu', 'neska@media.mit.edu', 'nkosmyna@media.mit.edu', 'jinmo@media.mit.edu', 'eijiy@media.mit.edu', 'sebzepf@media.mit.edu', 'cvx@media.mit.edu', 'jalboc@media.mit.edu', 'amores@media.mit.edu', 'victormr@media.mit.edu']",True,emotional-navigation-system,2018-04-02,"['interfaces', 'affective-computing', 'autonomous-vehicles']",Emotion Navigation,PUBLIC,https://enavigation.media.mit.edu/index.html,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-03-03T21:39:24.057Z,"AlterEgo is a  non-invasive, wearable, peripheral neural interface that allows humans to converse in natural language with machines, artificial intelligence assistants, services, and other people without any voice—without opening their mouth, and without externally observable movements—simply by articulating words internally.  The feedback to the user is given through audio, via bone conduction,  without disrupting the user's usual auditory perception, and making the interface closed-loop. This enables a human-computer interaction that is subjectively experienced as completely internal to the human user—like speaking to one's self.  A primary focus of this project is to help support communication for people with speech disorders including conditions like ALS (amyotrophic lateral sclerosis) and MS (multiple sclerosis).  Beyond that, the system has the potential to seamlessly integrate humans and computers—such that computing, the Internet, and AI would weave into our daily life as a ""second self"" and augment our cognition and abilities.  The wearable system captures peripheral neural signals when internal speech articulators are volitionally and neurologically activated, during a user's internal articulation of words. This enables a user to transmit and receive streams of information to and from a computing device or any other person without any observable action, in discretion, without unplugging the user from her environment, without invading the user's  privacy.  ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/04/04/cover_alterego.png,290498,,2021-04-15T14:41:10.940Z,"['arnavk@media.mit.edu', 'pattie@media.mit.edu', 'ewadkins@media.mit.edu']",True,alterego,2017-09-01,"['design', 'human-computer-interaction', 'artificial-intelligence', 'communications', 'health', 'human-machine-interaction', 'wearable-computing', 'interfaces', 'technology', 'internet-things', 'human-augmentation']",AlterEgo,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-03-07T19:16:55.416Z,"GUI3D Template is a generic implementation of the GUI components that anyone might want for a 3D simulation in Processing 3. Components include navigation, zooming, rotation, control sliders, radio buttons, and the means to select objects in 3D space with a mouse. This template can speed up implementation of 3D simulations in Processing.More information: https://ira.mit.edu/blog/gui3dJump directly to the GitHub Repository: https://github.com/irawinder/GUI3D",,[],https://dam-prod2.media.mit.edu/x/2018/03/07/Screen Shot 2018-03-07 at 1.24.47 PM.png,290559,,2018-04-27T14:47:52.452Z,['jiw@media.mit.edu'],True,gui3d,2017-12-01,"['gaming', 'human-machine-interaction', 'open-source', 'interfaces', 'social-science']",GUI3D,PUBLIC,http://ira.mit.edu/blog/gui3d,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-03-07T20:46:35.620Z,"In this project, we aim to examine the interplay between the brain and body using physiological sensors and functional magnetic resonance imaging (fMRI) simultaneously. The goal of the project is to identify the neural correlate that is involved in mediating the affect-related autonomic responses and how it may be affected in people with clinical conditions such as anxiety disorders and PTSD. ",2019-05-31,['affective-computing'],https://dam-prod2.media.mit.edu/uuid/6fc75993-681f-40d4-b636-7722d6888a47,290577,,2021-10-04T17:30:55.232Z,"['savio@media.mit.edu', 'picard@media.mit.edu', 'ktj@media.mit.edu']",True,brain-and-body-interaction-in-affective-responses,2018-01-01,[],BrainBody,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2018-03-07T20:48:48.684Z,"The Places dataset (website) is designed following principles of human visual cognition. Our goal is to build a core of visual knowledge that can be used to train artificial systems for high-level visual understanding tasks, such as scene context, object recognition, action and event prediction, emotion recognition, or theory-of-mind inference. You can try our online demo.",,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/03/07/Screen Shot 2018-03-07 at 3.50.48 PM.png,290585,,2018-10-22T19:53:47.491Z,['agata@media.mit.edu'],True,place-recognition-and-categorization-1,2014-10-01,[],Place Recognition and Categorization,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-03-07T21:04:46.006Z, In this project we explore how to recognize and localize affect in images.,,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/10/18/flowers.png,290593,,2018-10-22T19:54:06.978Z,['agata@media.mit.edu'],True,image-sentiment-anlysis,2017-01-02,['affective-computing'],Image Sentiment Analysis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-03-07T23:30:59.950Z,"The delivery of mental health interventions via ubiquitous devices has shown a lot of promise. A natural conversational interface that allows longitudinal symptom tracking and appropriate just-in-time interventions would be extremely valuable. However, the task of designing emotionally aware agents is still poorly understood. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this project, we explore the design and evaluation of EMMA (EMotion-Aware mHealth Agent).EMMA conducts experience sampling in an empathetic manner and provides emotionally appropriate micro-activities. We show the system can be extended to detect a user's mood purely from smartphone sensor data. We have conducted a three-week user study (N=58). Our results show that extroverts preferred EMMA significantly more, and that our personalized machine learning model was effective, as was relying on ground-truth emotion samples from users.",2021-04-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/03/07/EMMA.gif,290602,,2022-05-12T17:39:02.402Z,['asma_gh@media.mit.edu'],True,emma-an-emotionally-intelligent-personal-assistant-for-improving-wellbeing,2017-05-26,"['health', 'interfaces']",EMMA: An emotionally intelligent personal assistant for improving wellbeing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-03-08T00:59:12.290Z,"Can we modulate the way we hear the world around us to make it more calming or to induce focus? While technology is usually associated with causing stress, technology also has the potential to bring about calm. In particular, breathing usually speeds up with higher stress, but it can be slowed through a manipulation, and in so doing, it can help the person calm down. We are exploring a range of interventions to influence breathing without requiring any focused attention in order to be effective. In multiple projects, we have looked at dynamic composition of music, modulation of screen brightness, and headphone volume to create a seamless pulsating behavior, similar to breathing biofeedback, to indirectly influence breathing. Our preliminary analyses show promising results that such seamless modulation indeed have an influence on breathing rate and pattern.In this project, we explore modulating insertion gain on a headphone in harmony with affective signals, particularly breathing rate. We study the influence of this dynamic change between “inside” and “outside” sources of sound to induce a sense of calmness. We experiment in simulated environments that resemble different situations such as a library, a busy street, and a fireplace.We would like to thank Dan Gauger for giving us equipment and his thoughtful suggestions, including the project name. We would also like to thank Bose for making this project happen. ",2021-04-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/03/07/headphones.jpg,290610,,2022-05-12T17:41:20.357Z,"['asma_gh@media.mit.edu', 'picard@media.mit.edu']",True,worldbeat,2017-12-01,['affective-computing'],WorldBeat: Hearing the world differently,PUBLIC,http://www.ghandeharioun.com,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-03-13T15:44:18.683Z,"TuringBox is a platform that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms. It is a two-sided marketplace. On one side, AI contributors upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet. On the other side, AI examiners develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.",2019-12-01,"['ethics', 'scalable-cooperation']",https://dam-prod2.media.mit.edu/x/2018/03/13/turingbox_logo_white.png,290626,,2021-07-15T15:44:37.972Z,"['zive@media.mit.edu', 'blakeley@media.mit.edu', 'cebrian@media.mit.edu', 'irahwan@media.mit.edu', 'nobradov@media.mit.edu', 'felbo@media.mit.edu', 'groh@media.mit.edu', 'judyshen@media.mit.edu', 'dubeya@media.mit.edu']",True,turingbox,2018-03-21,"['artificial-intelligence', 'human-machine-interaction', 'social-science', 'machine-learning']",TuringBox: Democratizing the study of AI,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-03-15T00:30:28.218Z," A learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites.Robots have potential to revolutionize the way we interact with the world around us. One of their greatest potentials is in the domain of mobile health, where they can be used to facilitate clinical interventions. However, to accomplish this, robots need to have access to our private data in order to learn from these data and improve their interaction capabilities. To enhance this learning process, knowledge sharing among multiple robot units is the natural step forward. However, to date, there is no well-established framework which allows for such data sharing while preserving the privacy of the users, such as hospital patients. To this end, we introduce RoboChain: the first learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites such as hospitals. RoboChain builds upon and combines the latest advances in open data access, blockchain technologies, and machine learning. We illustrate this framework using the example of a clinical intervention conducted in a private network of hospitals. Specifically, we lay down the system architecture that allows multiple robot units, conducting the interventions at different hospitals, to perform efficient learning without compromising the data privacy.  ",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2018/03/14/Federated_Learning_Network_-_Fig_5.jpg,290645,,2019-04-19T14:53:57.705Z,"['sandy@media.mit.edu', 'orudovic@media.mit.edu', 'ecstll@media.mit.edu']",True,robochain-a-secure-data-sharing-framework-for-human-robot-interaction,2018-03-01,"['robotics', 'artificial-intelligence', 'data', 'health', 'human-machine-interaction', 'networks', 'security', 'systems', 'machine-learning', 'social-robotics', 'blockchain', 'trust', 'computer-science']",RoboChain: A secure data-sharing framework for human-robot interaction,PUBLIC,http://www.eduardocastello.com,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-03-15T02:52:40.385Z,"Swarms of robots will revolutionize many applications, from targeted material delivery to farming. However, the characteristics that make them ideal for certain future applications, such as robot autonomy or decentralized control, can also be an obstacle when transferring this technology from academia to real-world problems. Blockchain, an emerging technology, demonstrates that by combining peer-to-peer networks with cryptographic algorithms, a group of agents can reach agreements without the need for a controlling authority. The combination of blockchain with other distributed systems, such as robotic swarm systems, can provide the necessary capabilities to make robotic swarm operations more secure, autonomous, flexible, and even profitable.",,"['human-dynamics', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/1b64868c-bece-4b4f-98e9-517581a9aa23,290654,,2023-09-04T18:20:05.947Z,"['sandy@media.mit.edu', 'ecstll@media.mit.edu']",True,blockchain-a-new-framework-for-swarm-robotic-systems,2017-09-01,"['robotics', 'art', 'artificial-intelligence', 'cryptocurrency', 'data', 'economy', 'health', 'networks', 'privacy', 'security', 'systems', 'machine-learning', 'collective-intelligence', 'social-robotics', 'blockchain', 'trust', 'computer-science', 'data-science']",Blockchain: A new framework for robotic swarm systems,PUBLIC,http://www.eduardocastello.com,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-03-16T15:37:18.313Z,"Seeing through dense, dynamic, and heterogeneous fog conditions. The technique, based on visible light, uses hardware that is similar to LIDAR to recover the target depth and reflectance.The system relies on ultrafast measurements, used to computationally remove inclement weather conditions such as fog, and produce a photo and depth map as if the fog weren’t there (with contrast improved by 6.5x in dense fog conditions).  ApplicationsAutonomous and augmented driving in challenging weather.Airplanes and helicopters take off, landing and low level flight in dense fog conditions.Trains traveling at normal speeds during inclement weather conditions.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2018/03/16/sketch_car.jpg,290681,,2018-08-22T17:18:20.368Z,"['guysatat@media.mit.edu', 'raskar@media.mit.edu']",True,seeing-through-fog,2017-03-01,"['computer-vision', 'sensors', 'imaging']",Seeing Through Realistic Fog,PUBLIC,http://web.media.mit.edu/~guysatat/,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-03-17T16:24:04.841Z,"Predicting Urban Performance through Behavioral Patterns in Temporal Telecom DataThis study explores a novel method to analyze diverse behavioral patterns in large urban populations and to associate them with discrete urban features. This work utilizes machine learning and anonymized telecom data to understand which fragments of the city has greater potential to attract dense and diverse populations over longer periods of time. Finally, this work suggests a road map for building spatial prediction tools in an effort to improve city-design and planning processes.  Click here for an interactive visualization of this study Advisors: Kent Larson and Esteban MoroThanks to Andorra Telecom, ActuaTech, Núria Macià. Data was obtained by Andorra Telecom as part of MIT Media Lab City Science and the State of Andorra collaboration. ",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/03/17/ezgif-5-c9ca5235d3.gif,290692,,2020-07-23T21:42:53.537Z,"['noyman@media.mit.edu', 'doorleyr@media.mit.edu', 'emoro@media.mit.edu', 'alonsolp@media.mit.edu', 'kll@media.mit.edu', 'agrignar@media.mit.edu']",True,reversed-urbanism,2017-07-01,"['data', 'networks', 'urban-planning', 'machine-learning']",Reversed Urbanism,PUBLIC,http://ArielNoyman.com,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-03-20T19:08:40.994Z,"Civic Entertainment is a project based at the Center for Civic Media that explores the intersection of civic engagement with film, television, radio, theatre, literature, and digital entertainment. The project aims to study the modes in which entertainment can create greater knowledge of public institutions, motivate citizens towards democratic duties, and present effective strategies of social and political change.The research focuses on studying the ways in which fiction media can affect change in thought and behavior, develops case studies of past and existing films and television shows that reflect or carry elements of civic engagement, explores the representation of protest and activism in popular culture, and experiments with techniques to balance civic education with humor or drama within entertainment.The project has a key focus on Indian entertainment and works with Civic Studios (www.civicstudios.com), a Mumbai-based production firm, on creating civic entertainment content for young people across India.",2020-08-01,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2018/03/20/MEDYA_eO234k9.jpg,290712,,2022-06-10T13:29:45.182Z,"['anushkas@media.mit.edu', 'ethanz@media.mit.edu']",True,civic-entertainment,2018-03-20,"['civic-media', 'communications', 'entertainment', 'politics', 'social-science', 'social-change', 'civic-action']",Civic Entertainment,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-03-26T20:04:35.733Z,TerMITes are a wireless environmental sensor platform that enables designers and researchers with the necessary tools to capture environmental data. ,,['city-science'],https://dam-prod2.media.mit.edu/x/2021/03/02/Co2 TerMITe Carson Smuts page_small.jpg,292738,,2023-01-24T22:19:54.429Z,"['csmuts@media.mit.edu', 'cassiano@media.mit.edu', 'kll@media.mit.edu', 'aricom@media.mit.edu']",True,termites,2017-09-01,"['artificial-intelligence', 'sensors']",TerMITes,PUBLIC,http://termites.synthetic.space/,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2018-03-31T03:22:03.417Z,"We present Norman, world's first psychopath AI. Norman was inspired by the fact that the data used to teach a machine learning algorithm can significantly influence its behavior. So when people say that AI algorithms can be biased and unfair, the culprit is often not the algorithm itself, but the biased data that was fed to it. The same method can see very different things in an image, even ""sick"" things, if trained on the wrong (or, the right!) data set. Norman suffered from extended exposure to the darkest corners of Reddit, and represents a case study on the dangers of artificial intelligence gone wrong when biased data is used in machine learning algorithms. Norman is an AI that is trained to perform image captioning; a popular deep learning method of generating a textual description of an image. We trained Norman on image captions from an infamous subreddit (its name is redacted due to its graphic content) that is dedicated to documenting and observing the disturbing reality of death. Then, we compared Norman's responses with a standard image-captioning neural network (trained on MSCOCO dataset) on Rorschach inkblots–a test that is used to detect underlying thought disorders.Visit norman-ai.mit.edu to explore what Norman sees!",2018-06-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2018/04/03/norman.jpg,294308,,2020-07-23T18:46:10.029Z,"['pinary@media.mit.edu', 'cebrian@media.mit.edu', 'irahwan@media.mit.edu']",True,norman,2018-04-01,"['artificial-intelligence', 'social-media', 'storytelling', 'machine-learning']",Norman,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-04-02T00:49:31.476Z,"Temperature influences our perception and cognition both consciously and subconsciously. These effects are rooted in our bodily experiences and interactions with the environment, and are even embedded as metaphors in our language. By learning how temperature affects us in different contexts, we can make use of that knowledge to create interventions that help us with personal growth.This project seeks to apply thermal interfaces to assist with emotion and attention regulation. Stress and attention levels can be inferred using implicit user inputs such as electrodermal activity, heart rate variability, and relative facial temperature. This information can then be used to determine appropriate thermal feedback to implicitly modify the user’s perception and aid with emotional and attention regulation in a minimally disruptive fashion. Chill.out: a perceptual modifier.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/05/21/Screen Shot 2018-05-06 at 7.39.48 PM.jpeg,294622,,2020-05-21T07:34:01.216Z,"['tomasero@media.mit.edu', 'pattie@media.mit.edu']",True,chill-out,2018-02-01,['human-augmentation'],Chill.out,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-04-02T19:01:49.226Z,"This project aims to improve the prediction accuracy of wellbeing (stress, mood, and health levels) using temporal machine learning models. We extend our previous approach using Long Short-Term Memory models and time series data from the SNAPSHOT study. In addition, we consider adaptive methods to fill in missing data with time series information. We also develop the model using modifiable behavioral features such as bedtime, and examine how these contribute to wellbeing, so that people can get better control over how to improve their personal well-being.",,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/05/19/Terumi_v1.jpg,294939,,2019-05-24T18:28:52.701Z,"['picard@media.mit.edu', 'terumi@media.mit.edu', 'akanes@media.mit.edu', 'sataylor@media.mit.edu']",True,improving-well-being-prediction-performance-using-temporal-machine-learning-models,2018-04-02,"['machine-learning', 'affective-computing', 'wellbeing']",Improving wellbeing prediction performance using temporal machine learning models,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-04-02T19:08:40.131Z,"Excessive stress can decrease office workers' productivity and negatively impact overall health. This project aims to predict office workers' stress levels using physiological and behavioral markers based on heart rate, skin conductance, skin temperature, and acceleration. Building on knowledge and models developed for student populations in the SNAPSHOT study and collecting new data from worker populations, we plan to improve stress-level prediction performance for office workers. Furthermore, we will also study how to improve productivity by decreasing stress. In order to accomplish this, we will look for causal factors that increase stress and possible interventions that can be deployed to office workers to decrease these factors.",2020-04-30,['affective-computing'],,294947,,2022-05-12T17:44:32.951Z,"['terumi@media.mit.edu', 'akanes@media.mit.edu', 'sataylor@media.mit.edu', 'picard@media.mit.edu']",True,improving-well-being-for-office-workers,2018-04-02,['affective-computing'],Improving Wellbeing for Office Workers,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-04-05T19:50:20.272Z,"One of the eternal challenges of economic development is how to identify the economic activities that a country, city, or region should target. During recent years, a large body of research has shown that countries, regions, and cities, are more likely to enter economic activities that are related to the ones they already have. For instance, a region specialized in the exports of frozen fish and crustaceans can more easily start exporting fresh fish than heavy machinery. This research has illuminated a new chapter in the economic development literature, but has left an important question unanswered: what is the right strategy for countries wanting to diversify their economies? ",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2018/04/05/Screen Shot 2018-04-05 at 4.14.10 PM.png,295926,,2020-08-24T16:38:10.362Z,"['flaviopp@media.mit.edu', 'hidalgo@media.mit.edu']",True,strategic-diffusion,2018-04-02,"['data', 'economy', 'social-science']",What is the optimal way to diversify an economy?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2018-04-11T05:06:13.608Z,"We use high-resolution geospatial data collected from mobile phones to measure social segregation at an unprecedented resolution in cities across the United States. Social segregation happens when people of varying socioeconomic groups in a city have little opportunity to be exposed to people different than them.To construct this measure, we aggregate high-resolution data from over 4.5 million users in the principal metro areas in the US to characterize places in the city by how mixed their visitors are by income. Using this measure, rather than traditional residential metrics, reveals that social exposure in third places is crucial to understanding economic segregation patterns in cities. In fact, the social segregation of different economic groups is dependent on an extremely small proportion of overall venues in a city. We also look at how much individual citizens would need to change their behavior in order to make their patterns of exposure more integrated. Surprisingly, small changes in the amount of time people spend in different categories of places—changes as low as 2-5%—can reduce their social segregation by half. We're currently working on finalizing these results and exploring how we might translate these findings into policy.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2018/08/20/banner_sNgzYOy.png,297826,,2023-05-04T21:05:16.093Z,"['xdong@media.mit.edu', 'dcalacci@media.mit.edu', 'emoro@media.mit.edu', 'sandy@media.mit.edu', 'tyabe@media.mit.edu', 'bernard0@media.mit.edu']",True,measuring-and-reducing-social-segregation-in-cities,2017-10-01,"['social-networks', 'transportation', 'urban-planning', 'social-science', 'behavioral-science', 'social-change', 'covid19', 'cities']",Measuring and reducing social segregation in cities,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-04-14T18:26:09.989Z,Analyze and visualize urban interaction with computer vision and deep neural net. ,,['city-science'],https://dam-prod2.media.mit.edu/x/2018/04/14/180403_Screen-Recording-of-Unity-Viz_cut.gif,298816,,2018-05-03T15:51:35.403Z,['ryanz@media.mit.edu'],True,DUI,2018-02-14,[],Deep Urban Interaction,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-04-20T02:23:10.028Z,"Becoming Someone is a multi-modal concept that lives across Medium, Instagram, and human minds. It is comprised of The Ever Contracting Void,  an Instagram-based container by Micah Epstein commissioned by Manuel Cebrian and Iyad Rahwan as the visual metaphor for the  essay Becoming Someone. The Ever Contracting Void is a media container that exists within  The Directory of Worlds, an installment in Micah’s ongoing Instagram installation. The Directory is a series of accounts, called ""worlds,"" which serve as spaces for exploration and reprieve from Instagram’s visual overload and hyper-targeted marketing. Where his previous worlds were an opportunity, the Void is a threat. The Void contains nothing but the source code of popular social media websites, which use machine learning to drive the internet ever inwards upon the individual. Trapped in its center is an individual, whose very identity hinges on the viewer double-tapping the screen to like the image. What remains, when the Void closes upon the individual, leaving nothing but pristine white grids and information superhighways?Read the articulation here and explore the visual container on Instagram here. ",2018-04-26,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2018/04/19/5.jpeg,300711,,2018-04-23T17:54:11.668Z,"['irahwan@media.mit.edu', 'cebrian@media.mit.edu']",True,becoming-someone,2018-04-05,[],Becoming Someone,PUBLIC,https://medium.com/mit-media-lab/becoming-someone-54ed1798a1b7,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1
False,2018-04-20T20:44:22.627Z,"The Intertidal Experimentation Workshop will take place September 29 and 30 (9am to 2pm) at the MIT Media Lab, open to students ages 8-14. To register, please visit this link. Field Exploration in Boston's Intertidal Zone is a two-day, hands-on educational workshop for neurodiverse youth in the Greater Boston area, in which participants will use the city of Boston as a classroom, laboratory, and creative playground. Together, scientists, engineers, and artists will take to the field as explorers in order to answer questions related to ecology, biology, chemistry, art, and more. Workshop sessions will take place from 9am to 2pm on September 29 and 30, 2018. We will gather at the MIT Media Lab (75 Amherst Street, Cambridge) prior to traveling to field sites located within Greater Boston. Day I. Introduction to Intertidal Ecology and Experimental Design. Day I of this workshop will take place at the MIT Media Lab, where students will learn about the ecology of rocky intertidal zones as well as experimental design. Students will additionally work collaboratively in small groups to develop hypotheses about the phenomena occurring in Boston's urban intertidal zones which they will subsequently test on-site during Day II. Day II. Data Collection, Interpretation, and Presentation. On Day II, following the field investigation, students will regroup at the Media Lab to get a crash course on data interpretation and visualization. Thereafter, they will present their work to classmates as well as parents and community members through text, graphics, and more. Participation is free of charge, and all materials and meals are provided. For questions, or opportunities for involvement, please contact Avery Normandin (ave@media.mit.edu). Why Urban Oceans?Presently, over 40% of the world’s population lives within 100 kilometers of the coastline, often in seaside megalopolises. While it is known that urban-adjacent marine ecosystems are subjected to unique stressors—namely unparalleled amounts of pollution stemming from urban runoff—efforts related to ocean conservation, as well as marine ecological investigation, most frequently concern the open sea, beyond the immediate reaches of urban ecosystems.To better inform regulatory actions related to urban ocean protection, we must understand the unique qualities of these ecological bodies—the seas of cities—particularly as global changes (climate change, rapid urbanization) increase strain on these fragile systems. In parallel, given technological-driven paradigmatic shifts in our ability to characterize the unknown world, we are driven to generate innovative and novel platforms for education in the environmental sciences: experiential, instructional excursions which will empower and inspire urban populations to spearhead efforts to sculpt the future of their territories. Ideally, these sorts of experiences will cater to all individuals, regardless of gender, race, or cognitive differences.A Workshop for Neuroinclusivity ""Citizen science"" (or Open Science) movements have generated robust momentum for allowing communities to delineate the natural world—or speculate on its future—in hands-on and creative ways. As part of a larger effort to cultivate a future generation of environmentally engaged and justice-focused citizen scientists—and in line with the outreach efforts of the Media Lab's Open Ocean Initiative—we have developed Field Experimentation in Boston's Intertidal Zone: a two-day pilot workshop for Boston-area neurodivergent (e.g., autistic, dyslexic, dyspraxic, ADD, ADHD) youth, in which participants will learn about the ecology of rocky and intertidal systems, develop a hypothesis surrounding these bodies, and subsequently execute a field investigation to test this hypothesis. Students will have the option of approaching fieldwork as a scientist (or engineer), an artist, or a writer (poet, journalist).We envision that use of easy-to-access, public sites for the pilot workshop will further democratize the potential to recapitulate similar endeavors in ecological exploration and immersive learning.",2018-12-31,"['sculpting-evolution', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2018/04/25/Screen Shot 2018-04-25 at 9.46.54 PM.png,300744,,2019-04-22T17:58:25.679Z,"['ave@media.mit.edu', 'devora@media.mit.edu']",True,field-experimentation-in-boston-s-intertidal-zone,2018-04-02,"['ecology', 'learning-teaching', 'storytelling', 'urban-planning', 'engineering', 'ocean', 'climate-change']",Field Experimentation in Boston's Intertidal Zone,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-04-20T23:11:00.187Z,"Making 3D aniation easier by utilizing mobile AR, enabling manipulation of spatial objects using a spatial interface.",,[],https://dam-prod2.media.mit.edu/x/2018/04/20/animtool_MiobAMr.png,300762,,2018-04-20T23:24:12.477Z,['hbedri@media.mit.edu'],True,ar-animation-tool,,[],AR Animation Tool,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-04-20T23:40:10.819Z,"Audio on a mobile device can be used to form low-resolution 3D images. The resolution of audio ranging and imaging is heavily limited by bandwidth. Here is an attempt to increase the resolution through continuous basis pursuit, a grid-less sparse algorithm. Comparison of backprojection, basis-pursuit denoising, and continuous basis pursuit shown.Open access PDF: http://www.cv-foundation.org//openaccess/content_iccv_2015_workshops/w18/papers/Bedri_Exploring_the_Resolution_ICCV_2015_paper.pdf",,[],https://dam-prod2.media.mit.edu/x/2018/04/20/audioimage3_BnwHxZx.PNG,300770,,2018-04-20T23:52:23.241Z,"['hbedri@media.mit.edu', 'raskar@media.mit.edu', 'michaf@media.mit.edu']",True,grid-free-super-resolution-for-audio-imaging,,[],Grid-free super-resolution for audio imaging,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-04-20T23:56:40.245Z,"3D interaction is a magical way for users to interact with a system without touching it. This is usually accomplished using a camera, but that can intrude heavily on privacy. Using RF, we can create interaction mechanisms that do not take pictures of us and help to preserve our privacy.",,[],https://dam-prod2.media.mit.edu/x/2018/04/20/jellyflow_iDD7UHv.gif,300778,,2018-04-21T00:07:04.872Z,"['hbedri@media.mit.edu', 'michaf@media.mit.edu', 'otkrist@media.mit.edu', 'raskar@media.mit.edu']",True,through-wall-table-interaction-using-rf,,[],Through Wall/Table interaction using RF,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-04-21T00:10:07.352Z,"I'm showcasing a lightsaber with a beam that can be switched on and off. All the electronics and optics are contained within the handle, so when the lightsaber is off, all you see is the handle. When the lightsaber is on, a beam of light emerges that ends at the tip of the lightsaber. The lightsaber works in a fog-filled environment, where the beam can reflect off of the tiny fog-particles. The beam energy is safe, AKA it won’t be cutting through anything anytime soon , but it looks really cool :)",,[],https://dam-prod2.media.mit.edu/x/2018/04/20/gif_v0_DGcXkJu.gif,300786,,2018-04-21T00:35:58.195Z,['hbedri@media.mit.edu'],True,lightsaber-lightfields-diffusors,,[],LIGHTSABER! LIGHTFIELDS + DIFFUSORS,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-04-21T01:27:28.742Z,"KinderCity is a pilot project that attempts to understand the intangibles of city perception. It attempts to understand how people, across ages, children and adults, perceive places and spaces that are playful, creative and inspiring. ",,['city-science'],,301140,,2018-05-03T15:49:31.795Z,['layanasu@media.mit.edu'],True,kindercity,,[],KinderCity,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0
False,2018-04-23T18:18:35.189Z,"The view on Alzheimer’s Disease (AD) diagnosis has shifted towards a more dynamic process in which clinical and pathological markers evolve gradually before diagnostic criteria are met. Given the wide variability in data available per subject, inherent per-person differences, and the slowly changing nature of the disease, accurate prediction of AD progression is a significant, difficult challenge. The goal of this project is to devise novel Personalized Machine Learning Models that can accurately capture future changes in the key biomarkers and cognitive scores related to AD and other neurological conditions. As the basis for our framework, we use the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset–the largest publicly available dataset for AD research.  These data are highly heterogeneous and multi-modal, and include imaging (MRI, PET), cognitive scores, CSF biomarkers, genetics, and demographics (e.g. age, gender, race). The developed models are the break-through in machine learning for health-care as they allow personalized forecasting of the diseases' progression - in contrast to the traditional ""one-size-fits-all"" approaches. This capability is of great importance to both clinicians and those at risk of AD since it is critical to early identification of at-risk subjects, construction of informative clinical trials, and timely detection of AD.In the Affective Computing group, we have actively been working on the development of personalized machine learning models for future forecasting of AdasCog13 - a significant predictor of Alzheimer’s Disease(AD) in the cognitive domain – over the future 6, 12, 18, and 24 months, using the data of participants in the ADNI database. Specifically, in our latest work to be presented on August 09 in the premiere conference on Machine Learning for Healthcare (ML4HC), we introduced a modeling framework based on Gaussian Processes (GPs) that leverages the notion of ""meta-learning"" (learning how to learn). This approach learns automatically from previous participants'  data what is the best forecasting model to apply to a new participant: the population-level or personalized model. This is important in cases when the participants' data are highly noisy or missing, in which case the population-level GP models are suboptimal. Conversely, when we have a good-quality past data of the target participants, these are used to effectively personalize the target model to a new participant, largely outperforming the population-level model on the future data of that participant.  This has important implications for the design of clinical trials and also in gauging decisions of medical practitioners, allowing them to use smart and personalized AI when deciding what treatment to prescribe to their patients (by informing them of potential future outcomes for those patients, based on the medical history of the target patient but also large source of knowledge available from previous/other patients). For more details about this approach, check our paper (""Meta-Weighted Gaussian Process Experts for Personalized Forecasting of AD Cognitive Changes"")  that is provided below.",2019-12-31,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/04/23/PML-ADNI-Oggi.png,301771,,2022-05-12T17:41:57.228Z,"['orudovic@media.mit.edu', 'picard@media.mit.edu']",True,personalized-machine-learning-for-future-health,2017-11-01,"['human-machine-interaction', 'pharmaceuticals', 'machine-learning', 'affective-computing', 'human-augmentation']",Personalized Machine Learning for Future Health,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-04-25T19:25:32.553Z,"Social robots are increasingly being developed for long-term interactions with children in domains such as healthcare, education, therapy, and entertainment. In prior research, we have seen that children treat robots as more than mere artifacts, e.g., ascribing them mental states, psychological attributes, and moral standing. Thus, while children’s relationships with robots may not be like the relationships they have with their parents, pets, imaginary friends, or smart devices, they will form relationships of some kind. As such, we need to deeply understand how children’s relationships with robots develop through time, and find ways to characterize and measure these relationships. However, there are few validated assessments for measuring young children’s long-term relationships. Thus, we have adapted or created a variety of assessments for use in this context for children aged 5-6 years. Four of these assessments are presented in the associated paper.This paper shows that children can appropriately respond to these assessments with reasonably high internal reliability, and that these assessments are able to capture child-robot relationship adjustments  over a long-term interaction.",,"['ml-learning', 'personal-robots']",https://dam-prod2.media.mit.edu/x/2018/04/25/p101-c.jpg,302444,,2019-04-17T18:39:40.951Z,"['randiw12@media.mit.edu', 'cynthiab@media.mit.edu', 'haewon@media.mit.edu', 'jakory@media.mit.edu']",True,relationship-assessments,2017-04-01,"['kids', 'social-robotics', 'long-term-interaction']",Assessing Children's Relationships with Social Robots,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-04-26T15:16:29.875Z,"Vespers is a collection of masks exploring what it means to design (with) life. From the relic of the death mask to a contemporary living device, the collection embarks on a journey that begins with an ancient typology and culminates with a novel technology for the design and digital fabrication of adaptive and responsive interfaces. We begin with a conceptual piece and end with a tangible set of tools, techniques and technologies combining programmable matter and programmable life.The project points towards an imminent future where wearable interfaces and building skins are customized not only to fit a particular shape, but also a specific material, chemical and even genetic make-up, tailoring the wearable to both the body and the environment which it inhabits.Imagine, for example, a wearable interface designed to guide ad-hoc antibiotic formation customized to fit the genetic makeup of its user; or, consider smart packaging or surface coatings devices that can detect contamination; finally, consider environmentally responsive architectural skins that can respond to, and adapt—in real time—to environmental cues. Research at the core of this project offers a new design space for biological augmentation across a wide breadth of application domains, leveraging resolution and scale.The collection includes three series. The first series features the death mask as a cultural artifact. The final series features a living mask as an enabling technology. The second series mediates between the two, marking the process of ""metamorphosis"" between the ancient relic and its contemporaneous interpretation. The living masks in the final series embody habitats that guide, inform and ""template"" gene expression of living microorganisms. Such microorganisms have been synthetically engineered to produce pigments and/or otherwise useful chemical substances for human augmentation such as vitamins, antibodies or antimicrobial drugs. Combined, the three series of the Vespers collection represent the transition from death to life, or from life to death, depending on one’s reading of the collection.",2020-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2018/04/26/00_overview (1).jpg,302774,,2022-03-11T15:36:23.599Z,"['neri@media.mit.edu', 'bader_ch@media.mit.edu', 'jpcosta@media.mit.edu', 'ssunanda@media.mit.edu', 'rssmith@media.mit.edu', 'limulus@media.mit.edu']",True,vespers-iii,2018-04-01,"['design', 'art', 'artificial-intelligence', 'food', 'biology', 'materials', 'racial-justice', 'covid19']",Vespers III,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-04-26T20:08:43.334Z,"The development of biotechnologies since the era of recombinant DNA in the 1970s has occurred largely via the interaction of academic, industrial, and governmental institutions. Largely absent from this ecosystem are the informed inputs of grassroots communities at any point in the technology developmental cycle.The parallel rise of high-throughput, next generation DNA sequencing and advanced DNA synthesis technologies (reading and writing DNA), along with invention of precise genome editing technologies (e.g., CRISPR) has humanity at the brink of a new era, one where living technologies rule. Given the vital importance of living technologies, not only to human health, manufacturing, the economy, and environment, but to our social fabric and culture, we ask:How should living technologies be developed?How can we ensure there is broad, diverse participation in biotechnology?How can marginalized, under-represented and indigenous communities be agents of change in this era?What types of institutions and design practices can be employed to ensure just outcomes?How can humanity work harmoniously, in concert with nature, to co-evolve and flourish?We will host a workshop on this topic during the biennial Participatory Design Conference in Genk and Hasselt, Belgium.",,['community-bio'],https://dam-prod2.media.mit.edu/x/2018/04/26/AOSH.jpg,302795,,2019-09-09T14:55:21.444Z,"['dkong@media.mit.edu', 'nbakker@media.mit.edu']",False,participatory-biotechnology,2018-05-27,"['design', 'bioengineering', 'politics', 'synthetic-biology', 'ethics', 'biotechnology']",Participatory Biotechnology,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-04-26T21:23:39.919Z,"Mumble Melody uses musically altered sensory feedback as a potential treatment for stuttering.Several studies have shown improvement in speech fluency with delayed and pitch-altered auditory feedback.  In this project, we use sensory/auditory alterations that stimulate both the right (and left) hemispheres as a means of reducing the auditory feedback-mediated errors in basal ganglia-related motor selection     Stuttering is a condition characterized by involuntary, periodic disturbances in speech fluency, usually via speech sound repetitions, blockages, or prolongations. A host of other secondary features also accompany the condition (e.g. tongue thrusting, eye blinking, body movements), although these are not considered “core” features. Stuttering improves when an individual’s speech is played back to him or her in an altered manner, most famously when delayed by fractions of second, but also when the frequency is shifted, when masked with white noise, and when reading in choral speech. This rather interesting phenomenon of altered feedback-induced fluency is theorized to result from a reduced ability to detect small errors in articulation that occur in stuttering, which reduces its inhibition on speech initiation and output via the feedback mechanism.In people who stutter, there is both structural and functional evidence of atypical hemispheric lateralization of speech and language. People who stutter, when speaking fluently, tend to activate the right hemisphere a during speech tasks. The white matter integrity is disrupted on the left. This rightward shift of speech function may be compensatory (as opposed to causal). Trials comparing fluent versus non-fluent trials in people who stutter reveal the former to associated with activity in the right hemisphere and latter with the left hemisphere. In addition, white matter integrity is negatively correlated with severity of dysfluency on the left, and positively correlated on the right. The overall notion is that stuttering is associated with atypical left-sided speech mechanisms, and that this can be overcome, at least partially, when the right hemisphere is able to effectively compensate.  While most prominently explored in stuttering, the idea that left hemisphere lesions can be overcome by shifting the motor control of speech to the right is supported by other studies in post-stroke aphasia.In this light, altered auditory feedback—a fluency-inducing intervention in stuttering—is associated with activity in right hemispheric sensory, motor, and language areas. In addition, singing—another fluency-evoking task—is known to activate right hemisphere motor areas compared with non-musical speech production.",2019-01-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/04/26/influence envir copy.png,302803,,2018-10-22T16:47:03.821Z,['rebklein@media.mit.edu'],True,mumble-melody,2018-03-01,[],Mumble Melody,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-04-26T21:42:00.850Z,"Speech Companion is an exploration in the domain of real-time extraction of musicality from speech. Speech is one of the richest and most ubiquitous modalities of communication used by human beings. Its richness lies in the combination of linguistic and nonlinguistic information. Musicality is one of the most crucial nonlinguistic components of speech and covers tempo and rhythms of the speaker as well as the pitch variation and unique texture of the vocal sounds. Abstracting musicality from a speech in real time presents several challenges from latency to subjective pitch identification or recognizing voiced/unvoiced sounds. In this paper, we describe a new system for real-time extraction of the music present in everyday speech based on time and pitch quantization. Our system offers several modes from a simple synchronized melody line to a more complex accompaniment much like a singer accompanying herself at the guitar.  With such a system, we offer a proof of concept and a working prototype to explore the real-life situations where the music of speech impacts speakers or listeners such as in the contexts of infant-directed speech, language acquisition, human-animal communication, speech pathology, aphasia reeducation, or even music learning and musical composition.",,['opera-of-the-future'],,302811,,2018-04-27T15:50:50.384Z,['rebklein@media.mit.edu'],True,speech-co,2018-01-01,[],Speech Companion,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-04-27T04:36:55.280Z,"The design of next-generation bionic ankles and knees aims to improve bionic actuators on all metrics: range of motion, power density, bandwidth, and mass, while adopting a futuristic aesthetic. We are pushing the limits of materials and magnetics, combined with new control topologies to enforce a new paradigm in both autonomous and volitional controlled powered prostheses. Open access engrXiv pre-prints are available:Energetic Consequences of Series and Parallel Springs in Lower-Extremity Powered ProsthesesDesign and Preliminary Results of a Reaction Force Series Elastic Actuator for Bionic Ankle Prostheses",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2020/05/27/titlePage.png,303125,,2021-09-17T19:37:29.460Z,"['mcarney@media.mit.edu', 'jfduval@media.mit.edu', 'thhsieh@media.mit.edu', 'tonyshu@media.mit.edu', 'syeon@media.mit.edu', 'handford@media.mit.edu']",True,moment-coupled-cantilever-beam-series-elastic-actuator,2017-03-15,"['robotics', 'design', 'human-computer-interaction', 'art', 'bioengineering', 'communications', 'consumer-electronics', 'computer-science', 'biomechanics', 'technology', 'mechanical-engineering', 'alumni', 'biotechnology', 'electrical-engineering', 'engineering', 'mechatronics', 'prosthetics', 'bionics', 'neural-interfacing-and-control', 'prosthetic-design']",Personalized Bionics for Dynamic Gait,PUBLIC,http://matthematic.com/projects/mit/biomech/biomech_tf8.html,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-04-27T14:37:48.508Z,"Civic technology should empower us as citizens. But despite its breadth as a field, civic technology often takes its lead from Silicon Valley companies that espouse design goals potentially hazardous to participatory democracy. As an example, Facebook has been used to help organize democratic social movements around the world, but it has also allowed undemocratic actors to inflame partisanship and hate at the same time. I explore: How might we design civic technologies for citizen empowerment and evaluate their impact on this goal? With their increasingly important role as mediators of democracy, it is insufficient for civic technology designers to evaluate their designs in terms of ease of use and increased engagement with their platform. Research from political and developmental psychology shows the importance to lifelong civic engagement of learning experiences that cultivate a citizen's perception they can make change (political efficacy) and their belief in having responsibilities to the public good (civic identity). To achieve these positive feedback loops, we need a richer framework for civic technology design. This project proposes two solutions: 1) empowerment-based design principles for civic technology, and 2) a prototype toolkit for evaluating the impact of civic technology on political efficacy. Because empowerment is contextual, the proposals here focus on tools and platforms built to support ""monitorial citizenship,"" an increasingly popular form of civic engagement aimed at holding institutions accountable.",2018-06-08,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2018/04/27/Fist-Phone.png,303133,,2019-08-21T11:55:01.403Z,['erhardt@media.mit.edu'],True,empowering-civic-tech,2017-09-01,"['civic-media', 'civic-technology', 'politics', 'social-media', 'social-science']",Empowerment-based design and evaluation of civic technology,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-04-27T15:58:08.895Z,"Connected Coral integrates physical and digital elements in a visualization of the environmental impacts on reefs. This complex projection mapping uses multiple projectors, angled mirrors, and a motion sensor to create an interactive digital skin on a complex three-dimensional surface.To integrate the projected content with the physical design, the students fabricated the physical coral model based on photogrammetry scans of real coral, warped and blended the projected areas, and factored in hardware specifications. These modifications minimize visual distortion on the uneven surface and allow for an uninhibited interactive experience.This project was created through the Open Ocean Initiative and will be on display at the MIT Museum through Spring 2019.",2018-12-31,"['object-based-media', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2018/04/27/ConnectCoralBanner.png,303141,,2020-01-24T19:05:52.221Z,"['emilysa@media.mit.edu', 'katybell@media.mit.edu', 'nlutz@media.mit.edu']",True,connected-coral,2018-03-29,"['art', 'augmented-reality', 'interfaces', 'ocean']",Connected Coral,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2018-04-27T18:25:27.070Z,"A recent focus of our lab has been making use of Tangible Displays and Body Object Space to develop new assistive technologies. As a test case, we prototyped the Mario side-scrolling game for visually impaired users, using body movement analogies to control Mario in the game. Mario and 2D side scrollers present a particularly interesting case, as they keep the main character location in the center of the display and move the world around the character. The shape display itself provides spatial audio of enemy positions. We make use of the AUFLIP sensor platform to pick up body movements—walking and jumping, causing Mario to do the same in-game. This enables users to keep their hands engaged to understand the game landscape, while using their body to control Mario at the same time.",2017-05-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/2018/10/19/marioUp.gif,303168,,2020-06-09T17:28:03.759Z,"['kalli@media.mit.edu', 'dvlevine@media.mit.edu', 'ishii@media.mit.edu']",True,conjugate,2017-03-01,"['entertainment', 'gaming', 'technology']",CONJURE,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-04-27T20:37:38.490Z,"Gammalan is an interactive musical experience that uses music information retrieval techniques in conjunction with game design principles to engage audiences in creative behavior that combines the power of familiar songs with neural entrainment on multiple temporal scales.A preliminary system analyzes and processes existing recordings, manipulating properties such as rhythm and harmony, while introducing synthesized frequencies in a musically informed manner. The recordings are then presented in an exploratory 3D game-like environment that encourages active and playful engagement with the recorded music.",2019-07-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/08/18/gammalan_screenshot_8-18.png,303177,,2022-09-14T20:12:40.099Z,['davidsu@media.mit.edu'],True,gammalan,2018-02-01,[],Gammalan,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-04-27T21:22:42.730Z,Adaptive Music for Affect Improvement (AMAI) is a music generation and playback system with the goal of steering the listener toward a state of more positive affect. AMAI utilizes techniques from game music in order to adjust elements of the music being heard; such adjustments are made adaptively in response to the valence levels of the listener as measured via facial expression and emotion detection.,2019-07-01,"['affective-computing', 'opera-of-the-future']",https://dam-prod2.media.mit.edu/x/2018/04/27/fmod1.png,303185,,2022-09-14T20:15:10.221Z,"['picard@media.mit.edu', 'davidsu@media.mit.edu']",True,adaptive-music-for-affect-improvement,2017-10-01,"['music', 'affective-computing', 'technology']",Adaptive Music for Affect Improvement,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-04-28T00:15:33.865Z,"Nostalgia BoxA deep learning visualization of your own memoriesBy Aubrey Simonson Commissioned by Manuel Cebrian and Iyad RahwanNostalgia Box is a continually shifting soup of memories. Images, curated for their nostalgic emotional impact, are dreamed over one another using a neural style machine learning algorithm, based on the paper ""A Neural Algorithm of Artistic Style"" by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. The resulting images are then overlaid into a video which never fully solidifies into any one image, but instead is always several at once. To someone who is familiar with the content of the images, they are still recognizable. However, a stranger should see only a haze of vague shapes which occasionally contains the suggestion of a face. This is machine learning, stripped of the elements of spam. It is an amalgamation of its creator's digital history, but which is being used as a tool for reflection, rather than as a means to more effectively market products.  ",2018-12-31,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2018/04/27/39_Alec_Coat_7.jpg,303193,,2018-05-07T22:25:44.225Z,"['cebrian@media.mit.edu', 'irahwan@media.mit.edu']",True,nostalgia-box,2018-02-01,[],Nostalgia Box,PUBLIC,http://cs.wellesley.edu/~asimonso/nostalgiabox,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-04-28T15:30:49.395Z,"Embodied voice-based agents, such as Amazon’s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most people, these agents represent their first experience living with artificial intelligence in such private and personal spaces. However, little is known about people’s desires, preferences, and boundaries for these technologies. This project seeks to answer questions surrounding this space. How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people’s preferences, desires, and boundaries? What tools can be used to understand this space?",,['personal-robots'],,303508,,2018-07-15T03:35:18.476Z,"['akostrow@media.mit.edu', 'nikhita@media.mit.edu']",False,talking-machines-democratizing-the-design-of-voice-controlled-agents-in-the-home,2017-05-01,"['design', 'artificial-intelligence', 'data', 'interfaces', 'social-science', 'social-robotics', 'long-term-interaction']",Talking Machines: Democratizing the Design of Voice-Based Agents in the Home,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-04-28T21:13:06.301Z,"Deep Angel is an artificial intelligence that erases objects from photographs. The algorithm is hosted on http://deepangel.media.mit.edu, which enables anyone to interact with the AI and explore what it can disappear.Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.  Deep Angel draws from  Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could. The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the ""Publish to Deep Angel"" button. The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's Everything series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2018/08/06/_100 (51).jpg,303516,,2023-04-19T19:10:39.500Z,"['groh@media.mit.edu', 'cebrian@media.mit.edu', 'zive@media.mit.edu', 'nobradov@media.mit.edu', 'irahwan@media.mit.edu', 'niccolop@media.mit.edu', 'dubeya@media.mit.edu']",True,deep-angel-ai,2018-08-06,"['artificial-intelligence', 'social-media', 'imaging']",Deep Angel: The AI behind the aesthetics of absence,PUBLIC,http://deepangel.media.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-04-30T00:09:08.195Z,"Fluidics are promising foundational tools for synthetic biology. Unfortunately, fluidics are not broadly used, because they are difficult to manufacture and operate, and designs are currently not shared in a systematic fashion. Metafluidics is an open repository of both device and hardware designs to enable communities of users from around the world to share and remix bio-hardware. This repository is the hardware portal for the National Science Foundation-supported Living Computing Project. Share your fluidic devices with the global community today!",,[],https://dam-prod2.media.mit.edu/x/2018/04/29/metafluidics.png,303838,,2019-05-06T14:41:38.233Z,"['ninawang@media.mit.edu', 'dkong@media.mit.edu']",True,metafluidics,2017-01-01,[],Metafluidics,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-04-30T12:56:22.917Z,"Man of My Words is a wearable self-feedback voice changer for women to challenge internalized sexism. The experience is designed in two parts:Auditory The electronic part of the device consists of a Bluetooth microphone and earbuds. When the female user speaks into the microphone, her voice is altered into a male voice and returned through the earbuds. By giving the perception of speaking in a male voice, this device is intended to  break the association that the users have between themselves, their female, ""weak"" voice, and lack of authority.Visual The top hat and a fake mustache, apart from their functional purpose of holding the microphone, were designed to create a more immersive, satirical experience. This device aims to more easily approach a serious social issue through comfortable humor.",2020-07-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/04/30/manofmywords copy.jpg,304153,,2022-09-14T20:16:56.641Z,['hanelee@media.mit.edu'],True,man-of-my-words,,['voice'],Man of My Words,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2018-04-30T17:16:10.002Z,"More ocean data has been collected in the last two years than in all previous years combined, and we are on a path to continue to break that record. More than ever, we need to establish a solid foundation for processing this ceaseless stream of data. This is especially true for visual data, where ocean-going platforms are beginning to integrate multi-camera feeds for observation and navigation. Techniques to efficiently process and utilize visual datasets with machine learning exist and continue to be transformative, but have had limited success in the ocean world due to:Lack of data set standardization;Sparse annotation tools for the wider oceanographic community; and Insufficient formatting of existing, expertly curated imagery for use by data scientists. Building on successes of the machine learning community, we are developing a public platform that makes use of existing (and future) expertly curated data. Our efforts will establish a new baseline dataset, optimized to directly accelerate development of modern, intelligent, automated analysis of underwater visual data. This effort will ultimately enable scientists, explorers, policymakers, storytellers, and the public to know what’s in the ocean and where it is for effective and responsible marine stewardship.",2021-05-31,['open-ocean'],https://dam-prod2.media.mit.edu/x/2018/05/02/milos-prelevic-507074-unsplash.jpg,304169,,2022-06-23T12:52:06.770Z,"['katybell@media.mit.edu', 'gaikwad@media.mit.edu']",True,big-ocean-big-data,2018-04-02,"['data', 'environment', 'machine-learning', 'space', 'biology', 'ocean']","Big Ocean, Big Data",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-04-30T20:29:38.631Z,"Developed as part of the Playful Words research project at the MIT Media Lab's Laboratory for Social Machines, Learning Loops aims to make literacy learning—both on and off the screen—a family experience. We create small-scale coach-family networks centered around children’s play on custom-built, open-ended literacy learning apps. Building Coach-family networks helps to empower children as authors and facilitate their narrative development. StoryBlocks, a Learning Loops app, aims to promote literacy and social-emotional development through storytelling for children ages 6-10. StoryBlocks allows children to create and customize their own comic-style stories. These stories are analyzed using tools developed by the Learning Loops team to document children’s narrative development, and support Coaches as they provide personalized scaffolding for children’s narratives. How does the Learning Loop work?Data captured from a child’s use of StoryBlocks is streamed via the internet to cloud servers, and can immediately be accessed from a remote location by the child’s Coach. We have developed a Coach’s dashboard, called the Coach Console, powered by play analytics which enables a Coach to rapidly inspect play traces collected from a child’s activity and pull out their salient achievements, or meaningful moments. The Coaches then translate these moments into short personalized messages for the caregiver to inform them on their child’s narrative progress and provide suggestions for how to encourage new activities using StoryBlocks, together with background knowledge about their child’s path to literacy. Caregivers communicate with Coaches via text messages. Coaches can also help the children expand their sphere of learning and exploration by providing feedback on children's stories and suggesting new story starters  directly to the child’s device that are based on trends in the child’s play data.More info is available on the Learning Loops website.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2020/06/01/Logosadfsdf_rev.png,304191,,2020-06-09T14:43:44.333Z,"['anneli@media.mit.edu', 'jnazare@media.mit.edu', 'bonnerd@media.mit.edu', 'snehapm@media.mit.edu', 'sballing@media.mit.edu', 'jhgray@media.mit.edu', 'micshen@media.mit.edu']",True,family-learning-coach,2017-11-01,"['design', 'social-networks', 'human-machine-interaction', 'learning-teaching', 'storytelling']",Learning Loops,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-04-30T20:37:36.295Z,"OverviewWe have created a new customizable, multi-user research-through-play platform designed to facilitate social skill development for children with ASD. Through the highly motivating, individualized play environment, children can progress at their own pace while practicing social skills. Early results suggest that SPRING’s novel multi-player environment elicits social interaction in a way that can engage learners with very different interests.BackgroundPractitioners have long explored using motivating, personalized reinforcement to achieve developmental goals for children with Autism Spectrum Disorder (ASD) (Koegel & Mentis, 1985; Vismara & Lyons, 2007). SPRING–Smart Platform for Research, Intervention, and Neurodevelopmental Growth– is a customizable, interactive, research-through-play platform, built to systematically probe the effects of these reinforcement modalities on learning and physiological regulation (Johnson & Picard, 2017). SPRING has shown promise in facilitating increased engagement and skill development for children with autism and other neuro-differences, but it has lacked multi-user functionality and built-in means to prompt social skills, such as joint attention, turn taking, and cooperative play–until now. Adding this functionality allows practitioners to customize the reinforcement and developmental challenge of each individual SPRING unit while simultaneously encouraging social engagement by linking the units over a virtual network.ObjectivesHere, we present a Multi-SPRING system designed toStimulate early social experiences, such as joint attention, turn taking, and cooperative play;Facilitate simultaneous play between individuals with different skill levels, such as typical and autistic peers or siblings, while providing personalized reinforcement tuned to each individual’s motivating interests;Reduce anxiety associated with unaided social interactions and extend engagement in a multi-person activity; andPassively capture time-synchronized, quantitative measures of users’ affective states via wearable physiological sensors and data of users’ play progressions via SPRING.MethodsEach SPRING unit has a removable, modular center that can be adapted to the needs of an individual child by inserting different physical modules. An integrated smartphone and embedded LEDs allow user-selected customization of motivating reinforcement, such as favorite video clips, images, music, or colorful light displays. The smartphone also enables scaffolded difficulty levels within a single module so each child can be met with the “just right” challenge. Embedded digital sensors capture and store time-stamped data of a child’s interaction with SPRING. Paired with wearable physiological sensors, these data allow multimodal analysis of a child’s affective state and learning progression.The new Multi-SPRING system links multiple SPRING units in realtime through a virtual private room, much like a chat room. This method enables social multiplayer interactions, such as turn taking and cooperative play, while continuously capturing activity data logs from every child for future study.",2019-05-31,"['ml-learning', 'affective-computing']",https://dam-prod2.media.mit.edu/x/2018/04/30/INSAR2018_MultiSPRING_figure.png,304200,,2021-10-04T17:31:50.913Z,"['ktj@media.mit.edu', 'picard@media.mit.edu', 'fergusoc@media.mit.edu']",True,multi-spring,2017-08-01,"['cognition', 'learning-teaching', 'wearable-computing', 'behavioral-science', 'engineering', 'autism-research', 'physiology']",Multi-SPRING,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-01T01:54:41.219Z,"ModeSense is a full stack system that enables indoor environments to become aware of what is happening in them, and then enables the environment to locally inform (offline) all nearby phones and other electronic devices about the most appropriate operating mode in the present time and space.  We have developed an ultra-low cost, $5 device that can be installed in conference rooms, lecture halls, movie theaters, homes, and cars, which can dynamically determine the contexts in those areas and then locally broadcast the corresponding mode. Any phones in those areas are then aware of the mode in which they ought to be at that time, and can change their behavior accordingly.",2018-09-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2018/04/30/modeSense.PNG,304547,,2020-06-10T16:22:07.700Z,"['vmb@media.mit.edu', 'alims@media.mit.edu']",True,modesense,2016-10-01,[],ModeSense,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-05-01T02:20:54.342Z,"Teen Summit is a biennial week-long Youth Leadership event that brings Clubhouse youth together from each of the 100 Clubhouses internationally. Youth leaders explore issues relevant to them and propose solutions through the creative use of innovative, high-end technologies. The 2018 Teen Summit will take place in late July at Boston University, featuring a college and career fair, collaborative cross-cultural activities, and many other opportunities for educational, career, and personal growth.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2018/10/20/P1044915.JPG,304555,,2022-09-22T01:25:36.585Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'jaleesat@media.mit.edu']",True,teen-summit,2017-10-02,"['learning-teaching', 'makers']",Clubhouse Teen Summit,PUBLIC,http://www.computerclubhouse.org/teensummit,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-05-01T04:56:59.700Z,"Seventy percent of nations have deep-sea environments within their maritime Exclusive Economic Zones (EEZs), yet only 16 percent of them are able to explore those environments. This is especially true for less economically developed countries. The dearth of technological capability and knowledge leads to a lack of exploration, inappropriate or inadequate management decisions, and unaware populations. Our goal is to empower countries to explore their own deep-sea backyards using low-cost technology, while building lasting in-country capacity. Our project takes place in two small island developing states—the Republic of Kiribati, and Trinidad and Tobago. It utilizes Deep-Sea Drop Cameras developed by National Geographic’s Exploration Technology team (ExTech) and OpenROV’s Trident Remotely Operated Vehicles. Both technologies collect compelling imagery, but require minimal resources and expertise. In our pilot study during summer 2018, an engineer from ExTech and another team member traveled to each country to train a group of scientists, students, and communicators in the use of these technologies, which are to be left in-country until a scientist, a student, and a communicator from each country travel to the USA for further training in data analysis and creating outreach materials.",2021-05-31,['open-ocean'],https://dam-prod2.media.mit.edu/x/2018/05/01/octopus.png,304566,,2021-08-04T18:50:11.726Z,"['katybell@media.mit.edu', 'ahope@media.mit.edu', 'simun@media.mit.edu']",True,my-deepsea-mybackyard,2018-04-02,"['developing-countries', 'environment', 'history', 'wellbeing', 'ocean']","My Deep Sea, My Backyard",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-05-01T05:23:41.232Z,"Most of the underwater world remains far off the map. For many of the most exciting exploration challenges—from Maya cenotes to urban aquifers to archaeological treasures to coral reefs—map-making remains largely pre-industrial and time consuming. The difficulty and expense of mapping these spaces is a major barrier to storytelling for science, conservation, and stewardship. While many tools exist for open-ocean bathymetry (such as multibeam sonars), cost-effective diver-deployable tools for rapidly mapping complex and enclosed spaces are sorely lacking. Our goal is to create diver-deployable tools that are orders of magnitude faster, more precise, and less expensive than current practice–to enable mapping and imaging of these underwater resources at a societal scale.To this end we are developing low-cost, high-precision, diver-deployable underwater LIDAR and Depth-Imaging systems—3D scanning and navigation systems with which to quickly, safely, and beautifully map caves, aquifers, coral reefs, sunken cities, and other large-scale underwater spaces. To satisfy scientific and storytelling needs, these devices must be easy to use, have fine spatial resolution, map at swimming speed, produce data in industry-standard formats, and be completely open source at both hardware and software levels.",2021-05-31,['open-ocean'],https://dam-prod2.media.mit.edu/x/2018/05/01/PastedGraphic-1.png,304574,,2021-08-04T18:51:37.101Z,['katybell@media.mit.edu'],True,project-prometheus,2018-04-02,"['imaging', 'mapping', 'visualization', 'ocean']",Project Prometheus,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-05-01T06:39:32.154Z,"Microbes are the foundation upon which life on Earth depends: they set the boundaries of habitability for all plants and animals and create half of the oxygen we breathe. Ocean-dwelling microbes regulate the global climate and could hold the secrets to the origin of life. Put simply, we wouldn’t be here without microbes, yet most people don't realize how ubiquitous and important they are. The Micronauts project sought to build an emotional bridge over this gap. Microscopic creatures are, by definition, typically hidden from view, and the challenge of seeing them and perceiving their importance prevents emotional involvement and investment. We built an interactive installation that invites people from a general audience to step into the microscopic world. Visitors who step into the interactive projection represent sources of food or light, which allow for the microorganisms to live and reproduce. The reproduction and death rates for the visualization are based on mathematical models generated from the analysis of samples from Cape Cod. This kind of interactive interface has a special appeal to introduce children to Science concepts that are more abstract or far from their everyday experience. The exhibit happened during the 2018 National Ocean Exploration Forum on November 8 and 9, at the MIT Media Lab.Team MembersJeffrey Marlow (Harvard University)Benjamin Bray (MIT Sea Grant)Keith Ellenbogen (MIT Sea Grant)Craig McLean (MIT/WHOI Joint Program)Raquel Fornasaro (Contemporary Artist)Mark Adams (National Park Service/Painter)Caroline Rozendo (MIT Media Lab, Object-Based Media)",2018-11-30,"['object-based-media', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2019/02/27/micronauts-path.png,304582,,2020-03-19T01:48:20.761Z,['carolx@media.mit.edu'],True,micronauts,2018-02-28,"['art', 'augmented-reality', 'environment', 'learning-teaching', 'data-visualization', 'ocean']",Micronauts,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2018-05-01T07:09:13.941Z,"Throughout the ages bioluminescence has inspired myths. Long ago, sailors in the Indian Ocean encountered massive bioluminescent blooms as they sailed through the water, lighting the wakes of their ships like the spokes of a wheel carrying them to their destination in a chariot of wind and water.  They called this phenomenon “The Wheels of Poseidon."" Our goal is to harness the beauty of bioluminescence to create a new medium for artistic expression. We will generate a living, programmable bioluminescent display, with pixels and voxels built of bioluminescent plankton (Pyrosystis fuciformis) floating freely in the water column and stimulated to glow by a programmable pattern of pressure waves (acoustic waves) in the water. ",2018-12-31,['open-ocean'],https://dam-prod2.media.mit.edu/x/2018/05/01/free_download_sea_sparkle_01.jpg,304590,,2019-04-22T18:07:56.239Z,"['novysan@media.mit.edu', 'rssmith@media.mit.edu', 'danoran@media.mit.edu']",True,wheels-of-poseidon,2018-04-02,"['art', 'holography', 'ocean']",Wheels of Poseidon,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2018-05-01T14:00:38.430Z,"Interneternity is a dynamic musical composition built into the World Wide Web. The Internet changed the way that we listen to music, but music itself remains essentially unchanged. Can we create music that uses the Internet for more than a distribution medium? Can we make music that could not exist without the Internet?Interneternity is an experimental integration of music composition and the World Wide Web. Instead of releasing this music as a static file, it was released in the form of a website. When the site launched, users could play the composition continuously from beginning to end. But each new connection to the web server changes the musical structure in some small way…and eventually the original composition is unrecognizable.Certain interactions with the site may extend its lifetime, while certain interactions nudge the music towards inevitable disintegration. Will you get to hear the piece before it is broken forever?",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/05/01/Interneternity.png,304599,,2018-05-06T23:29:39.711Z,['holbrow@media.mit.edu'],True,interneternity,,[],Interneternity,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-01T14:33:02.708Z,The Gamma Instrument is a small-format interactive device hovering between the realms of musical instrument and medical instrument. A capacitive hand-following interface allows one to create abstract gamma sounds while surrounded by an orbit of gamma-frequency lights. Creating musical tones on the device could heighten gamma entrainment as it mimics higher-level cognition and gamma-band processing noted in musicians. This device is part of a larger exploration of 40 Hz frequencies and Alzheimer’s prevention/reduction within the context of the Aging Brain Initiative at MIT. The tabletop multi-sensory experience brings aspects of the Cognitarium to an interactive and portable platform.,,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/05/12/Gamma instrument.png,304607,,2020-02-15T17:09:35.533Z,['arieger@media.mit.edu'],True,gamma-instrument,2018-01-18,[],Gamma Instrument,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-01T14:48:59.398Z,"A conversational, voice-based interface for creating and playing Scratch projects makes Scratch accessible to children regardless of visual ability. Just as Scratch’s visual language lowers the barrier to entry for sighted children, the conversational interface lowers the barrier for children with visual impairments. The screenless interface is inspired by voice assistants and demonstrates the potential for programming through conversation.",2019-06-01,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2018/05/01/IMG_1313.JPG,304615,,2020-06-12T09:34:00.784Z,"['quacht@media.mit.edu', 'ascii@media.mit.edu']",True,making-scratch-accessible,2018-02-13,[],Agent-based programming interfaces for children,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-01T16:13:35.535Z,"Discreet Log Contracts (DLC) are a new type of smart contract which limit the information gained and influence of oracles, and can run on the very limited scripting system present in Bitcoin, without the need for more complex languages such as in Ethereum. DLC works by precomputing a wide range of potential outcomes for a given contract, and when the oracle announces an event, the event-dependent correct outcome can be published. There are a number of applications where this model applies, and the one that we’ll be starting with first is Bitcoin settled dollar futures. This use case introduces a useful tool to mitigate the volatility of Bitcoin.",2018-12-31,['digital-currency-initiative-dci'],,304636,,2018-05-01T20:05:09.760Z,"['joe@media.mit.edu', 'narula@media.mit.edu', 'adragos@media.mit.edu', 'tdryja@media.mit.edu']",True,bitcoin-settled-dollar-futures,2018-01-01,['cryptocurrency'],Building smart contracts with Bitcoin,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-01T18:11:35.679Z,"The project aims to investigate the relationships between emotion, wellbeing, skin, and skincare cosmetics. In our first study, we measured emotion/wellbeing, heart rate, and respiration using a mobile phone and a wearable sensor during consumer in-use test.",,['affective-computing'],https://dam-prod2.media.mit.edu/x/2018/05/01/black-and-white-close-up-eyes-407035_CJ5STMI.jpg,304663,,2018-05-01T19:59:13.741Z,['akanes@media.mit.edu'],True,emotion-wellbeing-x-skincare-cosmetics,2017-11-01,['wellbeing'],Emotion/wellbeing x skincare cosmetics,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-05-01T18:48:35.558Z," Research in Social Psychology has demonstrated that people's cognition is heavily influenced by an individual's sense of identity, which, in turn, is determined in part by their group relations.Using large amounts of data from online social systems, this project aims to uncover the role played in online polarization by this mechanism, especially in conjunction with the currently emerging plethora of alternative news sources.",2018-12-03,['scalable-cooperation'],,304679,,2018-05-07T17:26:55.950Z,['monsted@media.mit.edu'],False,Pol2,2018-05-01,[],Mechanisms of Political Polarization in Online Systems,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-05-01T19:17:40.971Z,"LUI is a scalable, multimodal web-interface that uses a custom framework of nondiscrete, free-handed gestures and voice to control modular applications with a single stereo-camera and voice assistant. The gestures and voice input are mapped to ReactJS web elements to provide a highly-responsive and accessible user experience. This interface can be deployed on an AR or VR system, heads-up displays for autonomous vehicles, and everyday large displays.Integrated applications include media browsing for photos and YouTube videos. Viewing and manipulating 3D models for engineering visualization are also in progress, with more applications to be added by developers in the longer-term. The LUI menu consists of a list of applications which the user can ""swipe"" and ""airtap"" to select an option. Each application has its unique set of non-discrete gestures to view and change content. If the user wants to find a specific application, they can also say a voice command to search or directly go to that application. Developers will be able to easily add more applications because of the modularity and extensibility of this web platform.For more information, contact graduate researcher Vik Parthiban at vparth@mit.edu.Advisors:V. Michael Bove, Director, Object-Based Media groupZach Lieberman, openFrameworks John Underkoffler, CEO, Oblong Industries; Scientific advisor, Minority Report and Iron Man interface",2019-08-31,['object-based-media'],https://dam-prod2.media.mit.edu/uuid/3a2dfab6-645e-41d4-8052-8945c3262933,304687,,2020-03-15T13:32:56.376Z,['vparth@media.mit.edu'],True,large-user-interface-with-gesture-and-voice-feedback,2018-11-01,"['virtual-reality', 'human-computer-interaction', 'augmented-reality', 'data-visualization', 'gesture-interface', 'autonomous-vehicles']",LUI: Large User Interface with Gesture and Voice Feedback,PUBLIC,https://vikparthiban.wordpress.com/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-01T19:43:32.467Z,"How something is presented can be as important as the message itself. We use various artificial intelligence techniques to model the ""subcarriers of information"" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. This would enable a broad-range, comprehensive analysis of how news presentation is trying to shape the public debate. Insights in this area are of vital importance in the age of political polarization and lead directly to applications that can break our bubbles.See:  viral.media.mit.edu/pub/tbd",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/15/trump.png,304705,,2019-04-18T16:50:14.749Z,"['lip@media.mit.edu', 'anderton@media.mit.edu']",False,newssense,2018-02-05,"['civic-technology', 'machine-learning']",Unspoken News,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-05-02T01:58:26.101Z,"The Māori of Aotearoa (New Zealand) have a long and deep connection to their island and ocean ecosystem. Concepts such as rāhui and mataitai are traditional methods of ocean protection that predate modern marine protected areas. Ocean Cultures hopes to support the Mahia Maori Council and their mataitai , a protected area of ocean where tangata whenua manage all non-commercial fishing, on the Mahia Peninsula, by co-developing  monitoring tools and programs for water analysis.  This partnership contributes to the ongoing documentation of the reefs in preparation for a species transplantation program for paua (abalone) no longer found in the waters. This project is funded in part by  by a Here be Dragons seed grant, a PGK Public Service Fellowship, and a National Geographic Early Career Grant. ",,"['responsive-environments', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2019/01/24/IMG_7181.JPG,305035,,2021-08-04T15:40:45.185Z,"['ave@media.mit.edu', 'devora@media.mit.edu']",True,oceancultures,2018-04-02,"['developing-countries', 'environment', 'genetics', 'collective-intelligence', 'ocean']",Ocean Cultures,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-05-02T15:40:45.059Z,"Embodied voice-based agents, such as Amazon’s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most, these agents represent their first experience of living with artificial intelligence in such private and personal spaces. However, little is known about people’s desires, preferences, and boundaries for these technologies. This projects seeks to answer questions surrounding this space: How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people’s preferences, desires, and boundaries? What tools can be used to understand this space?This work presents insights from a long-term exploration with over 70 children, adults, and older adults over a one-year period to interact with, discover, experience, reflect upon, and design voice-based agents. In addition, design tools and learnings from the experience have been developed into an open-source design kit to enable designers and researchers to explore these ideas with the broader population.For more information, please contact Nikhita Singh (nikhita@media.mit.edu) and Anastasia Ostrowski (akostrow@media.mit.edu).",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/05/02/IMG_3074.jpg,305056,,2018-07-15T03:34:47.660Z,"['cynthiab@media.mit.edu', 'akostrow@media.mit.edu', 'nikhita@media.mit.edu', 'haewon@media.mit.edu']",True,talking-machines-democratizing-the-design-of-voice-based-agents-for-the-home,2017-09-01,"['robotics', 'design', 'human-computer-interaction', 'artificial-intelligence', 'consumer-electronics', 'data', 'human-machine-interaction', 'interfaces', 'social-robotics', 'voice']",Talking Machines: Democratizing the design of voice-based agents for the home,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-02T16:45:37.866Z,"Privacy-preserving auditing on distributed ledgerszkLedger is a project that combines techniques from modern cryptography to analyze private data, while at the same time ensuring the integrity of that analysis by committing to the private data on a blockchain that is verified by all participants.zkLedger uses permissioned blockchains, zero-knowledge proofs, and additively homomorphic commitment schemes to create a tamper-resistant, verifiable ledger of transactions which hides the amounts, senders, and recipients of transactions, and still allows for rich auditing.Auditing complex systems increases confidence that said systems work as intendedLack of auditability or inaccurate results from auditing can have devastating effects, as demonstrated by the 2008 financial crisis. Traditionally, auditability for companies has been solved by the use of trusted third party auditors, such as the “Big Four”: Deloitte, PriceWaterhouseCoopers, Ernst and Young, and KPMG. Auditability for financial institutions and exchanges has been insured by federal and state government agencies such as the OCC, the FDIC, and SEC, to name just a few. Unfortunately, this type of auditing is a laborious, time-consuming process, that is far from real-time. Blockchain technology proposes an alternative, yet for that alternative to work, direct competitors would need to share information that they consider proprietary.Permissioned blockchainsRecently, financial institutions have formed consortia to investigate the use of a different architecture for securities settlement, inspired by blockchain technology. Bitcoin’s success has motivated institutions to consider upgrading their technical infrastructure by using permissioned blockchains, often maintained by participants with a consensus protocol. There are many strong players in this area that are making an impact, such as R3’s Corda system and IBM’s Hyperledger. With a large number of financial institutions already participating in these ledgers, what stands in the way of real-time auditing is a way to run computations on data while allowing participants to maintain the privacy of their data. This is where zkLedger can help.Zero-knowledge proofsUsing zero-knowledge proofs, one party can prove that they know some secret information without revealing what that information is. One way to understand this is to look at an example: suppose that Alice has two billiard balls, one red and one green (they are otherwise identical). Bob, who is colorblind, cannot tell the difference between the balls, so he assumes that they are the same color. Alice wants to convince Bob that they are in fact different without revealing the colors of the balls to Bob, so Bob takes both balls, puts them behind his back, and either switches them or keeps them in the same hand. If Alice can correctly answer each time whether they have been switched or not, then she has some knowledge about the balls, but has never revealed what the color of either ball is. If she were to answer incorrectly once, then we know that she was guessing each time. We use zero-knowledge proofs in APL to ensure that transactions added to the ledger are consistent, and that auditing computations are performed correctly. Going further, zero-knowledge proofs can impact many aspects of the financial sector by providing both secrecy and accountability to financial institutions, and we’re exploring new ways to leverage this technology.zkLedger uses and current statusWe are exploring non-financial uses cases for zkLedger.  We presented our paper ""zkLedger: Privacy-Preserving Auditing on Distributed Ledgers"" at NSDI 2018, released our prototype software on https://zkledger.org/ and are working on scalability extensions to this project.Read the paper below:",2021-10-01,['digital-currency-initiative-dci'],https://dam-prod2.media.mit.edu/x/2018/05/02/zkledger.jpg,305070,,2021-10-04T18:21:56.169Z,"['narula@media.mit.edu', 'madars@media.mit.edu']",True,zkledger-privacy-preserving-auditing,2018-01-01,[],zkLedger: Privacy-Preserving Auditing,PUBLIC,https://dci.mit.edu,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T17:05:06.351Z,"The LEGO Wayfinder project combines LEGO, robotics, and seawater into a playground of project-based learning and citizen science for budding engineers and explorers. As part of this outreach program, our team has developed a first prototype of a buildable LEGO marine exploration vehicle kit—addressing some of the design challenges of building for the underwater context.Our aim is to build an awareness of the state of the aquatic environment and instill a greater responsibility in shaping our interactions with the environment. To do so, young people will view underwater wonders of the world with their robots and get outside to explore their local waterway. Our approach embraces Seymour Papert’s model of ""low floors"" (where getting started is easy), and ""high ceilings,"" where students can pour their time and collaborative work efforts into creative engineering solutions to carry out a marine science experiment of their own design in the field.Going forward, we will be developing the next prototype focusing on watertight sensors and the ease and operability of the underwater build kit allowing for even more playful and creative transformations when in the hands of the students. With future prototypes we will adapt this programming for wider student audiences in after-school programs, classrooms, libraries, and community-based mentor clubs for youth.",2021-05-31,"['ml-learning', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2019/10/01/LEGOWayfinder.jpg,305078,,2021-08-04T18:50:28.354Z,"['katybell@media.mit.edu', 'ave@media.mit.edu', 'ps1@media.mit.edu', 'kamcco@media.mit.edu', 'jschow@media.mit.edu', 'novysan@media.mit.edu']",True,lego-wayfinder,2018-04-01,"['robotics', 'environment', 'kids', 'learning-teaching', 'makers', 'engineering', 'ocean', 'climate-change']",LEGO Wayfinder,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2018-05-02T18:56:09.161Z,"SCALE provides a framework for load data from distributed load-sensitive modules for exploring force-based interaction. Force conveys not only the force vector itself but also rich information about activities, including way of touching, object location and body motion. Our system captures these interactions on a single pipeline of load data processing. Furthermore, we have expanded the interaction area from a flat 2D surface to 3D volume by building a mathematical framework, which enables us to capture the vertical height of a touch point. These technical invention opens broad applications, including general shape capturing and motion recognition. We have packaged the framework into a physical prototyping kit, and conducted a workshop with product designers to evaluate our system in practical scenarios.",2019-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/05/16/thumbnail_b2eH2vd.png,305108,,2020-06-09T17:40:15.555Z,"['ishii@media.mit.edu', 'achituv@media.mit.edu', 'taka_y@media.mit.edu']",True,scale2018,2018-01-01,[],SCALE,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-02T19:49:40.774Z,"Find out more about the new version of the system, ""The Thinking Cap 2.0"" here.Peoples' mindsets, meaning their beliefs about their own intellectual abilities, affect their effort and thereby their performance on tasks. The goal of this project is to investigate if we can change peoples' mindsets using a technological intervention. The Thinking Cap is a wearable system that communicates praise for effort and ability in order to improve the resilience and self-esteem of the student wearing it and thus  positively influence their motivation and academic achievements (momentary learning). The Thinking Cap is built into a ""Sorting Hat"" from the Harry Potter franchise, which we equipped with an embedded electroencephalography (EEG) headset and a Bluetooth speaker. We chose this “magical” object from the well-known film/book franchise because popular press articles have suggested that people are likely to believe they possess the traits the Sorting Hat tells them they have, and consequently behave in related ways. One goal of this study is to investigate these findings in more depth. In our study we measure the self-esteem of children before and after the “intervention of the hat” to determine whether we observe any changes in their self-perception. The Sorting Hat could be replaced by any other object that a child may believe has ""magical"" powers. The hat uses established state-of-the-art Brain-Computer Interface (BCI) algorithms to recognize several mental processes like motor, auditory, or visual imagery as well as cognitive load and engagement level of the child (see also a related  project from our group called AttentivU). In an initial phase, the hat is used to recognize and report on the brain patterns of the child. We use supervised and unsupervised ML algorithms to train the system by asking the user explicitly to imagine/visualize either a simple movement or an object in their head (binary classification in most of the cases). The hat ""tells"" the child, via the Bluetooth speaker embedded in the hat, which of the two things he/she is thinking about. We hypothesize that, by demonstrating this basic capability of the hat to recognize their brain activity, the child will develop trust in the hat’s abilities to know him or her. Thus, when the hat in a later phase praises the child for their ability or effort on a task (e.g., a math test), the child is likely to be affected by its suggestions in their future performance (""You are doing well on this test now, let's do one more!""). We hypothesize that using the hat can thus lead to improved academic performance (momentary learning). If you are interested in participating in this study (your kid should be at least eight years old), please contact us at nkosmyna@media.mit.edu.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/02/6H7A0176.jpg,305124,,2021-09-30T11:30:11.103Z,"['pattie@media.mit.edu', 'nkosmyna@media.mit.edu']",True,thinking-cap,2017-12-01,"['kids', 'learning-teaching', 'wearable-computing', 'interfaces', 'cognitive-science', 'wellbeing']",Thinking Cap,PUBLIC,http://kosmina.eu,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-02T20:15:29.144Z,"Struggling to build your organization's ability to work with data? Use our hands-on learning program to kickstart your data culture.datacultureproject.orgData is everywhere right now. But many organizations like yours are struggling to figure out how to build capacity to work with data. You don't need a data scientist; you need a data culture.Use our self-service learning program to facilitate fun, creative introductions for the non-technical folks in your organization. These are not boring spreadsheet trainings. The free tools and activities on the linked website are hands-on and designed to meet people where they are across your organization, and build their capacity to work with data. Try to kickstart your data culture by running one activity per month as a brown bag lunch. Our videos and facilitation guides will lead you through running them yourself.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2018/05/02/dcp-big.png,305132,,2022-06-10T13:35:30.062Z,['rahulb@media.mit.edu'],True,data-culture-project,2017-09-01,"['data', 'learning-teaching']",Data Culture Project,PUBLIC,http://datacultureproject.org,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-05-02T20:27:32.078Z,"Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. It is, however, often expensive and reliant on experts to interpret the resulting images. We have developed a process for learning associations between standard white light images and both biomarkers and expert annotations of disease. Oral imaging is one particular example of biomarker imaging that can supplement expert knowledge; the biomarker porphyrin is associated with poor oral health and oral cancer. We report that our process learns to accurately predict the presence of porphyrin and expert-annotated conditions.Why is this work important?Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. Capturing biomarker images requires specialized and often expensive hardware, annotations, and analyses by experts, resulting in substantial diagnosis delays.What has been done before?Even when biomarker imaging is available, experts are often needed to interpret the resulting images. There is a rich literature on medical image segmentation, but many approaches—especially deep learning—require large amounts of images and operate on information from only a single given imaging modality.What are our contributions?We successfully learn assocations between images and union signatures of biomarker presence and expert disease annotations. By transforming the image-level segmentation problem into a region-based problem, we are able to learn from far fewer images than other approaches. We specifically test our approach on detecting the biomarker porphyrin and associated conditions in millions of image patches. Once trained, the classifiers predict the location of porphyrin in images without requiring specialized biomarker imaging devices or expert intervention.What are the next steps?We are developing processes incorporating numerous other biomarkers, conditions, and imaging modalities.Related projectsBiomarker Imaging with Mobile Phones",2022-08-26,['health-0-0'],,305140,,2022-10-14T21:42:58.812Z,['pratiks@media.mit.edu'],True,machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images,2017-02-01,"['health', 'imaging', 'machine-learning', 'point-care']",Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T20:40:03.758Z,"We have developed a new process to screen patients at the point-of-care with FDA-approved technology-enabled mobile health screenings (TES) and compare the results with routine health screenings. A study of nearly 500 patients was conducted to test the effectiveness of this new screening process. This is one of the first studies to investigate using TES to augment routine health examinations. We recommend using TES in synergy with routine health screenings to identify missing sick patients who might otherwise lack comprehensive primary care.Why is this work important?Providing good healthcare in low- and middle-income countries (LMIC) paradoxically requires expensive equipment, which may not be easily available because of resource limitations, for health monitoring and assessment. There is high variation in the degrees of healthcare access in LMIC, but such access is important because cardiovascular diseases, preventable blindness, oral cancer, and treatable neurological conditions constitute more than half of the disease burden in LMIC. Comprehensive TES may allow for more patients to be screened for more conditions in resource-limited settings, improving their access to primary healthcare. A lack of consensus exists about the usefulness of TES in augmenting primary health screenings in LMIC.What has been done before?Devices that allow TES have typically been evaluated in isolated silos, concentrating on individual devices or specific anatomical sites. They have additionally not been comprehensively evaluated alongside routine health screenings.What are our contributions?This is one of the first studies to investigate using multiple TES to augment routine health examinations. To facilitate this large-scale study, we developed and successfully used web examination platforms that enabled multiple physicians to diagnose health conditions remotely. We identified patients who would not have received the care they need in the absence of TES, and link TES to primary health outcomes.This study led to significant insights about strategies to develop technologies at MIT that are ready for deployment for effective and scalable primary care in the real world.What technology-enabled examinations were performed?Single-lead ECG: AliveCor Mobile ECGBlood oxygen saturation: Contec Medical Systems 50-DL Pulse OximeterOral imaging: ACTEON SoprocareRetinal scan: D-EYE direct ophthalmoscopy adapter attached to iPhone5s cameraTympanic membrane imaging: CellScope Oto with iPhone5 LEDs and cameraNeurological examinations: Microsoft KinectWhat are the next steps?We are actively working on automated diagnoses, analyses of disease co-occurrence, and patient risk stratification.Future studies that build on our technology-enabled screening process can evaluate the process for larger numbers of patients. A future longitudinal study may allow for additional insights into time-varying conditions.Related projectsMachine Learning and Automated Segmentation of Oral Diseases using Biomarker Images",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/x/2018/10/21/tes-fig-1.png,305148,,2022-10-14T21:42:26.541Z,['pratiks@media.mit.edu'],True,technology-enabled-mobile-phone-screenings-augment-routine-primary-care,2015-05-15,"['health', 'imaging', 'healthcare', 'public-health', 'point-care', 'clinical-trials']",Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-05-02T20:42:11.595Z,"Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Near-infrared imaging offers a non-ionizing alternative for dental analysis. We describe the construction and validation of a near-infrared imaging device to identify dental caries without the use of radiographs. It uses 850 nm light, allowing for a low-cost sensor and device construction.Why is this work important?While two-dimensional radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.What has been done before?NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. Previous studies have similarly neglected the extent to which indicators of dental health, especially early caries associated with the onset of more severe conditions, can be identified in 850 nm NIR images.What are our contributions?We describe the construction of a near-infrared imaging device to identify dental caries without the use of radiographs. Light-emitting diodes at 850 nm allow for the use of a low-cost imaging sensor. Its camera-wand design allows for multiple imaging configurations: reflectance, transillumination, and occlusal transillumination. We validate the diagnostic uses for the images produced by our device, determining that they provide insight into the location of caries without ionizing radiation. The camera-wand system was also capable of revealing demineralized areas, deep and superficial cracks, and other clinical features of teeth usually only visualized by X-rays.What are the next steps?Ongoing work is being done to analyze the extent of features made visible by our device and to model the interaction of light inside teeth in order to provide even more diagnostic power.Related projectsNear-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography ImagingNon-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/x/2018/10/21/15-16_2.jpg,305156,,2022-10-14T21:53:50.839Z,"['pratiks@media.mit.edu', 'kla11@media.mit.edu']",True,near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth,2016-06-01,"['computer-vision', 'bioengineering', 'health', 'imaging', 'engineering', 'point-care']",Near-Infrared Imaging for Detecting Dental Caries,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T20:43:28.356Z,"Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Cone beam computed tomography (CBCT) is also widely used for diagnosis but is expensive and relatively cumbersome. Near-infrared imaging (NIR) offers a non-ionizing alternative for dental analysis. We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination imaging modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients.Why is this work important?Two-dimensional radiographs and cone beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.What has been done before?NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. It is not well understood what clinical features, if any, are present in NIR dental images, especially at 850 nm. Our previous work has examined the sensitivity of 850 nm NIR images to early caries lesions, but if NIR is to synergistically augment X-rays and CBCT as the standard of care, we must evaluate how well such images represent other clinical features.What are our contributions?We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients. We also show that NIR imaging identifies clinical features associated with early dimineralization and enamel caries that are not apparent upon expert visual examination.What are the next steps?Ongoing work is being done to model the interaction of light inside the tooth in order to provide even more diagnostic power.Related projectsNear-Infrared Imaging for Detecting Dental CariesNon-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/x/2018/05/04/NIR Optics.jpg,305164,,2022-10-14T21:53:27.386Z,['pratiks@media.mit.edu'],True,near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging,2017-05-08,"['bioengineering', 'health', 'imaging', 'mechanical-engineering', 'engineering', 'point-care']",Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T20:45:07.076Z,"Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Our goal is to augment ultimately replace the prevalence of ionizing and expensive X-ray imaging and cone-beam computed tomography (CBCT) for dental care with near-infrared (NIR) imaging. Translucency of teeth in the NIR range offers non-ionizing and safe detection of dental features. NIR can be used in conjunction with multiple light sources to create three-dimensional images of teeth. By modeling the scattering of photons in teeth, we can effectively see several millimeters inside, providing additional diagnostic value.Why is this work important?Two-dimensional radiographs and cone-beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.What has been done before?NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. NIR light at 850 nm and 1310 nm, which strike a balance between enamel and water attenuation, have been shown to provide helpful diagnostics that visual examination alone lacks. Our previous work has demonstrated the sensitivity of 850 nm NIR images to early caries lesions and demineralization. For NIR is to synergistically augment or eventually replace ionizing radiation as the standard of care, we aim to expand its diagnostic potential to clinical features that exist beyond the surface of the tooth.What are our contributions?This is ongoing research. Preliminary work shows promise for augmenting the diagnostic power of NIR by modeling scattering.What are the next steps?Large-scale screenings can evaluate the effectiveness of our new imaging process.Related projectsNear-Infrared Imaging for Detecting Dental CariesNear-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/x/2018/05/04/Screen Shot 2018-05-04 at 11.45.25 AM.png,305172,,2022-10-14T21:52:03.185Z,"['kla11@media.mit.edu', 'gyauney@media.mit.edu', 'pratiks@media.mit.edu', 'arana@media.mit.edu']",True,replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis,2017-05-08,"['computer-vision', 'health', 'imaging', 'engineering']",Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T20:47:02.076Z,"Technical summary: Future of clinical development is on the verge of a major transformation due to convergence of large new digital data sources, computing power to identify clinically meaningful patterns in the data using efficient artificial intelligence and machine-learning algorithms, and regulators embracing this change through new collaborations. A perspective  led by Dr. Shah, with co-authors from the United States Food and Drug Administration , was published in Nature Digital Medicine that summarized insights, recent developments, and recommendations for infusing actionable computational evidence to accelerate clinical development of life-saving therapies. Other contributors were from academy, biotechnology industry, nonprofit foundations, regulators, and technology corporations. Analysis and learning from publicly available biomedical and clinical trial data sets, real-world evidence from sensors, and health records by machine-learning architectures were discussed. Strategies for modernizing the clinical development process by integration of AI- and ML-based digital methods and secure computing technologies through recently announced regulatory pathways at the United States Food and Drug Administration were outlined.  The key goal of this collaborations is increasing the impact of  novel digital algorithmic evidence to improve medical care for patients.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/511aa608-4193-4ffd-a1e5-afba62e115e5,305180,,2022-10-14T21:40:21.473Z,"['pratiks@media.mit.edu', 'fkendall@media.mit.edu']",True,artificial-intelligence-for-clinical-development,2017-01-23,"['computer-vision', 'human-computer-interaction', 'artificial-intelligence', 'bioengineering', 'data', 'extended-intelligence', 'genetics', 'health', 'imaging', 'machine-learning', 'medicine', 'clinical-trials', 'chemistry', 'clinical-science']",Artificial Intelligence and Machine Learning in Clinical Development: A Translational Perspective,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T21:04:11.658Z,"We report a novel method that processes biomarker images collected at the point of care and uses machine learning algorithms to provide a first level of screening against oral diseases. A machine learning classifier is trained to learn pixel-by-pixel mappings from RGB oral images and output areas with disease. This method can be adapted to  process biomarker images from other organs as well.Why is this work important?Visual inspection and probing techniques have been traditionally used for diagnosis of oral diseases in patients. These traditional methods are subjective and not scalable. We describe the use of RGB color images acquired by low-cost camera devices coupled with machine learning to detect areas with poor oral health.What has been done before?Currently the gold standard for oral diagnosis is visual inspections by a dentist followed by X-rays. These methods are expensive and invasive. What are our contributions?We implement a novel technique to combine medical expert knowledge with biomarker signatures.  We  use RGB color images taken directly at the point-of-care, using low-cost hand-held devices, to provide a first level machine-learning powered screening for patients.What are the next steps?We are expanding the repertoire of biomarkers that can be detected in RGB color images acquired at the point-of-care and pairing them with automated machine learning exams.Related projectsTechnology-Enabled Mobile Phone Screenings Augment Routine Primary Care",2022-08-26,['health-0-0'],,305188,,2022-10-14T21:44:39.819Z,['pratiks@media.mit.edu'],True,machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images,2017-04-24,"['computer-vision', 'health', 'imaging', 'machine-learning', 'engineering', 'point-care']",Machine Learning and Automated Segmentation of Oral Diseases Using Biomarker Images,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T21:06:26.114Z,"Imaging fluorescent disease biomarkers in tissues and skin is a non-invasive method to screen for health conditions. We report an automated process that combines intraoral fluorescent porphyrin biomarker imaging, clinical examinations, and machine learning for correlation of systemic health conditions with periodontal disease. 1,215 intraoral fluorescent images, from 284 consenting adults aged 18-90, were analyzed using a machine learning classifier that can segment periodontal inflammation. The classifier achieved an AUC of 0.677 with precision and recall of 0.271 and 0.429, respectively, indicating a learned association between disease signatures in collected images. Periodontal diseases were more prevalent among males (p=0.0012) and older subjects (p=0.0224) in the screened population. Physicians independently examined the collected images, assigning localized modified gingival indices (MGIs). MGIs and periodontal disease were then cross-correlated with responses to a medical history questionnaire, blood pressure and body mass index measurements, and optic nerve, tympanic membrane, neurological, and cardiac rhythm imaging examinations. Gingivitis and early periodontal disease were associated with subjects diagnosed with optic nerve abnormalities (p <0.0001) in their retinal scans. We also report significant co-occurrences of periodontal disease in subjects reporting swollen joints (p=0.0422) and a family history of eye disease (p=0.0337). These results indicate cross-correlation of poor periodontal health with systemic health outcomes and stress the importance of oral health screenings at the primary care level. Our screening process and analysis method, using images and machine learning, can be generalized for automated diagnoses and systemic health screenings for other diseases.Why is this work important?Standard practices like visual assessment and diagnosis of oral diseases using bleeding with a probe do not account for patient-to-patient variation or identify disease progression risk. This study uses a machine learning model to segment oral porphyrin biomarker levels from intraoral photographs and find correlations with and prognoses of systemic health conditions.What has been done before?Current methods to diagnose oral diseases include visual inspection by doctors and probing the gums. Positive correlations have been found between oral health and heart diseases, diabetes, tobacco use, and smoking, but all depend on visual examination by doctors.What are our contributions?We report a novel process for automated machine learning oral health examinations using images of fluorescent biomarkers and cross-correlations between oral and systemic health. We collect a novel dataset for the study and find correlations between oral health and systemic conditions like swollen joints, optical nerve abnormalities in retinal scans, and a family history of eye disease. Our approach can be generalized for predicting systemic health by analyzing other biomarker images.What are the next steps?We are actively expanding the work to a larger population to discover novel cross-correlations between other biomarkers and systemic health outcomes.Related projectsTechnology-Enabled Mobile Phone Screenings Augment Routine Primary CareMachine Learning and Automated Segmentation of Oral Diseases using Biomarker Images",2022-08-26,['health-0-0'],,305196,,2022-10-14T21:44:11.887Z,['pratiks@media.mit.edu'],True,machine-learning-from-biomarker-signatures-and-correlation-to-systemic-health-conditions,2017-09-18,"['computer-vision', 'artificial-intelligence', 'health', 'machine-learning', 'engineering', 'point-care']",Machine Learning from Biomarker Signatures and Correlation to Systemic Health Conditions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T21:09:49.690Z,"Paper diagnostics can be used for affordable and scalable biomarker detection. We propose new methods for stabilizing cellulose-based immunoassays, leading to a low-cost paper diagnostic assay for the detection of oral disease biomarkers in human saliva.Why is this work important?Low-cost detection of biomarkers associated with the majority of diseases is not feasible.What has been done before?Paper-based assays have been used to detect biomarkers in human serum or blood. The paper-based detection of biomarkers in saliva has not been extensively studied.What are our contributions?We propose methods for stabilizing paper-based immunoassays for robust and low-cost detection of salivary biomarkers. This method is low-cost and can facilitate rapid detection of oral biomarkers. We specifically describe the detection of biomarkers associated with oral diseases, MMP-8 and -9.What are the next steps?We are expanding this approach to assay biomarkers in serum and blood.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/b49d8eda-caa1-4d7b-ba79-e6a741444db7,305204,,2022-10-14T21:51:04.765Z,['pratiks@media.mit.edu'],True,detecting-biomarkers-with-printable-paper-diagnostics,2016-05-02,"['bioengineering', 'health', 'engineering', 'chemistry']",Detecting Biomarkers with Printable Paper Diagnostics,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-02T21:11:14.147Z,"A large proportion of the American population currently suffers from sleep disorders. Among them are patients with obstructive sleep apnea (OSA), who repeatedly stop breathing while asleep. Current screening methods and devices are impractical for widespread screening. We introduce a new model for low-cost OSA screening consisting of an at-home, wearable sleep mask that can easily track the wearer's sleep patterns. The data collected overnight by this sensory mask provides a determination of a patient's OSA risk.Why is this work important?There are 7-18 million Americans suffering from sleep disorders. Among them are patients with OSA, who stop breathing either completely or partially while asleep. This is a serious condition with few reliable low-cost devices available for primary diagnosis without expert supervision.What has been done before?The gold standard for OSA diagnosis is overnight polysomnography (PSG). Apart from that there are many home diagnostics devices available. However, many at-home devices offer poor diagnostic quality and some of them also require expert intervention, from installation of the device to analysis of the data.What are our contributions?We report the construction and validation of a design for low-cost OSA screening built around a simplified screening device embedded in an at-home wearable sleep mask. This simplified screening system allows for OSA diagnosis without imposing the costs or time commitment of a full PSG.What are the next steps?In the next iterations of the device, we aim to improve the mechanical design and ease of use, as well as automate data analysis and screening so that the device can be evaluated in larger studies.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/f4b53335-ae2f-4c0a-9849-1ddcf1657d26,305212,,2022-10-14T21:51:25.277Z,['pratiks@media.mit.edu'],True,at-home-sleep-apnea-screening,2016-05-09,"['health', 'engineering', 'point-care']",At-Home Sleep Apnea Screening,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-05-02T22:58:08.605Z,"PhonoBlocks is an app similar in design and purpose to SpeechBlocks. However, the blocks in this design are not letters, but sounds of the language (phonemes). While the sound of a letter in English changes depending on the word the letter is in, phonemes behave consistently in every word. Therefore, it might be easier for children to master usage of phoneme-based blocks, opening the avenue for early “writing”. Playing with phonemes is likely to help building phonological awareness (the ability to distinguish and manipulate the sounds within the words) - the key component of early literacy. When competence with phonemes is acquired, phoneme-to-letters correspondences may start to be introduced. PhonoBlocks is part of our exploration attempting to find out what designs facilitate early forms of meaningful writing.",,[],,305224,,2018-05-02T23:01:45.429Z,[],True,phonoblocks,2018-04-10,[],PhonoBlocks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-03T03:07:23.871Z,"Symbiotic guitar playing between human and machine fingers. The system can be used as a learning tool or a real-time augmentation to the human guitar player, offering previously impossible combinations of notes. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/02/IMG_5470.JPG,305565,,2018-05-07T01:50:19.235Z,['sangwon@media.mit.edu'],True,guitar-machine,2017-04-01,[],Guitar Machine,PUBLIC,http://www.sangww.net/2018/04/guitar-machine-i.html,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-03T03:17:08.246Z,"Exploring robotic sound generation mixed with human movements on the guitar. Guitar Machine II is a robotic guitar that responds to human gestures, as well as other input means such as midi controllers or algorithmic composition. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/02/IMG_7176.png,305573,,2019-07-24T13:53:11.882Z,['sangwon@media.mit.edu'],True,guitar-machine-ii,2017-09-01,"['robotics', 'human-computer-interaction', 'music']",Guitar Machine II,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-03T04:36:33.605Z,"Exploring the connection between our two minds: the one in our head and the one in our body. The mind-gut connection has flourished as a research area in the past few decades, elucidating the  key role of the enteric nervous system (ENS or ""gut-brain"") in stress, affect, and memory. However, this connection has not been explored for wearable technology—applications and research for cognitive phenomena remain biased towards the cerebrum. In this project, we are non-invasively acquiring gastric myoelectric activity from the abdomen to evaluate the potential for a new area of wearable technology that can inform users on affective, stress, or memory states based on signals produced by the ENS.",2018-02-28,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/08/x_hallway-evening-cropped.jpg,305581,,2019-11-06T22:25:23.971Z,"['avujic@media.mit.edu', 'pattie@media.mit.edu']",True,serosa,2017-10-01,['neural-interfacing-and-control'],Serosa I,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-05-03T11:56:06.557Z,"The Montessori Method is an educational approach that emphasizes independence and respect for a child's natural development process. Montessori materials are a hallmark of the Montessori
Method. These self-teaching tools encourage exploration of concepts in the
areas of mathematics, language, sensorial development, and practical life, and
allow children to direct their own learning with the light guidance of teachers
and peers. We envision a novel framework of unobtrusive sensor networks to understand and reflect on a child’s learning progress, by instrumenting existing Montessori learning materials using distributed sensing techniques.",,['social-machines'],,305589,,2018-05-04T11:39:56.455Z,['saquib@media.mit.edu'],True,instrumentation-of-montessori-learning-materials,2017-09-04,"['human-machine-interaction', 'learning-teaching']",Instrumentation of Montessori Learning Materials,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-03T12:02:36.927Z,"The World Health Organization estimates that more than 55 million people worldwide suffer from dementia. As the proportion of older people in the population is increasing in nearly every country, this number is expected to rise to 78 million in 2030 and 139 million in 2050. The MIT Media Lab Fluid Interfaces team and member company NTT DATA have been collaborating on a project called Rhizome. The project has the challenging goal of improving the lives of patients with an early stage of dementia, including Alzheimer’s. (The word Rhizome is of Greek origin and means a continuously growing mass of roots.) Rhizome is an intelligent digital memory book that aims to help dementia patients with their memories – a digital therapeutics tool based on reminiscence therapy.  Patients can use Rhizome to recollect, rehearse their memories, and reflect on their personal stories. The Rhizome system replaces handmade, paper-based memory books that are normally made by the patient's families and caregivers.  Patients can engage with Rhizome in several different modes to help keep their memories alive for longer, recover their lost memories, and feel more connected to their loved ones. Rhizome uses memory graph analysis and AI to map the patient’s memories. The tool’s functionality showcases memories, identifies friends and family members, and generates memories game questionnaires. We hope that the system will offer opportunities for doctors and caregivers to monitor the patient’s memory decline and evaluate the impact of specific treatments.   The following video is an overview of Rhizome, reflecting the team’s vision and collaboration.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2022/09/02/laura-fuhrman-73OJLcahQHg-unsplash.jpg,305597,,2022-09-16T23:02:36.684Z,"['mmv@media.mit.edu', 'alemari@media.mit.edu', 'diegoan@media.mit.edu', 'eijiy@media.mit.edu', 'ohsugi@media.mit.edu', 'dicostap@media.mit.edu']",True,rhizome,2018-01-01,[],Rhizome,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-03T15:05:12.192Z,"Ritual I: The Thing Itself consists of a choreographed robotic body that is in constant flux. It performs a dance of repetitive patterns that become a trance ritual of vibrations and movement. The thing or dancing body stands on a metal sheet that vibrates with every move it makes; this way the body affects its territory with every movement. In return, the vibrations of the metal add to the vibration of the thing itself while it moves, and in this way the body is affected by its territory. This is a feedback system, a cyclic loop, a transduction network, a ritual dance between a body and its territory. This ritual explores how agency becomes increasingly distributed among bodies and territories, which opens interactions of hybrid selves, blurring the limits of bodies and its environment, understanding them all as an assemblage of vibrant matter. The architecture comprises complex assemblages—nothing is something by itself, but things are themselves by being in a relationship with others. This is an entangled architecture of bodies. This is a way to explore and diversify the imaginative projections and potentials of a kinetic non-human body and how sound and vibration are key to trigger agency and vibrant presence.It is the thing itself that has been allowed to be deployed as multiple, and thus allowed to be grasped through different viewpoints, before being possibly unified in some later stage depending on the abilities of the collective to unify them.—Bruno Latour, Assembling the Social: An Introduction to Actor-Network-Theory (Oxford: Oxford University Press, 2005), 116.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/05/03/IMG_1938.jpg,305607,,2018-10-19T19:41:24.966Z,['nicolelh@media.mit.edu'],True,ritual-i-the-thing-itself,2018-05-01,"['robotics', 'performance']",Ritual I: The Thing Itself,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-05-03T18:09:55.926Z,"We built a low-cost and open source 405 nm imaging device to capture red fluorescence signatures associated with the oral biomarker porphyrin, demonstrating comparable performance to an expensive commercially available device.  We also provide a miniaturized mobile-adaptable version of the device. A step-by-step guide for device assembly and the associated computer vision algorithm are shared on the project website to facilitate open-source access to imaging technologies.Related projectsMachine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/d48d650f-01b7-4cb0-ac4c-bf75a1c2a5b6,305617,,2022-10-14T21:54:10.674Z,['pratiks@media.mit.edu'],True,porphyrin-imaging,2015-06-01,"['computer-vision', 'health', 'imaging', 'engineering', 'point-care']",Biomarker Imaging with Mobile Phones,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-05-03T19:25:23.134Z,"Industrial diversification is a path-dependent process that leverages knowledge, skills, and new technologies. Because such resources are difficult to move, geography plays a crucial role in determining the future economic activities of countries, regions, and cities. Yet most of the evidence on the geographic diffusion of economic activities is restricted to the last 60 years and relies on correlations. This paper analyzes the geographic diffusion of economic activities for Swedish towns between 1850 and 1950, using the evolution of the railroad network as a way to address endogeneity. We use the straight line between Sweden's 10 largest towns as an instrument for train adoption. Our instrumental variable estimates show that regions are more likely to diversify into sectors that are present in their train neighbors, suggesting that the impact of connectivity goes beyond access to markets: connectivity also promotes diffusion of economic activities, even at early stages of development.",2019-08-31,['collective-learning'],,305627,,2020-08-28T19:02:34.942Z,"['hidalgo@media.mit.edu', 'crisjf@media.mit.edu']",True,trains-of-thought-railroad-access-and-knowledge-diffusion-in-sweden,2017-11-01,[],Railroad Access and Diffusion of Industries: Evidence from Sweden during the Second Industrial Revolution,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-03T22:44:59.022Z,"There are currently  UROP openings for this project. AttentivU is  a  device, in the  form factor of a pair of glasses,  which senses brain activity (electroencephalography - EEG) as well as eye movements (electrooculography - EOG) to measure different cognitive processes in real time,  including  cognitive load, fatigue, engagement, and focus (please check our FAQ on this page for the definitions!). The device can be used for passive or active interventions, for example to monitor the state of the user, providing gentle audio or haptic feedback when the user is less attentive (driving scenario) or adapting the environment when cognitive overload is detected (blocking the notifications). The system can operate in a standalone, non-networked fashion to ensure privacy of the collected data.  The use of the AttentivU glasses can be limited to just those moments when the user decides they want to learn to sustain their attention.We have tested the first generation prototype of the device in workplace (check the paper here ) and learning settings (check the paper here and here) with over 100 subjects. We performed experiments with people studying or working by themselves, viewing online lectures as well as listening to a professor at the university.  We have now completed the first test of the glasses prototype (check the paper here) with more than 30 subjects who were performing a driving task in a simulator overnight when their attention waned, and the system reminded them with a sound or with a vibration to pay attention to the road . This research paper was rewarded with the honorable mention award at the 2019 AutoUI conference (paper will appear in the end of September 2019). Please check the dedicated page about the car use case scenarios here.To learn more about AttentivU  - please check its website. Please check the FAQ (bottom of this page) if you have more questions about the project and/or want to collaborate with us!",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/06/14/1.jpg,305638,,2023-08-18T21:51:40.545Z,"['nkosmyna@media.mit.edu', 'pattie@media.mit.edu']",True,attentivu,2018-01-01,"['human-computer-interaction', 'augmented-reality', 'health', 'neurobiology', 'wearable-computing', 'sensors', 'interfaces', 'machine-learning', 'technology', 'cognitive-science', 'wellbeing']",AttentivU,PUBLIC,https://braini.io,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-05-04T18:30:09.597Z,"When we form memories, not everything that we perceive is noticed; not  everything that we notice is remembered. Humans are excellent at  filtering and retaining only the most important parts of their  experience—what if our audio compression had the same ability? Our goal is to understand what makes sound memorable. With this  work, we hope to gain insight into the cognitive processes that drive  auditory perception and predict the memorability of sounds in the world  around us more accurately than ever before.  Ultimately, these models  will give us the ability to generate and manipulate the sounds that  surround us to be more or less memorable. We envision this research introducing new paradigms into the  space of audio compression, attention-driven user interactions, and  auditory AR, amongst others.A good overview of our dataset, experiments, and methodology can be found here: https://resenv.media.mit.edu/memory-dataset/demo.html",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/13/Slide8_ITjyV0f.png,305981,,2020-06-14T03:17:03.370Z,"['dramsay@media.mit.edu', 'ishwarya@media.mit.edu']",True,cognitive-audio,2017-11-01,"['artificial-intelligence', 'augmented-reality', 'cognition', 'music', 'sensors', 'technology', 'voice']",Modeling Auditory Memory,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2018-05-04T18:45:26.311Z,"Drawing inspiration from the notion of cognitive incongruence associated with Stroop's famous experiment, from musical principles, and from the observation that music consumption on an individual basis is becoming increasingly ubiquitous, we present the SoundSignaling system—a software platform designed to make real-time, stylistically relevant modifications to a personal corpus of music as a means of conveying information or notifications. From the substantial body of HCI research demonstrating the negative attentional implications of a daily inundation of notifications, we highlight two challenges associated with standard audio notifications—a ""switch cost"" that impedes productivity, and a lack of awareness of a user's cognitive load—that have the potential to be addressed by such a system without active activity estimation. Through this work, we suggest a re-evaluation of the age-old paradigm of binary notifications in favor of a system designed to operate upon the relatively unexplored medium of a user's musical preferences.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2018/05/04/Headphones.jpg,305989,,2020-06-14T03:32:02.171Z,['ishwarya@media.mit.edu'],True,soundsignaling,2017-11-01,"['human-computer-interaction', 'environment', 'music']",SoundSignaling,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-05-04T21:58:25.665Z,"From skin to light: an artistic dialog between cosmetics, lasers, and lighting between the human and microscopic scale. Cosmetics essentially diffract and scatter light in a way that makes the wearer appear different to the naked eye. In this project we propose an illumination grammar between cosmetic projects and coherent light lasers, as a suggestion for alternative interior lighting as well as artistic effect and expression. From skin to light, we explore these artistic archetypes.",2020-05-09,['object-based-media'],https://dam-prod2.media.mit.edu/x/2018/06/01/IMG_8230.jpg,306006,,2020-06-10T15:57:01.575Z,"['nlutz@media.mit.edu', 'vparth@media.mit.edu']",True,cosmetic-light,2018-02-06,[],Cosmetic Light,PUBLIC,https://ninalutz.github.io,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-04T23:45:01.219Z,"A Case for Breastfeeding InnovationBreastfeeding saves lives.If women globally were able to meet the WHO's public health goal to exclusively breastfeed for the first six months, we would prevent 823,000 infant deaths. For every 597 women who optimally breastfeed, one maternal or child death is prevented.Breastfeeding promotes long-term wellness for mother and baby.Breastfeeding protects against child infections and malocclusion, increases intelligence, and reduces the risk of obesity and diabetes for children¹. Breastfeeding decreases mothers' risk of breast cancer and optimal breastfeeding would lead to 20,000 fewer cases every year¹. It may also protect against ovarian cancer and diabetes¹. Women who are supported to successfully establish breastfeeding in early months have a lower risk for postpartum depression.Breastfeeding saves healthcare costs.If women in the US were able to meet the WHO's public health goal to exclusively breastfeed for the first six months, we would save $17.2 billion dollars in annual costs treating preventable events, including infant and maternal deaths, SIDS, ear infections and necrotizing enterocolitis in babies, and heart attacks, diabetes and breast cancer in mothers.Work environments and policies in the US are hostile to breastfeeding.The US is one of only three nations worldwide without paid parental leave. The other countries in this club are Papua New Guinea and Lesotho⁴. Women's return to work outside the home is the leading factor for early weaning⁵. Most US work environments do not provide material or policy-based support for breastfeeding women, including parental leave, flexible schedules, on-site daycare, breaks and spaces for nursing and pumping.The HackathonOur team is thrilled to produce a weekend with the leading innovators in breastfeeding and postpartum health, along with many mamas, papas, babies, students, and newcomers. This time around we have a focus on equity and inclusive innovation in breastfeeding. We want to catalyze the development of tech, products, spaces, clothing, programs and services that have an eye on affordability and access as well as cultural diversity.REFERENCES:Bartick, M. C., Schwarz, E. B., Green, B. D., Jegier, B. J., Reinhold, A. G., Colaizy, T. T., Stuebe, A. M. (2017). Suboptimal breastfeeding in the United States: Maternal and pediatric health outcomes and costs. Maternal & Child Nutrition, 13(1), e12366. http://doi.org/10.1111/mcn.12366Watkins, S., Meltzer-Brody, S., Zolnoun, D., & Stuebe, A. (2011). Early Breastfeeding Experiences and Postpartum Depression. Obstetrics & Gynecology, 118(2, Part 1), 214–221. http://doi.org/10.1097/AOG.0b013e3182260a2dBreastfeeding in the 21st century: epidemiology, mechanisms, and lifelong effect. Victora, Cesar G et al. The Lancet, Volume 387, Issue 10017, 475 - 490.https://en.wikipedia.org/wiki/Parental_leaveWhy invest, and what it will take to improve breastfeeding practices? Rollins, Nigel C et al. The Lancet , Volume 387, Issue 10017, 491 - 504.",2020-08-01,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2018/05/04/27919634588_a441896cb6_o.jpg,306014,,2022-06-10T13:29:45.220Z,"['ahope@media.mit.edu', 'dignazio@media.mit.edu']",True,make-the-breast-pump-not-suck-hackathon-2018,2017-04-02,"['human-computer-interaction', 'hacking', 'health', 'public-health', 'social-change', 'wellbeing', 'gender-studies', 'women']",Make the Breast Pump Not Suck Hackathon 2018,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-05-05T13:28:52.711Z,"Humans can accurately sense the position, speed, and torque of their limbs, even with their eyes shut. This sense, known as proprioception, allows humans to precisely control their body movements. Today’s conventional prosthetic limbs do not provide feedback to the nervous system. Because of this, people with amputated limbs cannot feel the position, speed, and torque of their prosthetic joints without looking at them, making it difficult to control their movement. In order to create a more complete prosthetic control experience, researchers at the MIT Media Lab invented the agonist-antagonist myoneural interface (AMI). The AMI is a method to restore proprioception to persons with amputation.",,['biomechatronics'],https://dam-prod2.media.mit.edu/uuid/77582407-555f-44b7-ba35-72031e071f99,306329,,2021-10-15T00:13:30.567Z,"['mcarty@media.mit.edu', 'shriyas@media.mit.edu', 'clites@media.mit.edu', 'hherr@media.mit.edu', 'lfreed@media.mit.edu', 'romka@media.mit.edu', 'mcarney@media.mit.edu', 'qiaoj@media.mit.edu', 'eisrael@media.mit.edu', 'hngnsong@media.mit.edu', 'samga@media.mit.edu', 'emrogers@media.mit.edu', 'tonyshu@media.mit.edu', 'syeon@media.mit.edu', 'thhsieh@media.mit.edu', 'gnf1@media.mit.edu']",True,agonist-antagonist-myoneural-interface-ami,2014-06-01,"['robotics', 'artificial-intelligence', 'bioengineering', 'civic-media', 'health', 'human-machine-interaction', 'neurobiology', 'imaging', 'social-science', 'biology', 'biomechanics', 'technology', 'biotechnology', 'wellbeing', 'prosthetics', 'medicine', 'bionics', 'neural-interfacing-and-control', 'prosthetic-design', 'biomedical-imaging']",Agonist-antagonist Myoneural Interface (AMI),PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-06T13:50:58.364Z,"Breathing actions are used to augment controller-based input by giving superpowers to players in two VR games. Blowing out long and strong turns you into a fire-breathing dragon, while holding your breath sends you into stealth mode.  By using  breathing as a directly controlled physiological signal, BreathVR can facilitate unique and engaging play experiences through natural interaction in single and multiplayer virtual reality games. Paper available here: http://web.media.mit.edu/~sra/breathvr.html",2018-07-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/06/fire-game.png,306643,,2018-08-20T16:27:18.375Z,"['sra@media.mit.edu', 'pattie@media.mit.edu', 'xuhaixu@media.mit.edu']",True,breathvr,2017-08-16,"['virtual-reality', 'physiology']",BreathVR,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-06T18:20:02.791Z,"PerForm explores the intuitive meanings associated with the shape of objects, and how a deformable tool can allow for interaction based on a language of form. How can we use intuitive associations from senses to create intuitive interfaces where the user communicates concepts through shapes? PerForm addresses that question by allowing users to transform a physical tool to fit their intentions. A user can play different musical instruments or take different actions in games, simply by varying the shape of the tool. Since the meanings associated with the shapes would be dependent on context, we are giving special focus to studying mappings of between timbral components of sound and characteristics of shape.SOUND-SHAPE CORRESPONDENCESPerForm explores how the associations between visual and auditory perception can be used in interaction design. We developed a physical interface that users can transform by bending to create geometric shapes or symbols. By investigating possible correlations, natural or forged, between perceptual components of shape and its correlates in sound, we enable the tool to become a new instrument, with different sound timbre depending on the geometry of the object.A DEFORMABLE GAME CONTROLLEROne of the applications of this shapeable device would be to enable different modes of interaction through changes in shape. Instead of having to buy multiple controller devices for each genre of gaming or kind of interaction, or simply using a single, fixed-form controller that limits the embodied experience, a device capable of transformation would enable users to have a more imaginative and creative gaming experience, even enabling new kinds of games in which the user can invent tools by varying shapes.",2019-09-01,['object-based-media'],https://dam-prod2.media.mit.edu/uuid/0079a53e-6eea-4134-ab3a-3d499affa1f1,306652,,2020-05-21T02:44:21.332Z,"['jaleesat@media.mit.edu', 'irmandy@media.mit.edu', 'carolx@media.mit.edu']",True,perform-1,2018-05-01,"['design', 'human-computer-interaction', 'music', 'perception', 'interfaces']",PerForm,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-06T18:55:03.196Z,"MotivationThe need for inexpensive, reliable, 3D, 360-degree display technologies grows as augmented reality applications continue to increase in popularity. There is room for innovation in the field, as many volumetric displays have moving components, are prohibitively expensive, aren’t 360-degree, or some combination of those factors. By creating a 360-degree autostereoscopic display that is cost-effective and reliable, we could expand the audience who would benefit from collaborative, interactive experiences. Additionally, while augmented reality headsets are increasingly popular, a volumetric display readily encourages accessible, collaborative interaction without separating users by wearable technology. This technology could impact a variety of industries and demographics as the connected media landscape continues to expand.Approach - HardwareThis system is comprised on three main components. First, a 4K monitor displays properly distorted, lenticularized content using a custom shader. That light-field information is then partitioned by a refracting medium, either a radial lenticular array or a holographic optical element. After the rays of light from the pixels bend through the refracting medium, the rays then bounce off the conical mirror, producing the proper image for each viewing angle.Hardware - Radial LenticularThe radial lenticular design is dictated by several parameters which all trade-off with one another. For the design of this lenticular, I plan to focus on maximizing the resolution and size of the image rendered in the cone, while also maintaining a reasonable number of viewing angles. Variables that can be changed include the number of cameras (which corresponds to the number of discreet viewing zones), the number of views per lenticular slice, and the number of times a particular sequence of views should repeat. Those variables dictate the minimum viewing radius size, the angular size of each lenticular slice, the angle of influence for each viewing zone, and the size of the image rendered in the cone.  It is important to consider what a user can reasonably see reflected in the cone, before the rays no longer bounce back to the user.Resolution is one over the number of views per each lenticular wedge. As the user moves radially around the display, the rendered views that are opposite the user (i.e., on the other side of the radial mirror) cannot be seen. Because of this assertion that the user doesn’t need to see all of the potential views from a static position, radial lenticulars operate differently than linear lenticulars. With a linear lenticular, every view that will be displayed needs to exist under every lenticular lens for the effect to work.  For example, if there are eight potential images lenticularized, there would be a slice of each of those eight views underneath each lenticular lens. However, with this setup, it is not necessary to produce every view under every lenticular lens, because each lenticular lens only needs to display the views that the user could potentially see from that lens. Because of that, there is a rolling priority system for which views to display under each lenticular lens.  If the user is standing directly in front of where a viewing zone is located for a particular image, the lenticular lenses closest to the user should have that view centrally located under the lenticular. The lenticulars that are adjacent to that area should still have information that can reach the user; however, the views have to shift to accommodate the off-axis position.  In that way, there is a rolling priority system, where the generated view that is closer to the user will appear centrally under the lenticular lens and incrementally move off-center and eventually disappear as the user moves radially around the display.Hardware - InteractivityFor hardware integration for interactivity, I plan to use Intel Realsense cameras, microphones, and arduino powered LEDs that all communicate with Unity. There is a plugin to use Intel Realsense in the unity environment, along with libraries that allow for more detailed signal processing and pose analysis.  I will use serial messaging in unity to send message to the arduino that will control the LEDs that will wrap around the housing of the device. The LEDs will be used to indicate important signals to users. Those signals include that the system can see the user and whether the user appears to be engaging as well as can signal when the program is performing a request that requires time.  Additionally, I will use unity to receive and process microphone inputs.Software - Custom ShaderUnity has built in support for creating complex shaders that can be applied real-time. In Unity, I will write a shader that appropriately partitions the views based on the number of cameras in the scene, the number of views per each lenticular lens, and the number of times each sequence of views should repeat. Before the views can be lenticularized the media from the camera must be rotated around the display to match where it will physically appear on the reflected cone.  If you don’t do this step, all of the views will be rendered on top of each other, resulting in an incorrect result. By rotating the views to their appropriate location, the imagery you should see when standing on the left of the display will appear on the left and same for all other directions. After the views have been rotated to the appropriate location, each of the camera’s views are sliced and reordered within the shader to produce the lenticularized result.Software - InteractivityIntel Realsense and similar depth sensing devices like Kinect and Leap Motion provide SDKs that allow developers to readily stream depth and color data into Unity. The Intel Realsense data can be further processed and interpreted using software like Nuitrack to do skeleton tracking which exposes body pose and joint positions.  This can be used to allow the system to know when a user is close enough to the display or standing a particular place. This is good for gesture tracking as well.  The color data from the Intel Realsense camera can be processed in OpenCV which has a Unity SDK.  With OpenCV, the color data can be analyzed for object detection, face detection, and recognition and face pose analysis.  This allows the system to recognize objects, people, and face pose (which could be used to interpret affective state).  Face and gaze detection could be used in lieu of trigger words like “OK Google” and “Alexa” as presumably the users have intent to interact with the system if they’re looking at the character/sensors.The sound recordings are used to detect volume, pitch, and speech. The speech is analyzed using a cloud-based service, which then streams the input and a response back to unity to influence how the character animates and responds. The speech analysis could be used to interpret special assigned words to trigger activities, games, or special animations or content in the display.  The response generated by the cloud-service can be used to animate the characters mouth if the character audibly responds to users. The coupling of depth/RGB data and audio allow for more nuanced understanding of a user’s intent and affective state.  In combination, this could be used to drive sympathetic animations from the character.  Because the RGB data allows for face recognition, the character can potentially store information about users to be retrieved whenever that user interacts with the system.Software - Procedural CharacterI chose an animated dog character as the embodied representative for several key affordances. The face and body of this character have already been rigged and those parameters are accessible in Unity. The design of the character is ideal for this system and for expressing emotion because the head size is large relative to the body and the facial features of the character are contrasted which will read better with this device. Additionally, because this character is familiar and has recognizable physical embodiments of emotion, the user will more readily understand key animation poses.  For example, if the character is happy, it will wag its tail and stick out its tongue, but if it’s nervous or upset, it will put its head and ears down. Based on the signals received and processed from the Intel Realsense camera and microphone, the dog character will animate.  Those inputs will impact the character’s emotional pose, LookAt( ) behavior, and special animation sequences.EvaluationThe ultimate goal of this project is to produce a low-cost, reliable, 3D, 360-degree autostereoscopic display. I will visually inspect the hardware/software implementation to determine if the desired 360-degree display is produced and measure the specifications of this display to other closely related devices. I will evaluate our software/hardware implementation by creating test patterns to discretize each individual view zone.  Additionally, we will perform user studies to verify that the interaction system is intuitive and engaging.User StudyThe questions I’d like to answer about this project lie at the intersection of the affordances of the display itself and the media users can interact with. Because I’m designing a procedural character for this display, I want to explore believability and engagement through the user study as well as whether placing a character in-situ in a space impacts user experience. The nearest neighbor devices for the study will be an AR headset and a 2D monitor.  In this individual-use user study, participants will engage in activities that evaluate the character’s ability to engage.  The participants will complete a close ended warm-up task both meant to acclimate and provide a concrete objective.  After the warm-up task is completed (e.g., playing catch), the participants will be asked to engage in a more exploratory capacity with the character on each of the displays.  The activity will involve engaging the character’s LookAt( ) behavior, object detection, face-pose detection, speech recognition, and visual accessories (like the LED light-strip). The order in which the participants use each display will be randomized to avoid conflating intuitive use with prior experience with the activity they’re asked to compete.If this device were to become commercially viable, it would have to be accessible for a wide variety of ages and expertise level. Because of that, I will look for a diverse group of participants with varying degrees of prior experience with technologies like video games, AR/VR, and voice assistants. As I design and implement features for my project, I will carefully prioritize scope that builds towards the user-study experience.Future WorkIf the radial lenticular works optimally, it would be amazing to explore a portable version of this novel volumetric display. Because the optical elements of this device have no moving parts and are lightweight, this display would be a great candidate for portability. It’d be fascinating to expand on the AI character’s capabilities by adding context aware infrastructure that changes how the character responds depending on localized metadata curation.  Ethically, this technology has the potential to make volumetric displays incredibly accessible at scale, because both the mylar cone and radial lenticular would cost less than one dollar to fabricate when productized. This would severely reduce cost on a market where displays typically cost thousands of dollars per unit.",2019-06-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2018/05/06/funnelvision_vhzjEpa.png,306660,,2020-06-10T16:02:37.978Z,['emilysa@media.mit.edu'],True,funnel-vision,2018-02-01,"['art', 'augmented-reality', 'entertainment']",Funnel Vision,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-06T22:33:32.435Z,"This is a joint proposal from graduate students Vik Parthiban (Media Lab) and Andrew Spielberg (MIT CSAIL, Distributed Robotics Laboratory). With the release of the Magic Leap Creator and Leap Motion NorthStar platform for new mixed-reality and augmented-reality applications, we propose a new computer-aided design (CAD) tool for simulating and deploying robots. Our goal is to better understand locomotion in 3D space and diagnose character movement under multiple constraints. We will develop the ""Magic Leap Design and Control Toolbox,"" a suite of new interactive algorithms and implementations to directly build structures within the environment using captured information about the world. A new system for gesturing the desired path or degrees of freedom of an object will automatically translate the gestures into robot and character control inputs. The system will be based on algorithms that synthesize natural gestural control based on the desired motion.",2019-09-24,['object-based-media'],,306668,,2019-09-25T14:52:58.742Z,['vparth@media.mit.edu'],True,mixed-reality-robots,2018-05-04,"['robotics', 'design', 'human-computer-interaction', 'augmented-reality', 'human-machine-interaction', 'interfaces']",Designing robots in mixed reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-07T15:20:59.769Z,"HyperCubes is an augmented reality platform to help children understand computational concepts drawn from their physical surroundings, from their most immediate and tangible reality. Children become creators and learn while tinkering with commands such as transformations in space that control little characters or geometry shapes. ",2018-12-31,['living-mobile'],https://dam-prod2.media.mit.edu/x/2018/08/06/DSC_0134.JPG,306984,,2018-10-18T01:45:06.375Z,['afuste@media.mit.edu'],True,hypercubes,2018-05-07,"['design', 'augmented-reality', 'learning-teaching', 'machine-learning']",HyperCubes: Learning computational concepts in Augmented Reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-05-08T00:21:59.290Z,"HeartBit is an interface designed for haptic heart rate biofeedback. A handheld heart beats alongside your own, mirroring the size, weight, and movement of a hidden internal organ, now external and tangible in real-time. HeartBit offers a medium for users to self-regulate in moments of stress, anxiety or exertion: Control your heart to control your breath and body—for relaxation, performance enhancement, or augmented self-awareness.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/07/DSC_0257-Edit-Edit.jpg,307004,,2019-04-18T17:08:14.374Z,['rosello@media.mit.edu'],True,heartbit,2018-05-01,"['augmented-reality', 'consumer-electronics', 'health', 'wearable-computing', 'interfaces', 'technology', 'wellbeing']",HeartBit,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-08T01:08:05.131Z,"The Launch Team: Christian McBride (6 Grammys), David Gage, Mas Hino and Topper Carew.",,['code-next'],https://dam-prod2.media.mit.edu/x/2018/05/07/20180327_154311.jpg,307318,,2018-05-08T01:55:43.662Z,[],False,creating-new-jazz-musical-instruments-and-new-jazz-idioms-the-launch-team-christian-mcbride-6-grammys-david-gage-mas-hino-and-topper-carew,2018-05-07,[],Creating New Jazz Musical Instruments and New Jazz Idioms 2,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2018-05-08T01:40:08.016Z,"For 18 months, the Smithsonian has been collecting art, photos, films, and interviews about the New Thing Art and Architecture Center, a Washington, DC inner-city cultural center that Topper Carew founded immediately after college. The work will be installed in the Smithsonian's personal collection.",,['code-next'],https://dam-prod2.media.mit.edu/x/2018/05/07/_DSC3568_M9oOk3d.jpg,307326,,2018-05-08T13:54:00.724Z,['tcarew@media.mit.edu'],True,creating-new-jazz-mus,2018-05-07,[],New Thing Art and Architecture Center at the Smithsonian,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-05-08T01:56:00.353Z,"The Launch Team: Christian McBride (winner of six Grammy Awards), David Gage, Mas Hino, and Topper Carew.",,['code-next'],https://dam-prod2.media.mit.edu/x/2018/05/07/20180327_154311_ICu5rKg.jpg,307343,,2018-05-08T12:03:39.097Z,['tcarew@media.mit.edu'],True,creating-new-jazz-musical-instruments-and-new-jazz-idioms-1,2018-05-07,[],Creating New Jazz Musical Instruments and New Jazz Idioms,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2018-05-08T03:09:51.045Z,"Integrating sensors and actuators using flexible electronicsCurrently, the manufacturing of self-actuating and self-sensing robots requires non-standard manufacturing techniques and assembly steps to integrate electrical and mechanical systems. In this work, we developed a novel manufacturing technique, where such robots can be produced at a flexible electronics factory. We developed the technique using standard industrial machines, processes, and materials. Using a lamination process, we were able to integrate air pouches or shape memory alloy (SMA) inside a polyamide-based flexible circuit to produce bending actuators. The bend angle of the actuators is sensed with a chain of inertial measurement units integrated on the actuator. Air-pouch actuators can produce a force of a 2.24N, and a maximum bend angle of 74 degrees. To demonstrate, we manufactured a five-legged robot with the developed actuators and bend sensors, with all the supporting electronics (e.g., microcontrollers, radio) directly integrated into the flexible printed circuit. Such robots are flat and lightweight (15 grams) and thus conveniently compact for transportation and storage. We believe that our technique can allow inexpensive and fast prototyping and deployment of self-actuating and self-sensing robots.",,"['tangible-media', 'responsive-environments']",https://dam-prod2.media.mit.edu/x/2018/09/28/IROS_Presentation_CircuitRobots_2018.001.jpeg,307351,,2022-06-10T13:56:33.013Z,"['joep@media.mit.edu', 'jifei@media.mit.edu', 'jieqi@media.mit.edu', 'artemd@media.mit.edu']",True,circuit-robots,2017-08-01,['robotics'],Circuit Robots: Mass manufacturing of self-actuating robots,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-05-08T03:43:17.420Z,"InspirationDreams are fascinating personally, therapeutically, and scientifically. Who hasn't woken up in the morning and wondered how they came up with that wild story, and what it could reveal? But studying dreams is limited  by the expensive technology used to track the sleep stages in which they occur. Masca is a flexible mask designed for easier, cheaper sleep stage detection. Our device is designed for eyelid motion detection, and adapts to the human body using conformable piezoresistive fabric and silicone to see the various physiological changes occurring as sleep states change without the need for wired amplifiers or sticky electrodes. Our vision is a democratized dream science, wherein people can try out all the laboratory protocols for lucid or therapeutic dreams  in the comfort of their own home. Scientific Background Eyes and eyelids change movement frequency predictably as sleep stage transitions occur—you can even try this at home, watching a friend or partner sleep, and you'll see an increase in their eye movement frequency when they hit the REM sleep stage. Trackers which are built for the eyes allow for a simpler, more portable system than the typical high-density EEG required for sleep tracking. This device is modeled after Prof. Robert Stickgold's Nightcap, who demonstrated you can track sleep stages with simple eye-worn sensors, and we are super grateful for his ongoing assistance with this project. Engineering and ExperimentationTracking and influencing of sleep cognition opens up doors to a future in which the consolidation of emotion, memory, and learning which happens in sleep is rendered controllable by wearable electronics. But first we have to know if Masca reliably tracks sleep stages! This device is a working prototype: We've only tested this device on a few people so far, and though results look promising, we'll wait to put those out publicly until we can publish in a peer-reviewed journal. For now, feel free to reach out to us with ideas, new strategies for sleep tracking, or if you want to run your own study using Masca and need a prototype. ",2021-07-02,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/07/adam normal masca.jpeg,307359,,2022-10-05T15:46:11.592Z,"['rosello@media.mit.edu', 'tomasero@media.mit.edu', 'irmandy@media.mit.edu', 'adamjhh@media.mit.edu']",True,masca,2018-04-01,[],Masca,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2018-05-12T20:52:40.297Z,"Identity is fluid. As we travel this journey of life, we get imprinted and we forget parts of ourselves. Older people's memories are erased and new memories are formed. As we go through this fluid process without actively knowing it, it can be helpful to pause, spend time with ourselves, and ponder where we started and where we are headed by reliving past conversations. A Date With Yourself is an interactive experience that lets you ponder the digital footprints you've left behind over the years on social media and encourages you to reflect on how you've changed as a person. Our prototype analyses a user's Facebook Archive to compute how their online persona has evolved over the years.Our analysis looked for emotions like happiness, surprise, disappointment, and anger. Only messages written by the researcher were analyzed—not those of the friends spoken to. Every message was decontextualized and when knitted together, created moments of the researcher's life where she was unhappy, inspired, or nostalgic, creating a realization of how our interactions with others have changed over time, we can learn about how they have moulded us.",2018-05-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/12/Ashris.jpg,308748,,2019-01-30T06:34:01.207Z,"['jsirera@media.mit.edu', 'ashris@media.mit.edu']",True,a-date-with-yourself,,[],A Date with Yourself,PUBLIC,https://github.com/iashris/adwy,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1
False,2018-05-23T17:38:41.218Z,"In-Vivo Networking (IVN) is the new technology that can wirelessly power and communicate with tiny devices implanted deep within the human body. Such devices could be used to deliver drugs, monitor conditions inside the body, or treat disease by stimulating the brain with electricity or light.  The implants are powered by radio frequency waves, which are safe for humans. In tests in animals, we showed that the waves can power devices located 10 centimeters deep in tissue, from a distance of one meter.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/uuid/f41c5219-618d-4492-a1c5-a6158a7d75d9,312234,,2019-02-14T19:47:21.377Z,"['yunfeima@media.mit.edu', 'zhluo@media.mit.edu', 'fadel@media.mit.edu']",True,ivn-in-vivo-networking,2017-09-01,"['robotics', 'environment', 'health', 'sensors', 'computer-science', 'technology', 'wellbeing', 'ocean']",In-Vivo Networking: Powering and communicating with tiny battery-free devices inside the body,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-05-23T20:28:59.557Z,"In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, and then show in an independent evaluation with 76 users that this model produced sketches that lead to significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model.",2019-07-30,['affective-computing'],,312242,,2022-05-12T17:45:12.136Z,"['jaquesn@media.mit.edu', 'picard@media.mit.edu']",True,learning-via-social-awareness-improving-sketch-representations-with-facial-feedback,2018-03-20,"['artificial-intelligence', 'machine-learning', 'affective-computing']",Learning via Social Awareness: Improving sketch representations with facial feedback,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-05-24T16:30:59.720Z,A wearable olfactory display that monitors cardio-respiratory information with the aspiration to support mental wellbeing.BioEssence is a wearable olfactory display that provides just-in-time release of scents based on the physiological state of the wearer. The device can release up to three scents and passively captures subtle chest vibrations associated with the beating of the heart and respiration through clothes. ,,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/05/24/top.jpg,312559,,2020-04-15T16:09:40.183Z,"['pattie@media.mit.edu', 'javierhr@media.mit.edu', 'artemd@media.mit.edu', 'amores@media.mit.edu']",True,bioessence,2017-12-01,"['consumer-electronics', 'health', 'wearable-computing', 'affective-computing', 'computer-science', 'technology', 'electrical-engineering', 'wellbeing']",BioEssence,PUBLIC,http://www.judithamores.com/essence,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-05-30T13:29:38.227Z,"We present a multimaterial voxel-printing method enabling the physical visualization of data sets commonly associated with scientific imaging. Leveraging voxel-based control of multimaterial 3D printing, our method enables additive manufacturing of discontinuous data types such as point cloud data, curve and graph data, image-based data, and volumetric data. By converting data sets into dithered material deposition descriptions, through modifications to rasterization processes, we demonstrate that data sets frequently visualized on screen can be converted into physical, materially heterogeneous objects. Our approach alleviates the need to post-process data sets to boundary representations, preventing alteration of data and loss of information in the produced physicalizations. Therefore, it bridges the gap between digital information representation and physical material composition. We evaluate the visual characteristics and features of our method, assess its relevance and applicability in the production of physical visualizations, and detail the conversion of data sets for multimaterial 3D printing. We conclude with exemplary 3D printed datasets produced by our method pointing towards potential applications across scales, disciplines, and problem domains.",2020-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2018/05/30/Line and Graph Brain_lGr3M6l.jpg,314432,,2021-05-21T14:39:08.276Z,"['bader_ch@media.mit.edu', 'kolb@media.mit.edu', 'ssunanda@media.mit.edu', 'limulus@media.mit.edu', 'jpcosta@media.mit.edu', 'neri@media.mit.edu']",True,making-data-matter,2018-05-30,"['design', 'manufacturing', '3d-printing']",Making Data Matter: Voxel-printing for the digital fabrication of data across scales and domains,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-06-09T23:58:53.850Z,"We'd like to introduce you to a very special neuroscience project that we are currently conducting in the setting of a traditional fine arts museum.Join the conversation on the Responsive Science Brainstorm project site. Responsive Science uses the PubPub platform, which allows for direct interaction. PubPub was developed at MIT Media Lab.",,['sculpting-evolution'],https://dam-prod2.media.mit.edu/x/2018/06/09/Slide1.jpg,317722,,2018-10-23T15:25:53.714Z,['lunshof@media.mit.edu'],True,brainstorm-anima-mundi,2018-06-09,"['art', 'genetics', 'synthetic-biology']",Brainstorm: Anima Mundi,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2018-06-11T15:59:36.007Z,"To explore future mobility modes, the City Science group is working with Media Lab member company Panasonic to explore the use and potential adaptations of the popular MamaChari bikes.  Like other mobility modes, the MamaChari bikes have developed and adapted over the past decades.  Bikes for women first became popular during Japan’s economic boom in the 1980s when many households benefited from one income, and women were encouraged to stay home and take care of their children.  Women used bikes to quickly navigate their cities and make frequent trips to shops and schools, kids in tow. Even as women gradually entered the workforce in the 1990s and 2000s, the stereotype of the Japanese biking woman remained.  By 2008, electric assist bikes were introduced to the market, and again they targeted women with children as the primary users. Today MamaChari bikes are stable, secure and ubiquitous in Japan, yet they have yet to enter other global markets.  The City Science group strives to understand current uses of the MamaChari and adapt the bike for new and future uses globally. Ideation workshops were completed in February and May 2018.Learn more about the first workshop here:  https://www.media.mit.edu/posts/mamachari/",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/06/11/Screen Shot 2018-06-11 at 12.08.51 PM.png,318342,,2018-06-19T18:01:03.323Z,['yasushis@media.mit.edu'],True,hackable-bike,2018-02-28,"['hacking', 'transportation', 'urban-planning']",Hackable Bike,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-06-12T15:54:28.655Z,"Data Fusion for Dynamic Traffic PredictionTraffic congestion has huge negative impacts on the productivity, health and personal lives of city dwellers. To manage this problem effectively, transportation engineers need to predict traffic congestion throughout the road network at all hours of the day. Prediction of traffic typically involves travel surveys that are expensive, time consuming and do not capture temporal variation in travel demand. However, anonymised location data from mobile phones present an alternative source of data which is passively collected, widely available and naturally captures temporal trends. On the other hand, these data contain other biases and so if we use these data for transportation models, we must take care to correct for these biases using more reliable data. As part of the City Science collaboration with Andorra, we used a Bayesian network to build a calibrated transportation model for the country based on geolocated telecoms data and validated using a small sample of traffic counts.",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/06/28/trafficResults.gif,318675,,2019-04-17T19:43:06.032Z,"['doorleyr@media.mit.edu', 'kll@media.mit.edu', 'agrignar@media.mit.edu', 'alonsolp@media.mit.edu']",True,traffic-andorra,2017-01-01,"['data', 'networks', 'engineering']",Dynamic Traffic Prediction in Andorra: a Bayesian network approach,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2018-06-12T21:23:21.317Z,"How can we get more value from the same buildings? Cities contain many different resources and spaces and typically, these resources operate as products with  a single function and a single owner and/or renter. However, the owner's demand for space often varies daily or seasonally, meaning that many buildings tend to be underutilized and are often vacant or partially vacant for large portions of each day. Meanwhile, the ""sharing economy"" has been one of the most significant economic shifts in the last 10 years, with companies like Uber and Airbnb experiencing explosive growth. Along these lines, Aalto University—a member of the MIT Media Lab City Science network—has developed the concept of City-as-a-Service, where building space and other resources are shared among institutions, businesses, and citizens in a community. Aalto has already begun experimenting with School-as-a-Service, as a prototype of City-as-a-Service on their campus in Espoo.",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/06/12/s3.gif,318702,,2019-12-17T21:09:35.616Z,"['doorleyr@media.mit.edu', 'alonsolp@media.mit.edu', 'kll@media.mit.edu', 'noyman@media.mit.edu', 'agrignar@media.mit.edu']",True,aalto-saas,2017-07-01,"['urban-planning', 'data-visualization']",Aalto Campus-as-a-Service Simulations,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2018-06-18T19:54:20.153Z,"City Science Lab Toronto was established in cooperation with Ryerson University in Toronto, Canada.  It  started in 2018 and is the newest city in the City Science Network. The lab will be embedded in the Faculty of Communications and Design and will be part of the University's Paradox Initiative. The two groups plan to build and work on the development and simulation of urban interventions, such as  micro-units for young people, shared work and collaboration spaces, educational facilities, financial services innovations, and new mobility and parking systems. This information will be analyzed and visualized using different platforms including the CityScope. The two groups  plan to  define new parameters  which may include financial modeling, design of innovation flow,  public health, new mobility systems, and/or energy networks. A number of large financial institutions, telecommunications companies, and hospitality groups are the founding corporate supporters of the initiative. The lab's director is Professor Hossein Rahnama, who is also a visiting faculty member at the MIT Media Lab.",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/06/18/Screen Shot 2018-06-18 at 4.00.25 PM.png,320583,,2023-09-07T14:41:06.026Z,['rahnama@media.mit.edu'],True,city-science-lab-toronto,2018-03-01,['urban-planning'],City Science Lab @ Toronto,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-06-19T12:52:16.617Z,"MIT City Science has developed an international network of cooperative City Science LabsWe are developing concepts and key technology that can be extended, deployed, and evaluated by our collaborators in unique contexts around the globe.  Over the next few years, we plan to develop new and exciting projects in less affluent, rapidly growing cities in Latin America, Africa, and India where the impact of a new process to address the challenges of urbanization may be the greatest.  ",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2018/06/18/Screen Shot 2018-06-18 at 4.00.25 PM.png,320927,,2023-09-07T23:29:09.374Z,"['ryanz@media.mit.edu', 'ptinn@media.mit.edu', 'mcllin@media.mit.edu', 'doorleyr@media.mit.edu', 'noyman@media.mit.edu', 'yasushis@media.mit.edu', 'agrignar@media.mit.edu', 'alonsolp@media.mit.edu', 'mdchurch@media.mit.edu', 'kll@media.mit.edu', 'heckbert@media.mit.edu', 'markuse@media.mit.edu', 'leticiai@media.mit.edu', 'vincy@media.mit.edu', 'gba@media.mit.edu', 'ilseh005@media.mit.edu', 'naroa@media.mit.edu', 'thomassl@media.mit.edu', 'csmuts@media.mit.edu', 'aricom@media.mit.edu', 'maitanei@media.mit.edu']",True,city-science-network,,"['robotics', 'design', 'human-computer-interaction', 'agriculture', 'architecture', 'data', 'transportation', 'urban-planning', 'data-visualization', 'sustainability', 'climate-change', 'cities']",City Science Network,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-06-19T14:34:23.483Z,"Games are a uniquely human endeavour, reducing stress and supporting mental well-being. Astronauts aboard the ISS have created their own games using materials at hand and pure creativity.  What if we could create games for them that took particular advantage of aspects of space, such as micro-gravity, and would help keep astronauts mentally engaged, socially connected, and physically relaxed?  Zero-G-ames is an ongoing series of workshops to explore, discuss, and design the history and future of games in constrained spaces and microgravity environments.Let’s play!",2022-08-01,['space-exploration'],https://dam-prod2.media.mit.edu/x/2018/06/19/TetrISS.jpg,320936,,2022-08-09T00:55:58.099Z,"['jhaas@media.mit.edu', 'novysan@media.mit.edu']",True,zero-g-ames,2018-03-10,"['design', 'gaming', 'health']",Zero-G-ames,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-06-26T14:29:31.381Z,"The remarkable Wallace–Bolyai–Gerwein theorem states that any simple polygon can be cut into finite pieces and rearranged to form any other simple polygon of equal area. We present an interactive application that visualizes the beautiful, constructive proof of this theorem. The interface allows the user to input their own initial and terminal polygons, and then will rigidly transform their initial polygon to their terminal polygon.",2016-06-13,[],https://dam-prod2.media.mit.edu/x/2018/06/26/Screen Shot 2018-06-26 at 10.30.09 AM.png,323159,,2018-11-21T17:16:01.155Z,[],True,scissor-congruence,,['data-visualization'],Visualizing Scissors Congruence,PUBLIC,https://dmsm.github.io/scissors-congruence/,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-06-26T18:45:06.227Z,"In architecture, the building skin is the primary interface for mediating the environment of the external with the internal. But today, this mediation is mechanical, deterministic, and static—often seeing the human as a generalizable and problematic input. With advances in material science however, there is great potential to disrupt these traditional manufactured environments of architecture and turn them into responsive mediated environments. What this thesis aims to explore is this idea of the receptive skin—a sensate and dynamic multi-material interface for environmental mediation. This suggests that by departing from the view that buildings are static artifacts, we may instead begin to see buildings as organic, living entities.Through the development of a working prototype, this project explores how such an interface may manifest itself, through dynamic material composites, instead of mechanical and electronic means. The final prototype is a “proof of concept,” a built example of this novel design methodology, which unites material performance with sensate technologies, as a way to enable new interactions between building and environment. ",,['city-science'],https://dam-prod2.media.mit.edu/uuid/b0ae93a8-e834-4a92-a041-ab132552dbe6,323169,,2018-06-26T21:12:49.214Z,"['kapeloni@media.mit.edu', 'kll@media.mit.edu']",True,receptive-skins,2017-05-01,"['architecture', 'human-machine-interaction', 'sensors', 'interfaces']",Receptive Skins: The Breathing Wall,PUBLIC,http://www.chrisoulakapelonis.com,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-06-28T16:53:32.990Z,"Expressing the hidden things inside, working with one of the most fragile of media.Hand carved illuminated Hen, Duck, Blue Duck and Goose egg shells. After emptying and cleaning the shells, I use a hand rotary tool to carve out the surface.",,[],https://dam-prod2.media.mit.edu/x/2018/06/28/threebirds_PDxMlwr.png,323828,,2018-06-29T19:09:48.567Z,['rebklein@media.mit.edu'],True,egg-shell-carving,2016-07-01,[],Egg Shell Carving,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-07-02T14:14:09.118Z,"Three-dimensional Digital Image Correlation (3D-DIC) is a non-contact optical-numerical technique for evaluating the dynamic mechanical behavior at the surface of structures and materials, including biological tissues. 3D-DIC can be used to extract shape and full-field displacements and strains with high resolution, at various length scales. While various commercial and academic 3D-DIC software exist, the field lacks 3D-DIC packages which offer straightforward calibration and data-merging solutions for multi-view analysis, which is particularly desirable in biomedical applications. To address these limitations, we present MultiDIC, an open-source MATLAB toolbox, featuring the first 3D-DIC software specifically dedicated to multi-view setups. MultiDIC integrates robust two-dimensional subset-based DIC software with specially tailored calibration procedures, to reconstruct the dynamic behavior of surfaces from multiple stereo-pairs. MultiDIC contains novel algorithms to automatically merge meshes from multiple stereo-pairs, and to compute and visualize 3D shape and full-field motion, deformation, and strain. User interfaces provide capabilities to perform 3D-DIC analyses without interacting with MATLAB syntax, while standalone functions also allow proficient MATLAB users to write custom scripts for specific experimental requirements. This paper discusses the challenges underlying multi-view 3D-DIC, details the proposed solutions, and describes the algorithms implemented in MultiDIC. The performance of MultiDIC is tested using a low-cost experimental system featuring a 360-deg 12-camera setup. The software and system are evaluated using measurement of a cylindrical object with known geometry subjected to rigid body motion and measurement of the lower limb of a human subject. The findings confirm that shape, motion, and full-field deformations and strains can be accurately measured, and demonstrate the feasibility of MultiDIC in multi-view in-vivo biomedical applications.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2018/07/02/FigE3c.gif,325074,,2021-09-17T19:55:24.698Z,"['hherr@media.mit.edu', 'kmoerman@media.mit.edu', 'danask@media.mit.edu']",True,multidic-a-matlab-toolbox-for-multi-view-3d-digital-image-correlation,2017-05-15,"['robotics', 'health', 'human-machine-interaction', 'prosthetics']",MultiDIC: a MATLAB Toolbox for Multi-View 3D Digital Image  Correlation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-07-06T14:15:08.495Z,"Visualization recommender systems aim to lower the barrier to exploring basic visualizations by automatically generating results for analysts to search and select, rather than manually specify. Here, we demonstrate a novel machine learning-based approach to visualization recommendation that learns visualization design choices from a large corpus of datasets and associated visualizations. First, we identify five key design choices made by analysts while creating visualizations, such as selecting a visualization type and choosing to encode a column along the X- or Y-axis. We train models to predict these design choices using one million dataset-visualization pairs collected from a popular online visualization platform. Neural networks predict these design choices with high accuracy compared to baseline models. We report and interpret feature importances from one of these baseline models. To evaluate the generalizability and uncertainty of our approach, we benchmark with a crowdsourced test set, and show that the performance of our model is comparable to human performance when predicting consensus visualization type, and exceeds that of other visualization recommender systems. ",2019-08-31,['collective-learning'],,326334,,2020-08-28T17:01:17.454Z,"['hidalgo@media.mit.edu', 'bakker@media.mit.edu', 'kzh@media.mit.edu']",True,vizml,2017-12-05,"['human-computer-interaction', 'machine-learning', 'data-visualization']",VizML: A Machine Learning Approach to Visualization Recommendation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-07-09T15:20:33.264Z," When the body senses itself internally and localizes its actions, it provides the basis for a material sense of self-existence. At the same time, the mind registers the sense of an agency with free will, the sense of being, the cause of voluntary action. Among all interoceptive experiences, respiration is the only one that we can regulate directly. There are many psychophysical breathing exercises to help self-regulation and reflection, that, combined with meditation and yoga, are designed to restore natural, smooth breathing appropriate to the physical needs of the body. ",2018-05-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/uuid/1ce47ad8-3c35-494a-89d0-9cbf4f77dd86,327283,,2018-10-20T21:47:28.195Z,"['xxxxxxin@media.mit.edu', 'pattie@media.mit.edu']",True,masque,2017-01-01,"['human-computer-interaction', 'cognition', 'wearable-computing', 'sensors', 'affective-computing', 'wellbeing']",Masque,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-07-13T16:20:47.648Z,"The  Interplanetary Gastronomy research  aims to address the unique challenges and opportunities associated with eating in space. What can we learn from food origins and gastronomy to create a new narrative for food futures and engage alternative approaches to food technology to improve health and wellbeing, in space and beyond? In order to support future crews in a new era of space exploration, space agencies will need to address the complex requirements of long-duration missions, and this includes how to provide future crews with safe, nutritious food for survival. What is at risk of being lost when engineering constraints are the sole drivers of innovation? Through the lens of food, this research prompts a more nuanced debate about health and nourishment, through the diversification of ingredients and food preparation. This research also considers the planet Earth, by working towards a framework for a sustainable closed-loop food system in space this research could be re-used in many Earth contexts. Research areas and flight experiments include zero gravity cooking tools and applications, novel food preservation and fermentation techniques, astronaut gut health, and the evolution of flavour off-Earth.Core Research Areas:Fermentation in SpaceRecipe Development Zero Gravity Cooking Equipment The Interplanetary Cookbook: A crowd-sourced guide to cooking and eating in space. Explore the public contributions and learn how to contribute here!  Flight Experiments & Missions: Space Miso ISS Mission: A sample of miso was sent to the International Space Station for a 30 days mission in spring 2021. Molecular Gastronomy in Zero GTasting Menu in Zero G Press: WIRED (cover story) Algae Caviar, Anyone? What We'll Eat on the Journey to Mars Humans are headed for the cosmos, and we’re taking our appetites with us. What will fill the void when we leave Earth behind? Featuring SEI's Maggie Coblentz on her work in space food and interplanetary gastronomy. The Tech What’s it like to design a meal that floats?Taking a taste of the sensory research of Space Exploration Initiative’s Maggie Coblentz, who believes that by designing novel space food rituals, astronauts will not just be able to survive in space, but thrive there as well.",2022-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2018/07/13/maggieCoblentz_paleBlueDot_02.jpg,328622,,2023-09-07T15:35:34.906Z,['mcoblent@media.mit.edu'],True,Interplanetary-Gastronomy,2018-09-01,"['design', 'virtual-reality', 'food', 'space', 'technology', 'wellbeing', 'zero-gravity', 'microbiology']",Interplanetary Gastronomy,PUBLIC,https://www.maggiecoblentz.com/,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-07-15T03:15:51.612Z,"Artificial intelligence (AI) agents in an embodied form, such as Jibo, Amazon Alexa, and Google Home, are increasingly becoming part of our daily lives and our homes. While there have been numerous studies in lab settings documenting short-term individual interactions with intelligent agents, we are at a point where we need to be exploring the larger impact of these technologies in the world, living with real people over longer periods of time.From a design research perspective, understanding and developing robots and AI that intersect with society is a “wicked problem,” a problem with many components that cannot be solved without interdisciplinary approaches. Design research within interdisciplinary applications has sought to develop approaches, methods, tools, and techniques to investigate the impact of technologies and inform future development. This work focuses on developing tools for exploring robots’ and AI’s impact on daily lives to better inform the development of these technologies by elucidating academia’s and industry’s requirements of tools for this domain.For more information, please contact Anastasia Ostrowski (akostrow@media.mit.edu).",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2018/09/27/ActionSheet_small.png,329251,,2023-09-04T18:05:13.583Z,"['haewon@media.mit.edu', 'akostrow@media.mit.edu', 'cynthiab@media.mit.edu']",True,tools-to-investigate-societal-impacts-of-robot-ai,2018-07-01,"['robotics', 'design', 'human-computer-interaction', 'artificial-intelligence', 'human-machine-interaction', 'social-science', 'social-robotics', 'social-justice']",Tools to investigate societal impacts of robots and AI,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-07-18T18:09:42.845Z,"Virtual reality can help realize mediated social experiences where we interact as richly with those around the world as we do with those in the same room. The design of social virtual experiences presents a challenge for remotely located users with differently sized, room-scale setups like those afforded by recent commodity virtual reality devices like the HTC Vive. This work explores how we can allow remote users to learn to dance together in VR by mapping their individual physical spaces to a shared virtual space.  Video and paper available here: http://web.media.mit.edu/~sra/dancing.html",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/12/13/dance-vr.png,330359,,2018-12-17T15:39:47.482Z,['sra@media.mit.edu'],True,learning-to-dance-in-vr,2017-02-01,"['virtual-reality', 'learning-teaching', 'interfaces']",Learning to dance in VR,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-07-18T18:38:43.666Z,"This work explores the design of techniques based on the cognitive illusion of ""inattentional blindness"" which is a failure to notice something happening in front of you when you are focused on something else. The aim is to direct a user's attention and manage their perception to create VR experiences with high levels of presence. The  techniques were tested through a VR implementation of a disruption-free natural walking experience.  Video and paper available here: http://web.media.mit.edu/~sra/vmotion.html",,['fluid-interfaces'],,330367,,2018-08-01T21:12:56.068Z,['sra@media.mit.edu'],True,inattentional-blindness-in-vr,,[],Cognitive Illusions and VR,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-07-23T13:56:15.757Z,"The Interplanetary Cookbook, developed by Maggie Coblentz, presents thought-provoking recipes and zero-g kitchenware for the future of life in outer space. Presently, most of space food is made for survival, yet food does not simply provide nourishment. It opens our imagination, engages our senses, and contains our rich heritage. As humans venture further into deep space and explore new planets, a unique food culture will emerge. By pushing the boundaries of space food as we know it, we hope to inspire a new field of Interplanetary Gastronomy that questions both what is possible, and what is desirable. This is less of a traditional cookbook with prescriptive tools and applications, but an expression of ideas about the flavors and artifacts of daily life that reflect our culinary future. The cookbook includes a collection of in-house designs, collaborations, and public contributions from around the world, to reflect the many ingredients and voices that shape our rich food culture.Explore the full collection of public contributions here. The digital cookbook is coming soon. Stay tuned! For inquiries about the project please email Maggie Coblentz (mcoblent@media.mit.edu). ",,['space-exploration'],https://dam-prod2.media.mit.edu/x/2018/07/23/1_wiFG3wtjlnKAcifHRliZQw.jpg,332016,,2021-05-07T14:12:21.293Z,['mcoblent@media.mit.edu'],True,interplanetary-cookbook,2018-09-01,"['design', 'virtual-reality', 'food', 'health', 'storytelling', 'space', 'wellbeing', 'zero-gravity']",The Interplanetary Cookbook,PUBLIC,https://www.maggiecoblentz.com/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-07-29T11:22:20.388Z,"The IEEE Standards Association (IEEE-SA) and the MIT Media Lab are joining forces to launch a global Council on Extended Intelligence (CXI) composed of individuals who agree on the following:One of the most powerful narratives of modern times is the story of scientific and technological progress. While our future will undoubtedly be shaped by the use of existing and emerging technologies – in particular, of autonomous and intelligent systems (A/IS) – there is no guarantee that progress defined by “the next” is beneficial. Growth for humanity’s future should not be defined by reductionist ideas of speed or size alone but as the holistic evolution of our species in positive alignment with the environmental and other systems comprising the modern algorithmic world.We believe all systems must be responsibly created to best utilize science and technology for tangible social and ethical progress. Individuals, businesses and communities involved in the development and deployment of autonomous and intelligent technologies should mitigate predictable risks at the inception and design phase and not as an afterthought. This will help ensure these systems are created in such a way that their outcomes are beneficial to society, culture and the environment.Autonomous and intelligent technologies also need to be created via participatory design, where systems thinking can help us avoid repeating past failures stemming from attempts to control and govern the complex-adaptive systems we are part of. Responsible living with or in the systems we are part of requires an awareness of the constrictive paradigms we operate in today. Our future practices will be shaped by our individual and collective imaginations and by the stories we tell about who we are and what we desire, for ourselves and the societies in which we live.These stories must move beyond the “us versus them” media mentality pitting humans against machines. Autonomous and intelligent technologies have the potential to enhance our personal and social skills; they are much more fully integrated and less discrete than the term “artificial intelligence” implies. And while this process may enlarge our cognitive intelligence or make certain individuals or groups more powerful, it does not necessarily make our systems more stable or socially beneficial.",,['directors-office'],,333918,,2018-07-29T23:53:45.243Z,"['drwood@media.mit.edu', 'minow@media.mit.edu', 'kadec@media.mit.edu', 'lessig@media.mit.edu', 'cbarabas@media.mit.edu', 'joyab@media.mit.edu', 'hidalgo@media.mit.edu', 'nsaltiel@media.mit.edu', 'tenzin@media.mit.edu', 'neri@media.mit.edu', 'picard@media.mit.edu', 'irahwan@media.mit.edu', 'kdinakar@media.mit.edu', 'joi@media.mit.edu', 'andreuhl@media.mit.edu']",True,council-on-extended-intelligence,,"['human-computer-interaction', 'artificial-intelligence', 'human-machine-interaction', 'ethics']",Council on Extended Intelligence,PUBLIC,https://globalcxi.org/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-08-01T14:55:50.957Z,"Biplanar fluoroscopy (BiFlo) enables three-dimensional bone kinematics analysis using x-ray videos and bone geometry from segmented CT. Hindered by a small capture volume relative to traditional optical motion capture (MOCAP), BiFlo applications to human movement are generally limited to single-joint motions with constrained range. Here, a hybrid procedure is developed for multi-joint gait analysis using BiFlo and MOCAP in tandem. Kinematic analysis of bones surrounding the knee, ankle, and foot was performed. Results show that this hybrid protocol effectively measures knee and ankle kinematics in all three body planes. Additionally, sagittal plane kinematics for select foot bone segments (proximal phalanges, metatarsals, and midfoot) was realized. The proposed procedure offers a novel approach to human gait analysis that eliminates errors originated by soft tissue artifacts, and is especially useful for ankle joint analysis, whose complexities are often simplified in MOCAP studies.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2018/08/01/flowChart3.png,334880,,2021-09-17T19:38:48.031Z,"['danask@media.mit.edu', 'kmoerman@media.mit.edu', 'hherr@media.mit.edu', 'dhill24@media.mit.edu']",True,biplanar-fluoroscopy-gait-analysis,,"['bioengineering', 'imaging', 'bionics']",Biplanar Fluoroscopy Gait Analysis,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-08-01T19:07:32.402Z,"During the gait cycle, the human ankle complex serves as a primary power generator while simultaneously stabilizing the entire limb. These actions are controlled by an intricate interplay of several lower leg muscles that cannot be fully uncovered using experimental methods alone. A combination of experiments and mathematical modeling may be used to estimate aspects of neuromusculoskeletal functions that control human gait. In this research, a three-dimensional neuromuscular model of the human ankle-foot complex based on biplanar fluoroscopy gait analysis is presented. Driven by kinematics, kinetics, and electromyography (EMG), the model seeks to solve the redundancy problem, individual muscle-tendon contributions to net joint torque, in ankle and subtalar joint actuation during overground gait. An optimization approach was employed to calculate sets of morphological parameters that simultaneously maximize the neuromuscular model’s metabolic efficiency and fit to experimental joint torques. Optimal morphological parameter sets produce estimates of force contributions and states for individual muscles.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2018/08/01/Neuro_model_extended_noMTP.png,334888,,2021-09-17T19:38:59.712Z,"['dhill24@media.mit.edu', 'hherr@media.mit.edu']",True,3d-neuromuscular-model-of-the-human-ankle-foot-complex,2018-08-01,"['bioengineering', 'imaging', 'biomechanics', 'bionics']",3D neuromuscular model of the human ankle-foot complex,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-08-02T00:43:50.044Z,"ArtBoat is a tool for communities to make collaborative light paintings in public spaces and reimagine the future of their cities.  Visit our project website at artboatcommunity.com.ArtBoat consists of a remote control boat with light strips that change color in response to a custom light design board. During ArtBoat events, community members can drive remote controlled boats, control the color of lights on the boats, or take long exposure photographs to make collaborative light graffiti that explores how art can help communities claim public spaces. Our first ArtBoat community event was on July 29, 2018 at Magazine Beach Park in Cambridge, MA. Thanks to our Magazine Beach host Cathie Zusy, our photographers Jorge Valdez (IG: jorgelvma), Ron Hoffmann (IG: archiscapes.us), Neil Gaikwad (IG: Neil Gaikwad Photography), Garance Malivel, and Jimmy Day, our boat facilitators, and our participants.  Our second ArtBoat event was on September 22, 2018  at Herter Park Amphitheater in Boston, MA, as part of the 15th anniversary Revels RiverSing! If you'd like to join our mailing list to find out about future ArtBoat events, email artboat@lauraperovich.com or fill out this form. ArtBoat is a sister project of SeeBoat. It is an open source project licensed under GPLv2 and Creative Commons.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2018/08/01/artBoat.jpg,334896,,2022-06-10T14:09:49.250Z,['perovich@media.mit.edu'],True,artboat,2018-04-01,"['art', 'environment', 'sensors', 'community']",ArtBoat,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2018-08-02T18:10:18.894Z,"Technical summaryUnstructured learning problems without well-defined rewards are unsuitable for current reinforcement learning (RL) approaches. Action-derived rewards can allow RL agents to fully explore state and action trade-offs in scenarios that require specific outcomes yet are unstructured by external reward. Clinical trial dosing choice is an example of such a problem. This study led by Dr. Shah reported the successful formulation of clinical trial dosing choice as an RL problem using action-based rewards and learning of dosing regimens to reduce mean tumor diameters (MTD) in patients undergoing simulated temozolomide (TMZ) and procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea, and vincristine (PCV) chemo- and radiotherapy clinical trials. The use of action-derived rewards as partial proxies for outcomes is described for the first time. Novel dosing regimens learned by an RL agent in the presence of action-derived rewards achieve significant reduction in MTD for cohorts and individual patients in simulated TMZ and PCV clinical trials while reducing treatment cycle administrations and dosage concentrations compared to human-expert dosing regimens. This approach can be easily adapted for other learning tasks where outcome-based learning is not practical.Glioblastoma (brain tumors) background: Glioblastoma is an aggressive type of cancer that can occur in the brain
or spinal cord. A hard-hitting treatment typically involves combining surgery
with radiation therapy and chemotherapy, which is necessary to combat the
aggressive nature of a glioblastoma. Chemo-and Radiotherapy Treatments (CRT) may
slow the progression of cancer and reduce signs and symptoms, but are unable to
cure the disease. Survival rates are low (about 14-18 months) and only about
10% of patients live five years or longer. CRTs are often given as a
combination of drugs. Procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea,
and Vincristine (PCV) is triple drug chemotherapy for glioblastomas and can be
toxic for the patients. Temozolomide (TMZ), is less toxic, has shown greater
efficacy when compared to radiotherapy alone, in the treatment of glioblastoma.
There is significant and urgent need for novel CRT dosing regimens in human subjects
to optimize for maximum therapeutic benefit for patients while reducing
toxicity.Reinforcement learning background: Reinforcement learning (RL) is an area of machine learning and AI inspired by behaviorist psychology. RL agents can self-learn how to solve complex tasks in a relatively unstructured environment so as to maximize some notion of cumulative rewards and reduce penalties set by human programmers. Reward functions in RL domains are typically derived from a measure that is external to the chosen representation of the states (data) and actions (steps) used for the self-training algorithm. Using RL to solve tasks without readily accessible external scalar outcomes is a relatively unexplored field, as many currently studied domains have well-defined outcomes and associated rewards as part of their definitions.Clinical trials background: Clinical trials to evaluate new drugs, therapies, and vaccines are among the most complex experiments performed in medicine. Nearly half of phase 2 and phase 3 trials fail. For oncology trials, the failure rate rises to two-thirds. A common theme is the difficulty of predicting clinical results in a wide patient base given limited knowledge of key parameters which need to be considered to test candidate molecules, eliminate adverse events, and identify the drugs half maximal inhibitory concentration. Optimal CRT dosing for patients enrolled in glioblastoma clinical trials thus provides one example of an open-ended problem characterized by complex interactions between different drug properties, dosage and timing of administrations (actions), and effects on tumors (state) and where survival (outcomes) may not be available.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/760d5de8-4b76-43bf-a8ac-36be2761a89f,335212,,2022-10-14T21:24:51.080Z,['pratiks@media.mit.edu'],True,self-learning-ai-model-learns-from-patient-data-to-design-novel-clinical-trials,2018-08-10,"['human-computer-interaction', 'artificial-intelligence', 'bioengineering', 'data', 'genetics', 'health', 'human-machine-interaction', 'machine-learning', 'medicine', 'clinical-trials', 'clinical-science']",Self-Learning AI Model Learns from Patient Data to Design Novel Clinical Trials,PUBLIC,https://www.pratiks.info/,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-08-07T19:26:29.984Z,"Did you know that submarines today still cannot wirelessly communicate with airplanes? For decades, communicating between underwater and the air has remained an unsolved problem. Underwater, submarines use acoustic signals (or SONAR) to communicate; in the air, airplanes use radio signals like cellular or WiFi. But neither of these signals can work across both water and air.We present TARF (Translational Acoustic-RF communication), the first technology that enables communication between underwater and the air. A TARF transmitter sends standard sound (or SONAR signals).  Sound travels as pressure waves; when these waves hit the surface, they cause it to vibrate. To pick up these vibrations, a TARF receiver in the air uses a very sensitive radar. The radar transmits a signal which reflects off the water surface and comes back. As the water surface vibrates, it causes small changes to the received radar signal, enabling a TARF receiver to sense the tiny vibrations caused by the underwater acoustic transmitter.The video below explains how TARF works and some of its applications.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2018/08/22/Thumbnail.jpg,336779,,2020-06-17T02:26:54.907Z,"['fadel@media.mit.edu', 'tonolini@media.mit.edu', 'rezagh@media.mit.edu']",True,translational-acoustic-rf-tarf-communication,2018-08-21,"['robotics', 'communications', 'networks', 'sensors', 'internet-things', 'ocean']",Wireless communication from underwater to the air,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-08-15T21:08:29.165Z,"Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas—a map of the social patterns and networks that exist on the playa. To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. We designed location-tracking vessels that we routed through Burning Man towards the goal of finding a particular person. Along the way, the vessels logged individual information and GPS data. The experiment was a play on Milgram's small world experiment, which routed letters through the US and established the concept of ""six degrees of separation."" Through tracking the voyage of the vessels across the desert, we were able to quantitatively map the connectivity of the Burning Man community and qualitatively understand how people engage with Burning Man culture.",2018-12-11,['scalable-cooperation'],https://dam-prod2.media.mit.edu/uuid/be10b8ee-b954-4b82-a926-1ae31ee5748d,339318,,2019-06-25T15:01:11.938Z,"['niccolop@media.mit.edu', 'groh@media.mit.edu', 'irahwan@media.mit.edu', 'cebrian@media.mit.edu', 'emoro@media.mit.edu', 'nobradov@media.mit.edu', 'zive@media.mit.edu']",True,black-rock-atlas,2018-08-15,"['social-networks', 'crowdsourcing', 'networks', 'social-science', 'collective-intelligence', 'mapping', 'data-visualization']",Black Rock Atlas,PUBLIC,https://blackrockatlas.mit.edu,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2018-08-16T02:23:40.227Z,"Our traditional manufacturing, fabrication, and electronics have been centered around synthetic and completely artificial techniques. Coming from an industrial revolution, this is not surprising. However, in recent years, the progress in material science has enabled us to work at a deeper substrate level, much beyond the chip-layer. This has helped us rethink form/structure, sources of power and ""hosts"" of future electronics. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/06/18/Cover.jpg,339663,,2022-03-08T21:18:04.884Z,['sareen@media.mit.edu'],True,argus-sensors-inside-plants,2017-06-01,"['robotics', 'design', 'environment', 'human-machine-interaction', 'alumni', 'biotechnology']",Argus: Water monitoring through nanosensors inside living plants,PUBLIC,http://www.harpreetsareen.com,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-08-17T09:30:37.424Z,"MODULAND is a playground kit for learning electronic music.MODULAND is an interactive project created by the MIT Media Lab Berlin – Signal & Noise prototyping workshop, within the track “Playful Machines that Make Music."" With MODULAND, playgrounds become modular synthesizers to raise curiosity, exploration, and connection to electronic music making.By creating playful machines that use LEGO bricks, sensors, and microcontrollers, it creates an embodied and interactive music lesson in an urban space.",2020-08-01,['ml-learning'],https://dam-prod2.media.mit.edu/x/2018/08/17/Playground_AkrWIpG.jpg,339979,,2022-06-10T14:06:49.101Z,"['akito@media.mit.edu', 'x_x@media.mit.edu']",True,moduland,2018-08-14,[],MODULAND,PUBLIC,https://medialabmoduland.wordpress.com/,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2018-08-18T13:37:40.839Z,"The MIT Knowledge Futures Group (KFG), a joint venture of the MIT Media Lab and the MIT Press, is an incubator for early-stage technologies that form part of a new open knowledge ecosystem. The partnership is the first of its kind between a leading publisher and a world-class research lab designing technologies of the future. The KFG seeks to incubate projects that enrich our open knowledge infrastructure, and leading by example, to spark a movement towards greater institutional ownership of that infrastructure. The KFG currently incubates PubPub, an open authoring and publishing platform initially developed as a Media Lab project, by deploying it with dozens of MIT Press books and journals. PubPub socializes the process of knowledge creation by integrating conversation, annotation, and versioning into short and long-form digital publication. One of the flagship publications on PubPub is the Journal of Design and Science, which forges new connections between science and design and breaks down the barriers between academic disciplines. We envision JoDS as the node in a global online community rooted in the Media Lab’s research and design ethos. The KFG also incubates The Underlay, an open, distributed knowledge store architected to capture, connect, and archive publicly available knowledge and its provenance. The Underlay provides mechanisms for distilling the knowledge graph from openly available publications, along with the archival and access technology to make the data and content hosted on PubPub available to other platforms. ",,[],,340578,,2018-08-23T19:38:55.666Z,"['trich@media.mit.edu', 'joi@media.mit.edu']",False,mit-knowledge-futures-group,2018-08-01,[],MIT Knowledge Futures Group,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-08-23T13:44:46.431Z,"Although significant progress is being made in identifying pH sensing materials and device configurations, a standard protocol for benchmarking performance of next-generation pH devices is still lacking. In particular, key properties of characterization systems, such as inherent component contributions, time plots for extended-gate field-effect transistor (EGFET) measurements, and the input resistance (Rin), often go unreported in studies of pH sensing systems. These properties strongly influence the characterization system and can lead to mistaken attribution of properties to the device. In this project, a series of essential characterization tests and parameters are reported to evaluate pH systems, such as the zinc oxide (ZnO) EGFET, in a standardized protocol. This EGFET ZnO sensor has a sensitivity of −58.1 mV pH−1, drift range from 2.5 to 14.2 μA h−1, and response time of 136 s. By using a ZnO sensing electrode, it is demonstrated that i) intrinsic contributions of reference electrode and commercial transistor (for EGFET) are not negligible; ii) time plots for EGFET configuration and defining a critical point at the onset of drift are essential for accurate sensitivity, response time, and drift reporting; and iii) the results of the pH sensing system are strongly dependent on the input resistance of the used characterization instruments.",,['conformable-decoders'],https://dam-prod2.media.mit.edu/x/2018/08/23/smtd.201800265_2.jpg,342315,,2019-07-19T21:58:05.569Z,"['mghoneim@media.mit.edu', 'canand@media.mit.edu']",True,a-protocol-to-characterize-ph-sensing-materials-and-systems,2018-08-23,"['health', 'biotechnology']",A Protocol to Characterize pH Sensing Materials and Systems,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-09-09T21:05:46.248Z,"For more details and recent updates visit:  https://web.mit.edu/deblina-sarkar/Nanoelectronics has the potential to enable radical tools for in-vivo interrogation of our biological systems in order to answer fundamental questions in biology as well as to provide novel technologies by combining diagnostics with automated, therapeutic effects at cellular precision. Realization of this promise, however, will require severe dimensional and power scaling of electronics, which is beyond the physical limitations of conventional nanoelectronics, dealing a hard blow to this dream. Our aim is to develop extremely energy-efficient and ultra-scalable, next-generation nano-machines that overcome these fundamental limitations and can make this dream come true, opening up entirely new avenues that were unthinkable earlier. These devices will possess the capabilities of energy harvesting, wireless communication with systems outside the body, and can be remotely controlled. They will be coated with biomolecules such that they can effectively camouflage and trick the body into thinking that it is a part of its own biological system. Such devices can cause a paradigm shift in life-machine synergism.The possibilities with such bioelectronic devices are endless, and we are exploring, among other opportunities, brain activity recording at a large scale with a precision of single neuron, activity recording in spinal cord and peripheral nervous system, monitoring tumor microenvironment, observing response to pathology development or external stimulus at a single cell level, along with integrated functionalities such as stimulation and drug delivery.Recently we have developed the technology called Cell Rover - first ultra miniaturized antenna that can work wirelessly inside a living cell in 3D biological systems. This technology can explore and augment the mysterious inner environment of the cell and can bring in the prowess of information technology inside a living cell to create cellular scale-cyborgs! This work has been featured as the Editors’ Highlight in Nature Communications which showcases the 50 best papers recently published.The versatility of electronics is that they are inherently fast and can be designed according to an engineer’s dream to perform unique functions, which are beyond the capabilities of biology. While our immediate aims are to develop electronic devices for probing and controlling/modulating (for therapeutics) the body and brain, our long-term goal is to achieve seamless integration of nanoelectronics-bio hybrid structures into biological systems to incorporate functionalities not otherwise enabled by biology—thus helping us transcend our biological constraints.",,"['nano-cybernetic-biotrek', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/09/15/3.png,347805,,2023-09-07T23:33:45.572Z,['deblina@media.mit.edu'],True,life-nanomachine-synergism,2017-01-01,"['robotics', 'bioengineering', 'data', 'synthetic-biology', 'sensors', 'biology', 'technology', 'biotechnology', 'medicine', 'data-science', 'microbiology', 'nanoscience']",Life-Nanomachine Synergism,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-09-10T06:14:58.772Z,"Electronics are ubiquitous in today’s world, with applications ranging from smart cities to health care, defense, economy, government, education, research, entertainment and the list goes on. While the computing demands of these applications are ever increasing, the capabilities of electronics have hit fundamental limitations and have plateaued. In the past, the growth in computing capabilities of electronics has been sustained by scaling both the dimensions as well as power-supply voltage of the Field-Effect-Transistors (FETs), that form the building blocks of the integrated circuits. However, as the scaling roadmap enters the sub-5 nm regime, today’s transistor reaches its physical limits in dimensional scalability and can no longer offer effective electrostatics, leading to exponential increase in static leakage power. In addition, the power supply voltage, which is the most effective way for lowering the dynamic power, can no longer be scaled effectively as in earlier technology generations. The voltage in-scalability has fundamental roots in the thermal distribution of carriers, which limits the steepness of turn-on characteristics or subthreshold swing of conventional FETs. These dimensional and voltage scalability issues, lead to exponential increase in power-density and has ushered in the dead end of the glorious growth episode of ""Information Society."" Moreover, the fundamental nature of the problem suggests the inability of evolutionary solutions to address this growing energy crisis and demands radically new innovations on multiple fronts.Our research involves a holistic approach towards solving this energy crisis, starting from exploration of beyond-Silicon nanomaterial technology, to in-depth understanding of the physics of fundamentally different device-working mechanisms and finally, experimental demonstration of novel, highly energy-efficient and scalable electronic devices. We invented the world’s thinnest channel (6 atoms thick) quantum mechanical transistor involving band-to-band-tunneling, which overcomes the fundamental thermal limitations in subthreshold swing and leads to record energy reduction by more than 75% [D. Sarkar et.al., Nature, 526(7571), 91-95, (2015)] [Nature (News and Views) 526, 51–52(2015)]. This atomically thin and layered semiconducting-channel tunnel FET (ATLAS-TFET), is based on the idea, that we conceived, of a unique tunneling heterojunction combining the best attributes of 3D (matured doping technology) and 2D (excellent electrostatics and ultra-low tunneling barrier) materials to achieve extremely efficient and controllable electron wave propagation through the energy barrier. This device is the first and only tunneling-transistor till date, in any architecture and any material platform, to achieve ITRS prescription of sub-thermal subthreshold swing over four decades of current at an ultra-low power-supply voltage of 0.1V(thus, allowing voltage scalability). Moreover, the atomically-thin 2D channel provides near-ideal device electrostatics, which allows dimensional scalability to beyond Silicon scaling era (sub 5nm). Thus, this device can crack the long-standing issue of simultaneous dimensional and power-supply voltage scalability.",,['nano-cybernetic-biotrek'],https://dam-prod2.media.mit.edu/x/2018/09/10/transistor_jRxXqIx.png,348119,,2018-09-21T13:47:00.959Z,[],True,nano-machine-life-synergism,,[],Ultra-Scalable and Energy-Efficient Nanoelectronics,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-09-10T07:50:28.106Z,"For more details and recent updates visit:  https://web.mit.edu/deblina-sarkar/Sensors, especially biosensors, are indispensable for modern society due to their wide applications in public healthcare, national and homeland security, and forensic industries as well as environmental protection. Detection of biomolecules at ultra-low concentration (sub-picomolar to 100 atto-molar) is necessary for screening many cancers, neurological disorders, and early stage infections such as HIV. However, current medical diagnostic tools either have low sensitivity (picomolar detection) or require bulky, expensive equipment and extensive procedures that cannot be performed outside a well-controlled lab environment. Our research established, for the first time, that the material and device technology which has evolved mainly with an aim of sustaining the glorious scaling trend of Information Technology, can also transform a completely diverse field of bio/gas-sensor technology [Nat. Nano. (Research Highlights) 7,275 (2012)] [D. Sarkar et. al., Appl. Phys. Lett., 102, 023110 (2013)] [D. Sarkar et. al., ACS Nano, 8, 3992–4003 (2014)] [D. Sarkar et. al., Nano Lett., 15, 2852–62 (2015)]. We experimentally demonstrated electrical bio- and gas-sensors based on molybdenum disulfide (MoS2), which provides extremely high sensitivity and at the same time offers easy patternability and device fabrication, due to its 2D atomically layered structure. Moreover, we proposed for the first time, biosensors based on low subthreshold swing electronic devices (steep turn-ON characteristics) and theoretically illustrated that they can break the fundamental limits in sensitivity of conventional electrical biosensors and lead to an increase in sensitivity by more than four decades. [D. Sarkar et. al., Appl. Phys. Lett., 100, 143108 (2012)] [D. Sarkar et. al., Appl. Phys. Lett., 102, 203110 (2013)] [D. Sarkar et. al., Appl. Phys. Lett., 102, 023110 (2013)]. We plan to integrate the ultra-sensitive biosensing capabilities of novel nanoelectronic transducers with read-out circuitry as well as sample processing steps to build a hand-held low-power device which can enable detection of extremely low concentration (single entity) of biomolecules from whole blood. This diagnostic device, being ultra-sensitive, will enable early disease detection and, being scalable and low-power, will allow point-of-care application and extend diagnosis to remote areas. Such technology can save billions of lives and cause massive reduction in health care costs.",,['nano-cybernetic-biotrek'],https://dam-prod2.media.mit.edu/x/2018/09/10/biosensor.jpg,348127,,2022-01-11T20:57:33.193Z,['deblina@media.mit.edu'],True,ultra-sensitive-electrical-biosensors-for-point-of-care-applications,,"['synthetic-biology', 'nanoscience']",Ultra-sensitive electrical biosensors for point-of-care applications,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-09-10T17:50:33.668Z,Rethinking photography optics in the time dimensionWhat if we could design optics in time instead of space?,,['camera-culture'],https://dam-prod2.media.mit.edu/uuid/c8f9c533-018b-4526-8c2a-6b1f6c1cb3c8,348148,,2018-09-24T18:15:42.266Z,"['barmak@media.mit.edu', 'guysatat@media.mit.edu', 'raskar@media.mit.edu']",True,time-folded-optics,2018-08-01,[],Time-folded optics,PUBLIC,http://web.media.mit.edu/~barmak/Time-folded.html,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2018-09-10T17:58:50.040Z,"For more details and recent updates visit:  https://web.mit.edu/deblina-sarkar/For understanding the brain structure, it is necessary to decipher the nanoscale organization of the biomolecular building blocks of the brain in 3D. However, the resolution of optical microscopes is limited by the diffraction of light while conventional super-resolution technologies require specialized expensive equipment and have challenges in scaling to 3D [K. R. Porter et. al., J. Exp. Med., 81, 233–246 (1945)] [E. Betzig, E. et al., Science, 313, 1642–1645 (2006)] [M. J. Rust et. al., Nat Methods, 3, 793–795 (2006)]. Recently, it has been shown that it is possible to break the diffraction limited resolution of optical microscopes by physically expanding the biological samples using electrostatic forces in a hydrogel: a methodology named expansion microscopy (ExM) [F. Chen et. al., Science, 347 (6221), 543–548 (2015)]. We have developed the technology to achieve the highest expansion factor, reported till date (100-fold linear expansion), of tissue-polymer hybrids [D. Sarkar et. al., Society for Neuroscience (2016)] . Such high physical expansion factors allow imaging of biological specimens at sub-10 nm resolution (i.e., 300 nm (diffraction limit) / 100 (expansion factor)), using conventional diffraction limited microscopes. This technology, which we termed Expansion Revealing (ExR), utilizes both electrostatic and mechanical forces to achieve extremely high expansion factors and is fundamentally different from expansion microscopy (ExM), or iterated-ExM, which involves solely the electrostatic repulsive forces in the polymer for expansion [D. Sarkar et. al., Nature Biomedical Engg., 1-17 (2022)]. Moreover, ExR is much simpler to implement, provides better yield and retention of biomolecules, allowing post processing. ExR enables precise mapping of the biomolecular building blocks of cells (proteins, transcriptomes (RNA), DNA) as well as the cellular interconnections that form large scale, 3D circuits, using hardware and reagents easily available in research laboratories. Thus, it is highly advantageous compared to conventional super-resolution imaging techniques, which are difficult to scale to 3D thick tissues and require forbiddingly expensive hardware and expert handling.We are now working on applying this technology for mapping the biomolecular building blocks of brain and provide in-depth insights into the pathology mechanisms that could lead to the discovery of new targets for treating neurological diseases.",,['nano-cybernetic-biotrek'],https://dam-prod2.media.mit.edu/x/2018/09/10/brain.png,348156,,2022-09-10T03:41:57.488Z,['deblina@media.mit.edu'],True,nanoscale-mapping-of-bio-molecular-building-blocks-of-brian,,"['synthetic-biology', 'biology', 'cognitive-science', 'nanoscience']",Nanoscale mapping of bio-molecular building blocks of brain,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-09-10T19:28:08.402Z,"For more details and recent updates visit:  https://web.mit.edu/deblina-sarkar/Electronics are ubiquitous in today’s world, with applications ranging from smart cities, health care, defense, economy, government, education, research, and entertainment. While the computing demands of these applications are ever increasing, the capabilities of electronics have hit fundamental limitations and have plateaued. In the past, the growth in computing capabilities of electronics has been sustained by scaling both the dimensions as well as power-supply voltage of the Field-Effect-Transistors (FETs) that form the building blocks of the integrated circuits. However, as the scaling roadmap enters the sub-5 nm regime, today’s transistor reaches its physical limits in dimensional scalability and can no longer offer effective electrostatics, leading to exponential increase in static leakage power. In addition, the power supply voltage, which is most effective way for lowering the dynamic power, can no longer be scaled effectively as in earlier technology generations. The voltage in-scalability has fundamental roots in the thermal distribution of carriers, which limits the steepness of turn-on characteristics or subthreshold swing of conventional FETs. These dimensional and voltage scalability issues lead to an exponential increase in power-density and have ushered in the dead end of the glorious growth episode of ""Information Society."" Moreover, the fundamental nature of the problem suggests the inability of evolutionary solutions to address this growing energy crisis and demands radically new innovations on multiple fronts.Our research involves a holistic approach towards solving this energy crisis, starting from exploration of beyond-Silicon nanomaterial technology, to in-depth understanding of the physics of fundamentally different device-working mechanisms, and finally the experimental demonstration of novel, highly energy-efficient and scalable electronic devices.We invented the world’s thinnest channel (6 atoms thick) quantum mechanical transistor involving band-to-band-tunneling, which overcomes the fundamental thermal limitations in subthreshold swing and leads to record energy reduction by more than 75% [D. Sarkar et.al., Nature, 526(7571), 91-95, (2015)] [Nature (News and Views) 526, 51–52(2015)]. This atomically thin and layered semiconducting-channel tunnel FET (ATLAS-TFET) is based on the idea, that we conceived, of a unique tunneling heterojunction combining the best attributes of 3D (matured doping technology) and 2D (excellent electrostatics and ultra-low tunneling barrier) materials to achieve extremely efficient and controllable electron wave propagation through the energy barrier. This device is the first and only tunneling-transistor till date, in any architecture and any material platform, to achieve ITRS prescription of sub-thermal subthreshold swing over four decades of current at an ultra-low power-supply voltage of 0.1V (thus, allowing voltage scalability). Moreover, the atomically-thin 2D channel provides near-ideal device electrostatics, which allows dimensional scalability to beyond Silicon scaling era (sub 5nm). Thus, this device can crack the long-standing issue of simultaneous dimensional and power-supply voltage scalability.",,['nano-cybernetic-biotrek'],https://dam-prod2.media.mit.edu/x/2018/09/10/transistor_QepZlmb.png,348168,,2021-05-13T21:31:51.069Z,['deblina@media.mit.edu'],True,ultra-scalable-and-energy-efficient-nanoelectronics,2018-09-10,"['synthetic-biology', 'engineering', 'nanoscience']",Ultra-scalable and energy-efficient nanoelectronics,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-09-12T18:16:13.184Z,"How to generate almost anything is a collaborative project between humans and AI, run by Pinar Yanardag Delul of the Scalable Cooperation group. The project combines expertise in making (inspired by Neil Gershenfeld’s How To Make Almost Anything class) from around MIT with generative adversarial neural networks (GANS). Each project chooses a focus for the human-machine collaboration — from music and fashion to pizza and perfume — to create outcomes that otherwise might never have been imagined!To read more about the project, it's fully documented at: https://medium.com/@howtogeneratealmostanything, and is written about here: https://venturebeat.com/2018/09/10/mit-students-use-ai-to-cook-up-pizza-recipes/",2018-09-30,"['center-for-bits-and-atoms-1', 'scalable-cooperation']",https://dam-prod2.media.mit.edu/x/2018/09/12/gan-dress.jpg,348830,,2018-09-17T15:49:17.289Z,"['pinary@media.mit.edu', 'agnescam@media.mit.edu']",True,how-to-generate-almost-anything,2018-07-01,[],How To Generate Almost Anything,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2018-09-20T01:21:48.648Z,"Translating ambient sounds of moment into tangible and shareable memories through animated paperWe present a tangible memory notebook—reMi—that records ambient sounds and translates them into a tangible and shareable memory using animated paper. The paper replays the recorded sounds and deforms its shape to generate synchronized motions with the sounds. Computer-mediated communication interfaces have allowed us to share, record, and recall memories easily through visual records. However, those digital visual-cues that are trapped behind the device’s 2D screen are not the only means to recall a memory we experienced with more than the sense of vision. To develop a new way to store, recall, and share a memory, we investigate how tangible motion of a paper that represents sound can enhance ""reminiscence.""",2019-05-31,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/fda139b8-9750-4ac0-80ff-34be15bd06ab,351350,,2019-04-10T14:41:04.282Z,['yun_choi@media.mit.edu'],True,remi,2017-10-30,"['art', 'consumer-electronics', 'interfaces']",reMi,PUBLIC,http://www.mallcong.com,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-09-21T18:56:40.353Z,"“Can a man know the truth and tell it to the greatest number and still be misunderstood? Can one man be of the many and still be known?” Schoenberg in Hollywood is the most recent opera by composer-inventor-professor Tod Machover that explores the complex relationship between uncompromising art and mass appeal, and of whether—and how—art can change the world. Arnold Schoenberg was a man of extraordinary contradictions: now considered one of the twentieth century’s greatest composers, during much of his lifetime Schoenberg was known for—and excelled at—composing music hated by the public and critics; a man whose only compass was his pursuit of pure ideas, Schoenberg also yearned for popularity; Schoenberg’s music absorbs tradition, but it is not hampered by it and always points forward. What happened—and might have happened—when such an uncompromising spirit settled in Hollywood, the epicenter of American popular culture, after he fled Hitler's Europe in 1935? Schoenberg in Hollywood begins with a meeting (one that did occur in history) between the legendary producer Irving G. Thalberg of Metro Goldwyn Mayer and Arnold Schoenberg. Thalberg asks Schoenberg to compose music for a film based on Pearl S. Buck’s The Good Earth, a best-seller about the life of peasants in a Chinese village. Although Schoenberg disdained the idea of composing music to please the public, the prospect of writing a Hollywood film score that would reach millions appealed to him greatly. In reality, Schoenberg was not offered the job when he demanded a $50,000 fee—an astronomical sum at the time—from Thalberg. However, Schoenberg in Hollywood exploits—and explores—the hypothetical scenario of what would have happened if Schoenberg had indeed composed for Hollywood. In the opera, Schoenberg imagines the events of his life through the lens of different film genres: silents, noir mysteries, Disney cartoons, musicals, and Westerns, making the movie—and projecting his vision well into the future—that Hollywood never allowed him to do.Commissioned and presented by Boston Lyric Opera, with much visionary  technology for sound, image, and staging created at the MIT Media Lab, Schoenberg in Hollywood is based on a scenario by the late Braham Murray, with a libretto by Simon Robson and directed by Karole Armitage. Schoenberg in Hollywood premiered at the Boston Lyric Opera in November 2018 and travels to the Vienna Volksoper in the 2019/2020 season.For more, see schoenberg.media.mit.edu.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/09/21/Schoenberg-Omar as Bogart-1_KcwO77T.png,351690,,2019-10-09T19:57:24.154Z,['tod@media.mit.edu'],True,schoenberg-in-hollywood,2018-09-01,"['art', 'artificial-intelligence', 'music', 'politics', 'storytelling', 'performance', 'creativity', 'technology', 'voice']",Schoenberg in Hollywood: A new opera by Tod Machover,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-09-22T02:35:55.065Z,"This 2018 AI and Governance Assembly project, EqualAIs, was a multifaceted investigation into the technical, policy and societal questions raised by the increasing capabilities of facial recognition software and the unprecedented  capability for automated, real time identification and tracking of  individuals.Our team demonstrated the technical feasibility of facial recognition adversarial attacks through the creation of a working prototype, successfully attacking the major APIs' facial recognition classifiers. We filed a FOIA with partners including the ACLU for information about US Customs' use of facial recognition algorithms and we sought to encourage public dialogue about facial recognition policy choices through a series of public talks and the creation of open source resources.",2018-04-20,['ethics-and-governance'],,352012,,2018-10-18T01:27:03.683Z,['ggreene@media.mit.edu'],True,equalais-adversarial-attacks-for-facial-recognition,2018-02-01,"['artificial-intelligence', 'politics']","EqualAIs: Facial Recognition, Adversarial Attacks and Policy Choice",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-09-22T03:24:37.699Z,"Trying to catch up to a rapidly advancing technology, government agencies, industry, and academia are looking for guidance and best practices around AI and ethics. Many applications of AI can be characterized as some form of algorithmic decision making which raises the question of when we, as a society, should attempt to delegate decision making making processes to computer programs.There is a difference between using a computer predicted risk score to decide which buildings to inspect and to decide a criminal sentence. As we create a theoretical taxonomy of decisions and their contexts, asking how their differences and similarities should be reflected in a society's norms, laws and policies around automation, we have a pressing practical application in mind. Governments and private actors are using AI to make decisions now.Working with governments internationally, we are developing best practices around government use and regulation of AI and AVs.We have designed surveys, conducted interviews, written case studies, directed legal research, created workshops, developed technical and policy tools, written guidance and facilitated regional and international conversations and collaboration between government leaders and other partners.Our goal is to help policy makers and practitioners around the world make good decisions around  AI and AV strategy, policy, ethics, governance and risk.",2020-06-02,['ethics-and-governance'],,352020,,2022-06-10T13:50:36.749Z,[],True,algorithmic-decision-making-and-governance-in-the-age-of-ai,2018-01-15,"['artificial-intelligence', 'government']",Algorithmic Decision Making and Governance in the Age of AI,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-09-26T13:38:09.903Z,"FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic ""agents"" designed to tune the material make-up of the structure being constructed on the fly as informed by their environment.Some of nature’s most successful organisms collaborate in a swarm fashion. Nature’s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. ",2019-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2018/09/26/03.jpg,353287,,2021-05-21T14:39:40.965Z,"['m_kayser@media.mit.edu', 'cail@media.mit.edu', 'sfalcone@media.mit.edu', 'bader_ch@media.mit.edu', 'nassia@media.mit.edu', 'jpcosta@media.mit.edu', 'darweesh@media.mit.edu']",True,fiberbots,2016-01-01,"['robotics', 'architecture', 'construction', 'manufacturing', 'fabrication']","FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-09-27T21:09:18.902Z,"Voice-user interfaces (VUIs), such as Amazon Echo and Google Home, are increasingly becoming present in domestic environments. Users attribute agency and personality traits to these AI agents. Due to the social attributes of these technologies, users try to understand the agents' characteristics based on social norms. These factors affect user experience quality and overall engagement, which, when considering first experiences, can impact continuous usage and engagement with VUI technology.Our work examines users’ first impressions and interactions with  VUI agents, such as Google Home, Amazon Echo, and Jibo, with varying brands and modalities. Using personality and experience questionnaires, we seek to understand how VUI modalities, form, and personality affect engagement with VUIs. In studies thus far, we have investigated how social embodiment, interpersonal movement, and branding influence users' perceptions of VUIs and their behaviors when interacting with VUIs.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/09/27/Screen Shot 2018-09-27 at 5.35.32 PM.png,353646,,2021-03-25T17:57:39.438Z,"['akostrow@media.mit.edu', 'haewon@media.mit.edu', 'cynthiab@media.mit.edu']",True,shaping-engagement,2018-08-13,"['robotics', 'design', 'human-computer-interaction', 'artificial-intelligence', 'social-science', 'machine-learning', 'social-robotics', 'voice', 'social-justice']",Engagement with Voice-User Interface Agents,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-01T19:17:33.377Z,"The CRISPR-Cas9 system has proven to be a versatile tool for genome editing, with numerous implications in medicine, agriculture, bioenergy, food security, and beyond. The range of targetable DNA sequences is limited, however, by the need for a short sequence of DNA beside the target site, called the PAM. In total, there are only a handful of CRISPR enzymes with a short enough PAM sequence to be able to target a large portion of the total DNA in a genome. In this study, we identify a natural Cas9 enzyme from the bacterial genome of Streptococcus canis that has a PAM sequence with only a single G as its PAM sequence (5’-NNG-3’), allowing flexible targeting of up to 50% of all DNA sequences in living organisms. This new molecular tool potentially grants unprecedented access to correct disease-related mutations, enhance agricultural methods, and expand research efforts.",,['molecular-machines'],https://dam-prod2.media.mit.edu/x/2018/10/01/CRISPRenzyme-Pranam.png,354943,,2018-10-25T16:09:08.650Z,"['pranam@media.mit.edu', 'njakimo@media.mit.edu', 'jacobson@media.mit.edu']",True,opening-wider-genomic-access-with-a-flexible-crispr-enzyme,2017-01-04,[],Opening wider genomic access with a flexible CRISPR enzyme,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-01T20:23:00.192Z,"Scratch BlockArt is an experimental visualization tool designed to let children discover their own computational patterns on Scratch. Existing methods often utilize data about the types of programming blocks used in children’s projects to generate a quantitative assessment of a project’s computational complexity based on limited criteria. BlockArt presents an alternative approach for revealing this data to young creators themselves. Rather than datafying children’s creations, BlockArt is designed to transform the data about their code into creative objects that can spark children's curiosity and enable them to reflect on their own styles and choices. For a given username, the tool dynamically generates colorful visualizations representing the number and diversity of programming blocks used in each of their shared projects over time. Children can also click to see the project behind the visualization. The idea is not to evaluate whether they use more or less 'complex' blocks, but to reveal how the types of blocks children use are based on their motivations and interests behind creating a specific project. It also shows how looking at the diversity of code in all their projects is a better representation of their learning trajectory than providing a quantitative assessment on individual projects without additional context. ",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2018/11/07/ba.png,354951,,2019-11-04T15:18:02.725Z,['shrutid@media.mit.edu'],True,scratch-blockart,2018-01-26,"['kids', 'learning-teaching', 'data-visualization']",Scratch BlockArt,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-03T18:08:55.015Z,"In the research realm of shape changing interfaces in human-computer interaction, a variety of pin-based shape display designs have been proposed. Using the hardware, researchers have explored and validated a range of applications for tangible and haptic interactions. While a variety of technical configurations for such shape display hardware have been explored (e.g., resolution, sizes, motion modalities), the interactivity that a single shape display hardware can offer is limited with its hardware configuration.TRANS-DOCK is a docking system for pin-based shape displays that expands their interaction capabilities for both the output and input. By simply interchanging the transducer modules, composed of passive mechanical structures, to be docked on a shape display, users can selectively switch between different configurations including display sizes, resolutions, and even motion modalities such as rotation, bending, and inflation.In our paper accepted to TEI 2020, we introduce a design space consisting of several mechanical elements and enabled interaction capabilities. Our proof-of-concept prototype explores the development of the docking system based on our previously developed 10 x 5 shape display, inFORCE. A number of transducer examples are shown to demonstrate the range of interactivity and application space achieved with the approach of TRANS-DOCK. In addition to the prototypes, we introduce potential use cases that take advantage of the interchangeability of TRANS-DOCK including education, entertainment, and prototyping interfaces. For example, TRANS-DOCK can be used for museums or schools which require a variety of tangible learning contents where different transducers can be selected. For gaming, users can switch between different physical interfaces to switch the contents for a game with physical I/O. For interface designers, TRANS-DOCK could be a platform to quickly iterate and compare different types of actuated interfaces without building them from scratch.Through TRANS-DOCK, we intend to expand the expressibility, adaptability, and customizability of a single shape display for dynamic physical interaction. By converting arrays of linear motion to several types of dynamic motion in an adaptable and flexible manner, we advance shape displays to enable versatile embodied interactions for a variety of digital/physical everyday applications.PresskitMore info on the Tangible Media group's project page.",2020-05-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/01/30/2_transdock_connection.jpeg,355604,,2020-06-10T19:34:01.434Z,"['ken_n@media.mit.edu', 'ishii@media.mit.edu']",True,trans-dock,2018-08-01,"['human-computer-interaction', 'interfaces']",TRANS-DOCK,PUBLIC,http://tangible.media.mit.edu/project/transdock/,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-05T19:08:04.063Z,"Lit, the lightweight Lightning Network software developed at the MIT Media Lab, works with multiple Bitcoin-like blockchains. The DCI team has created this demo to experience how Lightning can be used at point-of-sale terminals as well as allow users to do device-to-device transactions using multiple coins. We show how an electronic mobile wallet works with three currencies (Bitcoin, Litecoin, and USD).",2020-10-01,['digital-currency-initiative-dci'],https://dam-prod2.media.mit.edu/x/2018/10/11/Bitcoin vending machine.png,356247,,2021-10-04T18:15:14.352Z,"['narula@media.mit.edu', 'joe@media.mit.edu', 'adragos@media.mit.edu', 'tdryja@media.mit.edu']",True,bitcoin-vending-machine,2018-07-02,['cryptocurrency'],Bitcoin vending machine,PUBLIC,https://dci.mit.edu/lightning-network/,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2018-10-05T19:18:46.594Z,"Algorithmic FairnessSociety increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Substantial work in algorithmic fairness has surged, focusing on either post-processing trained models, constraining learning processes, or pre-processing training data. Recent work has proposed optimal post-processing methods that randomize classification decisions on a fraction of individuals in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concerns due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. Active FairnessThe present work proposes an alternative active framework for fair classification, where, in deployment, a decision maker adaptively acquires information according to the needs of different groups or individuals towards balancing disparities in classification performance. We propose two such methods where information collection is adapted to group- and individual-level needs, respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g., equal opportunity) and 2) parity in both false positive and false negative rates (e.g., equal odds). Moreover, we show that, by leveraging their additional degree of freedom, active approaches can outperform randomization-based classifiers previously considered optimal, while also avoiding limitations such as intra-group unfairness.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2020/06/01/hairplot.png,356255,,2020-06-01T12:46:55.438Z,"['sandy@media.mit.edu', 'bakker@media.mit.edu', 'noriega@media.mit.edu']",True,active-fairness,2018-01-31,['machine-learning'],Active Fairness in Algorithmic Decision Making,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-05T19:30:40.903Z,"The Jibo Research Platform is an in-the-field deployable Social Robotics experimentation and data collection infrastructure. Built upon the world's first commercial social robot for the home, it extends Jibo's design, hardware, and data security for research purposes.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2018/10/12/jibo_lights_ACPhaT3.png,356264,,2020-01-08T17:51:56.691Z,"['jon@media.mit.edu', 'haewon@media.mit.edu', 'samuelsp@media.mit.edu', 'cynthiab@media.mit.edu']",True,jibo-research-platform,2018-08-01,"['robotics', 'human-computer-interaction', 'artificial-intelligence', 'government', 'kids', 'learning-teaching', 'social-science', 'machine-learning', 'social-robotics', 'affective-computing', 'community', 'technology', 'mental-health', 'mechanical-engineering', 'open-access', 'diversity', 'women']",Jibo Social Robotic Research Platform,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-07T03:24:38.041Z,"Elowan is a cybernetic lifeform, a plant in direct dialogue with a machine. Using its own internal electrical signals, the plant is interfaced with a robotic extension that drives it toward light.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/11/01/BannerImage_v01.jpg,356897,,2019-06-18T21:50:45.542Z,"['sareen@media.mit.edu', 'pattie@media.mit.edu']",True,elowan-a-plant-robot-hybrid,2017-04-01,"['robotics', 'design', 'human-computer-interaction', 'art', 'environment', 'human-machine-interaction', 'interfaces', 'biology', 'technology', 'alumni', 'biotechnology', 'electrical-engineering']",Elowan: A plant-robot hybrid,PUBLIC,http://harpreetsareen.com,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1
False,2018-10-08T17:35:54.367Z,A tapestry where each pixel represents a pledge by an individual to vote. Anyone can participate and watch the growing and changing image that emerges as others agree to vote. The tapestry evolves and will encourage repeated attention. This tests local reinforcement to support actions.Follow this link for more information.,2018-12-31,['viral-communications'],https://dam-prod2.media.mit.edu/x/2018/11/05/votomosaic.png,357211,,2019-04-18T15:05:48.332Z,"['lip@media.mit.edu', 'kalli@media.mit.edu']",True,votomosaic,2018-08-01,"['civic-media', 'data-visualization', 'civic-action']",votoMosaic,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-08T18:04:49.875Z,"Many people think their vote doesn't count—that a single vote would not change an election's outcome—and they stay home on Election Day. However, your vote is a public statement of beliefs, amplified because many people in your voting district may agree with you, but do not (and in many cases cannot) vote. Your vote gives a voice to those many others.For example, congressional elections affect everyone in the district, regardless of whether they are eligible to vote. The same senators represent everyone in your state. The same ballot questions affect everyone in your community.Using data we've gathered from the United States Census Bureau, we have calculated just how many people you're representing with your vote. We directly compare the number of votes in recent congressional elections (by district) to the total population of the district.When you vote, you're not only voting for your own best interest, but you're also voting for the best interest of those around you.Visit https://www.relativotey.org and recognize your relative voting power!",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2018/10/29/Screen Shot 2018-10-29 at 2.42.37 PM_rOszUrk.png,357219,,2019-04-18T01:18:25.012Z,['lip@media.mit.edu'],True,relativoty,2018-09-04,"['civic-media', 'civic-technology', 'data-visualization']",RelatiVotey,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0
False,2018-10-08T18:09:57.998Z,"Only 40% of the eligible population votes in the typical US midterm election, and among young people turnout is even lower. In this experiment, we develop a game that encourages people to influence their friends to physically go to the polls. The system is reminiscent of Fifty Nifty, where people competed to amass points by both calling representatives and spreading the message to others. In addition to awarding points, vote.lol aims to motivate players by allowing them influence over the outcome of a shared narrative that develops in real-time before (and during) the election. Interactive stories with real-world game mechanics are characteristic of alternate reality games (ARGs), which have received scholarly attention for their potential to instigate viral communications among players who self-organize to solve complex problems. The purpose of this study is to test whether ARG techniques can motivate gamers to solve the intractable problem of getting their peers to vote.",2018-12-31,['viral-communications'],https://dam-prod2.media.mit.edu/x/2018/10/09/election-arg_As8PZzy,357235,,2019-04-18T00:52:49.802Z,"['smpsnr@media.mit.edu', 'mhjiang@media.mit.edu', 'agnescam@media.mit.edu', 'lip@media.mit.edu']",True,election-arg,2018-10-01,[],Election ARG,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2018-10-08T18:12:11.988Z,"A web browser extension that reveals less well-known aspects of corporate public behavior such as environmental respect and political bias. When one engages in a search, we place an image next to the link to a corporate site that graphically reveals relevant information. It might be a donkey versus an elephant, or a measure of ""greenness.""  We seed the system with public information and allow users to contribute to the database. Data collected can be further explored and mapped out using data visualizations, allowing  perception of network distributions and polarizations.",2020-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/05/boycott_eEnto63.png,357243,,2019-04-18T01:20:40.082Z,"['anderton@media.mit.edu', 'lip@media.mit.edu']",True,boycott,2018-10-01,['civic-technology'],Boycott!,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-10-09T17:41:22.678Z,"To better understand and improve the quality of our lives, there has been a need for measuring non-economic capital such as social capital and natural capital in addition to economic capital. Quantifying non-economic capital, however, is not easy and has not been widespread. In this project, we propose a system where individuals can start measuring their social capital, turning them into a real-world asset that enables the improvement their economic wellbeing, while preserving individual privacy and security.",,['human-dynamics'],,357584,,2019-12-27T01:09:08.610Z,"['nishikat@media.mit.edu', 'sandy@media.mit.edu']",True,social-capital-accounting,2018-06-01,"['social-networks', 'wearable-computing', 'social-science', 'community', 'technology', 'social-change', 'data-science']",Social Capital Accounting,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-10-09T17:52:55.354Z,A functioning society is predicated on an informed and active populace. Our work on broadcast news media analyzes how its unspoken cues incite reflexive responsive rather than reflective thought. We build apps to inspire the latter. We also work on systems that support grassroots organization to facilitate effective civic action. Our projects combine AI with video to parse and mediate presentational style as well challenges or gamelike behavior to inspire and propagate responsible civic action.,,['viral-communications'],,357592,,2020-10-24T06:38:19.342Z,['lip@media.mit.edu'],True,viral-political-action,2018-10-01,[],Theme | Civic Understanding and Participation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-10T12:51:29.095Z,"INSPIRE = Intelligence Never Seeks Perfection, Instead Requires Effort.Stories help us expand our sense of what's possible: by hearing from others like us who have overcome personal and societal obstacles in order to become activists, spoken word poets, and game designers (for example), we become more confident and hopeful in our ability to do the same.  Unfortunately, many children grow up in environments with little exposure to these stories, and subsequently pursue pathways through their education and careers that could have been very different had they received this exposure early on.INSPIRE is a program in partnership with a local middle school that seeks to expose students to stories of triumph and hope that they find personally meaningful.  A key part of the program is a mobile application that delivers short video stories of professionals talking about their life journeys.  Stories are sourced from our content partner Roadtrip Nation and delivered through a chatbot pal (""Jo Jo""), which personalizes video recommendations for students and prompts them to reflect after they watch videos.  The program also includes in-person community building and engagement programs at the middle school to supplement and extend what the mobile application can offer on its own.  Ultimately, the goal of INSPIRE is to help students' enhance their noncognitive and academic outcomes in ways that help them positively and productively shape their own unfolding narratives.",,['social-machines'],,357921,,2019-10-13T15:34:34.903Z,"['ngillani@media.mit.edu', 'dkroy@media.mit.edu', 'belen@media.mit.edu', 'snehapm@media.mit.edu', 'mhughes4@media.mit.edu']",True,pathways,2019-04-01,"['learning-teaching', 'networks', 'machine-learning']",INSPIRE,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-10-10T16:06:13.880Z,"​We have developed a wireless system that leverages the inexpensive RFID tags already on hundreds of billions of products to sense potential food contamination. Our system, called RFIQ (Radio Frequency IQ), aims at democratizing food quality and safety, bringing it to the hands of consumers. ",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2018/11/14/food-safety_Fw1dRRM.jpg,357932,,2019-02-13T16:42:33.755Z,"['yunfeima@media.mit.edu', 'jleng@media.mit.edu', 'unsoo@media.mit.edu', 'fadel@media.mit.edu']",True,learning-food-quality-and-safety-using-wireless-stickers,2018-09-01,"['artificial-intelligence', 'food', 'health', 'systems', 'sensors', 'networking', 'machine-learning', 'technology', 'rfid', 'internet-things', 'wellbeing']",RFIQ: Food quality and safety detection using wireless stickers,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-10T20:51:16.986Z,"In the 1980's media inventors were separated from creative users.  The same is true today with machine learning:  a cadre of inventors create tools and models for ""users"" who don't know how the system works.   We make learning systems that let you become the modeler, that explore the hidden innards of neural networks, that can play creative duets, and that inspire new ideas and ways to explore visual and conceptual material.  We want to put AI in the loop of human interaction to make better people.",,['viral-communications'],,357945,,2022-04-09T13:29:20.498Z,[],True,AI-Loop,2018-10-01,[],Theme | AI in the loop,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-10-10T20:58:12.746Z,"Here we include a suite of projects that migrate information to personal devices and systems instead of centralizes silos. In part this take you personal device seriously—it is more than a window into the cloud—it can store your information for your purposes.Blockchains have spawned new ways of looking at security, trust, and consensus. These are now design variables that allow diverse communities to develop networks with permanence  and agreement that have no central authority. We explore trust as a variable, building blockchain-based systems that separate transactions and currency from the utility of a shared, uneditable ledger.",,['viral-communications'],,357953,,2020-10-20T19:58:18.737Z,['lip@media.mit.edu'],True,decentralized-systems,2018-10-01,[],Theme | Decentralized Systems,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-10-11T01:29:29.171Z,"Participants in cryptocurrency markets are in constant communication with each other about the latest coins and news releases. Do these conversations build hype through the contagiousness of excitement, help the community process information, or play some other role? Using a novel dataset from a major cryptocurrency forum, we conduct an exploratory study of the characteristics of online discussion around cryptocurrencies. We find that coins with more information available and higher levels of technical innovation are associated with higher quality discussion. People who talk about serious coins tend to participate in discussion displaying signatures of collective intelligence and information processing, while people who talk about less serious coins tend to display signatures of hype and naïvety. Interviews with experienced forum members also confirm these quantitative findings. These results highlight the varied roles of discussion in the cryptocurrency ecosystem and suggest that discussion of serious coins may be oriented towards earnest, perhaps more accurate, attempts at discovering which coins are likely to succeed. ",,['human-dynamics'],,358283,,2019-04-19T14:41:02.807Z,"['sandy@media.mit.edu', 'emoro@media.mit.edu', 'eaman@media.mit.edu', 'suhara@media.mit.edu', 'pkrafft@media.mit.edu']",True,collective-sensemaking-in-cryptocurrency-community,2016-06-01,[],Collective sensemaking in cryptocurrency community,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1
False,2018-10-11T01:47:30.490Z,"Mobile phones are one of the fastest growing technologies in the developing world with global penetration rates reaching 90%. Mobile phone data, also called CDR, are generated every time phones are used and recorded by carriers at scale. CDR have generated groundbreaking insights in public health, official statistics, and logistics. However, the fact that most phones in developing countries are prepaid means that the data lacks key information about the user, including gender and other demographic variables. This precludes numerous uses of this data in social science and development economic research. It furthermore severely prevents the development of humanitarian applications such as the use of mobile phone data to target aid towards the most vulnerable groups during crisis. We developed a framework to extract more than 1,400 features from standard mobile phone data and used them to predict useful individual characteristics and group estimates. We here present a systematic cross-country study of the applicability of machine learning for dataset augmentation at low cost. We validate our framework by showing how it can be used to reliably predict gender and other information for more than half a million people in two countries. We show how standard machine learning algorithms trained on only 10,000 users are sufficient to predict individual’s gender with an accuracy ranging from 74.3 to 88.4% in a developed country and from 74.5 to 79.7% in a developing country using only metadata. This is significantly higher than previous approaches and, once calibrated, gives highly accurate estimates of gender balance in groups. Performance suffers only marginally if we reduce the training size to 5,000, but significantly decreases in a smaller training set. We finally show that our indicators capture a large range of behavioral traits using factor analysis and that the framework can be used to predict other indicators of vulnerability such as age or socio-economic status. Mobile phone data has a great potential for good and our framework allows this data to be augmented with vulnerability and other information at a fraction of the cost.",,['human-dynamics'],,358292,,2018-10-19T20:56:21.936Z,"['sandy@media.mit.edu', 'eaman@media.mit.edu', 'yva@media.mit.edu']",True,improving-official-statistics-in-emerging-markets-using-machine-learning-and-mobile-phone-data,2016-06-01,[],Improving official statistics in emerging markets using machine learning and mobile phone data,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-11T14:10:59.543Z,"MoCho (short for ""Mobility Choices"") is a CityScope module focused on mobility choices and societal impacts. This tool helps predict the choices of mobility modes made at the individual level throughout the entire Boston Metro area.Check out a live demo of MoCho predictions here. ",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/10/30/bg.gif,358301,,2019-08-12T16:14:26.739Z,"['yasushis@media.mit.edu', 'noyman@media.mit.edu', 'doorleyr@media.mit.edu']",True,mobcho,2018-06-01,"['architecture', 'transportation', 'urban-planning', 'machine-learning', 'data-visualization', 'public-health']",MoCho: Mobility choices and societal impacts,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-10-11T21:06:02.699Z,"We  build technologies that interface with the sleeping mind. We track different sleep-stages using brain activity, muscle tension, heart rate, and movement data and issue sleep based interventions using scent, audio, and electrical stimulation to affect sleep onset, sleep stages, sleep quality, and the content of dreams. ",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2018/12/10/MTT.jpg.1400x1400.jpg,358393,,2023-09-07T23:26:26.054Z,"['pattie@media.mit.edu', 'amores@media.mit.edu', 'adamjhh@media.mit.edu', 'gbernal@media.mit.edu', 'rosello@media.mit.edu', 'abyjain@media.mit.edu', 'tomasero@media.mit.edu']",True,theme-engineering-dreams,2018-09-01,"['design', 'wearable-computing', 'interfaces', 'technology', 'engineering', 'wellbeing']",Theme | Sleep + Dreams,PUBLIC,http://fluid.media.mit.edu,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-10-11T21:12:36.572Z,"We develop systems that support  health and wellbeing, with a special focus on mental health, using technologies  such as wearables, brain computer interfaces, and artificial intelligence.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/09/28/HeartBit.jpg,358401,,2023-09-07T23:37:16.157Z,"['avujic@media.mit.edu', 'abyjain@media.mit.edu', 'camorris@media.mit.edu', 'yzihan@media.mit.edu', 'patpat@media.mit.edu', 'nathanww@media.mit.edu']",True,theme-health-wellbeing,2018-09-01,[],Theme | Health + Mental Health,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-10-11T21:15:08.008Z,"One of the functions we are interested in supporting people with is decision making. We develop systems for supporting behavior change and habit formation, rational decision making and seamless interaction with wearable devices. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/uuid/d931d2c5-1568-4be4-be1b-c42ccc690bc7,358409,,2022-09-29T15:11:21.897Z,['pattie@media.mit.edu'],True,supporting-decision-making,2018-09-01,[],Theme | Reasoning & Decision Making,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-10-11T21:22:53.151Z,"Multiple projects related to the theme of how digital systems may support attention, memory, and optimal performance.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2022/10/25/324A6905_sm_ebyE9nJ.jpg,358417,,2023-10-23T15:35:33.408Z,['pattie@media.mit.edu'],True,theme-attention-mindfulness-and-health,2018-09-01,['human-computer-interaction'],"Theme | Attention, Memory + Optimal Performance",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-10-11T21:28:30.074Z,Our research group designs various hardware and software platforms that can be used to do scientific experiments or implement a particular application for cognitive support  in  naturalistic setting. Some of these platforms are made available to external collaborators.,,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/12/10/IMG_20180428_130707-2.jpg.1400x1400.jpg,358425,,2023-03-22T18:38:42.569Z,['pattie@media.mit.edu'],True,theme-platforms-for-sensing-and-interventions,2018-09-01,[],Theme | Platforms,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-10-11T21:32:08.360Z,"Several of our projects aim to support learning and creativity. While some systems focus on a specific skill to be strengthened, e.g., critical thinking or confident public speaking, others look at methods for improving motivation for learning, improving recall, and more.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/c8c62931-353b-4d63-8613-5a3595fdd70e,358433,,2023-09-07T23:11:25.427Z,['pattie@media.mit.edu'],True,theme-immersive-and-wearable-learning,2018-09-01,"['human-computer-interaction', 'artificial-intelligence', 'augmented-reality', 'cognition', 'learning-teaching', 'perception', 'creativity', 'cognitive-science', 'human-augmentation']",Theme |  Learning + Creativity,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-10-12T04:27:44.412Z,"Following the 2016 election, the entirety of the nation became conscious of its polarization. According to a study by the National Bureau of Economic Research*, polarization has increased among Americans since 1990. The study observes, however, that in eight of the nine measures of polarization, older individuals (70+ age group) show higher rates of increase in polarization than other age groups. This age group also utilizes social media less than other age-groups. Could it be that social media is not the root cause of polarization?In order to explore this further, we looked at polarization through talk radio, which is commonly thought to have political influence.",2018-12-31,['viral-communications'],https://dam-prod2.media.mit.edu/x/2018/10/12/conservative_listenership_AuFXRNM.png,358748,,2019-04-11T15:26:19.908Z,"['anderton@media.mit.edu', 'hbedri@media.mit.edu', 'kalli@media.mit.edu', 'lip@media.mit.edu']",True,radio-days,2018-10-12,"['civic-media', 'mapping', 'data-visualization']",Radio Days,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2018-10-12T14:01:45.261Z,"BeeMe a massive immersive social game that directly draws inspiration from popular culture, literature, performing arts, gaming, and YouTube streaming culture.On Halloween night (10/31/2018) at 11pm ET, an actor will give up their free will and let Internet users control their every action. The event will follow the story of an evil AI by the name of Zookd, who has accidentally been released online. Internet users will have to coordinate at scale and collectively help the actor (also a character in the story) to defeat Zookd. If they fail, the consequences could be disastrous.To play, simply connect to beeme.online on the night of Halloween at 11pm ET. We believe BeeMe will provide insight into ways we can fundamentally change the nature of online interactions and entertainment.",2018-11-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2018/10/30/BeeMe-2L.gif,358756,,2018-10-31T13:08:43.651Z,['niccolop@media.mit.edu'],True,beeme,2018-10-01,"['artificial-intelligence', 'gaming']",BeeMe,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2018-10-12T19:52:50.322Z,"Ubiquitous computing has been focusing on creating smart agents that are submerged into everyday environments, however, recent development on physical computing is demanding a shift from calm computing to a physically engaging form. Computing is no more limited to increasing our comfort through passive and pervasive deployment, they can now be created as being more actively and physically intermeshed into our tasks. We present L’evolved, autonomous ubiquitous utilities that assist in user tasks through active physical participation. They not only dynamically adapt to individual user needs and actions, but also work in close tandem with the users. Among explorations on potential applications, we harness drone technology to realize the design and implementation of example utilities that afford free motions and computational controls. Through various use scenarios of those exemplary utilities, we show how this new form of smart agents promises new ways of interacting with our physical environments. We also discuss design implications and technical details of our implementations.",,[],,358789,,2018-10-12T19:55:34.922Z,"['sangwon@media.mit.edu', 'harshit@media.mit.edu']",False,l-evolved,2015-09-11,[],L'evolved,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-16T04:50:41.383Z,"Leveraging sneaker culture to influence civic engagement. Marginalized groups have influence on digital platforms but are often unheard in other forums. We show how culturally resonant physical artifacts extend community reach and impact.  We leverage the sneaker, a growing political symbol and cultural influence, to impact community participation.",2019-09-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/17/KC.png,360088,,2020-06-13T18:28:57.648Z,['britneyj@media.mit.edu'],True,footwear-in-action,2018-10-11,"['fashion', 'health', 'social-media', 'social-science', 'technology', 'civic-action']",Kicks x Cliques,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-10-16T15:31:58.605Z,"Looking Sideways is an online inspiration browsing tool that seeks to provoke unexpected inspiration and guide pathways to new ideas by presenting a diverse range of content for each search query and suggesting connections for the concepts discovered.  At its core, Looking Sideways is a search engine. Yet, unlike other search engines it does not attempt to optimise the content presented to the user. For every search query, Looking Sideways collects content from a diverse range of online databases—Wikipedia, Wiktionary, Harvard Art Museum, Cooper Hewitt Design Museum, Giphy animated images, news, National Geographic, and New Scientist—and presents users with a random selection of that image or textual content. The Looking Sideways tool aims to make searching for inspirational content online feel like the lateral thinking experienced when jumping between ‘creative watering holes’ or juxtaposing content on a mood board, rather than the homogenous rabbit holes that today's search engines and image collections can feel like.If you are interested in learning more about this tool or would like to try it yourself or in your organization, Pip is continuing to develop her PhD research outside of the MIT Media Lab and would love to hear from you: https://lookingsideways.design/",2019-10-30,['object-based-media'],https://dam-prod2.media.mit.edu/x/2018/10/16/Screen Shot 2018-08-20 at 18.05.58_w6cFhhb.png,360113,,2020-05-27T23:12:40.489Z,['pip@media.mit.edu'],True,looking-sideways,2018-04-16,['design'],Looking Sideways,PUBLIC,http://www.pipmothersill.com/,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-16T16:04:16.934Z,"Design Daydreams is a digitally-augmented ideation table and post-it note that seamlessly connects the physical and digital content that designers use in their creative processes.The Design Daydreams augmented drafting table projects the Looking Sideways exploration tool onto a tabletop to allow a more tangible interaction with the information being explored. Combined with a low-tech augmented reality tool that uses any mobile device in a simple holder to project digital animations on top of objects around the user, the tool allows digital and physical concepts to be overlaid on top of each other to provoke new reinterpretations and creative inspiration. If you are interested in learning more about this tool or would like to try it yourself or in your organization, Pip is continuing to develop her PhD research outside of the MIT Media Lab and would love to hear from you: https://lookingsideways.design/",2019-10-30,['object-based-media'],https://dam-prod2.media.mit.edu/x/2018/10/16/IMG_9902.JPG,360121,,2020-05-27T23:56:06.287Z,['pip@media.mit.edu'],True,design-daydreams,2017-10-27,['design'],Design Daydreams,PUBLIC,http://www.pipmothersill.com/,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-16T17:09:13.108Z,"Seasons Change Together is a collaborative song construction experience for multiple simultaneous participants. It represents a first step towards the creation of a framework, designed for interactive multiplayer musical experiences, that explores the potential for technology-enabled systems to facilitate collaborative creativity through expression, the emotional affordances of musical storytelling, and the spatiotemporal boundaries of co-presence.Participants are presented with multiple interfaces that determine the musical texture, rhythmic patterns, and lyrical content of an interactive song. Individual participants can freely move between and share interfaces as they wish, allowing the experience to accommodate play sessions with variable user counts as well as encouraging participants to actively engage with all aspects of the song's construction through collaborative composition.Drawing inspiration from improvisational practice in addition to game design and interactive storytelling, Seasons Change Together strives to open up new possibilities for experiencing music, narrative, and creativity in a social environment.",2019-07-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/10/16/IMG_7349.JPG,360129,,2022-09-14T20:12:11.586Z,['davidsu@media.mit.edu'],True,seasons-change-together,2018-09-01,"['human-computer-interaction', 'gaming', 'music', 'interactive']",Seasons Change Together,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2018-10-16T19:56:57.788Z,"One of the six research methods used by the Space Enabled research group is creating models of complex systems by drawing on techniques from systems engineering, social science, and earth science. One of the key ways we do this is through the Environment-Vulnerability-Decision-Technology (EVDT) Framework,  a process for developing multi-disciplinary, interactive decision support systems (DSS) for a variety of sustainable development applications. This framework seeks to support the use of Earth Observation and socioeconomic data in a format usable by non-experts, while harnessing cloud computing, machine learning, economic analysis, complex systems modeling, and systems engineering. It is characterized by five basic elements: (1) the use of systems architecture & stakeholder analysis to identify needs and understand the context; (2) collaborative development of the DSS that continues stakeholder engagement past the initial systems architecting; (3) a concept of the sustainable development application as a complex socio-enviro-technical system, typically involving the Environment, Human Vulnerability and Societal Impact, Human Behavior and Decision-Making, and Technology Design; (4) an interactive decision-support system; and (5) A consideration towards modularity and re-use of DSS components in future applications. In particular, the EVDT Framework draws from the fields of systems architecture (and other systems engineering techniques), GIS, collaborative planning, and remote observation, each of which have complementary aspects that can be brought to bear on development challenges, particularly those of relatively small spatial scales (municipalities to metropolitan regions) that tend to be underserved by major international development programs.  Environmental models use physics-based simulations to estimate the behavior of natural features in the atmosphere, water, or vegetation. Human vulnerability and societal impact models estimate how people are impacted by environmental hazards, such as hurricanes, or ecosystems services, such as the benefits of forests, using physics-based simulations or economic regressions. Human decision-making models, which can be decentralized agent-based  or centralized decision-logic, simulate the actions taken by humans in response to environmental features. The technology design model allows humans to explore options for technical systems (such as earth observation satellites) to improve awareness of the state of the environment. While significant benefit has come from addressing each domain individually in existing models, and yet more from considering certain groupings (such as the economic valuations that combine both decision-making and societal impact), capturing all four together in an integrated software model can enable us to overcome important challenges that lie at the intersections of these domains.Over the past few years, the EVDT Framework has been used to develop DSSs for mangrove conservation in Brazil, flood resilience in Indonesia, invasive plant species management in Benin, cranberry bog renovation and wetland restoration in Massachusetts, and COVID-19 response in six major metropolitan areas around the world (for more information on the last of these, see the Vida Decision Support System project page). These projects involved both space and non-space stakeholders including the IPP (Rio de Janeiro’s municipal data agency), GGPEN (the Angolan space agency),  MICITEC (Chile’s science ministry), the Universitas Diponegoro of Indonesia, and the Yurok tribe of modern day California, among others. Through these applications and collaborations, the EVDT framework has developed from a conceptual framing to a concrete process that is shown to be repeatable across geographic contexts. For more examples, see the publications page and this recent thesis.",,"['space-enabled', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2020/06/15/EVDT_Screenshot.png,360146,,2023-09-07T22:31:58.950Z,['jackreid@media.mit.edu'],True,integrated-complex-systems-modeling,2018-09-03,"['agriculture', 'developing-countries', 'ecology', 'environment', 'social-science', 'engineering']",Environment-Vulnerability-Decision-Technology Framework,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-10-16T21:28:55.096Z,"Teaching multiple AI agents to coordinate their behavior represents a challenging task, that can be difficult to achieve without training all agents with a centralised controller, or allowing agents to view each others' reward functions. We present a new approach to multi-agent reinforcement learning (MARL), in which agents are given an incentive for being able to causally influence each others' actions. Causal influence is assessed using counterfactual reasoning. We show that this social influence reward gives rise to more coordinated behavior, better collective outcomes, and even emergent communication. In fact, the influence reward can be learn to train agents to use an explicit communication protocol in a meaningful way, when they cannot learn to do this under normal circumstances. Finally, we show that this reward can be computed by training each agent to model the actions of other agents. An agent can then ""imagine"" counterfactual actions it could have taken, and predict how this would have affected other agents behavior, thus computing its own influence reward. This mechanism allows each agent to be trained independently, representing a significant improvement over prior MARL work. ",2018-10-31,['affective-computing'],,360156,,2019-04-18T15:25:07.087Z,['jaquesn@media.mit.edu'],True,causal-influence-intrinsic-social-motivation-for-multi-agent-reinforcement-learning,2018-05-14,[],Causal Influence Intrinsic Social Motivation for Multi-Agent Reinforcement Learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-10-17T01:18:00.986Z,"The Telemetron is a musical instrument specially designed to be performed in microgravity environments. It is created to explore the poetics of movement in outer space and the relational aspects of an antigravitational performance between human and non-human bodies. Through this line of work, we explore how the creation of culture might evolve as we leave Earth. The Telemetron project proposes a space in space for everybody - a space to share, to create, to listen.During the summer of 2018,  the Telemetron was presented on different occasions:The Telemetron was exhibited at Ars Electronica as part of the exhibition ""A Glitch in the Stars"" curated by the MIT Media Lab Space Exploration Initiative. Nicole L'Huillier designed the exhibition along with Sands Fish and Xin Liu.The Telemetron was featured at Sónar+D, a festival that explores how creativity is changing our present and imagining new futures. Nicole was invited to speak in the ""Making Music in Space"" panel. She also gave a workshop called ""Antigravitational Luthiers"". Also, the Telemetron was part of The Zero Gravity Band Exhibition.We published the paper “Telemetron: a musical instrument for performance in zero gravity” at NIME, and Sands Fish presented it at the international conference on New Interfaces for Musical Expression, describing the technical design of our first Telemetron. Nicole also gave a talk about the Telemetron at the En Orbita Festival in NYC.The Telemetron was created by Nicole L'Huillier and Sands Fish. With the assistance of Thomas Sanchez Lengeling, Sarah Hua, and Matt Carney. It was created on the context of the Space Exploration Initiative first zero gravity research flight. We are currently working on more space instruments, stay tuned.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/10/16/Screen Shot 2018-10-16 at 9.59.41 PM.png,360507,,2019-02-14T19:38:59.554Z,"['sands@media.mit.edu', 'nicolelh@media.mit.edu']",True,the-telemetron-adventures,2018-06-13,[],The Telemetron Adventures,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2018-10-17T01:19:40.670Z,"Bricoleur allows makers of all ages to explore the creative possibilities of video and audio as programmable media on mobile devices. Using hand-drawn gestures and a Scratch Blocks-based interface, makers can quickly create complex interactive stories, animations, and artworks by capturing and programming images and sounds from the world around them.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2018/10/19/llk-sean-bricoleur-beach.png,360515,,2018-10-21T00:33:16.471Z,['hisean@media.mit.edu'],True,bricoleur,2018-06-01,"['human-computer-interaction', 'learning-teaching']",Bricoleur,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-17T02:30:04.150Z,"Hybrid Radio: A parasitic molecular infrastructureThis work opens a dialogue around the possibilities of re-thinking radio communication as an open tool for transmitting and receiving in order to create streams for civic communication, engagement, and expression. This text explores the history of radio, as well as free radio theories around the world, and proposes to re-appropriate the space of the airwaves that has been drastically regulated, privatized and institutionalized. Radio acts as an invisible and mobile architecture, having the characteristic of breaking down boundaries, territories, and walls. Understanding radio as a parasitic system can provide a setting to grow in an organic and molecular way. The objective is to explore the potentials in radio infrastructure, its invisibility and the possible ways of using it to foster expression, and trigger discussions about decentralized communication networks and open streams of coexistence.Hybrid Radio was curated by Nomeda and Gediminas Urbonas for the Swamp Pavilion (Lithuanian Pavilion) at the Venice Architecture Biennale 2018Tardigrade Radio, The Radio for Almost Invisible BeingsThis project is in the form of an installation and a series of community engagement and hybrid-radio building workshops. This project has the objective of deploying an open radio infrastructure and a layered invisible architecture of sound in the city. By combining sonic compositions and narratives from an interspecies perspective, the Tardigrade Radio works as a platform to give voice to different organisms (micro/macro/meta), humans, non-human agents, and matter.This piece proposes to re-appropriate the airwaves space, which has been drastically regulated, privatized and institutionalized. Radio pieces are caught by a “parasitic radio receiver module” built with biolab tools an installed in public spaces. Sonic compositions and narratives are taken from the inside of research laboratories to common spaces, blurring the boundaries between inside and outside, lab and nature, fiction and reality. The first installation consists of 1 “parasitic radio receiver module” and three sonic pieces inspired by the Lab Tardigrade’s story and environment. These compositions are meant to be absorbed in a non-linear narrative dispersed on space. Future iterations consider the installation of many “parasitic radio receiver modules,"" creating a large-scale choreography of radio transmitters and receivers. The piece uses radio as a spatial medium for mobile spaces, transversal structures, and build layered invisible architectures.",2018-11-30,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/10/16/Hybdrid Radio_ Photo credit Nicole L'Huillier_W3ryr2t.jpeg,360523,,2018-10-19T19:28:33.774Z,['nicolelh@media.mit.edu'],True,hybrid-radios,2017-11-01,[],Hybrid Radios,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2018-10-17T02:40:19.185Z,This project is a collaboration between the MIT Media Lab and the San Diego Zoo to design and build interactive sonic enrichment systems for animals in managed care. Our approach is based on the potential of animal-animal and human-animal relationship as an environmental enrichment for the welfare of zoo-housed animals specifically in terms of animal vocal communication.  Enrichment is a way for caregivers to provide animals with the opportunity to express natural behaviors and reduce stereotypic behaviors. ,,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/10/16/IMG_2617.jpg,360531,,2023-10-04T16:57:10.873Z,"['rebklein@media.mit.edu', 'janetb@media.mit.edu', 'gabem@media.mit.edu']",True,sonic-enrichment-at-the-zoo,2018-04-01,"['ecology', 'behavioral-science', 'technology', 'cognitive-science', 'alumni']",Sonic enrichment at the zoo,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2018-10-17T02:52:28.757Z,"Diastrophisms is a sound installation with a modular system that sends images through rhythmic patterns. It is built on a set of debris from the Alto Río building that was destroyed by the 27F earthquake in 2010 in Chile. With  Diastrophisms we were looking for a poetical, critical and political crossing between technology and matter, in order to raise questions about the relationship between human beings and nature, and to consider the construction of memory in a community by questioning the notion of monument, as well as to imagine new forms of communication in times of crisis.Work by: Nicole L’Huillier, Thomas Sanchez Lengeling, and Yasushi SakaiExhibited at Siggraph Art Gallery 2018, curated by Andres Burbano. A paper about this work was published in Leonardo Journal for the special edition of Siggraph 2018 Art Papers and Art Gallery Exhibition. The paper was written by Nicole L’Huillier and Valentina Montero.Diastrophisms was also exhibited as ""Diastrofismos"" at the Media Arts Bienal, Santiago de Chile, 2017, curated by Valentina Montero.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/10/16/Diastrophisms photo credit Nicole L'Huillier_VxJsExd.JPEG,360539,,2019-02-14T19:56:31.323Z,"['thomassl@media.mit.edu', 'yasushis@media.mit.edu', 'nicolelh@media.mit.edu']",True,diastrophisms,2017-10-01,[],Diastrophisms,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1
False,2018-10-17T03:12:29.275Z,"The internet  changed  how we create, distribute, and consume music and media. Modern digital tools for creating music and media provide ""cloud enabled"" features like automatic backups, an asset marketplace, and real-time collaboration. Despite these features, current  tools for creating music are still based on the personal computing paradigms of the 20th century. How will media production change in the 21st century? Internet engineering introduced the concept of cloud-native  applications. What would it mean for end-user experience to be truly cloud-native? This project shows a very early prototype that illustrates some of the possibilities. In this approach, the assets that make up a music project are securely exposed to the Internet, where they can be accessed and manipulated by digital services  and and human collaborators.The long-term goal is to give individual content creators control of their data, and a share of the benefits provided by machine learning analytics. A longer description and technical blueprint can be found in  Turning the Digital Audio Workstation Inside Out.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/10/21/charles-mw-2018-fall.jpg,360555,,2018-10-22T00:44:23.233Z,['holbrow@media.mit.edu'],True,imagining-the-future-of,2018-10-01,['music'],The SciFi Audio Workstation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-10-17T10:24:11.807Z,"In this SIG, lasting 18 months, MIT proposes to design and develop algorithms for novel low-cost & non-invasive mental condition estimation, in order to proceed towards the larger goal of unobtrusive, but medically acceptable preventive healthcare. MIT will continuously monitor TCS support services employees for 30 days using wearable sensors to examine the physiological, psychological and behavioral correlates of wellbeing. In addition, we will present surveys on their mobile phone in the morning and evening about employees’ mood, sleep, interaction with others and wellbeing. The mobile phone will also be used to monitor the activity and the location of the employees when they are not at work. In particular, we propose to explore the following research questions:
Are sleep irregularity
     and short sleep associated with stress, performance and other factors?
Is stress associated with
     factors such as hierarchical job ranking or team dynamics?
Are there apparent
     differences in stress across hierarchical levels?
What are the main
     predictive parameters of stress (e.g. sleep, ranking, sensor data)?
How is wellbeing related
     to stress at work?
Are work habits (work
     outside working hours) related to stress/wellbeing?
Are there any
     communication patterns (e.g., e-mail) and work settings (e.g., open/close
     office, number of meetings) correlated with stress/wellbeing?
How are daily mobility
     patterns related to stress/wellbeing?
Are working hour
     schedules related to employees’ performance, stress and mental health?",,[],,360563,,2019-02-14T19:55:12.974Z,[],True,psychophysiological-correlates-of-wellbeing-in-office-workers,2018-10-01,[],Psychophysiological correlates of wellbeing in office workers,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-10-17T16:42:15.275Z,"Modern cities have to respond to the growing demands of more efficient and sustainable urban development, as well as an increased quality of life. In this context, the cities of the future will need the ability to gain insight about current urban conditions and react dynamically to them. According to this view, ""smart cities"" can be seen as cybernetic urban environments in which different agents (e.g., citizens) and actuators (e.g., robots) exploit the city-wide infrastructure as a medium to operate synergistically. Urban Swarms explores the feasibility of swarm robotics systems in urban environments. By using bio-inspired methods, a swarm of robots is able to handle important urban systems and infrastructures, improving their efficiency and autonomy. A diverse set of simulation experiments were designed and conducted using real-world GIS data. Results show that the proposed combination is able to outperform current approaches. Urban Swarms not only aims to show the efficiency of our proposed solution, but also to give insights about how to design and customize these systems. CityScope Volpe ABM model has been customized to integrate Swarm behavior using the Gama Platform as an open source project. ",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2018/10/22/urbanSwarm2.0_GIF.gif,360592,,2019-03-12T15:37:36.573Z,"['kll@media.mit.edu', 'sandy@media.mit.edu', 'alonsolp@media.mit.edu', 'agrignar@media.mit.edu', 'ecstll@media.mit.edu']",True,urban-swarms,2018-10-01,"['robotics', 'artificial-intelligence', 'data', 'networks', 'security', 'systems', 'urban-planning', 'collective-intelligence', 'blockchain', 'trust', 'computer-science', 'engineering', 'data-science']",Urban Swarms,PUBLIC,http://www.eduardocastello.com,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-10-17T18:41:08.674Z,"We present an interactive shape-changing display—Dancing Membrane of the reSpire-  using the deformation of fabric and airflow control. To explore the new way of rendering an organic and natural experience with the shape-changing display, we have been experimenting with different types of fabric that can give soft textures. In order to create and control the local deformation of fabric, we developed a 6-DOF variable diameter nozzle platform which enables us to control the direction and pressure of airflow. By controlling those, we were able to create variable sizes of fabric deformation and vibration of fabric with the computed simulation results. The computational model we created allowed us to predict the responsive dynamic motion of fabric to the airflow. For the next step, we hope to exhibit it as an interactive art installation—as well as a shape-changing display in general for games, projection mapping with organic textures, and create a library of different types of fabric interacting with airflow for a computational simulation model.",2019-07-01,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/76dd5754-ea77-459f-8dd1-ea5f0029a88d,360608,,2020-09-01T19:44:35.672Z,"['yun_choi@media.mit.edu', 'vsumini@media.mit.edu']",True,dancing-membrane,2018-07-01,"['art', 'interfaces', 'mechanical-engineering', 'mechatronics']",reSpire: Self-awareness and Interpersonal Connectedness through Shape-changing Fabric Display,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-17T20:58:22.237Z,"Overview: The outermost skin of a space-based structure is designed using materials known to protect against the harsh elements of space. Simultaneously, the skin provides a unique opportunity to characterize the environment proximate to a spacecraft and to perform real-time damage detection. Thus, we are developing a space-resilient fabric that simultaneously senses and protects, emulating the dual protective and sensory capabilities of biological skin. Space resilient sensory skins will serve a key role in next generation haptic feedback systems for spacesuits (see SpaceTouch application area), as well as next generation thermal blankets for distributed detection of high velocity debris impact. Support: In-Space Deployment Support: ISS US National Lab, Aegis Aerospace,  Space BDManufacturing Support: JPS Composite Materials Specifics:  For example, Beta Cloth—the outermost layer of the International Space Station—is particularly resilient to atomic oxygen erosion and extended UV radiation exposure. It is also regularly exposed to high velocity debris impact. We draw from recent advances in functional fibers and electronic textiles in order to weave sensors directly into the Teflon-coated fiberglass that comprises Beta Cloth, enabling the skin to detect and characterize impact events. We seek to demonstrate that the well-characterized, protective properties of aerospace-grade woven materials can be preserved even when modified to include sensory functionality. ",2023-08-31,"['responsive-environments', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2021/10/14/MemberWkBkg.jpeg,360618,,2023-09-07T15:41:34.231Z,['cherston@media.mit.edu'],True,SpaceSkin,2018-05-01,"['design', 'architecture', 'consumer-electronics', 'data', 'environment', 'health', 'manufacturing', 'wearable-computing', 'sensors', 'space', 'electrical-engineering', 'materials', 'fabrication', 'wellbeing', 'zero-gravity', 'racial-justice', 'women']",Space Fabrics: resilient electronic textiles for distributed sensing in low Earth orbit,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-10-17T21:24:52.582Z,"The Lifelong Kindergarten group develops technologies and activities that engage people in creative learning. We’re exploring a few directions with our work in Africa:1. Supporting creative learning across contextsInstruction-focused learning and an emphasis on narrow outcomes continue to dominate formal and informal environments. As school systems seek to adapt to changes in technology, we’re exploring ways to support school systems, educators, and informal learning organizations in reconstructing outcomes and pedagogy that better prepare young people for lifelong, creative learning.2. Creative Learning CommunitiesWe’re exploring two kinds of communities in particular: (a) a maker-focused community that connects makers with employers while helping makers document and translate their skills to professional contexts and (b) supporting the development of creative learning communities—connecting individuals and organizations working to support creative learning in formal and informal learning environments. Building on the work of Aprendizagem Criativa no Brasil (Creative Learning in Brazil—a decentralized network of educators, designers, systems leaders, foundations, and companies all involved in or hoping to support creative learning) and other initiatives, we hope to first gather stories of educators across a range of contexts, connect them with one another (including at the 2019 Africa Scratch conference), support co-development of resources, and hopefully engage in movement building. 3. Creative vocational learningVocational, 21st-century skills trainings are in high demand across the world—and especially in areas with high unemployment. Trainings tend to be dominated by traditional instruction and often limited to a particular set of skills. Moreover, while some of these trainings might help a student get a specific job or progress on a particular skill, they don’t prepare students for lifelong, creative learning—and the world that students will be entering.In this environment, we’re exploring how to bring creative learning principles to the design and facilitation of “vocational trainings”—with the aspiration to create a model for more open-ended, playful, passionate, and peer-driven vocational learning that provides a springboard for lifelong learning.  We're piloting this work with the Mekatilili Initiative in Nairobi, Kenya.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2018/11/21/Growth Week Video.JPG,360626,,2022-09-22T01:30:02.699Z,"['muthui@media.mit.edu', 'yusufa@media.mit.edu']",True,creative-learning-africa,2018-10-03,[],Creative Learning Africa,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-10-18T03:10:40.333Z,"Split Learning: Distributed deep learning without sharing raw data Project Page: https://splitlearning.github.io/Abstract: Can a server utilize deep learning models for training or inference without accessing raw data from clients? Split learning naturally allows for various configurations of cooperating entities to train (and infer from) machine learning models without sharing any raw data or detailed information about the model.  Key idea: In the simplest of configurations of split learning, each client (for example, radiology center) trains a partial deep network up to a specific layer known as the cut layer. The outputs at the cut layer are sent to another entity (server/another client) which completes the rest of the training without looking at raw data from any client that holds the raw data. This completes a round of forward propagation without sharing raw data. The gradients are now back propagated again from its last layer until the cut layer in a similar fashion. The gradients at the cut layer (and only these gradients) are sent back to radiology client centers. The rest of back propagation is now completed at the radiology client centers. This process is continued until the distributed split learning network is trained without looking at each others raw data.Split learning’s computational and communication efficiency on clients:Client-side communication costs are significantly reduced as the data to be transmitted is restricted to initial layers of the split learning network (splitNN) prior to the split. The client-side computation costs of learning the weights of the network are also significantly reduced for the same reason. In terms of model performance, the accuracies of Split NN remained competitive to other distributed deep learning methods like federated learning and large batch synchronous SGD with a drastically smaller client side computational burden when training on a larger number of clients as shown below in terms of teraflops of computation and gigabytes of communication when split learning is used to train Resnet and VGG architectures over 100 and 500 clients with CIFAR 10 and CIFAR 100 datasets.Versatile plug-and-play configurations of split learningVersatile configurations of split learning configurations cater to various practical settings of i) multiple entities holding different modalities of patient data, ii) centralized and local health entities collaborating on multiple tasks, iii) learning without sharing labels, iv) multi-task split learning, v) multi-hop split learning and other hybrid possibilities to name a few as shown below and further detailed in our paper here (PDF) News stories about this workTechnology Review: A new AI method can train on medical records without revealing patient dataTechnology Review:  A little-known AI method can train on your health data without threatening your privacyThe Algorithm Newsletter: The privacy-preserving AI technique that will transform healthcareLes Echos: Medical secrecy, artificial intelligence and RGPD: irreconcilable? Not so sure…",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2019/10/11/Screenshot from 2019-10-11 13-50-04.png,360986,,2020-10-23T15:31:29.373Z,"['vepakom@media.mit.edu', 'raskar@media.mit.edu']",True,distributed-learning-and-collaborative-learning-1,2018-08-27,"['data', 'privacy', 'machine-learning', 'technology', 'public-health', 'data-science', 'covid19']",Split Learning: Distributed and collaborative learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-10-18T04:21:00.169Z,"Introducing the newest edition of the gamma musical/medical instruments - the Gamma MOON (Musical Omnisensory Orbital Neuroinstrument). This instrument features a capacitive interface which delivers multisensory gamma stimulation through audio, visual, haptic and tactile feedback. In collaboration with the Aging Brain Alzheimer’s Initiative at MIT, the Gamma MOON pilots a novel treatment form-factor with the goal of device deployment in large-scale clinical trials. Research reveals that gamma instrument interaction can strengthen cognitive function and sensory perception while increasing focus even in neurocognitively healthy individuals. Contact arrangement allows both patients and performers to create high-level musical abstractions as well as follow traditional notational melodies.  Gamma MOON's heightened sensorial engagement recruits increased cognitive entrainment, multimodal expression and creative freedom. ",,[],,360994,,2018-10-18T04:39:29.389Z,[],False,gamma-moon-musical-omnisensory-orbital-neuroinstrument,,[],Gamma MOON: Musical Omnisensory Orbital Neuroinstrument,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-10-18T04:40:33.594Z,"Introducing the newest edition of the gamma musical/medical instruments - the Gamma MOON (Musical Omnisensory Orbital Neuroinstrument). This instrument features a capacitive interface which delivers multisensory gamma stimulation through audio, visual, haptic and tactile feedback. In collaboration with the Aging Brain Alzheimer’s Initiative at MIT, the Gamma MOON pilots a novel treatment form-factor with the goal of device deployment in large-scale clinical trials. Research reveals that gamma instrument interaction can strengthen cognitive function and sensory perception while increasing focus even in neurocognitively healthy individuals. Contact arrangement allows both patients and performers to create high-level musical abstractions as well as follow traditional notational melodies. Gamma MOON's heightened sensorial engagement recruits increased cognitive entrainment, multimodal expression and creative freedom. ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/10/20/Gamma MOON Instrument_4Cw1HBp.jpeg,361002,,2018-10-20T15:22:00.051Z,['arieger@media.mit.edu'],True,the-gamma-moon-musical-omnisensory-orbital-neuroinstrument,2018-09-01,[],Gamma MOON: Musical Omnisensory Orbital Neuroinstrument,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-10-18T22:25:04.143Z,"Wearable Wisdom: An Intelligent Audio-Based System for Mediating Wisdom and AdviceHaving good mentors and role models is important for personal growth. However, they are not always available at the time of need. Some of our personal heroes have passed away leaving only their wisdom through writings and other artifacts. We present Wearable Wisdom, an intelligent, audio-based system for mediating wisdom and advice from mentors and personal heroes to a user. It does so by performing automated semantic analysis on the collected wisdom database and generating a simulated voice of a mentor sharing relevant wisdom and advice with the user. The results show that our platform is statistically superior in delivering relevant, yet abstract wisdom as well as providing more inspiration compared to control. We describe the implementation of the Wearable Wisdom system, report on a user study, and discuss potential applications of wisdom computation for supporting personal growth and motivation.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/03/23/Screen Shot 2020-03-23 at 11.55.06 PM.png,361012,,2020-04-28T19:50:35.004Z,"['patpat@media.mit.edu', 'tomasero@media.mit.edu', 'pattie@media.mit.edu']",True,wearable-wisdom,2018-10-18,"['human-computer-interaction', 'extended-intelligence', 'wearable-computing', 'interfaces', 'natural-language-processing', 'voice', 'human-augmentation']",Wearable Wisdom,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-10-20T18:24:32.855Z,"OverviewPAL is a  wearable platform for personalized, context-aware, and always-present user change.  PAL has multimodal sensors (camera, location, movement, heart rate, and on-device deep learning) to recognize user context context-aware, e.g., people, indoor locations, objects, etc. On-device deep learning minimizes computation time to provide real-time and offline context-aware support, and user data is also more private as raw data is not sent to the cloud. PAL offers personalized support for each user and users can train custom trainable low-shot models for personalized context detection and support. We are currently exploring  PAL for real-world-loop behavior change support using reinforcement learning for closed-loop behavior change interventions. PAL can also be used for real-time cognitive support (e.g., memory augmentation) and visual assistance (e.g., for visually impaired).PAL is made by my family of collaborators. PAL, by family :)",,[],https://dam-prod2.media.mit.edu/x/2018/10/20/PAL_poster_2018_pic2_56Gk4PE.png,361650,,2021-06-08T14:45:36.399Z,"['minakhan@media.mit.edu', 'utkarshs@media.mit.edu', 'glennfer@media.mit.edu', 'akashv@media.mit.edu', 'mmanuja@media.mit.edu']",True,pal,2018-06-01,"['human-computer-interaction', 'artificial-intelligence', 'health', 'sensors', 'machine-learning', 'behavioral-science', 'wellbeing', 'data-science']","PAL: a wearable on-device deep learning platform for personalized, context-aware, and closed-loop real-time support",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2018-10-21T07:26:22.711Z,"Research studies led by Dr. Shah in his laboratory have created new paradigms for using low-cost images captured using simple optical principles for point-of-care clinical diagnosis, and reducing  dependence on specialized medical imaging devices, biological and chemical processes . Recent peer-reviewed publications have also communicated interpretable tools and methods for generative, predictive and classification algorithms for obtaining medical diagnostic information of cells, tissues, and organs.  For example:Generalizability of deep learning models for segmenting complex patterns from images are not well understood and are based on anecdotal assumptions that increasing training data improved performance.  Research findings led by Dr. Shah published in a Cell Reports Methods paper  reported a novel an end-to-end toolkit for improving generalizability and transparency of clinical-grade DL architectures. Researchers and clinicians can  use this toolkit to identify hidden patterns embedded in images and overcome under specification of key non-disease and clinical labels causal to decreasing false-positive or negative outcomes in high-dimensional learning systems. The key findings from this study focussed on the evaluation of medical images, but these methods and approach should generalize to all other RGB and gray scale natural-world image segmentations. Methods for benchmarking, visualization, and validation of deep learning models and images communicated in this study have wide applications in biomedical research and uncertainty estimations for regulatory science purposes. (Project and publication link)In a collaboration led by Dr. Shah with Brigham and Women’s Hospital in Boston, MA,  a novel “Computational staining” system to digitally stain photographs of unstained tissue biopsies with Haematoxylin and Eosin (H&E) dyes to diagnose cancer was published. This research also described an automated “Computational destaining” algorithm that can remove dyes and stains from photographs of previously stained tissues, allowing reuse of patient samples. This method used neural networks to help physicians provide timely information about the anatomy and structure of the organ and saving time and precious biopsy samples. (Project and publication link) In a collaboration led by Dr. Shah with Stanford University School of Medicine and Harvard Medical School,  several novel mechanistic insights and methods to facilitate benchmarking and clinical and regulatory evaluations of generative neural networks and computationally H&E stained images were reported. Specifically, high fidelity, explainable, and automated computational staining and destaining algorithms to learn mappings between pixels of nonstained cellular organelles and their stained counterparts were trained.  A novel  and robust loss function was devised for the  deep  learning algorithms to preserve tissue structure. This research communicated that virtual staining neural network models developed in Dr. Shah's research lab were generalizable to accurately stain previously unseen images acquired from patients and tumor grades not part of training data. Neural activation maps in response to various tumors and tissue types were generated to provide the first instance of explainability and mechanisms used by  deep learning models for virtual H&E staining and destaining.  And image processing  analytics and statistical testing were used  to benchmark the quality of generated images.  Finally,  the computationally stained images were successfully evaluated for prostate tumor diagnoses with multiple pathologists  for clinical decision-making.  (Project and publication link) In a research study led by Dr. Shah, a complementary end-to-end deep learning framework for automatic classification, and localization of prostate tumors from non-stained and virtual H&E stained core biopsy images was  developed. A computationally H&E stained patch was first generated from a non-stained input image using the generative models described above and then was fed into a Resnet-18 classifier for classification as tumor or no tumors. A deep weekly-supervised learning  gradient backpropogation (GBP) algorithm was used to localize class-specific (tumor) regions on images outputted from the Resnet-18 classifier. If an input image patch was classified as tumor, the GBP localization module generates a saliency map) locating the tumor regions on computationally stained images. The core contributions were to extend the utility and performance of generative virtual H&E staining deep learning methods,  models and computationally H&E stained images for tumor localization and classification. (Publication link)In a collaboration led by Dr. Shah with Beth Israel Deaconess Medical Center in Boston, MA, the use of dark field imaging of capillary bed under the tongue of consenting patients in emergency rooms for diagnosing sepsis (a blood borne bacterial infection) was investigated. A neural network capable of distinguishing between images from non-septic and septic patients with more than 90% accuracy was reported for the first time. This approach can rapidly stratify patients and offer rational use of antibiotics and reduce disease burden in hospital emergency rooms and combat antimicrobial resistance. (Project and publication link)Dr. Shah led research studies that showed that signatures associated with fluorescent porphyrin biomarkers (linked with tumors and periodontal diseases) were successfully predicted  from standard white-light photographs of the mouth, thus reducing the need for fluorescent imaging at the point-of-care. (Project  and publication link) Research studies led by Dr. Shah reported automated segmentation of oral diseases by neural networks from standard  white-light photographs and correlations  of disease pixels with systemic health conditions such as optic nerve abnormalities in patients for personalized risk scores. (Project and publication link, Project and publication link)Examples described in this research area highlight contributions from Dr. Shah and his lab  towards designing next-generation of computational medicine algorithms and biomedical  processes that can assist physicians and patients at the point-of-care.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/x/2018/10/22/mit-research-1.jpg,361964,,2022-10-14T19:36:07.757Z,['pratiks@media.mit.edu'],True,research-area-medical-imaging-technologies-using-unorthodox-artificial-intelligence-for-early-disease-diagnoses,,"['computer-vision', 'artificial-intelligence', 'health', 'human-machine-interaction', 'machine-learning', 'healthcare', 'public-health', 'engineering', 'medicine', 'clinical-trials', 'biomedical-imaging']","Research Area | Application, Interpretability and Clinical Translation of Deep Learning Algorithms for Medical Images",PUBLIC,https://www.pratiks.info/,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-10-21T07:37:49.411Z,"Research studies led by Dr. Shah, in collaboration with the United States Food and Drug Administration (FDA),  have developed explainable AI and machine learning systems that learn from diverse and inclusive datasets for ethical clinical use and testing of experimental medicines. Real World Data (RWD) and Real World Evidence (RWE) are playing an increasing role in healthcare decisions to support innovative use of Electronic Health Records and other digital sources to benefit from experimental and existing treatments often tested on smaller cohorts of patients  (Project and publication link). For example: phase 3 clinical trials evaluating new therapies and vaccines are among the most complex experiments performed in medicine on small numbers of eligible patients. Around 50% of phase 3 trials fail and in some cases lead to adverse events. This high failure rate also counteracts patients consenting to treatments by experimental therapies as the last resort discussed in a review co-authored by Dr. Shah in a Cell press journal (Publication link) . The  FDA states that another significant challenge is the difficulty of predicting clinical results in a wider patient base in the real world vs. phase 3 randomized trials. Example of research projects, collaborations and resulting peer-reviewed publications  led by Dr. Shah are listed. In a research study led by Dr. Shah novel and non-trivial reward functions for self-learning Reinforcement Learning (RL) algorithms for dose de-escalation studies during clinical trials to alleviate chemotherapy toxicity have been developed . These algorithms learn reward contribution from physician actions and patient states/health without future survival/outcomes information to solve fundamental problems in clinical development of medicines (Project and publication link). Dr. Shah is the lead Principal Investigator on the Memorandum of Understanding (MOU) 'Health 0.0' between MIT and the United States FDA signed to engender AI and ML research for computational medicine and clinical development. Research activities under this MOU focus on three key themes for development of next-generation medicines by adoption of digital evidence generated by AI and ML: (1) validation and modernizing the clinical trials process, (2) strategies for rational use of AI- and ML-driven learning from real-world data and evidence and, (3) regulatory framework to improve health outcomes for patients and oversight for integration, explanation, and de-risking of AI/ML digital analytics in medical care for patients. Key summary of this MOU was published as a perspective in Nature Digital Medicine (Publication link).In collaboration with regulatory agencies and clinical partners,  a regulatory path for AI and ML software as a medical device and digital medicines developed  by Dr. Shah and his  laboratory has been initiated (Project and publication link,  Presentation link). This research classifies, predicts and enriches novel digital endpoints to benefit patient health, eliminate adverse events, and improve outcomes. This work has significant impact on the ethical decisions facing patients and their families, and regulatory decisions for US FDA and European Medical Agency (Project and publication link). Dr. Shah, with collaborators from Harvard Medical School and biostatisticians, are investigating the use of observational RWD and RWE to supplement randomized data to train neural networks cognizant of causal inference.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/01e5c28b-7440-4cc2-ab46-c4b6b1770420,361972,,2022-10-14T19:37:13.020Z,['pratiks@media.mit.edu'],True,research-area-novel-ethical-and-explainable-AI-and-digital-medicines,,"['computer-vision', 'artificial-intelligence', 'health', 'human-machine-interaction', 'synthetic-biology', 'machine-learning', 'computer-science', 'medicine', 'clinical-trials', 'clinical-science', 'data-science']",Research Area | Emergent Neural Networks to Redesign Clinical Testing of Medicines and Learning from Real World Evidence,PUBLIC,https://www.pratiks.info/,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-21T07:46:07.551Z,"Dr. Shah and his research lab at MIT has significant expertise and motivation to bring problems from the clinic and society into the laboratory to develop novel bioengineering solutions and perform real world prospective clinical evaluations. For example, in a study led by Dr. Shah, in collaboration with Harvard School of Public Health, published in the British Medical Journal evaluated the significance and efficacy of real world data generated from the United States Food and Drug Administration approved advanced technology-enabled, non-invasive diagnostic screening (TES) smartphones and other point-of-care medical imaging devices vs. conventional vital signs examinations (Project and publication link). This study led to significant insights regarding strategies for developing TES technologies at MIT that are ready for deployment and designed for effective and scalable primary care and real world evidence generation.Dr. Shah is a Co-Principal Investigator in a collaboration with MIT Mechanical Engineering for  developing an affinity ligand-based whole-cell capture, concentration, and culture for sensitive pathogen detection food safety tests. This microfluidic microbiological assay is suitable for implementation in an integrated bench top device for use by moderately-trained technicians directly at production sites in a format that preserves cells for further downstream testing (Project link).In collaboration with MIT Chemical Engineering and other researchers Dr. Shah contributed to developing and publishing biochemistry protocols for a lateral flow diagnostic strip and bio-digital wearable devices to detect biomarkers in human saliva samples. Biomarkers such as matrix metalloproteinases-8 and -9, pH and nitric oxide linked to oral diseases, stress and human physiology have been successfully integrated with our platforms (Project and publication link). A study of a low-cost mask to screen for sleep apnea through physiological monitoring: respiratory activity (airflow and nasal air pressure) and sleep behavior (motion and noise) was published (Project and publication link). Dr. Shah led research projects that reported construction and validation of low-cost, point-of-care, near-infrared imaging devices to diagnose dental caries, cracks, and demineralization without the use of ionizing X-rays  (Project and publication link, Project and publication link, Project  and publication link).Dr. Shah's research  has open-sourced the construction and the algorithm of porphyrin and other imaging device, ,and created a cell phone clip that can be used on a mobile phone camera (Project and publication link).Studies led by Dr. Shah have supported patients and physicians by providing innovative medical technologies using mobile phones, microfluidics and digital devices for generating at home and point-of-care real world data and evidence.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/x/2018/10/21/research-area-point-of-care-medical-technologies-for-real-world-data-and-evidence-generation.png,361980,,2022-10-14T21:20:57.188Z,['pratiks@media.mit.edu'],True,research-area-point-of-care-medical-technologies-for-real-world-data-and-evidence-generation,,"['bioengineering', 'developing-countries', 'sensors', 'healthcare', 'public-health', 'medicine', 'clinical-trials', 'clinical-science', 'data-science', 'biomedical-imaging']",Research Area | Rapid Fabrication and Integration of Medical Devices and Technologies,PUBLIC,https://www.pratiks.info/,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2018-10-22T12:36:08.172Z,"Antarctica is a unique and beautiful continent that is key to the global ecosystem and climate. Research based in Antarctica is helping scientists explore cutting-edge questions in oceanography, physics, climate science, ecology, and more. To most members of the public, however, Antarctica appears to be a cold and inhospitable place with little relevance to daily life. This project asks if combining a citizen science campaign and an immersive museum experience can increase a sense of stewardship to care for Antarctica within people around the world. For this project, the Space Enabled research group designed a concept for an interactive, room-scale multi-sensory presentation providing education regarding the Antarctic.This presentation is designed to be placed in an informal education venue such as an aquarium or science museum. The immersive presentation helps participants understand this dynamic region through an experience that combines photos, video, sound, smell,  and temperature changes. The information draws on multiple types of data from earth science satellites (i.e., ICESAT), airborne science platforms (i.e., IceBridge), and in-situ sensors (i.e., underwater video cameras and photographs). As participants go through the experience, they hear vignettes about specific research areas in Antarctica, including studies on penguin colonies, ice cores, meteorite searches, and ocean food chains. In addition to the physical, interactive presentation, the second aspect of the project involves a companion Citizen Science campaign. Specifically, citizen scientists and students located in countries near the Southern Ocean (including Chile, South Africa, New Zealand, and Australia) are invited to participate in a data collection campaign about their part of the Southern Ocean. A proposed mobile application in both English and Spanish could invite participants to submit photographs of the shoreline on the Southern Ocean. Our team is prototyping a small, low-cost kit that allows citizen scientists to take measurements such as temperature, salinity and pH measurements from coastal areas facing the Southern Ocean. Information from these citizen scientists could be incorporated into the physical presentation in the museum. Meanwhile, the visual aspects of the presentation may be provided online as photo or video files that  could be downloaded to host on their own platforms. ",,['space-enabled'],https://dam-prod2.media.mit.edu/x/2018/10/22/Installation_2.jpg,362311,,2019-04-19T18:42:30.362Z,['lizbethb@media.mit.edu'],True,design-for-a-citizen-science-and-public-engagement-project-celebrating-antarctica-and-the-southern-ocean,2018-03-09,[],Design for a citizen science and public engagement project celebrating Antarctica and the Southern Ocean,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-10-22T19:33:14.362Z,"Wax-based hybrid rocket propellants, including paraffin (common candlewax) and beeswax show promise as high-performing hybrid rocket propellants for chemical propulsion systems. Inherent safety and simplicity advantages and low cost (less than $4/kg) make waxes well-suited for widespread adoption for launch and in-space applications. Their benign nature compared to the toxicity and carcinogenicity which characterize currently-used propellants, such as hydrazine and nitrogen tetroxide, make paraffin an especially strong candidate for new entrants to the propulsion community.The Space Enabled Research Group is focused on the use of paraffin and beeswax for small satellite missions. Specifically, we are investigating the centrifugal casting of waxes into annular geometries on Earth as well as in microgravity. The research group envisions the repurposing of wax thermal insulation at end of life for deorbit maneuvers. However, such a mission would require centrifugal casting of wax in orbit — a task which has never been done before. The microgravity environment is expected to reduce rotation rate demands for uniform casting, and the overall experimental investigation aims to quantify the differences between 1-g and microgravity centrifugal casting.To date, two parabolic aircraft flights have been conducted, with one more upcoming.  Furthermore, three suborbital spaceflights are scheduled for 2021/22 alongside an ISS flight in order to test casting techniques in longer-term microgravity.  Image analysis of centrifugal casting tests have been employed to automate post-processing of solidification.",,"['space-enabled', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2020/06/16/JulietMicrogravity.jpg,362331,,2023-09-07T23:28:44.662Z,['stober@media.mit.edu'],True,candlewax-rockets-a-green-propellant-alternative,2018-04-01,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'environment', 'food', 'health', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry', 'racial-justice', 'women']",Candlewax Rockets: A Green Alternative for In-Space Propulsion,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-10-23T01:35:48.946Z,an anti-racist creative learning guide to engaging non-dominant youth in STEM activities,,[],,362657,,2019-09-01T23:10:40.256Z,[],False,uncovering-hidden-pathways,2017-11-01,[],Uncovering Hidden Pathways,PUBLIC,http://hiddenpathways.media.mit.edu,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-10-23T11:09:08.851Z,"General overview:Sepsis, a life-threatening complication of bacterial infection, leads to millions of worldwide deaths requires significant time and resources to diagnose. This disease is associated with very high mortality rates, making early detection crucial for treatment. Researchers have investigated direct clinical evaluation by using dark field imaging of capillary beds under the tongue of septic and healthy subjects for signatures of microcirculatory dysfunction associated with sepsis. Our published results, in collaboration with Beth Israel Deaconess Medical Center, have shown that machine learning and vision can learn higher-order hierarchical diagnostic and prognostic features for rapid and non-invasive diagnosis of sepsis using these dark field microcirculatory images. A neural network capable of distinguishing between images from non-septic and septic patients with more than 90% accuracy is reported for the first time. This approach can help physicians to rapidly stratify patients, facilitate rational use of antibiotics, and reduce disease burden in hospital emergency rooms.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/ae699990-6cc1-4871-b674-2708ef4cd9b5,362665,,2022-10-14T21:39:40.795Z,['pratiks@media.mit.edu'],True,machine-learning-algorithms-for-classification-of-microcirculation-images-from-septic-and-non-septic-patients-1,2018-08-01,"['computer-vision', 'human-computer-interaction', 'artificial-intelligence', 'extended-intelligence', 'health', 'machine-learning', 'computer-science', 'biology', 'medicine', 'biomedical-imaging']",Helping Emergency Care Physicians Diagnose Sepsis and Bacterial Infections  with Machine Learning and Vision,PUBLIC,http://bit.ly/2nMCs3C,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-10-23T11:18:12.456Z,"General OverviewStaining of tissues sections using chemical and biological dyes has been used for over a century for visualizing various tissue types and morphologic changes associated with contemporary cancer diagnosis. The staining procedure however is labor intensive, needs trained technicians, costly, and often results in loss of irreplaceable specimen and delays diagnoses. In collaboration with Brigham and Women's Hospital (Boston, MA), we  describe a “computational staining” approach to digitally stain photographs of unstained tissue biopsies with Haematoxylin and Eosin (H&E) dyes to diagnose cancer.  Our method uses neural networks to rapidly stain photographs of non-stained tissues, providing physicians timely information about the anatomy and structure of the tissue.  We also report a  ""computational destaining"" algorithm that can remove dyes and stains from photographs of previously stained tissues, allowing reuse of patient samples.  These methods and neural networks assist physicians and patients by novel computational processes at the point-of-care, which can integrate seamlessly into clinical workflows in hospitals all over the world.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/x/2018/10/23/forward_model_inputs.png,362673,,2022-10-14T21:39:18.324Z,['pratiks@media.mit.edu'],True,computational-histological-staining-and-destaining-of-prostate-core-biopsy-rgb-images-with-generative-adversarial-neural-networks-1,2018-05-01,"['artificial-intelligence', 'health', 'pharmaceuticals', 'imaging', 'machine-learning', 'computer-science', 'biology', 'public-health', 'medicine', 'clinical-trials']",AI Methods for Rapid Automated Staining and Destaining of Tissue Biopsies in Hospitals,PUBLIC,http://bit.ly/2nMCs3C,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-10-25T17:33:36.832Z,"What really divides Cambridge?Who can occupy certain spaces in Cambridge? What are the less obvious social and cultural factors at play? When moving to a new neighborhood, one of the first things you might think about is whether you can afford to live there. But for many people, housing is not only a matter of affordability but also a matter of access to community. For others, private housing may not even be an option, so the accessibility of homeless shelters and public indoor spaces become important factors.This project is in the prototype phase. It uses census data and augmented reality (AR) to examine the livability of three areas of Cambridge: Harvard Square, Central Square, and Kendall Square. In doing so, we hope to create an intersectional map that helps us understand housing accessibility.",2018-12-12,['civic-media'],https://dam-prod2.media.mit.edu/uuid/29403ef1-fea8-40de-bbc7-3d844d794610,363333,,2019-04-22T18:08:48.885Z,['mboya@media.mit.edu'],True,ar-neighbor-hood,2018-10-01,[],AR Neighbor/hood,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-10-25T17:35:32.939Z,"dataVisionaRi is an exploration of big data visualizations techniques in VR. The research generates new taxonomy and structures around data visualization in the relatively unexplored space of Virtual Reality. The first demonstration, NodeitAll, is a Media Cloud link map that experiments with various ways to present edges and nodes, playing with interaction and manipulability. Developed in Unity, the scripting is open source so that anyone may input their own CSV files and have a randomly generated network graph. ",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2018/10/25/vr.jpg,363341,,2022-06-10T13:35:30.074Z,['mboya@media.mit.edu'],True,datavisionari,2018-09-01,[],dataVisionaRi,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-10-26T17:35:37.236Z,"The ML Learning Initiative is built around a cohort of Learning Fellows from across the diverse research groups of the Media Lab.Meet our current cohort of 2018-19 Learning Fellows [link]--------------------------The ML Learning Fellows Program began in 2016. Since then, the ML Learning Initiative team has supported three cohorts of Learning Fellows, building a community of graduate student researchers at the Media Lab who are developing new technologies to cultivate creative learning in a variety of contexts. In addition to funding, the ML Learning Initiative team provides the students with additional mentorship, connections, and support for their research.Our Learning Fellows are supported in two ways, through the LEGO Papert Fellowship and the Learning Innovation Fellowship. LEGO Papert Fellowship: The LEGO Foundation, a long-time member of the Media Lab, has endowed this fellowship program to honor the legacy of educational-technology pioneer Seymour Papert, a founding Media Lab faculty member. Each year, the LEGO Papert Fellowship will fund the work of three graduate students who are working at the intersection of creativity, play, learning, and new technologies.Learning Innovation Fellowship: With a focus on creative learning environments and tools, early-childhood learning, life success skills, other learning mindsets, and adult learning, the Learning Innovation Fellowship supports  research that brings opportunities for creative learning to more people.",2019-09-01,"['ml-learning', 'digital-learning-studio']",https://dam-prod2.media.mit.edu/x/2018/10/26/LFs-2018-19-stacked-line-ml-website_3KKE7Ut.png,363665,,2020-12-02T01:11:52.426Z,"['kamcco@media.mit.edu', 'ps1@media.mit.edu', 'ktj@media.mit.edu', 'minakhan@media.mit.edu', 'reeddc@media.mit.edu', 'jaleesat@media.mit.edu', 'ave@media.mit.edu', 'anneli@media.mit.edu', 'jakory@media.mit.edu', 'hchen25@media.mit.edu', 'tarmelop@media.mit.edu', 'jwilbert@media.mit.edu', 'khanning@media.mit.edu', 'smithkim@media.mit.edu', 'akito@media.mit.edu', 'cdvm@media.mit.edu', 'shrutid@media.mit.edu', 'joyab@media.mit.edu', 'sdruga@media.mit.edu']",True,ml-learning-fellows-program,2016-09-01,['learning-teaching'],ML Learning Fellows Program,PUBLIC,https://learn.media.mit.edu,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2018-10-29T16:19:56.355Z,Media manipulation technologies have the power to vanish people from photographs. Yet their souls live on in the deep memory of these algorithms of omission.,2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2018/11/01/angelified_00001705_synthesized_image.jpg,364614,,2020-07-23T20:02:56.353Z,['groh@media.mit.edu'],True,ai_spirits,2018-10-22,[],AI Spirits,PUBLIC,http://spirits.media.mit.edu,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-11-08T15:13:04.100Z,"The City Science Lab@Guadalajara is a collaboration with the University of Guadalajara (UdeG), a network composed of 15 campuses within the state of Jalisco, Mexico. The University offers undergraduate and graduate studies to around 130,000 students. With this collaboration, UdeG and MIT aim to understand urban performance metrics using evidence-based decision making tools at the community scale. The teams hope to enable more equitable, secure, and resilient communities using the components of the City Science process.    Learn more below about City Science Guadalajara projects below:",,['city-science'],https://dam-prod2.media.mit.edu/x/2018/11/26/170704_recobra_esplendor_obra_de_orozco_en_el_paraninfo_ab-ag_1.jpg,367769,,2023-07-26T20:50:43.934Z,"['alonsolp@media.mit.edu', 'kll@media.mit.edu', 'mdchurch@media.mit.edu', 'agrignar@media.mit.edu', 'maitanei@media.mit.edu', 'gbabio@media.mit.edu', 'leticiai@media.mit.edu']",True,city-science-guadalajara,2018-11-01,"['design', 'human-computer-interaction', 'agriculture', 'architecture']",City Science Lab @ Guadalajara,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2018-11-26T17:28:49.385Z,"Data for refugees is a big data challenge whereby Turk Telekom opens a large dataset of anonymized mobile phone records to research groups for the purpose of providing better living conditions to Syrian refugees in Turkey.  We introduce different measures extracted from mobile phone metadata to study the integration of refugees along three dimensions: (1) social integration, (2) spatial integration, and (3) economic integration through signatures of employment activity. We use these measures to compare integration across different regions in Turkey and find striking differences both in the distributions of these dimensions and the relations between them. The paper is currently under review but will be shared soon.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2018/11/26/image-03.jpg,369497,,2019-04-19T14:42:49.278Z,"['singhv@media.mit.edu', 'alfredom@media.mit.edu', 'jobalbar@media.mit.edu', 'yleng@media.mit.edu', 'bakker@media.mit.edu', 'sandy@media.mit.edu']",True,data-for-refugees,2018-06-01,"['social-science', 'civic-action', 'data-science']",Data for Refugees,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-11-30T11:13:09.936Z,"ATENÇÃO: Saiu o resultado do Desafio Aprendizagem Criativa Brasil 2019! Clique aqui para conhecer os fellows e os projetos selecionados!----O Desafio Aprendizagem Criativa Brasil visa fomentar a implementação de soluções inovadoras que ajudem a tornar a educação brasileira mais criativa, prazerosa, relevante, colaborativa e inclusiva para crianças e jovens de todo o país.Organizado pela Rede Brasileira de Aprendizagem Criativa, e contando com o apoio da Fundação Lemann e do MIT Media Lab, o Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de aprendizagem criativa em escolas públicas (do Ensino Fundamental ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.Os representantes dos projetos selecionados ganharão uma Creative Learning Fellowship para ajudar a implementar seu trabalho. As inscrições vão até o dia  13 de janeiro de 2019 e devem ser feitas única e exclusivamente através do formulário abaixo.Clique aqui para a chamada de projetos completa.Clique aqui para o formulário de inscrição.Clique aqui para respostas às perguntas mais frequentes.Atenção:  esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no  fórum da Rede Brasileira de Aprendizagem Criativa.",2019-11-26,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2017/01/11/22531183118_2a2f2332c3_o.jpg,369546,,2020-01-29T20:14:25.932Z,"['mres@media.mit.edu', 'leob@media.mit.edu']",True,desafio-aprendizagem-criativa-brasil-2019,2018-12-03,[],Desafio Aprendizagem Criativa Brasil 2019,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2018-12-12T17:36:49.020Z,"Cocoon is our vision of a programmable dream machine from the future. This device would be enabled by the synthesis of many devices under development at MIT Media Lab’s Fluid Interfaces Group. As a dreamer descends into sleep, Cocoon tracks three sleep-stages using brain activity, muscle tension, heart rate, and movement data that are revealed through its dome.  External stimuli in the form of scent, audio, and muscle stimulation direct the content of the dreams. Crossing boundaries both disciplinary and experiential, Cocoon offers an embodied investigation of one’s own consciousness, a philosophy in the flesh; with it users can observe and engage the torsion of their senses, see and shape dreams which are otherwise entirely uncontrollable, unlinked, and unseen. We hope this speculative vision, and the conversations it inspires, help us reflect on how we develop our existing dream engineering technologies going forward, and how we combine them. Cocoon has been shown at Ars Electronica 2018, the Beijing Media Arts Biennale 2018, and the Han Shan Art Museum.",2019-09-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2018/12/12/Screen+Shot+2018-03-16+at+6.38.19+PM.png,369672,,2023-03-23T18:01:30.253Z,"['amores@media.mit.edu', 'adamjhh@media.mit.edu', 'rosello@media.mit.edu']",True,cocoon,2018-09-01,"['design', 'wearable-computing', 'interfaces', 'technology']",Cocoon: Speculative Dream Engineering,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2018-12-13T04:09:12.049Z,"Electrical stimulation (FES) is the current clinical stimulation modality used to restore function and provide therapy in a variety of clinical applications. However, its clinical implementation is riddled with challenges of fatigue, reverse order recruitment of motor units, and diffuse/non-specific excitation of surrounding tissues. Optogenetics is a recently evolving field, which involves genetically altering cells so that they can be activated with light. Optogenetic techniques have largely been used to probe neural circuits and study the brain's function. Work in our lab has focused on implementing optogenetics as a stimulation modality for peripheral tissues. Under optogenetic stimulation, many of the challenges associated with electrical stimulation are overcome. Most recently, we have utilized optogenetics in a closed-loop system to control a murine hind limb to follow desired movement patterns, mimicking climbing stairs and walking.  In an ideal future, similar techniques may be used to restore functional movement in patients with paralysis or other motor impairments. We demonstrate that our methods outperform traditional electrical stimulation methods by having less fatigue and smoother movement.  This system is the first proof-of-principle for peripheral limb control using closed-loop optogenetics and can perform with greater than 95% accuracy.",2018-12-31,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2018/12/13/shriya-01.jpg,369712,,2019-01-31T15:21:42.923Z,"['shriyas@media.mit.edu', 'bmaimon@media.mit.edu', 'hherr@media.mit.edu']",True,closed-loop-optogenetics-for-peripheral-nerve-control,2017-09-01,"['technology', 'prosthetics', 'bionics']",Closed-Loop Optogenetics for Peripheral Nerve Control,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-12-13T19:11:53.031Z,"Shrinking problems in 3D printing Although a range of materials can now be fabricated using additive manufacturing techniques, these usually involve assembly of a series of stacked layers, which restricts three-dimensional (3D) geometry. Oran et al. developed a method to print a range of materials, including metals and semiconductors, inside a gel scaffold (see the Perspective by Long and Williams). When the hydrogels were dehydrated, they shrunk 10-fold, which pushed the feature sizes down to the nanoscale.Lithographic nanofabrication is often limited to successive fabrication of two-dimensional (2D) layers. We present a strategy for the direct assembly of 3D nanomaterials consisting of metals, semiconductors, and biomolecules arranged in virtually any 3D geometry. We used hydrogels as scaffolds for volumetric deposition of materials at defined points in space. We then optically patterned these scaffolds in three dimensions, attached one or more functional materials, and then shrank and dehydrated them in a controlled way to achieve nanoscale feature sizes in a solid substrate. We demonstrate that our process, Implosion Fabrication (ImpFab), can directly write highly conductive, 3D silver nanostructures within an acrylic scaffold via volumetric silver deposition. Using ImpFab, we achieve resolutions in the tens of nanometers and complex, non–self-supporting 3D geometries of interest for optical metamaterials.",2020-07-01,['synthetic-neurobiology'],https://dam-prod2.media.mit.edu/x/2018/12/13/implosionfab.png,369722,,2020-09-10T17:27:28.065Z,"['esb@media.mit.edu', 'danoran@media.mit.edu', 'tillberg@media.mit.edu', 'chenf@media.mit.edu']",True,implosion-fabrication-1,2018-01-02,"['3d-printing', 'technology']",Implosion Fabrication,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2018-12-13T22:01:14.974Z,"Bauhaus artist Paul Klee considered a line to be  one of the atomic elements of art and architecture. He famously described a line drawing as ""a dot that went for a walk.""  What would a dot on a walk look like in three dimensions? This project uses our custom high-resolution AR rig to find out.  See the very first test footage:",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2018/12/13/Taking a line for a walk (test recording).jpg,369732,,2019-10-15T21:57:06.142Z,['holbrow@media.mit.edu'],True,taking-a-line-on-a-walk,2018-12-02,['augmented-reality'],Taking a line on a walk,PUBLIC,http://CharlesHolbrow.com,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2018-12-18T20:18:37.148Z,"As we know from our ‘maker’ classes and workshops, different capabilities and approaches can yield remarkable projects. Let’s facilitate a ‘smart’ building that is at least as much about the people as the tech.Because the walls and halls of E14 and E15 are subject to building codes and approvals, we decided to create our own walls that can contain and display, conceal or reveal, and in the true MIT innovative spirit, move. As a result, our ArtCube was born. Once we gather the remaining materials to finish construction, we will open for submissions (beginning of 2019).",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2018/12/19/ArtCube-HighKey_3DVl9h4.png,369745,,2022-06-10T13:35:30.086Z,"['lorrie@media.mit.edu', 'csbishop@media.mit.edu']",True,artcube,2018-12-18,"['art', 'interactive', 'community', 'exhibit', 'installation']",Art Cube: the Ambulatory Art Exhibit,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2018-12-24T07:33:12.017Z,"In December of 1968, the first human voyage to the moon catapulted the population of Earth into a new era of space exploration and self-reflection. It was during this voyage that astronauts Bill Anders and Jim Lovell recognized a familiar pale blue dot in the distance and snapped a photo, providing us with the first view of Earth from this distant vantage point. Since its release, this image has been the subject of various works of art and literature. After having seen the Earth from space, some astronauts reported a cognitive shift in awareness about the planet. This shift helped them recognize the fragility of Earth and has inspired feelings of global citizenship. Thanks to the writings of author Frank White, we now call this shift, “The Overview Effect.”The Media Lab’s Space Enabled research group asks if it is possible to create a similar cognitive shift in Earthlings through an experiential installation piece meant to inspire global citizenship as well as universal citizenship. “Earthrise: A 50 Year Contemplation” will celebrate the original Earthrise photo by creating a meditative space of reflection where participants are transported to the surface of Earth’s moon to reflect on themselves, Earth, and the solar system. The viewer will be immersed in the sensory experience that surrounds them. Through artificially creating “The Overview Effect,” and altering our perspective, might we also inspire a more sustainable approach in our exploration of the solar system?Frank White, author of The Overview Effect: Space Exploration and Human Evolution, is launching “The Human Space Program"" through his new book, The Cosma Hypothesis: Implications of the Overview Effect (Emergent Media; February 2019). The goal of the Human Space Program is to create a comprehensive, sustainable, and inclusive plan for exploring and developing the solar system. It is built around White’s “Cosma Hypothesis,” which addresses the question, “What is the purpose of human space exploration?  Why has the evolutionary process brought humanity to the brink of becoming a spacefaring species?” White’s surprising conclusion: Homo sapiens have a very significant role to play in the evolution of the universe (Cosma). Space Enabled appreciates the opportunity to dialogue with Frank about these fundamental questions shaping the moral compass of human exploration beyond earth.By Lizbeth B. De La Torre,  Rachael Petersen, Frank White  and  Danielle Wood",,['space-enabled'],https://dam-prod2.media.mit.edu/x/2019/01/04/EarthRise.png,369808,,2020-11-16T19:58:05.728Z,"['drwood@media.mit.edu', 'lizbethb@media.mit.edu', 'rachaelp@media.mit.edu']",True,earthrise-a-50-year-contemplation,2018-12-23,['space'],Earthrise | A 50 Year Contemplation,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2018-12-27T01:37:26.862Z,"Wearables are being widely researched for monitoring individual's health and wellbeing. Current generation wearable devices sense an individual's physiological data such as heart rate, respiration, electrodermal activity, and EEG,  but lack in sensing their biological counterparts, which drive the majority of an individual's physiological signals. On the other hand, biosensors for detecting biochemical markers are currently limited to one-time use, are non-continuous, and don't provide flexibility in choosing which biomarker they sense. We present ""wearable lab on body,"" a platform for active continuous monitoring of human biomarkers from the biological fluid. Via IEEE Engineering for Biology and Medicine Society (EMBC) - Pataranutaporn et. al., 2019The project is supported by NASA through Translational Research Institute for Space Health (TRISH) and MIT Media Lab Space Exploration Initiative",2020-12-31,"['fluid-interfaces', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/05/10/IMG_1102_Final.jpg,369826,,2023-09-07T15:43:09.663Z,"['patpat@media.mit.edu', 'abyjain@media.mit.edu', 'pratiks@media.mit.edu', 'pattie@media.mit.edu']",True,wearable-lab-on-body,2018-08-30,"['health', 'wearable-computing', 'sensors', 'biotechnology']",Wearable Lab on Body,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2018-12-27T01:39:40.573Z,"Living Bits : Opportunities and Challenges for Integrating Living Microorganisms in Human-Computer InteractionThere are trillions of living biological ""computers"" on, inside, and around the human body: microbes. Microbes have the potential to enhance human-computer interaction (HCI) in entirely new ways. Advances in open-source biotechnology have already enabled designers, artists, and engineers to use microbes in redefining wearables, games, musical instruments, robots, and more. ""Living Bits,"" inspired by Tangible Bits, is an attempt to think beyond the traditional boundaries that exist between biological cells and computers for integrating microorganism in HCI. In this work we: 1) outline and inspire the possibility for integrating organic and regenerative living systems in HCI; 2) explore and characterize human-microbe interactions across contexts and scales; and 3) provide principles for stimulating discussions, presentations, and brainstorms of  microbial interfaces. We aim to make Living Bits accessible to researchers across HCI, synthetic biology, biotechnology, and interaction design to explore the next generation of biological HCI.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/uuid/00364fc6-3d9f-43a0-aea3-06651765fd95,369834,,2020-08-11T06:37:38.798Z,"['patpat@media.mit.edu', 'avujic@media.mit.edu', 'pattie@media.mit.edu', 'dkong@media.mit.edu']",True,living-bits,2018-11-01,"['design', 'human-computer-interaction', 'bioengineering', 'consumer-electronics', 'synthetic-biology', 'interfaces', 'biology', 'biotechnology', 'human-augmentation']",Living Bits,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-01-02T21:29:51.273Z,"We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/01/03/Gabi CityScope.gif,369855,,2022-03-22T15:43:36.563Z,"['aberke@media.mit.edu', 'alonsolp@media.mit.edu', 'gba@media.mit.edu', 'agrignar@media.mit.edu', 'gbabio@media.mit.edu', 'maitanei@media.mit.edu', 'csmuts@media.mit.edu', 'yasushis@media.mit.edu', 'noyman@media.mit.edu', 'ryanz@media.mit.edu', 'kll@media.mit.edu', 'mdchurch@media.mit.edu', 'markuse@media.mit.edu', 'doorleyr@media.mit.edu']",True,cityscope-cooper-hewitt,2018-11-01,[],CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-01-07T17:51:01.625Z,"ABSTRACTThe impending introduction of autonomous vehicles (AVs) has posed regulatory and ethical questions regarding how they should operate. Much of the previous literature on this subject has explored these questions with an underlying model of streets based on the present.This paper takes a different approach by putting forward a future vision for streets where privately owned and operated vehicles are no longer dominant and shared transit is more pervasive. In doing so, this paper expands the current discussions around individual AVs to the system of streets they will occupy.  It views the topology of streets and the rules that govern them, coupled with the vehicles that move through the streets, as an autonomous system, or machine.  This project proposes updates to this autonomous system in order to build a more equitable system for a future where AVs will be ubiquitous.  The paper presents a design of two parts in order to ensure that AVs operate in the public’s best interests:An update to the laws that govern the use of roads, vehicle regulations and safety standards.  A requirement that AV decision making code be open sourced.",2020-09-01,['city-science'],https://dam-prod2.media.mit.edu/x/2019/01/07/shared-av-world.png,369906,,2019-01-08T20:15:32.709Z,"['kll@media.mit.edu', 'aberke@media.mit.edu']",False,a-future-forward-proposal-for-a-system-of-streets-and-autonomous-vehicles,2018-12-01,[],A Future-Forward Proposal for a System of Streets and Autonomous Vehicles,PUBLIC,https://github.com/aberke/moral-machine-simulation,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-01-22T14:51:13.509Z,"How do we raise conscientious consumers and designers of AI?Children today live in the age of artificial intelligence. On average, US children tend to receive their first smartphone at age 10, and by age 12 over half of all children have their own social media account. Additionally, it's estimated that by 2022, there will be 58 million new jobs in the area of artificial intelligence. Thus, it's important that the youth of today are both conscientious consumers and designers of AI. This project seeks to develop an open source curriculum for middle school students on the topic of artificial intelligence. Through a series of lessons and activities, students learn technical concepts—such as how to train a simple classifier—and the ethical implications those technical concepts entail, such as algorithmic bias.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2019/08/05/6H7A2826 (1).JPG,370045,,2020-02-27T16:44:20.781Z,"['cynthiab@media.mit.edu', 'blakeley@media.mit.edu', 'dipaola@media.mit.edu']",True,ai-ethics-for-middle-school,2018-08-01,"['design', 'artificial-intelligence', 'gaming', 'kids', 'learning-teaching', 'machine-learning', 'ethics', 'technology', 'covid19']",AI + Ethics Curriculum for Middle School,PUBLIC,http://blakeleyhoffman.me/,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-01-24T23:21:57.550Z,"Algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, yet scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study; 2) presents new performance metrics from targeted companies IBM, Microsoft, and Megvii(Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018; 3) provides performance results on PPB by non-target companies Amazon and Kairos; and 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within seven months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and  22.50%  for the darker female subgroup, respectively. While algorithmic fairness may be approximated through reductions in subgroup error rates or other performance metrics, algorithmic justice necessitates a transformation in the development, deployment, oversight, and regulation of facial analysis technology. Consequently, the potential for weaponization and abuse of facial analysis technologies cannot be ignored, nor the threats to privacy or breaches of civil liberties diminished even as accuracy disparities decrease. More extensive explorations of policy, corporate practice, and ethical guidelines are thus needed to ensure vulnerable and marginalized populations are protected and not harmed as this technology evolves.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/uuid/9ef57767-4d98-4bdd-8537-87eb415ed64a,370085,,2022-06-10T13:35:30.100Z,['joyab@media.mit.edu'],True,actionable-auditing-coordinated-bias-disclosure-study,2019-01-24,"['computer-vision', 'artificial-intelligence', 'civic-media', 'civic-technology', 'social-science', 'machine-learning', 'ethics', 'computer-science', 'technology', 'civic-action', 'racial-justice']",Actionable Auditing: Coordinated bias disclosure study,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-02-04T15:33:35.455Z,"LinkedOut aims to define and build solutions to facilitate societal reentry for formerly incarcerated individuals.In collaboration with the Office of Returning Citizens (ORC), an office under the City of Boston that facilitates reentry, LinkedOut is working to design a reentry infrastructure to shut the revolving door of incarceration and reincarceration.Our work seeks to break down the structural and societal barriers to successful reentry. At the structural level, we are building technologies to develop richer data biographies of the lives of returning citizens, and in doing so provide policymakers with the robust data required for effective reentry programs. Importantly, our goal is to embed values in technology design to humanize the reentry process. On the societal level, we have begun to expose holes in the moral fabric of society as we draw attention to the dehumanization and stigmatization of returning citizens. These invisible societal norms disempower returning citizens from rebuilding their lives. Thus, our work demands a reimagination of our current justice system—one that rehabilitates rather than retributes, that embraces rather than excludes—to design for a successful reentry journey.Ongoing ProjectPathfinder: How it worksPathfinder is a personalized case management platform that helps case managers design and keep track of returning citizens’ transition back into society. It makes real-time recommendations in the categories of healthcare, housing, employment, education, financial health, and community engagement based on the unique needs of each returning citizen. Beyond tracking the reentry journey of returning citizens, Pathfinder helps service providers identify and track community service gaps in meeting the needs of their clients.In summary, Pathfinder has three key aims:Develop individualized case management plans for returning citizensStreamline work process for service providers to improve efficacy of servicesCapture community-level determinants of reentry for more informed reentry policies and programsCollaboratorsPathfinder is a collaboration between, with, and for returning citizens. We are partnering with Coders Beyond Bars, a non-profit that teaches returning citizens to code; through this partnership, returning citizens will co-design and co-develop Pathfinder with the MIT Media Lab.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2019/02/04/LinkedOut.png,370217,,2022-06-10T13:35:30.112Z,"['ethanz@media.mit.edu', 'rubezc@media.mit.edu', 'mavipasi@media.mit.edu']",True,linkedout,2018-09-17,"['design', 'human-computer-interaction', 'civic-technology', 'government', 'ethics', 'social-change', 'marginalized-communities']",LinkedOut: Codesigning societal reentry with returning citizens,PUBLIC,https://rubezchong.com/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-02-06T14:48:17.884Z,"New work:  Fisheries in the Pacific Islands operate in an opaque market where an auditing tool built on a distributed ledger would be beneficial to the local economy by providing an immutable ""stamp""  that could leverage semi-trusted, third party auditors and vet fisheries for:
labor conditions on-board
types of fish caught (Skipjack, Yellowfin, Bigeye)
sustainability of fishing method (fish aggregating device (FAD), seine net fishing, longline)
",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/02/20/skipjack1.jpg,370250,,2020-06-12T15:01:22.696Z,['oceane@media.mit.edu'],True,provenance-in-pacific-island-fisheries,2019-02-01,[],Distributed ledgers for ocean conservation and fisheries,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-02-07T13:05:50.474Z,"This pedagogy of experiments seeks to accurately model and manufacture a new form of lighting fixtures and framework around architectural lighting. This is an ongoing project, so please be expecting more updates and materials. These new fixtures are made of polyester resin and a particle distribution called a colloid. Unlike traditional light bulb models, rather than having illumination come from individual lighting fixtures that are wired to a common electric source, we are using one lighting source in the form of a laser to illuminate each fixture.By using controlled resin casting alongside numerical simulation of structured colloids, we can program and evaluate different substances held in distribution with a 9 dimensionality lighting model. This allows us to understand the type of lighting output that can be achieved with various materials and shapes, as well as predict the aesthetic properties that these lighting sources may allow us. From various colors to different glittery reflects, this framework allows both technical and artistic framework.",2020-04-01,[],https://dam-prod2.media.mit.edu/x/2019/07/01/IMG_8769.jpg,370272,,2020-10-19T21:18:44.691Z,['nlutz@media.mit.edu'],True,pigmented-lumens,2018-08-01,[],Pigments and Lumens,PUBLIC,https://ninalutz.github.io,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-02-07T13:35:47.138Z,"~~~This project is on indefinite hold due to COVID-19~~~What if we could leverage technology and modern cosmetics for good? To empower people to explore identities? To give anyone, regardless of skill level, the tools to bend, express, and explore various gender identities and traits?The goal of this project is to use mathematics and computation to create a tool that will allow a user to add more feminine, masculine, or androgynous traits to themselves with cosmetics. After interacting with renders and the tool's preferences, the user will receive a list of cosmetics and then follow along with the tool in real time while they apply their makeup.This tool is meant to be accessible, to be free and open source technology, and made for users of all skin tones, shapes, and preferences. To allow anyone to express their identities.This work is ongoing and will have regular updates, beta releases, and user surveys coming soon.",2020-04-01,"['object-based-media', 'future-sketches']",https://dam-prod2.media.mit.edu/x/2019/10/02/IMG_8841.jpg,370280,,2020-10-19T21:18:21.086Z,"['nlutz@media.mit.edu', 'havasi@media.mit.edu', 'novysan@media.mit.edu']",True,cosmetics-empowered-identities,2018-08-01,[],Gender Affirming Makeup,PUBLIC,https://ninalutz.github.io,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-02-11T19:25:55.648Z,"What is FindingPlaces?In reaction to the sudden arrival of tens of thousands of refugees in the city of Hamburg (DE) in 2015, the Lord Mayor requested the CityScienceLab (CSL) at HafenCity University to facilitate a public discussion and decision-making process on locations for refugee accommodation in Hamburg neighborhoods. With highly sensitive socio-political implications, this project demanded a well-designed technological and procedural approach. CSL employed an innovative Human-Computer Interaction tool, CityScope, to facilitate public participation and urban decision-making. A workshop process was also designed to help multiple participants and stakeholders interact effectively. Running from May to July 2016, the FindingPlaces (FP) project enabled about 400 participants to identify 160 locations accepted by Hamburg’s citizens, out of which 44 passed legal confirmation by the authorities. Overall, on a qualitative level, the project facilitated surprisingly constructive and collaborative interaction, raising awareness and a sense of ownership among participants.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/02/11/IMG_2445.jpg,370322,,2019-02-12T19:05:58.708Z,"['kll@media.mit.edu', 'noyman@media.mit.edu']",True,finding-places,2016-01-01,"['human-computer-interaction', 'architecture', 'government', 'urban-planning']",FindingPlaces,PUBLIC,https://findingplaces.hamburg/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-02-12T03:35:55.674Z,"Paper Dreams explores how human creativity can be supported by artificial intelligence.Prior research on AI and creativity has primarily focused on using machine intelligence to learn “creativity” from humans, from transfer learning of artistic styles to generation of original paintings, poems, and music by the machine. However, the application of current machine learning algorithms to the augmentation of human creativity is a relatively unexplored area. By creating a dynamic human-machine back-and-forth and working with representations inside machine learning models, we can offer people new ""smart"" tools for brainstorming and creative expression.Our system in its current form provides users with a canvas where they draw parts of a sketch that the machine tries to recognize and offers to complete. We do this by building a neural network which takes a small number of input variables, called latent variables, and produces the entire sketch as output.In addition, the system also helps users move forward with their creation. To date, Paper Dreams augments the drawing experience in three different ways: by adding textures/colors, suggesting other elements/drawings for the scene, and introducing serendipity. To adjust the level of serendipity, the user has control of a dial that determines how ""predictable"" vs ""unpredictable/out there"" they want these machine additions to be.While the current version of Paper Dreams works with 2D sketches, the same human machine collaborative creativity approach could be applied to other domains such as 3D models and more.Some of the driving questions for this project are:To what extent are these new tools enabling creativity? Can they be used to generate ideas which are truly surprising and new, or are the ideas clichés, based on trivial recombinations of existing ideas?Can such systems be used to develop fundamental new interface primitives? How will those new primitives change and expand the way humans think?",2019-10-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/04/19/storyboard.png,370332,,2023-03-15T20:32:43.366Z,['gbernal@media.mit.edu'],True,paper-dreams,2018-03-01,"['design', 'art', 'artificial-intelligence', 'human-machine-interaction', 'politics', 'social-science', 'machine-learning', 'computer-science', 'technology', 'cognitive-science', 'electrical-engineering']",Paper Dreams,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-02-12T18:07:52.256Z,"We must proactively tackle the economic, social, and societal implications that accompany the widespread deployment of AI technology. In service to this goal, examining the evolution of AI research itself could provide a valuable input into models of AI's impact (e.g., models of the future of work). ",2020-07-01,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2019/02/12/Screenshot from 2019-02-12 13-14-54_QkcFPYm.png,370342,,2020-07-23T18:36:56.545Z,"['mrfrank@media.mit.edu', 'cebrian@media.mit.edu', 'irahwan@media.mit.edu']",True,the-science-of-ai-research,2018-05-01,"['human-computer-interaction', 'artificial-intelligence', 'computer-science', 'technology', 'academia', 'industry']",The Science of AI Research,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-02-14T21:53:02.693Z,"Digital machine knitting is a highly programmable manufacturing process that has been utilized to produce apparel, accessories, and footwear. Our research presents three classes of textile sensors exploiting the resistive, piezoresistive, and capacitive properties of various textile structures enabled by machine knitting with conductive yarn. ",2019-05-01,"['tangible-media', 'responsive-environments', 'synthetic-neurobiology', 'hacking-manufacturing']",https://dam-prod2.media.mit.edu/x/2019/02/14/instrument01s.jpg,370379,,2020-06-09T17:57:39.198Z,"['jifei@media.mit.edu', 'danoran@media.mit.edu', 'ddh@media.mit.edu', 'joep@media.mit.edu', 'ishii@media.mit.edu']",True,sensorknits,2017-09-01,"['design', 'manufacturing', 'wearable-computing']",SensorKnits: Architecting textile sensors with machine knitting,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-02-15T17:35:27.823Z,"Communication technologies, from printing to social media, affect our historical records by changing the way ideas are spread and recorded. Yet, finding statistical evidence of this fact has been challenging. Here we combine a common causal inference technique (instrumental variable estimation) with a dataset on nearly forty thousand biographies from Wikipedia (Pantheon 2.0) to study the effect of the introduction of printing in European cities on Wikipedia’s digital biographical records. By using a city’s distance to Mainz as an instrument for the adoption of the movable type press, we show that European cities that adopted printing earlier were more likely to become the birthplace of a famous scientist or artist during the years following the invention of printing. We bring these findings to recent communication technologies by showing that the number of radios and televisions in a country correlates with the number of globally famous performing artists and sports players born in that country, even after controlling for GDP, population, and including country and year fixed effects. These findings support the hypothesis that the introduction of communication technologies can bias historical records in the direction of the content that is best suited for each technology.",2019-08-31,['collective-learning'],https://dam-prod2.media.mit.edu/x/2019/02/18/raphael-schaller-88040-unsplash.jpg,370399,,2020-08-28T19:02:22.687Z,"['crisjf@media.mit.edu', 'amy_yu@media.mit.edu', 'hidalgo@media.mit.edu']",True,medium-shapes-message,2015-12-01,"['social-science', 'social-change', 'history']",How the medium shapes the message: Printing and the rise of the arts and sciences,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1
False,2019-02-18T20:29:44.156Z,"Facilitating opportunities for non-dominant youth to learn how to use technology as a tool to engage civically and create positive change in their communities is both powerful and revolutionary. The K-8 curriculum is designed to expand young people’s civic imagination, develop their civic agency, and increase their efficacy in their ability to effect change in their communities. Preliminary results have demonstrated that minoritized youth understand computer science as an accessible field of exploration that can be used to develop their conceptions of identity, race, and power. Our goal is to empower youth with the agency to imagine and create their own futures leveraging computer science to accomplish that goal.This curriculum engages youth in producing computational artifacts using block-based programming tools like Scratch and OctoPlay. Opportunities for promoting awareness and advocacy are incorporated throughout the curriculum, based on constructionist and creative learning practices. This work builds off the many, and often unnamed, community organizers and activists in marginalized communities that continue to fight for better lives for all people—emphasizing the importance of civic engagement and creating positive change in our own communities. We apply an anti-racist framework, with the belief that everyone has the right to have access to computer science education and create spaces for themselves that reject customs that often force conformity and assimilation into the white supremacist traditions of computer science.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2019/10/18/Screen Shot 2019-10-18 at 8.51.09 AM.png,370420,,2022-05-26T17:40:53.399Z,"['jaleesat@media.mit.edu', 'csadler@media.mit.edu']",True,purpose-based-creative-computing-with-scratch,2018-09-26,"['learning-teaching', 'computer-science']",Computing for a Purpose,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-02-19T15:08:51.403Z,"Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.",2021-05-21,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2019/04/02/019_Totem.jpg,370428,,2022-03-11T15:35:09.130Z,"['neri@media.mit.edu', 'ssunanda@media.mit.edu', 'bader_ch@media.mit.edu', 'fkraemer@media.mit.edu', 'rssmith@media.mit.edu', 'josephk@media.mit.edu', 'jpcosta@media.mit.edu']",True,biodiversity,2019-02-27,"['design', 'architecture', 'art', 'synthetic-biology', '3d-printing', 'biology', 'materials', 'engineering', 'covid19']",Totems,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-02-19T15:24:27.727Z,"""Distributed Personalization"", ""Humanization of and Algos and Recommendations""Your data can't be used against you.Project 1: SIOProject 2:  Product personalityGet my data - portofolio of products, product personality test https://www.sciencedirect.com/science/article/pii/S0142694X08000859http://brandpersonalityquiz.com/take-the-quiz/And your resultant product personality https://www.myersbriggs.org/my-mbti-personality-type/mbti-basics/home.htm?bhcp=1   Then onto suggestions. Nice easy and fun.",,['viral-communications'],,370436,,2019-04-05T20:23:07.136Z,[],False,decentralized-self,,[],Decentralized Self,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-02-19T17:19:33.134Z,"TurboTrack is a 3D localization system for fine-grained robotic tasks, with unique capability to localize backscatter nodes with sub-centimeter accuracy without any constraints on their locations or mobility. We showed that TurboTrack can work in multiple collaborative applications with robotic arms and nanodrones including indoor tracking, packaging, assembly, and handover.This research is partially funded by the National Science Foundation (NSF) and a Google Faculty Research Award.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/uuid/ae75e436-f3df-4667-920b-5f91cc535049,370461,,2019-04-17T18:08:41.636Z,"['yunfeima@media.mit.edu', 'fadel@media.mit.edu', 'zhluo@media.mit.edu']",True,turbotrack-3d-backscatter-localization-for-fine-grained,2017-06-01,"['robotics', 'environment', 'health', 'sensors', 'computer-science', 'technology', 'rfid', 'engineering', 'internet-things', 'wellbeing', 'ocean']",TurboTrack: 3D backscatter localization for fine-grained robotics,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2019-02-22T22:26:48.592Z,"The Open Water Data project explores data physicalization as a path to community engagement and action on important environmental issues. For our 2018 installation Chemicals in the Creek, we released glowing lanterns representing water quality permit violations from local facilities onto the river as part of a performance of local environmental challenges that informed a community conversation on these issues.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2019/04/22/Rio Asch Phoenix.jpg,370514,,2022-06-10T14:09:49.269Z,['perovich@media.mit.edu'],True,open-water-data,2017-01-01,[],Open Water Data,PUBLIC,http://datalanterns.com/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2019-03-01T15:43:36.169Z,"Segregation is hurting our societies and especially our cities. But economic inequality isn't just limited to neighborhoods. The restaurants, stores, and other places we visit in cities are all unequal in their own way. The Atlas of Inequality  shows the income inequality of people who visit different places in the Boston metro area. It uses aggregated anonymous location data from digital devices to estimate people's incomes and where they spend their time. Using that data, we've made our own place inequality metric to capture how unequal the incomes of visitors to each place are. Economic inequality isn't just limited to neighborhoods; it's part of the places you visit every day.Try it yourself here:The Atlas of InequalityThe Atlas of Inequality is a project from the Human Dynamics group at the MIT Media Lab and the Department of Mathematics at Universidad Carlos III de Madrid.It is part of a broader initiative to understand human behavior in our cities and how large-scale problems like transportation, housing, segregation, or inequality depend in part on the emergent patterns of people’s individual opportunities and choices.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2019/03/01/atlas_of_inequality.png,370612,,2021-08-12T13:20:13.171Z,"['emoro@media.mit.edu', 'sandy@media.mit.edu', 'dcalacci@media.mit.edu', 'xdong@media.mit.edu']",True,the-atlas-of-inequality,2019-03-01,"['design', 'human-computer-interaction', 'civic-technology', 'data', 'economy', 'urban-planning', 'social-science', 'data-visualization', 'industry', 'data-science', 'covid19', 'cities']",The Atlas of Inequality,PUBLIC,https://inequality.media.mit.edu,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1
False,2019-03-02T17:54:22.244Z,"For decades, the International Space Station (ISS) has operated as a bastion of international cooperation and a unique testbed for microgravity research. Beyond enabling insights into human physiology in space, the ISS has served as a microgravity platform for numerous science experiments. In recent years, private industry has also been affiliating with NASA and international partners to offer transportation, logistics management, and payload demands. As the costs of flying projects to the ISS decrease, the barriers limiting non-traditional partners, like emerging space nations and startups, from accessing the ISS as a platform also decrease.  However, the ISS in its current form cannot be sustained forever. As NASA looks towards commercialization of the low Earth orbit (LEO) space and the development of a cislunar station, concrete plans for shifting the public-private relationship of the ISS are unclear. With the consistent need to continue microgravity research—from governments and private industry—understanding the socio-technical and policy issues that affect the marketplace for future microgravity platforms is essential to maintaining an accessible and sustainable space economy. How will the US and other governments design public-private partnerships to pursue economic and social goals in the LEO microgravity ecosystem? What governance structures will influence who is eligible to operate platforms for activities including tourism, research, manufacturing, and outreach? How will international collaboration occur in the future LEO microgravity ecosystem? This project contributes to progress on these questions by offering technology policy insight using methods from Systems Engineering. Through case study research and numerous expert interviews, this project examines the stakeholders, needs, objectives, system functions and forms for the ISS and microgravity research platforms now and in the future. Particular attention is paid towards explaining the market dynamics that affect the administrative and economic barriers to entry for emerging space nations and non-traditional spaceflight participants.",,['space-enabled'],https://dam-prod2.media.mit.edu/x/2019/03/04/ISS_pic.png,370625,,2019-07-22T16:17:33.440Z,"['drwood@media.mit.edu', 'cjoseph1@media.mit.edu']",True,accessibility-of-the-microgravity-research-ecosystem,2018-04-01,"['environment', 'health', 'politics', 'sensors', 'social-science', 'space', 'technology', 'wellbeing', 'zero-gravity', 'racial-justice', 'women']",Accessibility of the microgravity research ecosystem,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0
False,2019-03-03T21:00:14.523Z,"Try Affective Network!Emotional contagion in online social networks has been of great interest over the past years. Previous studies have mainly focused on finding evidence of affection contagion in homophilic atmospheres. However, these studies have overlooked users' awareness of the sentiments they share and consume online. In this work, we present an experiment with Twitter users that aims to help them better understand which emotions they experience on this social network. We introduce Affective Network (Aff-Net), a Google Chrome extension that enables Twitter users to filter and make explicit (through colored visual marks) the emotional content in their news feed.The extension is powered by machine learning algorithms that classify tweets into different sentiment categories: positive posts tend to use happy or surprising language; negative posts tend to use sad, angry, or disgusting language; and posts without strong emotional language are classified as neutral.Affective Network aims to help social media users better understand which emotions they tend to consume on social media, and how these emotions can spread through their social networks. It was built by researchers at the Laboratory for Social Machines and the Affective Computing group at the MIT Media Lab.Note that Affective Network does not necessarily reflect the official position of the MIT Media Lab regarding the benefits and drawbacks of filtering out specific emotional content.Try Affective Network!",,[],https://dam-prod2.media.mit.edu/x/2019/04/11/promotion-aff-net.png,370633,,2020-06-30T01:05:49.400Z,"['belen@media.mit.edu', 'picard@media.mit.edu', 'dkroy@media.mit.edu']",True,affective-network,2018-12-01,[],Affective Network,PUBLIC,https://affectivenetwork.media.mit.edu,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-03-07T18:57:24.792Z,"We are working on communication interfaces for people with speech and language disorders, incorporating technologies like physiological sensing and personalized machine learning. As part of our research, we conducted a survey to better understand how existing devices are used, and what architectures and features might be useful in new devices.De-identified quantitative data can be downloaded and viewed in GUI via the attached file MATLAB_Publish.zip.  Please cite use of this data as:Narain, J., and Maes, P.  ""Understanding AAC Usage and Needs through a Web Survey with Device Users and Families"".  Proceedings of the 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society.  2020. Sources for logo: Spell deaf talk speech, Information Computer Technology Digital Binary, Alternative Handicapped Accessible sign",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/04/09/AAC.png,370700,,2020-08-10T17:43:58.470Z,['jnarain@media.mit.edu'],True,aac-survey,2019-01-13,"['design', 'human-computer-interaction', 'health', 'technology']",User needs for augmentative communication interfaces,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-03-14T18:47:07.111Z,"Programmable Water-Based Biocomposites for Digital Design and Fabrication across ScalesAguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.",2025-07-31,['mediated-matter'],https://dam-prod2.media.mit.edu/uuid/1dec6bd8-37a1-465f-9ef8-623659bcaf3f,370755,,2021-05-24T15:36:15.208Z,"['neri@media.mit.edu', 'limulus@media.mit.edu', 'ssunanda@media.mit.edu', 'jpcosta@media.mit.edu', 'bader_ch@media.mit.edu', 'dlizardo@media.mit.edu', 'dumo@media.mit.edu', 'darweesh@media.mit.edu', 'nah6cz@media.mit.edu', 'asling@media.mit.edu', 'yjtai@media.mit.edu', 'jvanzak@media.mit.edu', 'j_duro@media.mit.edu']",True,aguahoja,2016-01-01,"['design', 'architecture', 'art', 'environment', 'synthetic-biology', '3d-printing', 'biology', 'materials', 'engineering', 'covid19']",Aguahoja,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-03-21T16:09:47.447Z,"Should workers worry about automation and AI?Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.",,['scalable-cooperation'],https://dam-prod2.media.mit.edu/x/2019/03/21/robot.jpg,370832,,2019-03-22T16:27:19.712Z,"['groh@media.mit.edu', 'hyoun@media.mit.edu', 'emoro@media.mit.edu', 'cebrian@media.mit.edu', 'irahwan@media.mit.edu', 'mrfrank@media.mit.edu']",False,towards-understanding-the-impact-of-ai-on-labor,2018-03-01,"['data', 'technology']",Towards Understanding the Impact of AI on Labor,PUBLIC,http://www.media.mit.edu/~mrfrank,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-03-25T20:29:55.579Z,"Interoceptive Technologies: Inducing emotions from the body upInspirationThere’s a feeling that comes tingling down the spine in certain moments, when a favorite moment of a favorite song comes on, or when a turn of phrase in a powerful speech moves us.  This experience resonates with us so much that it elicits an intense bodily reaction, a physical feeling of shivers down your spine and tingling on the nape of your neck and the back of your arms. That feeling is called aesthetic chills, or frisson. Our body and our mind feel this feeling in unison, challenging our models of mind-body dualism, and meaning that we can physically engage with emotional states by actuating the body. Scientific BackgroundFrisson is a physically felt signature of an emotion, a somatic marker. Like nausea and disgust, or a rapid heartbeat and anxiety, this feeling in the body coincides with an emotion in the mind (and thus makes the body-mind distinction much more blurry). The specific somatic marker of frisson is experimentally tied to peak emotional experiences and the most meaningful moments during exposure to different stimuli, such as  songs or speeches or art pieces. And amazingly, aesthetic chills seem to be an almost universal marker of peak emotional experiences across a wide range of cultures and continents. This universality is really rare—usually, expressions of emotion are quite different across cultural contexts—and this means that we can potentially use chills as a way to study emotion in the body in diverse peoples and places. We have a remarkable embodied feeling (a somatic marker) which is tied to this abstract emotion (meaningful moments), as if we could reach out and directly touch 'meaning-making'.The field of embodied cognition, in addition to finding body-based correlates of emotional experience, has also illuminated many links between our physical and psychological experiences. Studies on misattribution of arousal show us we can drive cognition by driving physical sensation, for instance increasing people's heart rate to increase their likelihood of romantic attraction (people feel a fast heart rate and think they must be attracted to someone, reasoning from the body upwards). This points to opportunities, because if we can artificially induce frisson perhaps we can also drive the downstream cognitive effects of frisson: These include pleasure, inspiration, openness to experience, relief in stress, increase in empathy, and experience of meaning .Engineering and ExperimentationThis project unites embodied cognition and on-body device design to ask questions about the origin of emotions and the potential to hack our brains and behavior by hacking the body.  At once transcendent and physiological, the sublime literally cascades across skin. So we built a device meant to trigger frisson. Alongside Félix Schoeller, a scientist who specializes in researching chills at the Paris CRI, we have shown that our device can 1) reliably induce chills in participants at experimenter-chosen moments and 2) can recreate the downstream cognitive effects of chills, including increases in pleasure and empathy. This is a working, tested device, and we've already published results from studies including: Nature Scientific Reports, describing our experiment using the Frisson device to generate synthetic chills and improve pleasure and empathy during viewing of a film.Physics of Life Review outlining our vision for affective and interoceptive neuroscience that uses devices to change affect by intervening on interoception. ACM IMWUT focused on designing a new model for wearable interfaces which centers on interactions beyond conscious control.Frontiers in Psychology writing on possible combinations of sensor-actuator systems to direct social emotion.Our vision is psychophysiology driving thought from the spine upwards. Emotion prosthetics and somatosensory interfaces like the Frisson device offer new possibilities for affective neuroscience inducing human emotions from the bottom-up, modulating their associated feelings and downstream effects through wearable technology.  Surprisingly little research has been devoted to the topic of how technology may be used to enhance or generate emotion from modulating and somatic markers (i.e., somatosensory interfaces). Our results suggest that in the long term, emotion prosthetics and somatosensory interfaces may provide novel, non-invasive tools for intervention on higher cognitive functioning through controlled stimulation of body signals, for instance in mood disorders. Further, these tools let us understand links between our interoception (our sense of our body's internal state) and the interoception-related disorders like addiction, somatic sensation disorders, and anxiety. We're also hopeful that these devices are relevant for opportunities in arts and entertainment, as tools to guide the emotional arc experienced by audiences, where composers could imagine writing a score for the body alongside the ears or filmmakers could enhance empathy for characters by creating shared physical experience between actor and audience.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/03/26/44AF87A8-1F9E-4E5F-AB54-1E0E521F542C.gif,370903,,2021-01-11T18:26:00.491Z,"['abyjain@media.mit.edu', 'tomasero@media.mit.edu', 'adamjhh@media.mit.edu', 'felixsch@media.mit.edu']",True,frisson,2019-01-01,"['design', 'human-computer-interaction', 'cognition', 'consumer-electronics', 'health', 'wearable-computing', 'interfaces']",Frisson,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-03-26T17:29:55.484Z,"We introduce SociaBowl, a dynamic table centerpiece to promote positive social dynamics in 2-way cooperative conversations. A centerpiece such as a bowl of food, a decorative flower arrangement, or a container of writing tools, is commonly placed on a table around which people have conversations. We explore the design space for an augmented table and centerpiece to influence how people may interact with one another. We present an initial functional prototype to explore different choices in materiality of feedback, interaction styles, and animation and motion patterns. These aspects are discussed with respect to how it may impact people’s awareness of their turn taking dynamics as well as provide an additional channel for expression. Potential enhancements for future iterations in its design are then outlined based on these findings.",2019-01-07,['tangible-media'],https://dam-prod2.media.mit.edu/x/2019/03/26/SB-apps-spill.jpg,370926,,2020-01-15T17:48:09.435Z,"['joaleong@media.mit.edu', 'ishii@media.mit.edu']",True,sociabowl,2018-11-12,['human-computer-interaction'],SociaBowl: A Dynamic Table Centerpiece to Mediate Group Conversations,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2019-03-26T18:18:55.764Z,"A technology that is thousands of years old, knitting is imbued with tradition, myth, and storytelling—to knit is to embody the work of our ancestors. In a hand knitted piece, each loop passes through needles and fingers, leaving small clues about its maker. Depending on our mood and the day, we might knit relaxed loops, tense knots, or daydream and slip a stitch. Unlike machines, we humans have unique quirks and tendencies, and this extends to our knitting. With the proliferation of factory mass production, knits have become standardized, with anonymous stitches that lack any mark of a sentient maker.During this project, I spent a month working in a factory in China to learn about manufacturing and labor. Between rows of hundreds of identical machines, there are still many humans scurrying around to fix jammed needles, trim loose threads, stack and transport finished pieces. I was struck by how these workers live in service of machines, their labor and sweat as ghosts within the identical and perfect end products. This resulting project attempts to reintegrate emotional mark of the worker back into the process of mass production, to begin telling a story of each individual in a landscape of anonymous labor.Automation has many functions and plays a critical role in our technological advancement, but is it possible to embed automation with gestures as intimate as the mark of a brushstroke in a painting? Taking the cognitive signatures of a human via their EEG signals, I translated these brain activities back into the knitting process. Depending on the EEG signal, the tensility of the weaving was programmed to vary, reflecting the mark of the worker's “cognitive” hand. Depending on the mental stress levels over a day of work, the tensility varies and the fabric ripples in empathy.  This project stitches a portrait of the factory worker, through their fluctuating mental states throughout the day—capturing moments of frustration, focus, and meditative work flow. The resulting fabrics tell a story, and each one is unique to the worker and particular moment in time.There are many ways humans express themselves—what does this expression look like in the age of mechanical production? Is there a way to insert the mark of being human into the process? This project offers a moment of reflection—for both the consumer and the worker—to reflect on the labor, both mechanical and human, that is involved in our economy.Learn more: https://ani-liu.com/mind-in-machine",2017-08-02,['hacking-manufacturing'],https://dam-prod2.media.mit.edu/x/2019/03/26/psychemanufacturing.jpg,370938,,2019-03-26T19:13:50.509Z,['wonder@media.mit.edu'],True,mind-in-the-machine,2017-06-02,[],Mind in the Machine: Psyche in the age of mechanical production,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-03-26T18:23:19.480Z,"Structure and function studies of membrane proteins, particularly G protein-coupled receptors (GPCRs) and multipass transmembrane proteins, require detergents after removing them from cell membranes. We have invented a simple tool, the QTY code, that is named for three amino acids: glutamine (Q), threonine (T), and tyrosine (Y) for making water-insoluble domains become water-soluble without detergents. Despite substantial transmembrane domain changes, the detergent-free QTY variants maintain stable structures and ligand-binding activities. We believe the QTY code will be useful for designing water-soluble variants of membrane proteins and other water-insoluble aggregated proteins. The QTY code designed detergent-free chemokine receptors may be useful in many applications. The QTY variants may not only be useful as reagents in deorphanization studies, but also for designing biologic drugs to treat cancer, autoimmune, or infectious diseases. The QTY code allows membrane proteins to be systematically designed through simple, specific amino acid substitutions. The QTY code is robust and straightforward: it is the simplest tool to carry out membrane protein design without sophisticated computer algorithms. Thus, it can be used broadly. The QTY code has implications for designing additional GPCRs and other membrane proteins, or for rendering water-insoluble and aggregated proteins to become water soluble.",,[],,370946,,2020-04-16T15:17:47.491Z,"['shuguang@media.mit.edu', 'ruiqing@media.mit.edu', 'junni@media.mit.edu', 'vex@media.mit.edu']",True,the-qty-code-a-simple-tool-for-protein-design,2019-04-01,"['biology', 'public-health', 'medicine', 'covid19']",The QTY code: A simple tool for protein design,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-03-26T20:29:25.171Z,"There are 20 chemokine receptors that bind their respective chemokines. It is not currently understood how these structurally similar receptors distinguish their ligands; namely, how EC loops and transmembrane domains of these receptors are involved in ligand-binding activities. With the detergent-free GPCRs, we show that it is now possible to design and produce chimeric receptor proteins to study ligand-binding mechanisms. We exchanged the N terminus and 3 EC loops of natural chemokine receptor CXCR4 to append them onto the 7TM α-helices of detergent-free variant CCR5QTY, and systematically studied which ligands it binds. These designer chimeric receptors provide insight into how natural receptors bind their respective ligands. These chimeric receptors with tunable functionality may have applications for bioelectronics sensing devices.QTY-designed, detergent-free chemokine receptors have been expressed in SF9 insect cells, as well as produced using a low cost and simple E.coli system with much higher throughput. These QTY-designed receptor variants exhibit remarkable heat stability in the presence of arginine additive, retaining ligand binding activity after 100°C treatment. New protein variants can also be designed using the same alpha-helical segments but switching the extracellular (EC) loop, e.g., using variant B’s EC loop to directly attach to variant A’s alpha-helical segments. This approach helps our understanding of the binding mechanism of QTY variants and natural membrane receptors, as well as enables a novel pathway for the design and production of multi-functional, water-soluble membrane receptors, with tunable properties for in vitro and in vivo applications.",,['molecular-machines'],,370958,,2020-04-15T23:48:56.013Z,"['shuguang@media.mit.edu', 'ruiqing@media.mit.edu']",True,water-soluble-transmembrane-protein-receptors-with-exchangeable-tunable-ligand-affinity,2019-04-01,"['design', 'bioengineering', 'synthetic-biology', 'medicine']",Water-soluble transmembrane protein receptors with exchangeable and tunable ligand affinity,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-03-26T21:17:23.054Z,"In collaboration with Prof. Uwe Sleytr and Dr. Andreas Breitwieser (BOKU, Vienna, Austria).Bioelectronics is an emerging antidisciplinary field which utilizes biomolecules in electronics, mimics biological architectures, or builds electronic-living organism interfaces. One important aspect of the field is to fabricate sensors for label-free biomolecules detection. Researchers previously designed sensors based on i) metal-oxide-semiconductor (MOSFET), ii) polymers, and iii) inorganic crystalline materials which produce good sensitivity, but lack selectivity. Recent efforts are devoted to directly connecting biological receptors with electronic systems. G protein-coupled receptors (GPCRs) serve as suitable candidates as they are the largest family of membrane receptors that detect information (molecules and lights) and transduce to cell internal signals to regulate body functions. There are ~1,000 GPCR proteins in human cells, each one highly specific to a particular signal. QTY-designed, detergent-free GPCR receptors can be modified and attached to recombinant-SbpA proteins, which are capable of reproducing two-dimensional crystalline monolayers on various electronic surfaces, as demonstrated by Prof. Uwe Sleytr in Vienna, Austria. SbpA 2D crystalline guides the orientation of the attached GPCRQTY receptors and exposes their active binding sites. The self-assembly yields functional molecules with high density >10^12 molecule/cm^2. The bioelectronics platform yields detectable electrical, electrochemical, or optical signals in response to the biological stimuli from the receptor layer. When coupled with different types of receptors, this approach may be a platform for bioelectronics and ultrasensitive-sensing systems.",,['molecular-machines'],,370966,,2020-04-15T23:58:50.626Z,"['shuguang@media.mit.edu', 'ruiqing@media.mit.edu']",True,membrane-receptor-based-high-molecular-density-bioelectronic-platform,2019-04-01,[],Membrane receptor-based high molecular density bioelectronic platform,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-03-26T21:36:27.513Z,"In collaboration with Prof. Horst Vogel and Dr. Horst Pick (EPFL, Lausanne, Switzerland).G protein-coupled receptors (GPCRs) are vital for diverse biological functions, including vision, smell, and aging. They are also involved in a wide range of diseases, and are among the most important targets of medicinal drugs. Tools that facilitate GPCR studies or GPCR-based technologies or therapies are thus critical to develop. We used QTY code (glutamine, threonine, tyrosine) to systematically replace 29 membrane-facing leucine (L), isoleucine (I), valine (V), and phenylalanine (F) residues in the transmembrane alpha-helices of the GPCR CXCR4. This variant, CXCR4QTY29, became more water-soluble, while retaining the ability to bind its natural ligand CXCL12. When transfected CXCR4QTY29 gene into HEK293 cells, the translated CXCR4QTY29 receptor inserted into the cell membrane and retained its cellular signaling activity. This QTY code can significantly improve GPCR and membrane protein studies by making it possible to design functional hydrophilic receptors. The QTY code can be applied to diverse a-helical transmembrane proteins, and may aid in the development of other applications, including clinical therapies.",,['molecular-machines'],,370974,,2020-04-16T15:18:07.140Z,['shuguang@media.mit.edu'],True,cxcr4-designed-using-qty-code-becomes-more-hydrophilic-and-retains-cell-signaling-activity,2019-04-01,"['biology', 'public-health', 'medicine', 'covid19']",QTY CXCR4 becomes more hydrophilic and retains cell signaling activity,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-03-26T21:43:20.061Z,"In collaboration with Prof. Tao Fei’s lab (Shanghai Jiaotong University, China)In addition to a large number of membrane proteins that comprise most alpha-helix transmembrane segments, there are also many transmembrane proteins with mostly beta-sheets, called beta-barrel membrane proteins. These beta-barrel proteins are often involved in molecular transports. Some are enzymes that are involved in lipid metabolism. We now use the QTY code to design water-soluble beta-barrel transmembrane proteins. Such water-soluble, beta-barrel proteins will facilitate studies of the molecular mechanisms of high selectivity of molecular transport and how these beta-barrel membrane proteins carry out lipid catalysis and metabolism. New insight gained from these studies may be useful for further designs of new molecular devices.  ",,['molecular-machines'],,370982,,2020-04-16T15:18:19.203Z,['shuguang@media.mit.edu'],True,qty-code-designs-for-water-soluble-barrel-transmembrane-proteins,2019-04-01,"['biology', 'public-health', 'medicine', 'covid19']",QTY code designs for water-soluble beta-barrel transmembrane proteins,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-03-27T06:09:10.528Z,"How to Train Your Robot is a curriculum for students in 5-8th grade to explore artificial intelligence and ethics. In this course, students participate in a range of hot-topic discussions and hands-on, creative activities to learn about how artificial intelligence is impacting society today. Students design robot companions to solve real-world problems and use machine learning to make them intelligent. At the end of the week, there is a showcase where students can share their inventions with their communities and the public.CollaborationWe are working with i2 Learning to develop this curriculum and deliver it to general education teachers in schools around Massachusetts. i2 Learning is an organization that develops STEM curricula and professional development sessions so that all teachers can feel confident in their ability to bring hands-on STEM lessons to their classrooms. In addition to i2 Camps over the summer, i2 Learning also leads the Massachusetts STEM Week where teachers flip their classrooms upside-down for a week-long dive into STEM.Teacher TrainingWith i2, we trained teachers to deliver this week-long course to their students. Our hope is to empower teachers to understand AI well enough to facilitate accurate and meaningful conversations about the impact of technology on society and the role that their students can play in shaping the future. We aim to study how teachers engage with the curriculum and their students so that we can scale learning about AI in a formal education setting.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2020/02/26/20191025_143332_REV.jpg,371006,,2022-02-02T15:50:02.264Z,"['blakeley@media.mit.edu', 'safinah@media.mit.edu', 'dipaola@media.mit.edu', 'cynthiab@media.mit.edu', 'randiw12@media.mit.edu']",True,ai-5-8,2019-05-01,"['artificial-intelligence', 'kids', 'learning-teaching', 'social-robotics', 'ethics', 'technology']",How to Train Your Robot,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-03-27T19:54:14.456Z,"Mobility is a key issue for city planners. Being able to evaluate the impact of its evolution is complex and involves many factors including new technologies like electric cars, autonomous vehicles and also new social habits like vehicle sharing. We need a better understanding of different scenarios to improve the quality of long-term decisions. Computer simulations can be a tool to better understand this evolution, to discuss different solutions and to communicate the implications of different decisions. In this paper, we propose a new generic model that creates an artificial micro-world which allows the modeler to create and modify new mobility scenarios in a quick and easy way. This not only helps to better understand the impact of new mobility modes on a city, but also fosters a better-informed discussion of different futures. Our model is based on the agent-based paradigm using the GAMA Platform. It takes into account different mobility modes, people profiles, congestion and traffic patterns. In this paper, we review an application of the model of the city of Cambridge.",,['city-science'],,371025,,2019-03-27T20:04:59.772Z,['agrignar@media.mit.edu'],True,abmobility,2016-04-15,[],ABMobility,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-03-29T18:01:13.181Z,"Mekatilili is a learning initiative that provides a platform for African youth to enhance technical skills through creative learning approaches that strive to empower learners and enable access to broader job opportunities and meaningful work. Founded in 2016, the program has reached over 400 young people whose average age demographic ranges from 14 – 25 years. The program is conducted through hands on, interactive workshops focusing on human-centered design, rapid prototyping, electronics, computer science, and professional development. Mekatilili Fellowship ProgramIn 2019, the initiative launched the Mekatilili Fellowship Program (MFP), which is an annual gathering of African innovators that aims to foster open ended, playful, and peer-driven learning to promote the development of appropriate and sustainable local technical solutions.",2020-05-29,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/uuid/d7e06815-4785-485c-bb1d-922e5b7e38bf,371070,,2020-05-24T16:39:57.407Z,"['jaleesat@media.mit.edu', 'muthui@media.mit.edu', 'jwanyiri@media.mit.edu']",True,mekatilili,2018-07-03,"['design', 'learning-teaching', 'makers', 'technology', 'engineering', 'social-change']",Mekatilili Fellowship Program,PUBLIC,http://mekatilili.media.mit.edu/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-04-02T13:43:44.409Z,"Scratch in Practice (SiP) shares stories, strategies, and resources from the Scratch Team and educators around the world. The SiP website features new themes to explore and discuss, and the team hosts conversations on social media to connect with educators using the hashtag #scratchinpractice.SiP is developed by the Scratch Team and the Lifelong Kindergarten group at the MIT Media Lab. The goal of SiP is to help more educators integrate Scratch into their practice in a way that supports a creative learning approach. In response to current events, the SiP site is sharing resources for educators and families who are supporting learners in using Scratch at home.",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/uuid/49f140b0-247e-480f-b97c-b39b9ab935a0,371093,,2022-09-22T01:12:24.672Z,"['nrusk@media.mit.edu', 'gabaree@media.mit.edu', 'mres@media.mit.edu', 'eschill@media.mit.edu', 'muthui@media.mit.edu', 'christan@media.mit.edu', 'sotts@media.mit.edu', 'champika@media.mit.edu', 'kathywu@media.mit.edu', 'bowman@media.mit.edu', 'calla@media.mit.edu', 'leob@media.mit.edu', 'hisean@media.mit.edu', 'yusufa@media.mit.edu', 'carolcr@media.mit.edu', 'jaleesat@media.mit.edu', 'tarmelop@media.mit.edu', 'kosiecki@media.mit.edu', 'yumikom@media.mit.edu', 'rupal@media.mit.edu']",True,scratch-in-practice,2019-02-01,['learning-teaching'],Scratch in Practice,PUBLIC,https://sip.scratch.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-04-02T23:12:32.418Z,"We present a mobile heart rate regulator—ambienBeat—which provides closed-loop ambient biofeedback via subliminal tactile stimulus based on a user's heartbeat rate variability (HRV). We applied the principle of interpersonal physiological synchronization to achieve our goal of effortless regulation of HRV, which is tightly coupled with mental stress levels. ambienBeat provides various patterns of subliminal tactile stimuli, which mimics the feeling of a heartbeat pulse, to guide a user's HRV to resonate with its rhythmic, tactile patterns. The strength and rhythmic patterns of tactile stimulation are controlled to a level below the cognitive threshold of an individual's tactile sensitivity on their wrist so as to minimize task disturbance. Here we present an acoustically noiseless, soft voice-coil actuator to render the ambient tactile stimulus and present the system and implementation process. We evaluated our system by comparing it to ambient auditory and visual guidance. Results from the user study shows that the subliminal tactile stimulation was effective in guiding a user's HRV to resonate with ambienBeat to either calm or boost the heart rate using minimal cognitive load. ",2020-02-11,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/01/15/keyboard.jpg,371113,,2020-10-28T00:07:02.667Z,"['yun_choi@media.mit.edu', 'ishii@media.mit.edu']",True,ambienbeat,2019-01-01,"['design', 'health', 'wellbeing']",ambienBeat: Mobile tactile biofeedback for subliminal heart rate rhythmic regulation,PUBLIC,http://www.mallcong.com,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-04-04T13:24:16.887Z,"MoveU is a wearable vestibular stimulation device for providing proprioceptive haptic feedback in virtual reality (VR).  The device induces sensations of motion corresponding to virtual motion, thereby increasing immersion in VR and reducing cybersickness. MoveU non-invasively stimulates the vestibular system using a technique called galvanic vestibular stimulation (GVS). GVS is a specific way to elicit vestibular reflexes using electrical current. This technique has been used for over a century to study the function of the vestibular system. In addition to GVS, the device supports physiological sensing by connecting heart rate, electrodermal activity, and other sensors  using a plug-and-play mechanism offering flexibility for further development. MoveU supports multiple categories of virtual reality applications with different types of virtual motions such as driving, navigating by flying, teleporting, or riding. The device is a working prototype and we evaluated the effects of synchronous vestibular stimulation on mitigating cybersickness and increasing the sense of immersion in a VR environment (see publications for more info). ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/04/05/moveu_device.jpg,371210,,2019-10-17T15:47:23.527Z,"['abyjain@media.mit.edu', 'sra@media.mit.edu', 'pattie@media.mit.edu']",True,move-u,2018-07-15,"['human-computer-interaction', 'wearable-computing', 'interfaces']",MoveU,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-04-04T14:16:51.107Z,"Canadian artist Emily Carr once said, “You come into the world alone and you go out of the world alone yet it seems to me you are more alone while living than even going and coming.” This observation seems more piercingly accurate now, in our 21st century reality, than it was in Carr’s early 20th century reality. If we consider how individuals connect and how their connections evolve into community, we may extend that thinking toward how to facilitate those connections—those sparks—through which individuals who know very little about each other realize they indeed share something in common, in sometimes unexpected or surprising ways. Through the medium of photography, YOU:PLURAL aims to create that spark of connection in an all-inclusive way. Beginning with everyone in our neighborhood—E14 and E15, including the Media Lab, CBA, CMS, and ACT—and hopefully beyond, we invite you to explore connections to people you may often see but don’t know well. (And we will help you!) We will then document those nascent connections as photographic images and associated metadata. These collected portraits will be displayed as part of our first show on the Civic Media ArtCube in early 2019.If you’re interested in building and enhancing community here in E14 and E15 and want to have fun in the process, let us know. We’d love to hear your ideas for making creative, compelling images of our community.—Cindy Bishop + Lorrie LeJeune",2020-08-01,['center-for-civic-media'],https://dam-prod2.media.mit.edu/x/2019/04/04/Screen Shot 2019-04-04 at 10.18.41 AM_wbaWJmi.png,371218,,2022-06-10T13:29:45.238Z,"['csbishop@media.mit.edu', 'lorrie@media.mit.edu']",True,youplural,2019-04-04,[],You:Plural,PUBLIC,http://www.youplural.org,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-04-04T18:40:14.818Z,"Let's play: Waste at MIT is a game about trash; it’s also a game about understanding civic infrastructure. This project explores the use of games to engage citizens, impacting real-world actions in complex civic systems. Working with waste management and sustainability efforts on campus, this project uses MIT as a living lab to study the link between interaction and action in civic media. More generally, this project looks at ways in which participatory, critical and exploratory games can give people agency over the complex systems that surround them.Participate in the project by playing! Visit wasteatmit.media.mit.edu to learn more. The code for this project is open-sourced at: http://github.com/agnescameron/trashgame",2019-06-07,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/04/tyler-screenshot.png,371230,,2019-06-04T19:37:53.019Z,['agnescam@media.mit.edu'],True,waste-at-mit,2019-01-01,"['civic-technology', 'gaming', 'civic-action']",Let's play: Waste at MIT,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2019-04-05T20:03:48.622Z,"Utreexo: a dynamic accumulator for bitcoin state A description of research by Thaddeus DryjaOne of the earliest-seen and most persistent problems with Bitcoin has been scalability. Bitcoin takes the idea of ""be your own bank"" quite literally, with every computer on the bitcoin network storing every account of every user who owns money in the system. In Bitcoin, this is stored as a collection of ""Unspent transaction outputs,"" or ""utxos"", which are somewhat unintuitive, but provide privacy and efficiency benefits over the alternative ""account"" based model used in traditional finance.It's important to distinguish between the transaction history and the current state of the system. The transaction history in Bitcoin is currently 200GB, and contains every transaction since Bitcoin was launched in early 2009. The size of this history can of course only increase with time. The current system state, however, is much smaller, at under 4GB, and deals with only who owns what right now. This state size has generally increased over time, but has in fact decreased a bit this year.The history, despite its much larger size, is not in fact the scalability concern, as it is not used in any time-critical fashion; one can discard the history after processing with no loss of security. The increasing state size, however, is a concern—one which utreexo solves.Utreexo is a novel hash based dynamic accumulator, which allows the millions of unspent outputs to be represented in under a kilobyte—small enough to be written on a sheet of paper. There is no trusted setup or loss of security; instead, the burden of keeping track of funds is shifted to the owner of those funds.  Current transactions specify inputs and outputs, and verifying an input requires you to know the whole state of the system. With Utreexo, the holder of funds maintains a proof that the funds exist, and provides that proof at spending time to the other nodes. These proofs are compact (under 1KB) but do represent the main downside in the utreexo model; they present an additional data transmission overhead, which allows much smaller state.Utreexo pushes the costs of maintaining the network to the right place: an exchange creating millions of transactions may need to maintain millions of proofs, while a personal account with only a few unspent outputs will only need to maintain a few kilobytes of proof data. Utreexo also provides a long-term scalability solution, as the accumulator size grows very slowly with increasing size of the underlying set (the accumulator size is logarithmic with the set size).",2022-12-31,['digital-currency-initiative-dci'],https://dam-prod2.media.mit.edu/x/2021/10/04/Utreexo.png,371251,,2023-03-30T19:27:49.532Z,['tdryja@media.mit.edu'],True,utreexo,2018-11-28,[],Utreexo,PUBLIC,https://dci.mit.edu/utreexo,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-04-07T15:48:05.158Z,​Interested in learning more about the Commalla project?  Fill out this form! ,,"['affective-computing', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2019/10/16/echos-banner.png,371302,,2021-10-07T17:29:41.793Z,"['jnarain@media.mit.edu', 'ktj@media.mit.edu', 'fergusoc@media.mit.edu']",True,commalla,2019-03-16,"['design', 'human-computer-interaction', 'machine-learning', 'interactive', 'autism-research']",Commalla: Communication for All,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-04-08T01:00:15.278Z,"Chemical constituents are the most essential components of the fresh produce items consumed by people around the world. That chemistry can be extremely variable based on a number of factors—but wealthy or poor, privileged or vulnerable, we humans are ultimately subject to the destiny meted out by the chemistry of the food we consume.Indeed, nutritional content, flavor, color, texture, aroma, and even medicinal benefits are dictated by this chemistry, yet there has never been a thorough and comprehensive analysis of that chemistry in fresh produce items. The National Produce Chemotyping Project sets the stage to assess the chemical variability, or chemotypes, of those items. Ultimately this project will lead to:Unprecedented studies seeking to correlate specific chemical attributes of food to health and well-being for all people.Improved understanding of how the agricultural pipeline impacts food at the point of consumption, with the specific inclusion of food sources located in underserved communities.The piloting of a free, public-facing resource (The National Produce Chemotyping Database) with interpreted data available in a format that is user-friendly for even the most vulnerable populations.",2020-04-30,['open-agriculture-openag'],https://dam-prod2.media.mit.edu/uuid/3ac4daf6-b47f-46ea-be00-4b319a1ac9bb,371310,,2020-06-17T15:08:48.271Z,"['rebekahj@media.mit.edu', 'calebh@media.mit.edu']",True,npcp,2019-01-01,[],National produce chemotyping project,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-04-08T01:55:52.822Z,"Bubble is a pneumatically actuated wearable system that enables people with handdisabilities to use their own hands to grasp objects without fully bending their fingers. Bubble offersa novel approach to grasping, where slim, ultra-lightweight silicone actuators are attached to thefingers. When the user wishes to grasp an object, the silicone units inflate pneumatically to fill theavailable space around the object. The inflatable units are interchangeable, can be independentlyinflated, and can be positioned anywhere on the fingers and in any orientation, thereby enabling a wide variety of grasping gestures.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2019/04/07/1.jpg,371318,,2022-06-10T14:09:49.301Z,['alims@media.mit.edu'],True,robotics,2019-01-01,['human-computer-interaction'],Bubble: Wearable assistive grasping augmentation based on soft inflatables,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-04-08T17:02:40.751Z,"inSight is a brain decoding system. It uses  generative models (BigGAN and MusicVAE) to stimulate the brain with synthetic, but natural-looking videos and melodies. The brain activity of the user is then recorded using an EEG headset. The recorded data is then processed to train an encoder to interpret the brain activity in terms of the latent space of the generative model, effectively allowing the system to generate video and music from the user's brain activity. inSight can be used for applications such as neurofeedback therapies, creativity, human-human communication and search. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/04/08/DNF-01_IHlgqLt.png,371336,,2021-01-24T19:36:27.800Z,['mmv@media.mit.edu'],True,insight-deep-neurofeedback,2018-12-01,"['extended-intelligence', 'health']",inSight: Deep Neurofeedback,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-04-08T17:21:20.009Z,"With the exponential increase of personal data in the forms of images, videos, emails, and social media posts, the time is ripe for building personal AIs that utilize these data to enhance the productivity and creativity of the users. Training AI algorithms require labeled and processed data.However, annotating data is time-consuming and often regarded as the bottleneck of supervised learning. Most tools used for data labeling are tailored for the needs of data-scientists and researchers and are far from being useful for general users.  The users of these systems need to know the ontology of possible labels beforehand and use complex interfaces and workflows to maintain the consistency and quality of the resulting dataset. ""Q"" aims to reformulate data annotation as an engaging conversation by asking appropriate questions and automatically highlighting possible regions of interest. To come up with relevant questions, Q learns from the Wikidata public knowledge graph by computing the probable properties and relationships of entities. It also utilizes the previously annotated pieces of data to speed up the process.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/04/08/Qpipe_sfgQFow.png,371344,,2019-04-08T18:21:50.443Z,['mmv@media.mit.edu'],True,q-ai-that-makes-the-graph-of-your-memories,2019-01-01,[],Q: An intelligent conversational interface for personal data labeling,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-04-09T19:57:09.990Z,"How something is presented can be as important as the message itself.In the age of political polarization and election meddling, it is of vital importance to understand which factors contribute to the formation of public opinions. Television is one of the main sources of information for a large portion of the general population.In order to understand discrepancies in news perception, what they are caused by and which implications they might have on shaping public political debate, we first need to understand how television news are constructed and define presentational aspects of the news. Given that, we can build tools that analyze news consumption by the public.We aim to use various artificial intelligence techniques to model the ""subcarriers of information"" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. Our goal is to develop an algorithmic understanding of journalistic choices in the way news content is presented. We also attempt develop an understanding of higher-level characteristics of television news such as television set atmosphere or political bias. This altogether would enable a broad-range, comprehensive algorithmic analysis of how news presentation is trying to shape the public political debate.Project updates via PubPub",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/09/symbolic_aspects_wRyVqDy.png,371357,,2019-04-18T16:51:12.445Z,"['eickhoff@media.mit.edu', 'lip@media.mit.edu', 'mhjiang@media.mit.edu']",True,unspoken-news,2018-11-01,['machine-learning'],Unspoken News,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-04-09T20:39:02.134Z,"Seemingly an old TV box which turns out to offer an abundance of control over content selection: each aspect of news can be selected for using old-style controls. The main goal is to make people aware of their choices and biases and to make them curious to tune the controls and to watch something different. Hitting the TV resets the controls to random values and lets user discover completely new unexpected content. The main underlying assumption behind the idea of YouTune is that while it is very difficult to influence people’s opinions and change their beliefs explicitly without a willingness on their part to change these beliefs, we could attempt to influence people implicitly by providing a tool that, first, would make users aware of the choices they take: to watch what they want or are used to, the user has to explicitly set controls. Unlike traditional broadcast and online media platforms, YouTune does not allow users to select content based on channel or program, so in order to find content that the person is usually interested in, with YouTune they would need to manually select characteristics of the content they want to watch. And second, we believe that YouTune will make people curious to change the control values they are used to and therefore to discover unexpected content that could potentially influence their opinions in an implicit way. With YouTune the user can choose between different characteristics and tune into some value of the chosen characteristic to see short news stories. The idea is to make statistics experienceable without showing graphs: a person using YouTune would subconsciously aggregate statistical patterns and understand which tricks and stylistic tools channels tend to use to cover certain topics.Project updates via PubPub",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/09/people.png,371366,,2019-11-01T17:22:43.952Z,"['eickhoff@media.mit.edu', 'lip@media.mit.edu']",True,youtune,2019-01-01,[],YouTune,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-10T15:14:39.886Z,"Evergreen Blues is a suite of interactive songs that together provide a collaborative musical narrative experience in the form of a multiplayer operatic game.Two players simultaneously control the construction and direction of a piece of music through the use of a real-time lyrical conversation system, allowing for granular control of musical expression. Choices made in one song influence the outcomes of the next, paving the way for multi-scene interactive experiences grounded in narrative principles of persistence and emotional consequence.This work seeks to provide a novel means of creating and understanding multi-user, interactive music systems in which users participate in active and collaborative music-making in conjunction with narrative engagement. It is the goal that this work will open up new possibilities for experiencing music, narrative, and social interaction. ",2019-07-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/04/10/Screen Shot 2019-04-10 at 11.16.29 AM.png,371394,,2022-09-14T20:11:19.837Z,['davidsu@media.mit.edu'],True,evergreen-blues,2019-01-01,"['human-computer-interaction', 'gaming', 'music', 'interactive']",Evergreen Blues,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-10T17:55:42.026Z,"Concerned about your privacy online? Worried with whom and how photos of you and/or your family might be shared on social media? You probably should be. Revelations about social networks (like Facebook) sharing your personal data with unethical actors (like Cambridge Analytica) are a major cause for concern. According to a recent Pew research study, 91 percent of Americans worry that social networks might misuse or resell their sensitive personal data.That's why we built RockStar, a fully-featured social network in which everyone’s identity is correlated to a rock. RockStar most literally ensures rock solid privacy. Get My RockStar right now!!!Because humans are excellent at recognizing patterns, we can see faces in non-human forms— something that machine learning algorithms have difficulty replicating. As such, our most secure Rockelgangers come from samples from inorganic material—a collection of rocks at The Japanese Museum of Rocks that Look like Faces. We captured 1,500 unique rock face images that we estimate resemble 85 percent of the human population. Using the innovative Nimoy-Brenner Algorithm developed at MIT Center for Civic Media, we match your face to that of a celebrity and from that to one of these 1,500 rocks. By cloaking your identity via your Rockelganger, you can use social media freely without worry that your or your family’s data is being resold or compromised.Worried that you might be part of the 15 percent whose face doesn't map neatly to a rock? Using cutting-edge generative adversarial neural network techniques, we can generate a custom Rockelganger just for you. These rocks look recognizably like you, but are sufficiently inorganic, enough to fool even the most advanced facial recognition systems.ADDITIONAL BENEFITS!!!Escape your echo chamber!Enhanced privacy is not the only benefit of RockStar. We know that social networks can lead toward echo chambers of information where you hear only from your friends and people that you spend time with offline. RockStar offers a whole new way to make friends and meet people online.Make new friends!With only 1,500 default rocks, it's almost certain that you have the same Rockelganger as someone else online. Why not meet that person and find out what interests you share in common?Learn about geology! Meet  RockStar members who share your same type of rock. Is your rock face an igneous rock? Meet other igneous rocks or branch out and search for sedimentary or metamorphic rocks. Rockstar—smashing privacy violations and the filter bubble, one rock at a time.",2020-08-01,['center-for-civic-media'],https://dam-prod2.media.mit.edu/x/2019/04/22/Screen Shot 2019-04-20 at 1.57.15 PM.png,371402,,2022-06-10T13:29:45.251Z,['csbishop@media.mit.edu'],True,rock-and-roll-spirit,2019-01-01,[],RockStar-ai,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-04-11T19:27:41.002Z,"Computational Food is a series of experiments around the shape-changing nature of food and its associated unique sensory experiences. We used food as a medium and a platform to develop dynamic, shape-changing prototypes that are edible or that enhance the culinary experience.",2014-12-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/04/11/jud.JPG,371434,,2019-04-29T22:10:01.940Z,"['amores@media.mit.edu', 'viirj@media.mit.edu', 'inamura@media.mit.edu', 'djain@media.mit.edu']",True,computational-food,2014-09-16,[],Computational Food,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-11T19:35:11.072Z,"Media filter bubbles sacrifice shared reality amongst US citizens. We aim to burst these echo chambers by presenting short, automatically summarized news clips to users through a mobile app. The user watches these short clips in sequence and has the option to press a lightbulb that signifies whether the news segment have been enlightening. These clips are generated from SuperGlue metadata and is based on News*2. It uses an “Anti-recommender system” that actively expands the user’s horizon—contrary to traditional recommender systems that aim to thicken the walls of echo chambers.Open enlightened.media.mit.edu in your mobile phone and start watching short videos while discovering your own media biases.",2021-07-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/11/IMG_20190411_162148.jpg,371442,,2023-09-27T03:52:20.143Z,"['mhjiang@media.mit.edu', 'lip@media.mit.edu']",True,enlightened,2019-01-01,"['storytelling', 'computer-science', 'natural-language-processing']",Enlightened: Broaden Your Views,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-04-12T19:25:43.450Z,"Autonomous vehicles (AVs), drones, and other types of robots will revolutionize our way of traveling and understanding urban space. In order to operate, all of these devices are expected to collect, analyze, and share sensitive data about our daily activities. However, current approaches of handling these datasets relies on centralized models (e.g., corporate databases, cloud-based services, etc). Recently, it has been shown that the security and privacy-preserving capabilities of these centralized models present significant challenges for future deployments. For this reason, this project proposes BASIC, the Blockchained Agent-based Simulator for Cities. This tool aims to verify the feasibility of using blockchain as a communication layer between agents (e.g., citizens, robots, etc.) in simulated urban scenarios. In order to test the proposed tool, we propose a car-sharing scenario populated with AVs within the city of Cambridge (Massachusetts, USA).",,"['human-dynamics', 'city-science']",https://dam-prod2.media.mit.edu/x/2019/08/12/Basic_8mdPG64.png,371489,,2019-08-12T16:42:18.449Z,"['ecstll@media.mit.edu', 'alonsolp@media.mit.edu', 'sandy@media.mit.edu', 'kll@media.mit.edu', 'agrignar@media.mit.edu']",True,basic,2018-09-03,"['urban-planning', 'blockchain', 'autonomous-vehicles']",BASIC: Blockchained Agent-based Simulator for Cities,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-04-13T19:45:04.397Z,"Rotational  musical  instrument  interfaces  are  a  very  intuitive representation of the fundamentals of music itself, as they  are  inherently  simple  oscillators  that  can  run  from low control frequencies up to audio. The downside of many rotating instruments is that they are bounded by the rotating mechanisms themselves, that being a fixed axle where the rotation is driven by a hand or a motor.  This leads to a reduction of the performer’s expressive ability to influence the generated sound by constraining them to fixed locations and/or positions. In this work, we avoid these drawbacks,  but maintain the affordance of the rotational interface by incorporating non-contact sensing and wireless data transfer.We designed an untethered digital synthesizer that can be held and manipulated while broadcasting audio data to a receiving off-the-shelf Bluetooth receiver.  The synthesizer allows the user to freely rotate and reorient the instrument while exploiting non-contact light sensing for a truly expressive performance. The system consists of a suite of sensors that convert rotation, orientation, touch, and user proximity into various audio filters and effects operated on preset wave tables, while offering a persistence of vision display for input visualization.  ",2019-01-31,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2019/04/13/cd_synth.jpg,371505,,2020-04-24T21:27:46.051Z,['chwalek@media.mit.edu'],True,cd-synth,2018-10-01,[],CD-Synth,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-15T14:07:06.062Z,"Talk radio exerts significant influence on the political and social dynamics of the United States, but labor-intensive data collection and curation processes have prevented previous works from studying its content at scale. Over the past year, the Laboratory for Social Machines and Cortico have created a talk radio ingest system to record and automatically transcribe audio from more than 160 stations around the country. Using these transcripts, we propose novel compression-based methods for unsupervised summarization of spoken opinion in conversational dialogue. By relying on an unsupervised framework that obviates the need for labeled data, the summarization task becomes largely agnostic to human input beyond necessary decisions regarding model architecture, input data, and output length. As a result, trained models are able to produce a more accurate depiction of opinion. Using the outputs of my proposed methods, we conduct a case study to examine the variability of public opinion across America. In the interests of reproducibility and further research, we open-source all code and data used.",2019-06-07,['social-machines'],https://dam-prod2.media.mit.edu/x/2019/04/15/Screen Shot 2019-04-15 at 10.12.25 AM_gJ77OKg.png,371516,,2019-04-17T00:15:52.766Z,"['dkroy@media.mit.edu', 'shayneob@media.mit.edu']",True,spoken-opinion-summarization,2018-08-01,[],Spoken Opinion Summarization,PUBLIC,https://github.com/shayneobrien,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-04-16T02:17:44.126Z,"What is Cortico and the Local Voices Network?Cortico, a non-profit 501(c)(3) in cooperation with MIT’s Laboratory for Social Machines, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a public conversation network called the Local Voices Network (LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.LVN combines in-person and digital listening to host, analyze and connect community conversations at scale. Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network is designed around three core efforts:Facilitating in-person community dialogue that enables participants to listen, learn, and be heardConnecting facilitators and conversations digitally across boundariesOpening a new listening channel for journalists, leaders, and the community at largeWhy is this work important?Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2019/04/15/LVN.png,371532,,2019-04-16T16:12:25.376Z,"['chrwang@media.mit.edu', 'wesc@media.mit.edu', 'wschen@media.mit.edu', 'dougb5@media.mit.edu', 'dkroy@media.mit.edu', 'russell5@media.mit.edu', 'ajking@media.mit.edu', 'pbeshai@media.mit.edu', 'eyi@media.mit.edu']",True,local-voices-network,2018-09-01,[],Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-04-16T19:53:56.508Z,"As we move towards an increasingly IoT-enabled ecosystem, we find that it is easier than ever before to capture vast amounts of audio data.  However, there are many scenarios in which we may seek a ""compressed"" representation of an audio stream, consisting of an intentional curation of content to achieve a specific presentation—a background soundtrack for studying or working; a summary of salient events over the course of a day; or an aesthetic soundscape that evokes nostalgia of a time and place.In this work, we present a novel, automated approach to the task of content-driven ""compression,"" built upon the tenets of auditory cognition, attention, and memory.  We expand upon our previous experimental findings, which demonstrate the relative importance of higher-level gestalt and lower level spectral principles in determining auditory memory, to design corresponding computational implementations enabled by auditory saliency models, deep neural networks for audio classification, and spectral feature extraction.  We demonstrate the approach by generating a number of 30 second binaural mixes from eight-hour recordings captured in three contrasting locations at the Media Lab, and conduct a qualitative evaluation illustrating the relationship between our feature space and a user's perception of the resulting presentations.  Through this work, we suggest rethinking traditional paradigms of compression in favor of an approach that is goal-oriented and modulated by human perception.A brief demo and explanation of our work can be found here: https://resenv.media.mit.edu/audio-summarization/",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/13/Slide7_MWBL7ty.png,371578,,2020-06-15T16:02:52.670Z,"['ishwarya@media.mit.edu', 'dramsay@media.mit.edu']",True,compression-by-content-curation,2019-03-01,"['artificial-intelligence', 'music', 'technology']",Cognition-Driven Audio Summarization,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-16T19:56:15.756Z,"The Language Simplification project is developing automatic methods to simplify complex texts to be more easily read and understood by a broader audience, such as children and non-native English speakers. Using neural networks, complex words and phrases can be substituted, the sentence can be split and rephrased, and the overall text can be summarized and compressed. These capabilities can be wrapped into a reading assistance tool for end users, or as a pre-processing step for other NLP tasks.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2019/04/16/Extension2_xdL7N7M.png,371586,,2019-04-17T20:51:49.717Z,"['echu@media.mit.edu', 'exposito@media.mit.edu', 'ngillani@media.mit.edu']",True,language-simplification,2019-02-01,[],Language Simplification,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-04-16T20:04:48.218Z,"Generative audio models based on neural networks have led to considerable improvements across fields including speech enhancement, source separation, and text-to-speech synthesis. These systems are typically trained in a supervised fashion using simple element-wise l1 or l2 losses. However, because they do not capture properties of the human auditory system, such losses encourage modeling perceptually meaningless aspects of the output, wasting capacity and limiting performance. Additionally, while adversarial models have been employed to encourage outputs that are statistically indistinguishable from ground truth and have resulted in improvements in this regard, such losses do not need to explicitly model perception as their task; furthermore, training adversarial networks remains an unstable and slow process.In this work, we investigate an idea fundamentally rooted in psychoacoustics. We train a neural network to emulate an MP3 codec as a differentiable function. Feeding the output of a generative model through this MP3 function, we remove signal components that are perceptually irrelevant before computing a loss. To further stabilize gradient propagation, we employ intermediate layer outputs to define our loss, as found useful in image domain methods. Our experiments using an autoencoding task show an improvement over standard losses in listening tests, indicating the potential of psychoacoustically motivated models for audio generation.A summary of the work and sound examples can be found here: https://ishwaryaanant.github.io/small-network-perceptual-loss/demo",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/13/Slide6_Gqub35x.png,371594,,2020-06-15T16:04:28.823Z,['ishwarya@media.mit.edu'],True,codec-perceptual-loss,2019-02-01,[],Towards a Perceptually-Motivated Loss Function for Audio Neural Networks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-04-16T20:26:53.237Z,"Bias by us envisions a future of media diversity by understanding the bias of today.Our work seeks to understand how the US media ecosystem reports on underrepresented minorities. By using natural language processing algorithms and data-intensive models, we aim to uncover underlying stereotypes, associations, and modes of narration that media produces and reproduces when covering minority related events. We perform a multi- and cross-platform analysis, capturing media dynamics on different social media platforms and traditional media outlets across the political spectrum. Besides understanding how media language portrays underrepresented minorities, we locate effects on and associations to political saliency and bias-motivated crime.By understanding media bias and its effects on underrepresented minorities, we reflect on the conditions that can ensure a diverse and inclusive US media ecosystem.",2019-06-15,['civic-media'],https://dam-prod2.media.mit.edu/x/2019/04/16/bius_.png,371602,,2019-04-21T19:06:53.374Z,"['ethanz@media.mit.edu', 'orestis@media.mit.edu']",True,bias-by-us,2019-02-18,"['civic-media', 'politics', 'social-media', 'natural-language-processing', 'data-science']",Bias by us,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-04-16T22:59:18.909Z,"To help prevent the mindless sharing of content and promote the sharing of thoughtful content, a socially acceptable “stamp of approval” was created, backed by distributed ledger technology (DLT) that would integrate itself into a post. These stamps of approval are social indicators to one's network that the person sharing the content (and receiving the stamp of approval) has reviewed the content before sharing.DLTs, specifically blockchains, have the architectural benefit of providing a public ledger platform where all recordings/transactions are immutable and verifiable, thus being an excellent platform for audits and verifying provenance—even though this isn’t what we care about. In traditional blockchain architecture, miners perform a computational intensive process called PoW (proof-of-work) in order to prove to the complete blocks that maintain the network. Using “patience” as a variable to allow a user-defined block completion algorithm for every assertion made, we created proof-of-patience. A user defines the level of patience—time and computational resources they are willing to give up, from a scale of one to three and then they mine their block.When the user is done mining for their block, we created PoPBot (""Proof of Patience""-bot)to tweet at the user the content they want to share with a badge that is representative of the work/patience they put in and the user can then retweet it to their network. ",2019-01-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/16/Screen Shot 2019-04-16 at 7.54.58 PM_wAqE0f9.png,371621,,2020-01-08T20:20:22.664Z,"['oceane@media.mit.edu', 'dmarquez@media.mit.edu']",True,proof-of-patience,2018-11-11,[],Proof of Patience,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-04-16T23:09:00.893Z,"Loneliness is becoming a global epidemic. As many as 33 percent of Americans report being chronically lonely, with similar percentages being reported in countries around the world. Additionally, this percentage has risen in recent years. Many are turning to online forums as a way to connect with others about their feelings of loneliness and to begin to reduce these feelings. However, oftentimes, posts go unresponded to and online conversations do not take place, perhaps because those conversing did not find a connection with each other, potentially leaving the poster feeling even more lonely. This research explores the how health of conversation should be defined in online support conversations and analyzes the characteristics of conversation that contribute to healthier conversation.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2019/04/16/Screen Shot 2019-04-16 at 4.12.45 PM.png,371629,,2019-04-17T13:57:33.414Z,"['dkroy@media.mit.edu', 'lnfrat@media.mit.edu']",True,conversational-health-loneliness-on-reddit,2018-09-01,[],Conversational Health: Loneliness on Reddit,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-04-17T01:24:02.110Z,"The Joy Branch project explores different user interfaces to allow parrots to shape their sonic environment.  Animal agency—control of the environment—is an important and underutilized element of captive care.  Parrot species are vocal learners, and as such are highly attuned to their sonic environment. Much of their brains are involved in the production and analysis of sound, and yet their sonic environment in managed care does not provide a rich experience. In this project, we assess the efficacy of new enrichment techniques that have the potential to improve the lives of these birds through music.  The project involves the placement of sonic enrichment elements into the birds’ enclosures under controlled and supervised conditions.The ""joystick branch"" element exposes only a standard wooden perch to the birds. The aim is to create naturalistic interactive methods for birds to generate sounds, and to assess their optional engagement with these new modes of control. ",2021-12-31,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/04/17/Screen Shot 2019-04-16 at 8.33.29 PM copy.jpg,371637,,2023-10-04T16:57:00.001Z,"['rebklein@media.mit.edu', 'gabem@media.mit.edu', 'davidsu@media.mit.edu']",True,joy-branch,2019-01-01,"['human-computer-interaction', 'ecology', 'behavioral-science', 'technology', 'cognitive-science', 'alumni']",JoyBranch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-17T01:58:26.371Z,"ConceptNet, or Open Mind Common Sense, is a long-standing project designed to help computers understand the meanings of words that people use. Starting in 1999, we were the first crowd-sourced project used to train an Artificial Intelligence and one of the first uses of crowdsourcing. ConceptNet originated from the crowdsourcing project Open Mind Common Sense, which was launched in 1999 at the MIT Media Lab. It has since grown to include knowledge from other crowdsourced resources, expert-created resources, and games with a purpose. It is currently an open data project providing freely-available knowledge graphs and NLP models in 73 languages. Over the years this project has had many collaborators and have lived in the Society of Mind, Software Agents, and Digital Intuition groups as well.",2020-08-01,['object-based-media'],,371645,,2022-06-10T14:09:49.313Z,[],True,conceptnet-new,1999-09-15,"['artificial-intelligence', 'natural-language-processing', 'data-science']",Common sense for artificial intelligence,PUBLIC,http://conceptnet.io/,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-04-17T02:09:15.668Z,"We want to create immersive, personalized, and scalable digital experiences. In order to do that, we need to fundamentally rethink the way intelligent agents are built. Currently, agent technology is designed to quickly get a user with a single problem to their solution; success is how quickly you accomplish this task. We are rethinking agents to be about story quality, experience, and interaction design. We plan on separating the bits of the agent: the dialogue generation, world modeling, conversational planning, and emotion. In order to do that, we must first design NLP that work within the confines of a world that is often very different than our own—with limited data to do it!",2020-08-01,['object-based-media'],,371653,,2022-06-10T14:09:49.333Z,[],True,conversational-characters,2018-12-17,"['artificial-intelligence', 'entertainment', 'gaming', 'storytelling', 'interfaces', 'natural-language-processing', 'data-science']",Conversational Characters:  AI agents for entertainment,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-17T02:21:42.324Z,"This project delivers a series of Github-signal based metrics that would be useful to both the layman and investor, helping look beyond the rocky trends of the cryptocurrency market to the actual viability of the token as a utility or security from the perspective of developer dynamic. By doing a quantitative analysis of source code and developer activity within a community, we can provide a rich set of insights into the health of a token’s foundation.Assessment MethodologyBuilding upon previous research, we gathered a ranked list of 13,695 repositories that correlate with 1,011 tokens—where we assess a token (i.e., Bitcoin) which has multiple repositories (i.e., bips, bitcoin, libbase58). The repository metadata information includes url data, forks_count, subscribers_count, network_count, open_issues_count, watchers_count, stargazers_count, size, created_at, updated_at, pushed_at, has_wiki, has_downloads, has_issues, is_fork. We have built a classifier that accepts this metadata as input features  and returns a label to help us assess ranking qualities of the specified token.Co-authored with Yashashree Kokje",2019-12-31,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/16/network.jpg,371661,,2022-10-11T13:27:26.493Z,['oceane@media.mit.edu'],True,depicting-token-quality,2019-01-01,"['cryptocurrency', 'blockchain']",Humanized Cryptoassets,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0
False,2019-04-17T12:17:49.813Z,"QR Websites serves websites without the need for an Internet connection. The approach uses a minified site, encoded as a Data URI on a QR code, to enable a smooth interaction: scan the QR code, and open the URI in the address bar of any browser. Applications that involve sending data, such as webforms or quizzes, work by using a response formatting engine to send the data as SMS messages. Using this approach, developers can create forms, quizzes, educational tools, games, and demonstrations without a connection to the Internet, all while using what people likely have in their pockets: a mobile phone with a browser, and any QR code scanning app. ",2019-09-30,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/04/17/QRSite.png,371677,,2020-10-23T18:16:37.775Z,['lip@media.mit.edu'],True,qr-sites,2019-04-01,[],QR Sites,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-04-17T13:11:29.341Z,"""The sense of freedom…entails not simply the absence of frustration but the absence of obstacles to possible choices and activities—absence of obstructions on roads along which a man can decide to walk."" —Isaiah BerlinCan we understand freedom as a subjective sensation?The Freedom Simulator is a set of three experiences that aim to induce a feeling of freedom. These experiences question the manifestation and significance of freedom in our everyday lives.Each experience is based on a modern political philosophy perspective on freedom:  positive freedom, negative freedom, and freedom in light of ethical individualism. Various techniques such as spatial audio, motion tracking, and real-time video projection are utilized.",2020-07-01,['opera-of-the-future'],https://dam-prod2.media.mit.edu/uuid/65853371-070f-4ef0-bf53-75de386b57a3,371685,,2022-09-14T20:16:35.709Z,['hanelee@media.mit.edu'],True,freedom-simulator,2018-10-01,[],Freedom Simulator,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2019-04-17T15:13:26.994Z,"The Gamma SENSE (Sensory Engaging Nebulised Scent Experience) is a pioneering instrumental addition to the gamma instrument series. The device delivers multisensory gamma stimulation through auditory, visual, and olfactory channels. This medical/musical instrument is based upon our groundbreaking pilot studies revealing (even non-synesthetic) humans link scent to sonic pitches. Due to the steep fall-off rate once gamma stimulation concludes, this olfactory mechanism probes the possibility of slowing 40 Hz frequency attenuation in patients. As research continues, the Gamma SENSE device was developed to aid in the testing and identification of the olfactory perceptive triggers which recruit and sustain identified cognitive frequencies. In collaboration with the Aging Brain Alzheimer’s Initiative at MIT, the Gamma SENSE pilots a novel, testing form-factor with the goal of deployment in large-scale clinical trials. The Gamma SENSE instrument also features one of the first touchpad arrangements designed to support EEG testing via the low-motion/high output format. As motion creates unwanted noise artifacts during electroencephalograms, this instrument requires the subtlest of finger-motions to trigger dynamic musical expression. The Gamma SENSE propels myriad frontiers through expression, function, and design.",,['opera-of-the-future'],,371709,,2020-06-15T14:53:58.425Z,['arieger@media.mit.edu'],True,gamma-sense,2019-01-02,"['medicine', 'neural-interfacing-and-control']",Gamma SENSE: Sensory Engaging Nebulised Scent Experience,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2019-04-17T15:19:37.884Z,"Songs released on music streaming services are static, never changing after their initial release. Evolving Media proposes a content production and publishing pipeline, enabling artists and content creators to release media that evolves and matures as it is consumed. To take advantage of this capability, we are re-thinking the tools and processes used to create and update media content. The current implementation integrates our custom augmented reality stack to rapidly iterate and publish synchronized audio/video content to the web. ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/04/23/2019 Spring MW - Taking a line for a walk.jpg,371725,,2019-07-26T15:33:02.553Z,['holbrow@media.mit.edu'],True,evolving-media,2019-03-01,"['artificial-intelligence', 'music', 'technology']",Evolving Media on the Internet,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-04-17T15:32:31.815Z,"Symphony for the Koreas will be the latest installment of the celebrated City Symphony series. Over the next few years, Tod Machover and his team will collaborate with citizens from both South and North Korea to create a symphony that reflects what both sides have in common, where conflicts remain, and what might be effective, realistic, and peaceful ways to resolve conflicts through music. Through collaboration with the Lindenbaum orchestra and festival, a South Korean organization dedicated to bringing peace to the Korean Peninsula through music, Machover and his team plan to invite musicians from both Koreas to participate in ongoing creative activities—as well as to live performances—of Symphony for the Koreas. The Lindenbaum organization has been granted unprecedented permission by the South Korean government to communicate and collaborate with the North Korean Government. It has also secured an MOU with the North Korean Ministry of Culture to hold a joint concert between the two Koreas. The final performance is expected to take place at or near the Korean Demilitarized Zone, and will then be toured worldwide.Since 2012, Tod Machover and his Opera of the Future Group have created City Symphonies for Philadelphia, Detroit, Lucerne, Perth, Edinburgh, Miami and Toronto. In addition to Symphony for the Koreas, Machover is currently working on new City Symphonies for Chennai (India) and for Boston (in collaboration with HUBweek).",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/04/17/20190318_092804.jpg,371735,,2019-08-12T17:35:40.986Z,['tod@media.mit.edu'],True,symphony-for-the-koreas,2019-04-17,"['music', 'politics', 'community', 'technology']",Symphony for the Koreas,PUBLIC,,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-04-17T17:40:35.111Z,"The ability to automatically understand and infer characters' goals and their emotional states is key towards better narrative comprehension. Reasoning about mental representations of various characters in a narrative has been referred to as Theory of Mind (ToM) reasoning. In this work, we propose an unsupervised neural network that  exploits the personal stories on social media and incorporates commonsense knowledge about characters' motivations and reactions to generate interpretable trajectories of characters' mental states. We find that our model is capable of learning coherent mental representations from characters' actions and their affect states. We evaluate our model using a publicly available dataset for mental state tracking of characters in short commonsense stories. ",,['social-machines'],https://dam-prod2.media.mit.edu/x/2019/04/17/Screen Shot 2019-04-17 at 11.27.36 PM.png,371753,,2020-04-03T16:41:05.158Z,"['pralav@media.mit.edu', 'bridgitm@media.mit.edu', 'dkroy@media.mit.edu']",True,story-comprehension,2019-02-10,[],Story Comprehension,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-04-17T20:59:28.086Z,"An alternative viewing experience for large-scale conversations on Twitter. Each conversation tree visualizes the structure of replies to a single tweet. The purpose of this project is to explore a hidden dimension of virality. Not all tweets that elicit a large volume of comments behave the same under the hood.  These structural patterns, when combined with sentiment analysis and/or toxicity, can reveal a deeper story.",,[],https://dam-prod2.media.mit.edu/x/2019/04/17/Screen Shot 2019-04-17 at 4.59.09 PM.png,371770,,2019-04-18T14:50:10.698Z,"['dkroy@media.mit.edu', 'bridgitm@media.mit.edu', 'bcroy@media.mit.edu']",True,conversation-trees-1,2019-04-17,"['social-networks', 'social-media', 'data-visualization']",Conversation Trees,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-04-17T22:48:50.274Z,"Maiden Flight is an autonomous biological laboratory environment designed for studying the impact of space flight on the sole reproductive node of a bee colony: the queen bee and her retinue.  It represents the first space module of its kind built specifically to cater to queen bees. The hybrid-ecology of the capsule was created to take into account the distributed and uniquely non-human nature of bee biology, in order to consider how to extend the bee reproductive system for environmental extremes. This aim is reflected in the structure of the capsule interior, which was assembled by humans and augmented by the bees’ natural fabrication. In May 2019, the Mediated Matter group traveled to Texas to launch two laboratory capsules on Blue Origin’s sub-orbital rocket system, New Shepard. Each custom-designed metabolic support capsule comprised an experimental environment for one queen bee and an attending retinue of 10-20 nurse bees for a parabolic flight to a 100-kilometer micro-gravitational space apogee, and back.",2021-05-21,"['mediated-matter', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/04/28/MediatedMatterGroup_MaidenFlight_BeesinCapsule.jpg,371780,,2021-05-21T14:41:27.185Z,"['neri@media.mit.edu', 'renri@media.mit.edu', 'rssmith@media.mit.edu', 'fkraemer@media.mit.edu']",True,maiden-flight,2019-04-19,"['robotics', 'design', 'agriculture', 'architecture', 'art', 'bioengineering', 'manufacturing', 'synthetic-biology', 'space', 'materials', 'engineering', 'industry']",Maiden Flight,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-04-18T14:37:07.681Z,"This project aims to create a modular platform for exploring micro-kitchens that are culture specific. Cooking is a personal experience that has cultural attributes. This project explores new modes of cooking using robotically enabled cabinets and appliances to minimize the footprint of the kitchen, while maximizing the ability for users to cook large meals, socialize, and utilize the same space during non-meal times for work. Piccolo kitchen is one of the components of the micro-units that are currently under development as part of the CityHome 02 projects.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/12/06/Piccolo Kitchen Automation.gif,371788,,2020-02-04T15:31:54.239Z,"['maitanei@media.mit.edu', 'alhadidi@media.mit.edu', 'kll@media.mit.edu', 'apaolaza@media.mit.edu', 'adim@media.mit.edu', 'agarciag@media.mit.edu', 'skots@media.mit.edu']",True,piccolo-kitchen,2018-09-01,"['robotics', 'design', 'food']",Piccolo Kitchen,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-04-18T14:40:28.042Z,"An astronaut's ability to leverage the sense of touch is substantially reduced when wearing a protective, pressurized spacesuit. We imagine reinstating the biological skin's sense of touch by mapping inputs on the protective suit's exterior layer to a haptic feedback system on the biological skin. This concept is an illustrative application area for the SpaceSkin project, showcasing how aerospace-grade electronic textiles  might improve astronaut situational awareness, as well as strengthen a sense of connection to fellow explorers. V1 and V2 are early prototypes.  V2 and V3 formed the basis of a conference publication in IEEE Aerospace 2021.Excerpted graphics from conference paper:",2022-08-01,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2021/07/03/SpacesuitHaptics.jpg,371796,,2023-05-31T12:28:49.132Z,"['cherston@media.mit.edu', 'irmandy@media.mit.edu', 'honnet@media.mit.edu', 'vsumini@media.mit.edu']",True,spaceTouch,2019-04-01,"['environment', 'sensors', 'space']",SpaceTouch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-04-19T17:14:41.645Z,"All over Africa, experts use satellite Earth Observation (EO) data for applications such as monitoring crop health or assessing the risk of disease vectors. These applications are often done at a national scale meaning there is a challenge to ensure that end users such as small companies, rural communities or otherwise marginalized groups benefit from EO systems. This project explores an EO application with the enterprise Green Keeper Africa (GKA) based in Cotonou, Benin, that addresses the management of an invasive plant species that is threatening local economic activities such as fishing. GKA helps control the infestation of the water hyacinth on Lake Nokoue by repurposing the plant into a product that absorbs oil-based waste. The EO application is an online Environmental Observatory that utilizes satellite, aerial and ground data to map the location of the water hyacinth over time, providing valuable information for government, private and public users. The research outcomes presented in this project address processes that (i) outline the steps for a small company in Benin to setup and operate a new EO technological capability, and (ii) enable low cost data collection of parameters describing the coastal water ecosystem. In the observatory, the technique Normalized Difference Vegetation Index (NDVI) is applied to free satellite data to identify likely locations of the hyacinth in the target region of Lake Nokoue. ",,['space-enabled'],https://dam-prod2.media.mit.edu/x/2019/04/19/densehyacinth (2)_CCrhjLv.jpg,371832,,2020-10-23T21:45:19.758Z,"['ufuoma@media.mit.edu', 'drwood@media.mit.edu']",True,technology-design-and-assessment-for-coastal-water-ecosystem-management-a-case-study-of-benin,2018-08-01,"['data', 'environment', 'technology', 'water']",Low-cost invasive species management in coastal ecosystems: A case study in Benin,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-04-19T17:27:19.493Z,"As bikes navigate city streets after dark, they are often equipped with lights. The lights make the bikes visible to cars or other bikers, and the hazards of traffic less dangerous.Imagine that as solitary bikes come together, their lights begin to pulsate at the same cadence. The bikers may not know each other, or may only be passing each other briefly, but for the moments they are together, their lights synchronize. The effect is a visually united presence, as groups of bikes illuminate themselves with a gently pulsing, collective light source.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/04/20/bikes-in-list-pulsating-cropped.gif,371840,,2020-10-28T15:46:11.990Z,"['kll@media.mit.edu', 'thomassl@media.mit.edu', 'aberke@media.mit.edu']",True,bike-swarm,2019-02-01,"['design', 'human-computer-interaction', 'architecture', 'art', 'civic-technology', 'networks', 'open-source', 'privacy', 'transportation', 'urban-planning', 'wearable-computing', 'collective-intelligence', 'social-robotics', 'computer-science', 'community', 'technology', 'electrical-engineering', 'engineering', 'industry']",[bike] swarm,PUBLIC,http://aberke.com,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2019-04-19T18:04:15.683Z,"SpaceHuman is a soft robotics device designed to facilitate the exploration of environments with reduced gravity in a view of democratization and openness towards access to space and its exploration.  It is based on the idea that one day, people who have not received a long preparation and training, as happens today with the astronauts, will be able to have access to the space having a type of conformation and physical configuration that is not adapted to this kind of setting. The analysis of the unique seahorse's tail structure became the insight of the overall biomimetic design process. In fact, seahorse tail movement, gripping and protection to the seahorse while floating. Moreover, seahorses do not use their tails to swim; instead, they use them to grasp objects in their environment while they camouflage to hide from predators and hunts for prey. Flexibility and resiliency are key features that enable these behaviours.SpaceHuman is an additive prosthesis or otherwise definable as a ""supernumerary robot."" SpaceHuman will facilitate the use of space in zero gravity or reduced gravity restoring the right motion and balance of our body and assigning a new function to a part of our body that until now has not been fully exploited except for the transport of loads, our back. Users will thus be able to build a new poetics of the body and its movements within this radically different space through SpaceHuman, creating new scenarios of its application. Through air chambers specifically designed to be able to change their shape and bend along a reinforcing rib of the material, the people who will use SpaceHuman will be able to cling to useful surfaces inside orbital housing or in lunar or martian villages. ",2021-12-31,"['responsive-environments', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/04/19/IMG_9919.JPG,371848,,2023-09-06T22:39:17.193Z,"['joep@media.mit.edu', 'vsumini@media.mit.edu', 'muccillo@media.mit.edu', 'aekblaw@media.mit.edu']",True,spacehuman,2018-08-01,"['robotics', 'design', 'human-computer-interaction', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'prosthetics', 'chemistry', 'prosthetic-design']",SpaceHuman 2.0,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2019-04-19T18:13:19.675Z,"The Tidmarsh Living Observatory Portal is a research project that focuses on the design and fabrication of a pavilion that will generate an immersive physical telepresence experience of this enchanting natural wetland.The Tidmarsh site has been restored from a former cranberry farm to natural wetland and, through extensive Responsive Environments research that lasted decades, it has been possible to retrieve several environmental live data thanks to the networked and outdoor installed instrumentation.The portal will generate a real-time telepresence by recreating visuals and sounds representative of the site, streaming live data as well as past recorded data, allowing also a travel across time. The overall experience, for maximizing its immersion, is tailored for a single user and lasts about three to five minutes. The user will interact by selecting a location of the Tidmarsh map and a specific time.Architectural designThe architectural design of the pavilion is inspired by the idea of recreating an individual Tidmarsh ecosystem wherever the Portal is going to be located. Therefore, the bio-inspired egg shape enclosure represents the optimal metaphor of a micro-ecosystem within a macro-ecosystem.The research explored different ways of geometrically describing an egg shape using computational techniques to allow a parametric design flow while considering a user-centered design approach.Some of the  variables are the overall dimensions of the Portal and the different mathematical approaches to define a spatial egg geometry. The coded Rhino-Grasshopper algorithms allowed us to define a set of possible design solutions that respect some physical  boundary conditions  and optimize the visual experience.",2021-12-31,"['responsive-environments', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/10/17/Interior_Vale.jpg,371864,,2023-09-07T15:38:11.784Z,"['duhart@media.mit.edu', 'vsumini@media.mit.edu', 'bmayton@media.mit.edu', 'joep@media.mit.edu', 'ddh@media.mit.edu', 'patpat@media.mit.edu']",True,tidmarsh-living-observatory-portal,2019-04-19,"['design', 'human-computer-interaction', 'architecture', 'data', 'environment', 'health', 'learning-teaching', 'music', 'sensors', 'interactive', 'community', 'technology', 'covid19']",Tidmarsh Living Observatory Portal,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-19T21:20:27.071Z,"What is ELSA?ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.You can try some of the current ELSA bots in this online demo. How does ELSA work?Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.  More specifically, we build hierarchical recurrent neural network models that can converse like people  and use transfer learning of topic and emotional tone recognition models to improve our final model.What are the applications of ELSA?Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.  In particular,  we aim to make ELSA useful for:Eliciting journalingSuggesting behavioura interventionsUsing Cognition Behavioral TherapyDetecting individuals at risk of depression or suicideWork in progressELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online demo.   ",2020-09-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/2019/04/19/elsa.png,371884,,2021-10-07T16:49:07.200Z,"['jaquesn@media.mit.edu', 'asma_gh@media.mit.edu', 'judyshen@media.mit.edu', 'ncjones@media.mit.edu', 'fergusoc@media.mit.edu', 'agata@media.mit.edu', 'picard@media.mit.edu']",True,elsa,2019-03-03,"['computer-vision', 'artificial-intelligence', 'extended-intelligence', 'human-machine-interaction', 'machine-learning', 'affective-computing', 'natural-language-processing']","ELSA: Empathy learning, socially-aware agents",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-04-20T20:33:43.925Z,"A significant number of college students suffer   from   mental   health   issues   that   impact   their   physical,  social,  and  occupational  outcomes.  Various  scalable technologies  have  been  proposed  in  order  to  mitigate  the negative  impact  of  mental  health  disorders.  However,  the evaluation   for   these   technologies,   if   done   at   all,   often reports  mixed  results  on  improving  users’  mental  health. We need to better understand the factors that align a user’s attributes  and  needs  with  technology-based  interventions for positive outcomes. In psychotherapy theory, therapeutic alliance  and  rapport  between  a  therapist  and  a  client  is regarded as the basis for therapeutic success. In prior works, social robots have shown the potential to build rapport and a  working  alliance  with  users  in  various  settings.  In  this work, we explore the use of a social robot coach to deliver positive psychology interventions to college students living in on-campus dormitories. We recruited 35 college students to participate in our study and deployed a social robot coach in their room. The robot delivered daily positive psychology sessions   among   other   useful   skills   like   delivering   the weather  forecast,  scheduling  reminders,  etc.  We  found  a statistically  significant  improvement  in  participants’  psychological wellbeing, mood, and readiness to change behavior for improved wellbeing after they completed the study. Furthermore, students’ personality traits were found to have a significant association with intervention efficacy. Analysis of the post-study interview revealed students’ appreciation of the robot’s companionship and their concerns for privacy. In comparison to task-oriented health technological tools, a social robot has unique opportunities to build therapeutic alliance with its users and to leverage that rapport to further enhance the effectiveness of the interventions it provides.*Best Paper Award* at RO-MAN 2020",2021-01-31,['personal-robots'],https://dam-prod2.media.mit.edu/x/2020/09/03/jibo_wellness_station.png,371894,,2021-07-12T09:03:46.357Z,"['haewon@media.mit.edu', 'sooyeon6@media.mit.edu', 'aymerich@media.mit.edu', 'agata@media.mit.edu', 'picard@media.mit.edu', 'cynthiab@media.mit.edu', 'sharifah@media.mit.edu']",True,personalized-emotional-wellness-coach,2018-09-01,"['robotics', 'social-robotics', 'wellbeing']",Robotic Positive Psychology Coach for College Students,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2019-04-22T16:59:12.173Z,"Definition:combining formPrefix: allo-other; different.""allopatric""im·ag·i·na·tionnoun: imagination; plural noun: imaginationsthe faculty or action of forming new ideas, or images or concepts of external objects not present to the senses.allo-i(s) is a project exploring alternative imaginations through the use of interactive, immersive experiences.How can we leverage new technologies in low-cost environments to create location based experiences similar to theme-parks? Arwa is developing a mixed reality tool kit that can be used by creators to create immersive location based experiences at a low cost. She plays with projection mapping, AR/VR/XR, sensor technologies, theatre design, and narrative/story structures.We further investigate the socio-economic and mental health impact of immersive spaces in low income communities across Africa. ",2020-08-01,['civic-media'],,371915,,2022-06-10T13:35:30.126Z,['mboya@media.mit.edu'],True,allo-i-s,2019-01-01,[],allo-i(s),PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2019-04-22T17:40:25.062Z,"Biological neuronal networks are highly complex and interconnected with superior information processing capabilities. Such networks have previously served as a model for creating inorganic synapses [1] through silver compounds. However, efficiently generating such complex networks through conventional fabrication remains a challenge. Our mission objective was to grow nanometer/sub-micrometer scale silver dendrite networks in microgravity and characterize them for nanoscale manufacturing on Earth.",2020-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/04/24/Zero-G_oa6zvGS.gif,371924,,2023-09-08T14:43:55.838Z,['sareen@media.mit.edu'],True,floral-cosmonauts,2018-09-01,"['design', 'manufacturing', 'space', 'engineering', 'industry', 'fabrication']",Floral Cosmonauts: Self-assembling silver dendrite networks in microgravity,PUBLIC,http://www.harpreetsareen.com,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0
False,2019-04-22T18:38:27.348Z,"The Project on Affinities and Language (PAL) is designed to help us understand what happens in a child’s brain when they engage with their interests, passions, or hobbies – also known as “affinities.” These deep passions are common in all children, and they are especially prevalent in individuals with autism spectrum disorder (ASD). The PAL study using functional magnetic resonance imaging (fMRI) to view the brains of children aged 7-12 -- both with and without autism -- while they watch videos and listen to stories of their unique affinity. Each neuroimaging session is personalized to the child to ensure that we are truly capturing the neural response to deep interests. In particular, we are examining the brain response to language about one's affinity in a novel paradigm. The results of this study will inform educational and therapeutic paradigms for young children, especially those with autism.",2021-08-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2020/06/16/mcgovern-mri.png,371934,,2022-05-12T17:48:39.800Z,['ktj@media.mit.edu'],True,pal-project-on-affinities-language,2018-03-01,"['learning-teaching', 'neurobiology', 'language-learning', 'affective-computing', 'autism-research', 'diversity']",PAL: Project on Affinities + Language,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-04-22T21:45:17.487Z,"🚨Application to the 2022 Co-Design Experience here bit.ly/codesign2022 🚨In the Civic Media and Space Enabled groups, we are exploring new theoretical frameworks for design, and novel design education methodologies that serve a future celebrating the pluriversal (Escobar, 2018) rather than the universal. The “Co-Design Experience: Technology Design for Coffee Production” is a program exploring this concept. During IAP 2019, this experience  convened 16 participants from seven countries to explore technological, social, and business solutions for small-scale coffee production along with coffee growers in rural Colombia. The course was a unique, multidisciplinary, multicultural design experience in which people came together to co-design technologies and to connect with rural coffee growers inventive practices in rural Colombia.The course immersed participants into different agricultural practices, primarily coffee growing, as well as in the ontologies traditional to these practices. In an effort to expose participants to non-mainstream design methods and mechanisms of invention, our research team focused on surfacing local knowledge through research materials and hands-on activities.",2022-06-30,"['civic-media', 'space-enabled']",https://dam-prod2.media.mit.edu/uuid/6dcd869f-5a60-494a-a4e6-b3909b92b50e,371943,,2022-01-05T19:58:40.569Z,['pcuellar@media.mit.edu'],True,technology-design-for-coffee-production,2019-01-07,['design'],A Co-Design Experience: Technology Design for Coffee Production | 2019 Edition,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-04-23T04:52:25.608Z,"Automatic emotion recognition has become a well-established machine learning task in recent years. The sensitive and subjective nature of emotions may give rise to societal challenges manifesting from incorrect or misinterpreted predictions. In this work, we make the argument that emotion recognition models have an obligation to quantify their uncertainty (or similarly, provide confidence bounds). We provide demonstrations of how classical network architectures can be altered to give measures of epistemic and aleatoric uncertainty using established probabilistic inference techniques. We also explore what these uncertainties explain about the data and predictions and how it can reveal a lack of diversity in training data. We demonstrate how difficult and subjective training samples can be identified using these learned uncertainty measures.",2021-04-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2019/04/23/uncnet.png,371953,,2022-05-12T17:49:09.138Z,"['asma_gh@media.mit.edu', 'picard@media.mit.edu']",True,uncnet,2019-02-01,"['computer-vision', 'artificial-intelligence', 'extended-intelligence', 'human-machine-interaction', 'machine-learning', 'affective-computing', 'natural-language-processing']",UncNet: Modeling uncertainty in deep learning for inherently subjective tasks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-04-23T15:21:33.891Z,"Machines powered by artificial intelligence (AI) increasingly                           mediate our social, cultural, economic, and                           political interactions. Understanding the                           behavior of AI systems is essential to our                           ability to control their actions, reap their                           benefits, and minimize their harms. We argue                           this necessitates a broad scientific research                           agenda to study machine behavior that                           incorporates but expands beyond the discipline                           of computer science and requires insights from                           across the sciences. Here we first outline a                           set of questions fundamental to this emerging                           field. We then explore the technical, legal,                           and institutional constraints facing the study                           of machine behavior.",2020-07-01,"['human-dynamics', 'personal-robots', 'scalable-cooperation']",https://dam-prod2.media.mit.edu/x/2019/04/23/cover_2_landscape.png,371964,,2020-07-23T20:03:43.785Z,"['cynthiab@media.mit.edu', 'sandy@media.mit.edu', 'cebrian@media.mit.edu', 'nobradov@media.mit.edu', 'irahwan@media.mit.edu']",True,machine-behavior,2019-04-24,"['artificial-intelligence', 'human-machine-interaction', 'learning-teaching', 'social-science', 'machine-learning', 'behavioral-science', 'social-robotics']",Machine Behavior,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-04-24T18:22:16.664Z,"Metafluidics is an open source, community-driven repository that hosts digital design files, assembly specifications, and the bill of materials necessary for users to make and operate a fluidic device. The site enables the global microfluidics community, from trained scientists and engineers to hobbyists, students, and amateur makers alike, the ability to submit designs and reproduce and remix devices with the ultimate goal of democratizing microfluidics.This repository is the hardware portal for the National Science Foundation-supported Living Computing Project. Share your fluidic devices with the global community today!",,['community-bio'],https://dam-prod2.media.mit.edu/x/2020/06/16/Screen Shot 2020-06-16 at 12.50.25 AM.png,372014,,2020-06-16T04:51:51.981Z,"['dkong@media.mit.edu', 'desireed@media.mit.edu', 'bestrada@media.mit.edu']",True,metafluidics-openrepository,2017-01-01,"['microbiology', 'microfabrication']",Metafluidics: Open repository for fluidic systems,PUBLIC,https://metafluidics.org/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-04-24T19:41:32.347Z,"This project asks, how can we transmit experiences across space and time? The launching point of the project is to create a drawing that could only have been made in space, and to capture an emotional aspect of space travel, through an art piece based on notions of telepresence. For my payload, a mark making object (graphite) is placed in the nano lab, which is lined with paper.  As the payload makes its journey, the mark making object the MMO will float about making a unique drawing of its experience. A sensor inside the box will simultaneously record its movements and position. This data and footage will be used to recreate the flight paths and movements of the graphite in an identical “sister” box with a CNC back on Earth, post flight. This project occurs in two stages, and involves [1] the capture of an object’s experience in zero gravity through a sensor and a drawing made in zero g, and [2] the recreation of that object’s flight experience back on Earth through a robotic arm.The resulting objects for exhibition would include the original box that went up for the launch, the drawings inside of it, and the ""sister"" box that shows the recreated movements of the graphite back on Earth.",2020-09-30,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/04/29/Ani Liu Space Drawing 3.PNG,372031,,2023-09-08T14:45:01.706Z,['wonder@media.mit.edu'],True,telepresent-drawings-in-space,2017-05-01,"['agriculture', 'architecture', 'art', 'manufacturing', 'space', 'engineering', 'industry']",Telepresent Drawings in Space,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-04-24T20:47:07.533Z,An environment of plants and mirrors that extends beyond the terrarium walls. ,,['city-science'],https://dam-prod2.media.mit.edu/x/2019/04/24/traces.png,372039,,2019-11-24T18:14:17.597Z,['aberke@media.mit.edu'],True,ar-enhanced-wall-plants-escape-pod,2018-11-30,"['agriculture', 'architecture', 'augmented-reality', 'energy']",AR Enhanced Wall Plants: Escape Pod,PUBLIC,http://aberke.com,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2019-04-27T16:54:59.619Z,"EBIFA (Everything Beautiful Is Far Away) is a crystalline robotic device that carried a tooth to outer space. EBIFA's form and function follow an unusually personal approach to our technological space futures, one centered on visceral, active, empathic, and poetic engagement.",2021-12-31,['space-exploration'],"https://dam-prod2.media.mit.edu/x/2019/04/29/Living Distance_1_Credit Tim Saputo, Xin Liu.jpg",372064,,2023-09-08T14:47:16.080Z,"['xxxxxxin@media.mit.edu', 'gershon@media.mit.edu']",True,living-distance,2018-09-03,"['robotics', 'architecture', 'art', 'manufacturing', 'performance', 'space', 'alumni', 'engineering', 'industry']",The EBIFA robot (Everything Beautiful Is Far Away),PUBLIC,http://xxxxxxxxxinliu.com,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2019-04-28T04:18:36.729Z,"Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/10/28/digital-dna.gif,372101,,2019-10-29T00:56:44.618Z,"['rosello@media.mit.edu', 'patpat@media.mit.edu', 'amores@media.mit.edu', 'abyjain@media.mit.edu', 'avujic@media.mit.edu', 'pattie@media.mit.edu', 'dkong@media.mit.edu']",True,Biological-Enhancement,2019-02-03,"['health', 'synthetic-biology', 'wearable-computing', 'space', 'biotechnology', 'wellbeing']",Theme | Wearable Biotech Enhancement,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-04-29T22:11:34.928Z,"Automatic and real-time sleep scoring is necessary to develop user interfaces that trigger stimuli in specific sleep stages. However, most automatic sleep scoring systems have been focused on offline data analysis. We present the first, real-time sleep staging system that uses deep learning without the need for servers in a smartphone application for a wearable EEG. We employ real-time adaptation of a single channel Electroencephalography (EEG) to infer from a Time-Distributed Convolutional Neural Network (CNN). Polysomnography (PSG)—the gold standard for sleep staging—requires a human scorer and is both complex and resource-intensive. Our work demonstrates an end-to-end, smartphone-based pipeline that can infer sleep stages in just single 30-second epochs, with an overall accuracy of 83.5% on 20-fold cross validation for 5-stage classification of sleep stages using the open Sleep-EDF dataset.",2019-09-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/05/09/Screen Shot 2018-10-26 at 7.07.19 PM copy.png,372129,,2023-03-23T17:57:17.697Z,['amores@media.mit.edu'],True,sleep-staging-EEG,2018-09-01,[],Real-time Smartphone-based Sleep Staging using 1-Channel EEG and Machine Learning,PUBLIC,http://www.judithamores.com,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-05-06T11:49:38.382Z,"Researchers currently rely on ad hoc datasets to train automated visualization tools and evaluate the effectiveness of visualization designs. These exemplars often lack the characteristics of real-world datasets, and their one-off nature makes it difficult to compare different techniques. In this paper, we present VizNet: a large-scale corpus of over 31 million datasets compiled from open data repositories and online visualization galleries. On average, these datasets comprise 17 records over 3 dimensions and across the corpus, we find 51% of the dimensions record categorical data, 44% quantitative, and only 5% temporal. VizNet provides the necessary common baseline for comparing visualization design techniques, and developing benchmark models and algorithms for automating visual analysis. To demonstrate VizNet's utility as a platform for conducting online crowdsourced experiments at scale, we replicate a prior study assessing the influence of user task and data distribution on visual encoding effectiveness, and extend it by considering an additional task: outlier detection. To contend with running such studies at scale, we demonstrate how a metric of perceptual effectiveness can be learned from experimental results, and show its predictive power across test datasets.",2019-08-31,['collective-learning'],,372223,,2020-08-24T21:13:07.183Z,['kzh@media.mit.edu'],True,viznet-towards-a-large-scale-visualization-learning-and-benchmarking-repository,2018-06-01,"['human-computer-interaction', 'data-visualization']",VizNet: Towards A Large-Scale Visualization Learning and Benchmarking Repository,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-05-08T18:12:29.122Z,"Plants can sense the environment, other living entities and regenerate, actuate or grow in response. Our interaction and communication channels with plant organisms in nature are subtle - whether it be looking at their color, orientation, moisture, position of flowers, leaves and such. This subtlety stands in contrast to our interactions with artificial electronic devices that are centered in and around the screens, requiring full attention and induce cognitive load.  We envision bringing such interaction out from the screens back into natural world around us. Beyond external indicators, plants also have electrochemical signals and response mechanisms inside them that make them very similar to our electronic devices. To tap into such capacities already built in nature, we propose a new convergent view of interaction design. Our goal is to merge and power our electronic functionalities with existing biological functions of living plants. Through Cyborg Botany, we re-appropriate some of these natural capabilities of plants for our interactive functions. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/05/08/titlegid-nosub.gif,372261,,2019-05-09T14:53:38.490Z,"['pattie@media.mit.edu', 'sareen@media.mit.edu']",True,cyborg-botany,2017-01-03,"['robotics', 'design', 'human-computer-interaction', 'agriculture', 'bioengineering', 'environment', 'interfaces', 'biology', 'technology', 'biotechnology']","Cyborg Botany: Augmented plants as sensors, displays, and actuators",PUBLIC,http://harpreetsareen.com,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-05-09T16:37:40.721Z,"We present an interactive virtual reality (VR) experience that uses biometric information for reflection and relaxation. We monitor in real-time brain activity using a modified version of the Muse EEG and track heart rate (HR) and electro dermal activity (EDA) using an Empatica E4 wristband. We use this data to procedurally generate 3D creatures and change the lighting of the environment to reflect the internal state of the viewer in a set of visuals depicting an underwater audiovisual composition. These 3D creatures are created with the aim to unconsciously influence the body signals of the observer via subtle pulses of light, movement, and sound. Our goal is to decrease heart rate and respiration by subtle, almost imperceptible light flickering, sound pulsations, and slow movements of these creatures to increase relaxation.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/05/09/Screen Shot 2019-05-09 at 1.13.07 PM.png,372302,,2020-11-19T21:13:59.200Z,"['amores@media.mit.edu', 'afuste@media.mit.edu', 'richer@media.mit.edu']",True,deep-reality,2017-04-01,"['virtual-reality', 'human-computer-interaction', 'augmented-reality']","Deep Reality: An underwater VR experience to promote relaxation by unconscious HR, EDA, and brain activity biofeedback",PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-05-10T22:52:37.261Z,"​The lotus flower is an ancient symbol that has been associated with spiritual awakening or enlightenment.In yoga and meditation, the lotus flower has been used as a symbolic support for the mind. The heart can be visualized as a lotus flower unfolding at the center of the chest. In Egyptian mythology, Nefertem was the lotus god of healing and perfume. Inspired by these mythologies, symbols, and practices, we created Lotuscent.",,[],https://dam-prod2.media.mit.edu/x/2019/05/10/2019-05-07_dHRHr3x.jpg,372343,,2019-12-23T05:32:01.263Z,"['amores@media.mit.edu', 'pattie@media.mit.edu']",True,lotuscent,2018-11-01,[],Lotuscent: Targeted memory reactivation for wellbeing using scent and VR biofeedback,PUBLIC,http://www.judithamores.com,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-05-15T20:05:29.588Z,"Unfolding the way we move. Mobility has shaped the built environment since humans started settling together. From industrial towns to post-industrial innovation and service hubs, the mobility mode of the era was key in shaping not only the physical attributes of cities, but also the efficacy. In order to allocate the massive migration from rural areas, cities are growing and becoming more dense. Although high density can minimize transportation cost and energy, several problems start to appear if they are not planned carefully. Urban ventilation potential is reduced and open seen spaces are limited, compromising our experience and life quality. Residential, office, and retail get closer but remain arranged in conventional ways. A two-dimensional street that organizes the way we live and keeps transportation methods are in permanent conflict. Too fast for those who live in it, and too slow and congested for those that go by.Urban mobility is becoming more electric, more autonomous, more shared, and more connected, indicators that call for a mobility revolution, and designers have the chance to reinvent the way city is experienced.Today, more than ever, the scale and rate of urban expansion is making mobility solutions a key concern, which will impact large segments of the global population since it is estimated that by 2050, more that two thirds of the global population will be living in cities. We  propose a new experience and mobility around cities, unfolding the city networks and using its third dimensions, different mobility, speed modes (static, mass transportation, internal transportation), public areas appearing in rooftops, and mix-use spaces in intersticial parts of buildings. Through simulation as a tool, we can understand the impact of this new disrupting mobility system, avoiding to repeat mistakes like those made in the past.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/05/15/Screen Shot 2019-05-15 at 4.08.22 PM.png,372384,,2019-06-12T14:47:59.274Z,"['gbabio@media.mit.edu', 'kll@media.mit.edu', 'agrignar@media.mit.edu']",True,3d-mobility,2018-07-09,[],3D Mobility,PUBLIC,https://guadalupebabio.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2019-05-16T19:03:21.098Z,"College of Design and Innovation of Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform to support the urban decision making that promotes urban vibrancy and innovation potential. The “NICE2035 LivingLine” project in Shanghai, China, is a design-driven, community-based urban innovation initiated by Professor Yongqi Lou, Dean of College of Design of Innovation. LivingLine is a crowdsourcing and co-creation project aiming at building an ecosystem of innovation and entrepreneurship on the internal street of a typical gated residential neighborhood. By introducing radical programs such as living labs, co-working space, and startup-incubators into underutilized storefront space, LivingLine’s goal is to revitalize the urban space and to prototype diverse future lifestyles.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/05/16/BQ8A3018.JPG,372408,,2021-06-24T20:12:48.671Z,"['kll@media.mit.edu', 'thomassl@media.mit.edu', 'liuymit@media.mit.edu', 'ryanz@media.mit.edu', 'chenhan@media.mit.edu', 'markuse@media.mit.edu', 'alonsolp@media.mit.edu']",True,cityscope-livingline-shanghai,2018-03-01,[],CityScope LivingLine Shanghai,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-05-16T21:53:44.500Z,"A molecular gastronomy experiment was flown on a zero gravity flight in August 2019, using spherification techniques to create recipes in zero g (e.g. Algae Caviar). Molecular Gastronomy in Zero G explores new modes of preparing food in space. Studying how food is created and consumed in zero gravity can help shed light on how our experiences of food extend far beyond mere nourishment. Food is not simply fuel—it’s part of what makes us human. Debriefs with astronauts tell us that food is a key creature comfort in spaceflight, and it will play an even more significant role on long duration spaceflight and future space habitats. The current space food system offers some variability in menu items, but does little outside of sustenance. It’s freeze-dried and pre-packaged in ways consistent with the demands of present day space travel. For longer trips these self-contained meals will be detrimental to astronauts’ mental health. Promoting new culinary techniques and interactions between different sensory modalities will assist in improving the experience of food in space.",2019-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/09/03/ZG492_steve-boxall-20190815_0590 copy.jpg,372417,,2023-09-06T22:36:44.925Z,['mcoblent@media.mit.edu'],True,molecular-gastronomy-in-zero-g,2018-09-01,"['design', 'virtual-reality', 'food', 'space', 'technology', 'wellbeing', 'zero-gravity']",Molecular Gastronomy in Zero G,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-05-16T21:56:25.084Z,"This research highlights opportunities to bring the rich culture of Earth-based fermentation practices to space to design beneficial applications for astronauts and novel ways of managing waste. To support future crews on long duration deep space missions, they will require advanced space food systems to provide nourishment and improve the quality of limited fresh ingredients. In space, fermentation could be leveraged to repurpose food waste for a closed-loop system, preserve limited fresh ingredients, diversify food selection, grow nutrients, and improve astronaut gut health.Fermentation is one of the oldest methods of food preservation and preparation. Fermentation gives food a variety of sensory attributes, such as flavors and textures, and nutritional values. In food production, fermentation is the process of transforming organic substrates such as proteins, carbohydrates, lipids, or other types of organic material through the action of enzymes produced by different microorganisms. Fermentation is largely impacted by the environment with temperature, humidity, and air quality being main factors. It is fascinating to imagine how the environment of space could uniquely alter this process. As space agencies prepare for a new era of space exploration, and future long duration missions to the Moon or Mars, they will need to address the complex requirement of providing crews with safe, nutritious food for survival. Crews will not be able to bring everything with them and will need to learn how to produce their own food from limited resources. To achieve sustainable food systems in closed loop life support applications such as space vehicles and habitats, we propose fermentation-based food production technology and systems. ",2022-08-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/05/04/spaceFermentation_MLpage.jpg,372425,,2023-09-21T15:24:12.360Z,['mcoblent@media.mit.edu'],True,space-fermentation,2019-01-01,"['food', 'space', 'zero-gravity']",Space Fermentation,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-05-21T08:23:37.703Z,"In honor of May being Mental Health Awareness month, the collaboration between Footwear x Mental Health needs YOUR participation in a call to action.According to the National Alliance on Mental Illness (NAMI), approximately 46.6 million adults in the US live with a mental illness and half of all lifetime mental health conditions begin by age 14 and 75 percent begin by age 24 [1]. Additionally, African Americans are “20 percent more likely to experience serious mental health problems than the general population” [2].Barriers to treatment and engagement for African Americans include lack of cultural competence by health professionals, and shame and stigma within the community surrounding the topic. Other barriers affecting engagement with mental health treatment include an inability or unwillingness to use creative and innovative approaches to engagement, and an inability to work effectively within and across diverse cultures [3].This is why we need your help in unleashing the power of footwear to prove it’s more than just a shoe or a product. Sneakers impact communities.[Embed video]To participate:Post a sneaker (designed, already owned, or one you admire) on Twitter, along with one to two sentences related to how it links to mental health and wellness. You may answer one of the three prompts below or share thoughts of your own:How does this sneaker inspire you or a friend in overcoming a difficult situation?Describe how this sneaker makes you feel.How can footwear provide an innovative approach to destigmatizing mental health?Include in your post the hashtags: #FootwearxMentalHealth and #BlackMentalHealthMatters (or #MentalHealthAwareness ).Tag two people in your post, and encourage them to also share how their kicks collaborate with mental health and wellness.The content you share will be reviewed to see how footwear may assist in destigmatizing the conversation surrounding mental health.For more mental health resources, please visit:The National Alliance on Mental IllnessNational Institute of Mental HealthSilence the ShameAsk Dr. Jess",,['viral-communications'],,372478,,2019-05-22T14:21:12.337Z,['britneyj@media.mit.edu'],False,footwear-x-mentalhealth,2019-05-21,[],Footwear x Mental Health: A Collaboration,PUBLIC,,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-05-21T17:56:12.221Z,"Beautiful Me has been designed to address the problem of acid attacks in southeast Asian countries and parts of Europe. This form of assault is done primarily to disfigure someone's body or in an attempt to kill them. As there are very few legal checks pertaining to the buying and selling of acid, we have developed a foundation (topical cream) which can act as a barrier against the acid. The foundation formula creates a thin film, which reacts with the acid to reduce its effects and prevents the acid from penetrating to the skin. This first layer of defense is oleophilic,  hence the acid can be washed off easily. ",2019-06-01,[],https://dam-prod2.media.mit.edu/x/2019/05/21/OLEOPHILIC TISSUE ACID- COMPARISON.jpg,372490,,2021-07-15T18:23:11.911Z,['manisham@media.mit.edu'],True,beautiful-me,2016-01-01,[],Beautiful Me,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2019-05-23T16:51:54.955Z,"The MIT Media Lab Space Exploration Initiative is pushing the boundaries of space exploration innovation across the Media Lab’s many healthcare-focused and healthcare-adjacent research thrusts. The program focuses on the protection, long-duration preservation, and predictive adaptation of life beyond the earthbound, integrated throughout the Space Exploration Initiative’s portfolio. This research platform builds towards a near future where space-faring humans can both survive and thrive wherever the future takes us—back and forth to the surface of Earth, to low Earth orbit, to the moon, Mars, and beyond. In addition to technology for the sake of deep space exploration, we note a dual opportunity for the technologies developed as part of this program to enhance and improve healthcare treatment regimens on Earth, in the long tradition of NASA spinoffs yielding benefit to wider populations. We aim to jumpstart a creative, interdisciplinary approach to space healthcare innovation, in the spirit of the Space Exploration Initiative’s cross-cutting technological development across multiple fields (from synthetic neurobiology to human-robotic interaction and AI).Research AreasNeural Oscillations. The Alzheimer's research aims to use non-invasive sensory-induced gamma entrainment to attenuate space-induced physiological impairments. On an upcoming parabolic flight, we will test the effect of microgravity on gamma entrainment in humans. Mouse models will be used to evaluate the microglial response to sensory-mediated gamma entrainment. Microglia are macrophage-like immune cells in the central nervous system that scavenge extracellular debris, prune synapses, and support neural function. Tardigrade Cryptobiosis. Explores the feasibility of reverse engineering Tardigrade species cryptobiosis capabilities (i.e., desiccation, radiation and temperature-swing resistance) into other organisms, to “flip the paradigm” of organism survival in space—make the organism inherently space-tolerant, rather than relying strictly on life support systems.Space Food. How can we best meet the nutritional, performance, and emotional needs of astronauts through food? Our space food research area aims to address the unique challenges associated with eating in space—from the microbiome scale to the “envirome” scale—including fermentation and probiotics,  improving waning or shifting appetite, and preservation of freshness and nutrient quality. This research area explicitly addresses earth-based markets as well, as the foods  and eating experiences developed for space can be re-used in many Earth contexts. Personal Robots in Space. Can we enable social connectivity between astronauts and people on Earth through an embodied agent? While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.Sensory Synchrony. The primary goal of this research project is to investigate vestibular system stimulation techniques to combat motion sickness and create more intuitive experiences when being in a non-natural gravity environments. A prototype built for multipole vestibular stimulation for simulating acceleration in roll and pitch axis will be tested on the upcoming zero gravity flight for minimizing the effects of alterations between micro and hyper gravity phases.Physio Freefall. This project seeks to examine the effects of altered gravity on an individual’s physiology during parabolic flight. Specifically, we will collect flight participants’ heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea.Mediated Atmospheres in Space. Designing the atmosphere and sensorial qualities of physical space can have a remarkable influence on human experience and behavior. This project envisions a workspace or space station that is capable of dynamically transforming to enhance occupants’ work experience and cognitive ability, via both subtle and overt customizations tailored to bio-signal inputs.Testing and Product DevelopmentZero Gravity Flight: We charter an annual parabolic flight (20 zero-g parabolas, 25 researchers, 15 experiments), with a focus on prototypes uniquely designed for the affordances of microgravity. Research Collaboration with NASA Translational Research Institute for Space Health (TRISH): We are actively developing a suite of health-focused prototypes that will mediate human interaction with interior space habitats to improve cognitive performance and overall wellness.  ISS Interior and External Deployment Tests: Coming in the next 12 months.",,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/05/24/Screen Shot 2019-05-23 at 1.41.39 PM_QYuZlik.png,372501,,2020-11-16T19:58:59.499Z,['aekblaw@media.mit.edu'],True,space-health,2019-05-01,['space'],Space Health,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2019-05-29T16:50:24.576Z,"​The past two decades have seen unprecedented progress in the development of novel materials, form factors, and functionalities in neuroimplantable technologies, including electrocorticography (ECoG) systems, multielectrode arrays (MEAs), Stentrode, and deep brain probes. This review highlights the key considerations for the development of such devices intended for acute implantation and chronic use, from the perspective of biocompatible hybrid materials incorporation, conformable device design, implantation procedures, and mechanical and biological risk factors. These topics are connected with the role that the US Food and Drug Administration (FDA) plays in its regulation of neuroimplantable technologies based on the above parameters. Existing neuroimplantable devices and efforts to improve their materials and implantation protocols are first discussed in detail. Then the effects of device implantation with regards to biocompatibility and brain heterogeneity are explored. Topics examined include brain-specific risk factors, such as bacterial infection, tissue scarring, inflammation, and vasculature damage, as well as efforts to manage these dangers through emerging hybrid, bioelectronic device architectures. The current challenges of gaining clinical approval by the FDA—in particular, its relationship to biological, mechanical, and materials risk factors—are summarized. This work concludes by discussing the available regulatory pathways to accelerate next-generation neuroimplantable devices to market.",,['conformable-decoders'],https://dam-prod2.media.mit.edu/x/2019/06/18/CD-slide-6.png,372557,,2019-06-22T23:07:21.354Z,"['canand@media.mit.edu', 'farita@media.mit.edu']",True,the-future-of-neuroimplantable-devices-a-materials-science-and-regulatory-perspective,2019-06-20,"['design', 'bioengineering', 'health', 'neurobiology', 'public-health', 'engineering', 'neural-interfacing-and-control']",The Future of Neuroimplantable Devices: A materials science and regulatory perspective,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-05-31T18:08:08.491Z,"The Hol-Deep-Sense project (funded by the EU’s Horizon 2020 Marie Skłodowska-Curie grant) aims at holistic machine perception of human phenomena such as personal attributes (e.g., age, gender), emotion, health, as well as cognitive and physical states. The machine learning methods developed in this project help personalize AI technologies and enable natural human-machine communication. In particular, this project addresses the shortcoming of today's recognition systems that regard affective states as isolated patterns. Using novel multi-task and transfer learning techniques, we aim to shed light on the interrelations between the facets of human phenomena. The overarching goal of the Hol-Deep-Sense project is to create an end-to-end, multi-input, and multi-output learning framework that learns from multi-modal sensory inputs (e.g., audio, visual, physiological signals), an acoustic model to jointly recognize multiple output targets.",2020-04-15,['affective-computing'],https://dam-prod2.media.mit.edu/x/2019/05/31/holistic.png,372609,,2020-01-02T15:36:09.895Z,['yuefw@media.mit.edu'],True,hol-deep-sense,2018-10-15,[],HOL-DEEP-SENSE,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2019-06-06T14:46:00.074Z,"NeuroKnit is the interplay between the physical and virtual that is explored in response to the current lack of culture, expression, and emotions in VR  experiences; we propose a two-fold solution. First, the integration of bio-signal sensors into the HMD and techniques to detect aspects of the emotional state of the user. Second, the use of this data to generate expressive avatars.Scroll down to see more about the project.",2022-05-07,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/06/06/P1220169.JPG,372705,,2022-05-11T19:06:21.447Z,"['gbernal@media.mit.edu', 'pattie@media.mit.edu']",True,neuroknit,2017-07-01,[],Emotional Beasts Parte Dos: NeuroKnit,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-06-07T19:23:25.195Z,"Every day, data about our environment, education, routes, and other aspects of our lives become available. Yet most people often struggle to understand, use, and relate to these data, especially because the access to all this data is often in formats that only few can retrieve, read, and understand.In this project, we seek to disrupt the way we present and interact with datasets, using art like dancing and acting as vehicles to tell our stories.  Data Moves is a data experience. “A data experience takes data off the screen and puts it into the physical world” [Perovich L., 2015]. This project explores the ways in which data can be physicalized through human expression. We are working with members of the community to co-compose dance performances informed by datasets and validate the use of dance and acting as methods to enhance data literacy.",2020-05-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2019/06/07/ahmad-odeh-761627-unsplash.jpg,372810,,2020-04-15T14:47:56.606Z,"['ethanz@media.mit.edu', 'rahulb@media.mit.edu', 'mavipasi@media.mit.edu', 'perovich@media.mit.edu']",True,dancing-data,2019-05-01,[],Data Moves,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-06-15T23:02:38.051Z,"How do you image a city that doesn't exist? DeepScope is a novel platform for interactive, real-time, and setup-less urban design visualization. It attempts to substitute common practices of urban design with a machine-learnt, generative visualization approach. By implementing a deep convolutional generative adversarial network (DCGAN) and CityScope, a tangible user interface, this project  allows for real-time prototyping and visualizations of urban design processes. ",,['city-science'],https://dam-prod2.media.mit.edu/uuid/fd33c431-8d26-4dea-892f-30b9a433ebe5,373242,,2020-03-02T15:09:21.748Z,"['noyman@media.mit.edu', 'kll@media.mit.edu']",True,deep-image-of-the-city,2019-02-01,"['design', 'human-computer-interaction', 'architecture', 'artificial-intelligence', 'urban-planning', 'machine-learning', 'visualization']",The Deep Image of the City,PUBLIC,http://ArielNoyman.com,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2019-06-19T18:53:20.339Z,"The Democratizing Science project is opening up MIT Media Lab research in the form of STEAM learning programs in three elemental areas: ocean, earth, and space.These creative, Media-Lab style programs will challenge and excite young people as they engage in learning experiences that are antidisciplinary, interest-based, and hands-on.Democratizing Science is a collaborative project from ML Learning, Open Ocean, OpenAg, and the Space Exploration Initiative. Each of our three elemental focus areas is based on current Media Lab research. You can learn more at our project website: learn.media.mit.edu/science.",2020-08-01,['ml-learning'],https://dam-prod2.media.mit.edu/x/2019/06/19/elements-shapes-square2.png,373454,,2022-06-10T14:06:49.115Z,['kamcco@media.mit.edu'],True,democratizing-science,2018-05-01,['learning-teaching'],Democratizing Science,PUBLIC,https://learn.media.mit.edu/science/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2019-06-29T18:49:02.633Z,"Moments is a prototype designed for users to save special moments and to experience those moments in the future. Firstly, a multi-sensor equipment is designed to collect environment data in user’s daily life. Secondly, the data integration system can collect relevant information based on both photos and user data from an open data platform. At last, an immersive virtual reality environment is provided for the user to explore his or her past experiences.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2019/09/11/appScene3.jpg,373608,,2019-09-23T21:26:29.388Z,"['joep@media.mit.edu', 'fredjf@media.mit.edu']",True,moments,2018-10-15,"['design', 'virtual-reality', 'sensors']",Moments,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-07-08T00:54:31.369Z,"For more details and recent updates visit:  https://web.mit.edu/deblina-sarkar/We are developing nano-devices using meta-materials that can non-invasively and remotely monitor and modulate our biological system. The requirements of the system are: 1) they should be as small as possible such that the volume displacement of tissue due to the placement of the device is minimal, and 2) they should be untethered/wireless such that they can be remotely controlled. Such a device will sense the biological environment and send the information to a system outside the body in real time. The device will also have the capability to do internal analysis of the sensed data and depending on the analysis results, take further action such as electrical stimulation or drug delivery. The device will harvest energy from external applied fields for its functioning and also modulate the external fields for communicating sensed data.The possibilities with such bioelectronic devices are endless, and we are exploring, among others, brain activity recording at a large scale with precision of a single neuron, activity recording in spinal cord and peripheral nervous systems, monitoring tumor microenvironment, observing response to pathology development or external stimulus at a single cell level, along with integrated functionalities such as stimulation and drug delivery.This work is recognized by the National Institute of Health with  a perfect and rarely achieved impact score and the NIH Director's New Innovator Award. We achieved an impact score of “10”—the highest score possible!",,['nano-cybernetic-biotrek'],https://dam-prod2.media.mit.edu/x/2022/04/10/PS2.jpg,373744,,2023-08-17T20:11:51.380Z,['deblina@media.mit.edu'],True,wireless-sensing,2019-07-07,"['synthetic-biology', 'nanoscience']",Nano-Implants for Wireless Brain Interfacing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-07-08T01:44:31.081Z,"An affective robotic ring which allows users to interact with multiple robots using finger gestures.- Runs on Robotic Operating System (ROS)- Affective: measures real-time electrodermal activity (EDA) of the user- Allows the user to select from multiple robots, and guide different actions through finger movements- Haptic feedback to user as they select various actions for a robot- Provides a user Interface on android app, to visualize the actions of multiple robots and real-time EDA readingA final project for the course ""Sensor Technologies for Interactive Environments""  (MAS.836).",2025-05-31,['personal-robots'],,373752,,2020-04-30T18:26:44.744Z,"['suryaj@media.mit.edu', 'manjola@media.mit.edu', 'tejwanir@media.mit.edu']",True,affective-robotic-ring,2019-04-01,"['robotics', 'artificial-intelligence', 'sensors', 'social-robotics', 'affective-computing']",Affective Robotic Ring,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-07-10T01:46:24.710Z,"In microgravity, the removal of constant loading on the skeleton leads to loss of bone density and muscle mass. In order to counteract this, humans living in microgravity must exercise extensively. Exercise devices exist on the International Space Station (ISS), including a treadmill, cycle ergometer, and resistive squat-type machine, but these devices generally consume a large volume of space in the ISS. For a long duration flight to Mars, space is far more limited—to a relatively small crewed capsule. This necessitates the design of more compact exercise device. Rowing ergometers offer the benefit of a diverse resistive and cardiovascular exercise in a compact space. We aim to estimate the kinematics of the rowing motion in microgravity using two techniques: optical motion capture and wearable inertial measurement units (IMUs). We will attempt to answer whether wearable IMUs are a feasible solution to human pose estimation in microgravity and how rowing form in microgravity changes—useful to inform exercise planning on future missions.",2019-09-30,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/07/09/IMG_20190705_171019.jpg,373794,,2019-12-02T16:38:45.676Z,[],True,microgravity-rowing,2018-09-01,"['space', 'technology', 'zero-gravity']",Microgravity Rowing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-07-10T02:03:44.387Z,"Life on Mars, if it exists, could have been transferred between Earth and Mars due to meteorite impacts. The Search for Extra-Terrestrial Genomes (SETG; setg.mit.edu) is under development to enable the search for nucleic acid (DNA, RNA) based life on Earth and for diverse other space-related applications (e.g., Carr et al. 2017). In our prior ZeroG experiment (November 2017) we demonstrated a robust sequencing capability during parabolic flight using a small commercial device (Oxford Nanopore MinION Mk-1B) weighing ~ 100 g and powered by a USB port. This device detects double stranded DNA or single stranded RNA after appropriate library preparation, which consists of many enzymatic steps to prepare the nucleic acids (DNA, RNA) for sequencing. Life beyond Earth may or may not use standard nucleic acids, thus, the astrobiology community seeks devices capable of detecting not only DNA or RNA, but nucleic acids or other informational polymers. In addition, targeting other biomolecules that are stable over geologic time, such as amino acids, is highly desirable.Here we propose to test an element of the Electronic Life-detection Instrument (ELI), specifically a solid-state single molecule detector. We propose to target amino acids and IPs, including nucleic acids, though note that, in principal, many other types of molecules could be detected. This versatility will support unambiguous life detection and detection of forward contamination. ELI relies on solid-state quantum electronic tunneling (QET) nanogap sensors (Fig. 1), which can detect and discriminate among single amino acids, and detect RNA and DNA, including bases and (in a very limited fashion) sequences.The research goals of the experiment are to 1) Quantify the impact of g-level on the nanogap device, and 2) Quantify changes in noise due to g-level and vibration. In addition, we aim to perform single molecule detection of amino acids.In the 2021 flight (originally planned for 2020), we are testing upgraded hardware that permits automatic real-time sub-nanometer gap control to improve the measurement fidelity of the system.",2019-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/uuid/ec0a5d18-f780-4106-bc6c-af2c26742725,373802,,2023-09-06T22:38:25.142Z,['aekblaw@media.mit.edu'],True,ELI,2018-09-01,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",ELI: Electronic Life-detection Instrument,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-07-10T02:19:57.435Z,"Mechanical actuators for deployable systems on satellites are often complex, expensive, only designed for a single use, and too large or heavy for use in nanosatellites. The Folded Lightweight Actuated Precision System (FLAPS) uses shape memory alloys (SMAs) to actively control the position of solar arrays on a CubeSat. SMAs “remember” distinct shapes when heated above their transition temperatures, selected at 90 °C in this project. These shapes can be characterized and trained by annealing at high temperatures (approximately 500 °C). The SMA strips are placed in opposition inside a hinge assembly, which connects the satellite bus to the actuated surface. The multi-use SMA actuators would allow the solar panels to take on different angles, providing power generation optimization and active drag management. Other potential FLAPS applications include aperture positioning, radiators deployment, and antenna steering. Going forward, FLAPS could also increase accessibility by decreasing cost, especially within the CubeSat format. Given the recent push for the democratization of space, these types of reliable, low-cost tools are important in making that access complete.Team MembersPaula do Vale Pereira - Graduate Student, Aeronautics and Astronautics Department, MITKatherine Chun - Graduate Student, Aeronautics and Astronautics Department, MIT",2019-09-30,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/08/13/FLAPS-Hinge.jpg,373810,,2019-12-02T16:39:54.632Z,[],True,FLAPS,2018-09-01,"['space', 'technology', 'zero-gravity']",FLAPS: Folded Lightweight Actuated Positioning System for CubeSat Deployables,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-07-11T19:32:18.988Z,"In 2015, the director general of the European Space Agency (ESA), Jan Wörner, introduced the concept of the “Moon Village.” Inspired by the unparalleled level of cooperation achieved by the International Space Station, the Moon Village represents an extension of this paradigm of deep space activities. It is a vision to apply international cooperative principles to exploration beyond low Earth orbit. The Massachusetts Institute of Technology, in collaboration with Skidmore, Owings & Merrill and the European Space Agency, are investigating concepts for the first permanent human settlement on the lunar surface. This collaboration aims to demonstrate the potential of an international private-public partnership to advance human space exploration through cross-disciplinary cooperation. The Moon Village project presents a holistic approach to the planning of a lunar development, centering on the need for habitation systems, designed as adaptive space environments to enable versatile surface operations. The team has proposed a design located near the South Pole where there is a convergence of resources such as near perpetual light, as well as water ice in the nearby permanently shaded craters, in order to maximize in-situ resource utilization.",2020-12-31,"['responsive-environments', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/07/11/Moon_Village_1.jpg,373865,,2023-09-07T15:42:41.670Z,"['vsumini@media.mit.edu', 'joep@media.mit.edu']",True,moon-village,2019-01-01,"['design', 'architecture', 'construction', 'space']",Moon Village,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-07-12T15:33:00.470Z,"Space/Craft explores sculpting in zero gravity. Making artistic works by hand is a fundamentally human act, but how will it transform in space? What non-existent forms of artistic expression does different gravity enable?Prior work has explored the adaptation of mark-making techniques, like painting, in zero gravity (e.g., Frank Pietronigro’s creativity chamber for drift painting, 1998). Less explored in zero gravity are sculptural works, although 3D printing has been shown to work normally in space (NASA technical publication, 2017).Digital modeling tools allow us to break the laws of physics as we create, but we can’t replicate those processes on Earth. Space/Craft will explore the artistic processes and possibilities enabled by zero gravity by using a hot glue gun to  ""draw in 3D.""  During each cycle of microgravity, the artist will use the mark-making tool to sculpt shapes inside of a containment cube. The thin strings of glue will float into forms that could not be created on Earth by the same process, and then cool in order to “freeze” in that position. The containment cube will be video recorded as the works are created, to be shared after the flight with communities of craft and design. The cooled sculptures will also be preserved so they can be exhibited after the flight. This project will inform reflective writing about the evolving nature of hand-craft in new environments that humans will likely inhabit in the future.",2019-12-31,"['civic-media', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/08/17/Screen Shot 2019-08-17 at 10.28.37 AM.png,373877,,2023-09-06T22:38:48.241Z,"['ahope@media.mit.edu', 'tsavas@media.mit.edu']",True,space-craft,2019-07-12,"['art', 'space', 'technology', 'zero-gravity']",Space/Craft,PUBLIC,http://www.alexishope.com,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-07-24T00:39:52.189Z,"See ACL 2020 video here!See in its GitHub repoSharing personal narratives is a fundamental aspect of human social behavior as it helps share our life experiences. We can tell stories and rely on our background to understand their context, similarities, and differences. A substantial effort has been made towards developing storytelling machines or inferring characters' features. However, we don't usually find models that compare narratives. This task is remarkably challenging for machines since they, as sometimes we do, lack an understanding of what similarity means. To address this challenge, we first introduce a corpus of real-world spoken personal narratives comprising 10,296 narrative clauses from 594 video transcripts (http://roadtripnation.com/). Second, we ask non-narrative experts to annotate those clauses under Labov's sociolinguistic model of personal narratives (i.e., action, orientation, and evaluation clause types) and train a classifier that reaches 84.7% F-score for the highest-agreed clauses. Finally, we match stories and explore whether people implicitly rely on Labov's framework to compare narratives. We show that actions followed by the narrator's evaluation of these are the aspects non-experts consider the most. Our approach is intended to help inform machine learning methods aimed at studying or representing personal narratives.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2019/12/05/Screen Shot 2019-12-05 at 11.52.27 PM.png,374038,,2020-07-05T04:28:41.277Z,"['belen@media.mit.edu', 'dkroy@media.mit.edu']",True,personal-narratives,2019-07-01,[],Personal narratives,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-07-26T15:48:09.639Z,"Opioid therapy is the cornerstone of management of pain in the ICU. However, opioids present numerous side effects and are highly addictive. In fact, it is estimated that over 130 Americans die every day from an opioid overdose. Adequate opioid therapy, personalized to each patient's needs, is therefore essential. Unfortunately, ICUs are frenetic environments and clinicians are often unable to make optimal decisions or to continuously adapt therapy in real time based on the evolving patient physiological state. To address many of these issues and augment physicians' decision making with information about what an optimal therapeutic approach may look like, we propose to leverage the latest advancements in artificial intelligence. Specifically, we focus on deep reinforcement learning, which can learn optimal state-action policies using training data that does not represent optimal behaviors. We are therefore able to train the machine learning model to recommend optimal opioid interventions using training data that does not contain optimal decisions.Opioid analgesia in the ICU is a complex decision problem influenced by multiple factors, and extensive work will be required to develop systems that can be deployed in real clinical environments.",2020-07-31,['affective-computing'],https://dam-prod2.media.mit.edu/uuid/c19436c6-5484-40a1-abac-9874faf155c5,374085,,2019-09-12T16:18:44.187Z,"['dlmocdm@media.mit.edu', 'picard@media.mit.edu']",True,deep-reinforcement-learning-for-pain-management,2019-07-01,"['artificial-intelligence', 'health', 'neurobiology', 'machine-learning', 'computer-science', 'technology', 'public-health', 'wellbeing', 'medicine']",Deep Reinforcement Learning for Pain Management,PUBLIC,http://www.daniellopez.eu,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-08-06T20:14:22.166Z,"We present Piezo-Acoustic Backscatter (PAB), the first technology that enables backscatter networking in underwater environments. PAB relies on the piezoelectric effect to enable underwater communication and sensing at near-zero power. Its architecture is inspired by radio backscatter which works well in air but cannot work well underwater due to the exponential attenuation of radio signals in water.PAB nodes harvest energy from underwater acoustic signals using piezoelectric interfaces and communicate by modulating the piezoelectric impedance. Our design introduces innovations that enable concurrent multiple access through circuit-based frequency tuning of backscatter modulation and a MAC that exploits the properties of PAB nodes to deliver higher network throughput and decode network collisions.We built a prototype of our design using custom-designed, mechanically fabricated transducers and an end-to-end battery-free hardware implementation. We tested our nodes in large experimental water tanks at the MIT Sea Grant. Our results demonstrate single-link throughputs up to 3 kbps and power-up ranges up to 10 m. Finally, we show how our design can be used to measure acidity, temperature, and pressure. Looking ahead, the system can be used in ocean exploration, marine life sensing, and underwater climate change monitoring.This work is funded by the Office of Naval Research.If you are interested in this, check out  our other ocean research projects:How to enable scalable and ultra-low power ocean IoT.Battery-free Underwater GPS.How to increase the throughput of PAB nodes using higher modulation schemes.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2019/08/15/backscatter-gopro-photo.jpg,374219,,2020-11-16T20:22:53.651Z,"['fadel@media.mit.edu', 'junsuj@media.mit.edu', 'osvyrd@media.mit.edu', 'jmuguira@media.mit.edu', 'rezagh@media.mit.edu', 'afzals@media.mit.edu']",True,oceans,2018-09-01,"['communications', 'data', 'energy', 'environment', 'networks', 'sensors', 'computer-science', 'mapping', 'technology', 'internet-things', 'ocean', 'water']",Battery-Free Subsea Internet-of-Things,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-08-07T21:21:18.930Z,"Bird is a hand-controlled pointing system that translates a user's finger movements and positions into the motion of a 3D pointer in a virtual space. The name Bird stems from its similarity to a computer mouse, named for an animal occupying the ground, evolved with the addition of a third degree of freedom into an animal occupying the air.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2019/10/05/workingvolumes.png,374232,,2022-06-10T14:09:49.368Z,['asimonso@media.mit.edu'],True,bird,2018-01-02,"['virtual-reality', 'interfaces']",Bird: a 3D Cursor for 3D Interaction in Virtual Reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-08-08T19:13:23.609Z,"The Power of WITHOUT is a research theme in the City Science group. The theme proposes that heavy infrastructure solutions are not financially feasible and cannot meet the needs of rapidly urbanizing communities. Instead, we must research and propose new lightweight and decentralized solutions to solve the massive challenges faced globally.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/01/28/Comp 1.gif,374265,,2023-10-10T17:23:21.175Z,"['kll@media.mit.edu', 'alonsolp@media.mit.edu', 'gbabio@media.mit.edu', 'maitanei@media.mit.edu', 'mdchurch@media.mit.edu', 'gba@media.mit.edu', 'leticiai@media.mit.edu']",True,power-of-without-1,2018-09-03,"['design', 'human-computer-interaction', 'architecture', 'art', 'civic-technology', 'data', 'developing-countries', 'energy', 'environment', 'food', 'open-source', 'systems', 'transportation', 'urban-planning', 'collective-intelligence', 'computer-science', 'mapping', 'data-visualization', 'marginalized-communities', 'data-science', 'water']",The Power of Without,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-08-09T19:50:28.572Z,"In Solange Knowles’ 2016 single “Don’t Touch My Hair” she explained the sensitivities of Black women like herself who don’t want people to touch her hair. She believes that people don’t understand the disrespect and violation of personal space this action indicates. Solange was not the beginning of this movement calling for respect for Black women and their bodies. Black feminist figures such as Toni Morrison and bell hooks have written extensive literature on the mistreatment of Black female bodies, and forms of media from children’s books to mobile games like “Hair Nah” have brought awareness to the issue of touching a Black woman’s hair without consent.  Despite this, a physically interactive experience addressing this issue hasn’t been explored. (Don’t) Touch My Hair is an interactive sculpture of a Black woman with an Afro. The interaction storyline will teach users how to ask for consent to touch her hair and how to be respectful of her response, whether it’s a yes or a no. This work will help teach users how to respectfully explore their curiosity of Black hair while appreciating the history of Black females who have championed against regulation of Black female bodies. More broadly, this project expands the conversation of consent in the context of personal space and physical intimacy to include the intersection of identity and hair.",2019-08-09,['object-based-media'],,374277,,2019-11-20T16:30:44.860Z,['mbuford@media.mit.edu'],True,don-t-touch-my-hair-exploring-consent-education-through-interactive-art,2019-06-09,[],(Don't) Touch My Hair: Exploring Consent Education Through Interactive Art,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0
False,2019-08-12T02:23:48.982Z,"Scratch Mondays is an initiative  to share the Scratch Day spirit to schools in Brazil. After having a big Scratch Day event in 2018 in my hometown, the year after I decided that it would be more interesting to support schools and teachers on their own effort to organize their own events.In April 2019,  I experimented together with collaborators in Brazil what I call the Scratch Mondays (Segundas de Scratch), gathering school teachers who want to learn about Scratch and organize a Scratch Day in their own school. For more than six weeks,  these educators were spending every Monday night together, experimenting Scratch, organizing their events and sharing ideas on how to practice creative computing with their students. After almost two months, I thought it was time to share with these educators something that could inspire them to keep exploring Scratch in many ways, even after the Scratch Mondays. So I prepared what I call the Scratch Day kit: a kit full of resources and materials that can inspire people to organize and create activities for a Scratch Day. The kit was homemade and pretty simple, but it was prepared with lots of love, paying attention to details: the materials in the box are things that they've experimented on Scratch Mondays. Others were complementary for the event and based on their interests along our meetings, making the kit not only inspiring but meaningful for these educators.Here are photos of the box and of the simple and happy night we had sharing it with teachers during a Scratch Monday. This is the  list of items for its first version. You’re very welcome to make comments and suggestions.NEXT EDITIONIn 2020, Scratch Mondays will be organized nationwide, as part of the ""Scratch & Creative Computing"" program lead by the Brazilian Creative Learning Network. HOW ABOUT YOU? Do you lead or know any initiative to promote and support schools in their Scratch Days?What do you think it would be useful in a Scratch Day kit?",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2019/12/13/fullsizeoutput_e19.jpeg,374288,,2023-05-25T21:05:27.469Z,"['carolcr@media.mit.edu', 'leob@media.mit.edu']",True,scratch-mondays,2019-04-10,"['design', 'kids', 'community']",Scratch Mondays,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-08-12T15:28:54.767Z,"How might zero gravity space affect silk spinning, and what benefits might such material properties provide for digital fabrication and additive manufacturing once back on Earth? Project ZG Stardust explores the broader theme of material formulation and fabrication in microgravity, particularly the spinning of silk by silkworms. Experimental research associated with the project focuses on the spinning dynamics of the silkworm Bombyx mori. We study how microgravity may impact the worms across all stages of life—egg, larva, cocoon, and adult moth—through changes in their spinning patterns pre- and post-flight. The goal is to implement insights and results in the design of fiber-based goods otherwise not producible on Earth while enabling natural metamorphosis.",2021-05-21,"['mediated-matter', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/08/14/wormvsmoth.jpg,374309,,2021-05-21T14:41:51.593Z,"['neri@media.mit.edu', 'fkraemer@media.mit.edu', 'josephk@media.mit.edu', 'ssunanda@media.mit.edu']",True,zg-stardust,2019-07-01,"['space', 'zero-gravity']",ZG Stardust,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2019-08-12T16:12:59.273Z,"The Lut Desert of Iran is one of the most similar regions to Mars. Close to that area, there are vernacular adobe architectures that are hundreds of years old which were built by primitive construction methods, but with interesting and complex geometry. Among all city structures, bathhouses represent the unique example of an introvert architecture that has a deep commonality with greenhouse closed-systems. A bathhouse not only isolates active interior life from the outer world, but also insulates the temperature and humidity from the extreme environment of the Lut Desert. It usually also has a closed clean water system, open-air water flow circulation, and water reservoir and safe power access, all features that should be present in a greenhouse design concept for Mars. The main architectural feature of the Persian bathhouse architecture is the presence of one or more domes as a roof system. These masonry domes, named Karbandi, are characterized by ease of construction and use of local materials, absolutely consistent with the In Situ Resources Utilization requirements for building on Mars. Indeed, this research focuses on exploring the structural performance of bathhouse architecture for designing a 3D printed regolith dome—a shelter to provide micrometeoroid and radiation shielding for a  greenhouse infrastructure on Mars.",2021-12-31,"['responsive-environments', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/08/12/persian_website_EQ66pXX.gif,374317,,2023-09-08T14:42:23.302Z,"['vsumini@media.mit.edu', 'joep@media.mit.edu']",True,persian-domes-for-human-space-exploration-on-mars,2019-06-01,"['design', 'architecture', 'construction', 'space']",Persian Domes for Human Space Exploration on Mars,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-08-12T18:38:22.308Z,"A multi-course tasting menu was flown on a zero gravity flight in August 2019.  Five specially crafted dishes were consumed in a custom space food helmet to investigate the sensory and physiological experience of eating in zero gravity. The space food helmet prototype is under development to become a personalized multi-sensory and fully immersive floating ""restaurant"" for astronauts and future space tourists. It will include image projection, scent, and sound, in addition to  interchangeable dishware and utensils uniquely designed for zero gravity. MenuSounds of Mirepoix. A special aroma was created in collaboration with researchers from the International Flavors and Fragrances (IFF). The scent of onions and vegetables sauteed in butter were diffused into a custom space food helmet while listening to the sounds of butter sizzling in a pan. Astronauts suffer from decreased appetites and this type of multi-sensory pre-meal experience could enhance their anticipation of a meal in enclosed space environments where food is pre-packaged and pre-prepared.  Champagne Toast. Tailor-made champagne flavored popping candy could remind us of our Earth-based food rituals and enhance mouthfeel in space, where most of the current space food is uniform in texture. Algae Caviar. Algae Caviar is a recipe made with ingredients that can be grown and prepared in space using molecular gastronomy spherification techniques to create refreshing bursts of flavor. A suite of algae-based recipes, that can be grown and created in situ, are being developed for enclosed self-reliant space habitats. Miso. Miso was taste tested in zero gravity to assess first-hand if umami (i.e. savory taste) could be harnessed to help intensify flavor perception in space. Astronauts report a decreased sense of taste in space, in part due to fluid shifts in the body. Learn more about Maggie’s ongoing fermentation research here. Silicone Bones. Food-safe silicone bones were tested as a new space utensil, to add a sense of play to eating in zero gravity. These dishes will be included in The Interplanetary Cookbook project.",2019-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/09/03/ZG492_steve-boxall-20190815_0729 copy 2.jpg,374328,,2023-09-06T22:37:41.697Z,['mcoblent@media.mit.edu'],True,tasting-menu,2018-09-01,"['design', 'virtual-reality', 'food', 'space', 'wellbeing', 'zero-gravity']",Tasting Menu in Zero G,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-08-14T17:12:05.708Z,"The RadioTalk corpus includes speech recognition transcripts sampled from talk radio broadcasts in the United States between October of 2018 and March of 2019. The corpus is intended for use by researchers in the fields of natural language processing, conversational analysis, and the social sciences. The corpus encompasses approximately 2.8 billion words of automatically transcribed speech from 284,000 hours of radio, together with metadata about the speech, such as geographical location, speaker turn boundaries, gender, and radio program information. The paper introducing it summarizes why and how we prepared the corpus, gives some descriptive statistics on stations, shows and speakers, and carries out a few high-level analyses.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2019/08/14/radio-2433311_1920.jpg,374352,,2019-11-08T18:07:25.421Z,"['wbrannon@media.mit.edu', 'dkroy@media.mit.edu', 'dougb5@media.mit.edu']",True,radio-talk,2018-04-01,"['civic-media', 'networks', 'politics', 'social-science', 'behavioral-science', 'technology', 'natural-language-processing', 'data-science']",RadioTalk,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2019-08-19T19:56:42.558Z,"Dr. Shah was an invited speaker and a contributor at The National Cancer Policy Forum developed workshop series at the The National Academies of Sciences, Engineering, and Medicine to examine strategies to improve cancer diagnosis and care.  The workshop was held on October 29 – 30, 2018 to examine the use of high-dimensional omics data and computational methods in precision oncology care.  Proceedings from the workshop was subsequently published by theThe National Academies of Sciences, Engineering, and Medicine.Talk title: Digital Clinical Trials for Oncology Patients with Novel Machine Learning and AI ArchitecturesTo examine opportunities to improve cancer diagnosis and care in the new precision oncology era, the National Cancer Policy Forum developed a two-workshop series. The first workshop focused on patient access to expertise and technologies in oncologic imaging and pathology and was held in February 2018.The second workshop, conducted in collaboration with the Board on Mathematical Sciences and Analytics, was held in October 2018 at The National Academies of Sciences, Engineering and Medicine to examine the use of multidimensional data derived from patients with cancer, and the computational methods that analyze these data to inform cancer treatment decisions. The workshop convened diverse stakeholders and experts, including clinicians, researchers and statisticians, and patient advocates, as well as representatives of health care organizations, academic medical centers, insurers, and federal agencies. The workshop included presentations and panel discussions on the current state of computational precision oncology and its opportunities, challenges, and limitations. Topics explored included • Data quality, completeness, sharing, and privacy; • Preclinical and clinical validation of the reliability, safety, and effectiveness of diagnostic tests and clinical decision support tools; • Regulatory oversight and reimbursement; • Communication of omics findings to clinicians and patients; and • Lessons from the use of computational precision oncology in clinical practice. The published workshop proceedings highlights suggestions from individual participants, including Dr. Shah and his research at MIT, regarding potential ways to improve the translation of computational precision oncology into clinical practice.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/347647e2-6a80-4a4d-b895-d67eda13f916,374383,,2022-10-14T21:38:01.221Z,['pratiks@media.mit.edu'],True,digital-clinical-trials-patients-with-novel-machine-learning-and-ai-architectures,2018-10-29,"['artificial-intelligence', 'synthetic-biology', 'machine-learning', 'public-health', 'engineering', 'medicine', 'clinical-trials', 'clinical-science']",Digital Clinical Trials with Novel Machine Learning and AI Architectures,PUBLIC,https://www.media.mit.edu/groups/health-0-0/overview/,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-08-19T20:13:58.222Z,"For my ""How to Make Almost Anything"" [HTMAA] class,  I attempted to build holographic glasses for augmented reality. These glasses would function differently from a normal AR wearable device. This device would use encoded holographic interference patterns to see content in true 3D dynamically.The most difficult part of this project was integrating the photonics with the RF circuitry. Special thanks to Sunny Jolly for his mentorship and advice in the process. Learn more about the project here.",2019-09-24,['object-based-media'],,374391,,2019-09-25T14:51:29.524Z,['vparth@media.mit.edu'],True,holographic-glasses-for-augmented-reality,2017-09-06,"['augmented-reality', 'holography']",Holographic Glasses for Augmented Reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-08-26T17:36:39.395Z,"Is the placement of bike-share docks equitable?This interactive map explores the question visually.The map shows the addition of bike-share docks, as well as the changes in income and race throughout the years the bike-share program has been in service. You can toggle the display to see only income, or race, or bikes data, or any of their combinations.https://aberke.github.io/income-race-bikesThe project includes bike-share networks for the cities of New York, Boston, Washington DC, Chicago, and Philadelphia.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/08/26/boston-income-race.gif,374410,,2022-10-07T19:20:34.588Z,"['kll@media.mit.edu', 'aberke@media.mit.edu']",True,income-race-bikes,2019-08-12,"['design', 'civic-technology', 'cartography', 'data-visualization', 'marginalized-communities']","Income, Race, Bikes",PUBLIC,http://aberke.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2019-09-03T04:01:36.434Z,"This project takes the idea of the unity between heaven and man of the ancient Chinese philosophy I ching (Book of Changes), combines the Bagua (eight trigrams) of the I ching with multidimensional data and multimedia, and uses an immersive interactive environment to present a novel form of narrative visualization. The main theme of this project is the “Unity between Heaven and Man,” namely the harmony of nature and humans. It systematically expounds the relationship between man and nature that man must follow the law of the universe, to respect and protect nature, to have an insight into the truth that harmony can produce all things. According to the main theme, we design this project under the form of a virtual reality environment by integrating personal data, environmental data, and media materials.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2019/09/03/baguaMarsh.jpg,374858,,2019-09-23T00:18:49.218Z,"['fredjf@media.mit.edu', 'ddh@media.mit.edu', 'joep@media.mit.edu']",True,baguamarsh,2019-07-01,"['virtual-reality', 'storytelling', 'visualization']",BaguaMarsh,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-09-05T10:47:59.699Z,This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.A program to promote ethics and empathy-based learning and leadership in Supreme Court Judicial Systems in Colombia. The group works with presidents and judges of all four Supreme Courts. Our efforts brought together all the judges for the first time since 1991 when the new constitution was set in motion.,2019-12-01,['ethics'],https://dam-prod2.media.mit.edu/x/2019/09/05/1becf2f4b19810aa7995a27d841a07ec.jpeg,374943,,2020-02-28T17:42:13.332Z,['tenzin@media.mit.edu'],True,ethical-leadership-in-judicial-systems,2019-08-01,[],Ethical Leadership in Judicial Systems,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-09-05T18:30:29.820Z,"This work is being continued through The Dalai Lama Center for Ethics and Transformative Values at MIT.A project in partnership with Nia Tero that focuses on transference of indigenous leadership values and skills to groom the next generation of leaders in the respective communities. The first cohort included members of communities from Rarotonga, Cook Islands; Federation of Micronesia, Polynesia; Lake region, North Canada; and Surinam, Guayana. These future leaders will lend their voice on issues ranging from conservation to climate change—issues that directly and severely impact these communities.",2019-12-01,['ethics'],https://dam-prod2.media.mit.edu/x/2019/09/05/4194c23c619d98538c623855fa798053.jpeg,374952,,2020-02-28T17:40:32.202Z,['tenzin@media.mit.edu'],True,transformative-leadership-in-indigenous-communities,2019-03-01,[],Transformative Leadership in Indigenous Communities,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-09-05T20:27:31.639Z,"A lightweight research platform to develop autonomous technology Designed to transform bicycle-sharing systems into an on-demand mobility solution, generating a more efficient and sustainable transportation network in cities.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/09/10/DSC_1183.jpg,374961,,2021-10-20T14:38:04.577Z,['naroa@media.mit.edu'],True,AutonomousBicycleProject,2019-02-10,"['robotics', 'design', 'energy', 'environment', 'manufacturing', 'transportation', 'urban-planning', 'technology', 'mechanical-engineering', 'engineering', 'climate-change', 'autonomous-vehicles']",The MIT Autonomous Bicycle Project,PUBLIC,https://www.linkedin.com/in/naroacoretti/,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-09-20T05:54:21.452Z,"When contemplating the future of life in space, we dream not only of rockets and off-world societies, but also of art, of leisure, and frequently of music. Human life in space will be not only operational, but also culturally rich. In order to explore this future, Nicole L'Huillier, Sands Fish, and Thomas Sanchez Lengeling are building the Telemetron Orchestra, a collection of novel musical instruments designed explicitly to be performed in microgravity. Born from speculation and microgravity experimentation, the Telemetrons take advantage of the poetics of the weightless environment to create a musical performance based on motion in space. The instruments are both devices and sculptures at once. They are both kinetic and aesthetic experiments, as well as explorations of interaction design in an environment where the gravitational rules we are used to do not apply. ",2022-01-01,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/09/20/ZG492_steve-boxall-20190815_0680.jpg,375105,,2020-06-23T16:37:00.955Z,"['thomassl@media.mit.edu', 'nicolelh@media.mit.edu', 'sands@media.mit.edu']",True,telemetron-orchestra,2019-08-15,"['design', 'music', 'interfaces', 'space', 'technology', 'zero-gravity']",The Telemetron Orchestra,PUBLIC,http://telemetron.space,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-09-25T07:55:54.331Z,"Several research projects have recently explored the use of physiological sensors such as electroencephalography (EEG - brain activity) or electrooculography (EOG - eye movements) to measure the engagement and vigilance of a driver. However, these systems still suffer from limitations such as an absence of a socially acceptable form-factor and use of impractical, gel-based electrodes. We have developed AttentivU,  a  device, in the  form factor of a pair of glasses,  which senses brain activity EEG as well EOG to measure different cognitive processes in real time,  including  cognitive load, fatigue, engagement, and focus. The device can be used for passive or active interventions, for example to monitor the state of the user, providing gentle audio or haptic feedback when the user is less attentive or adapting the environment when cognitive overload is detected. The system operates in a standalone, non-networked fashion to ensure privacy of the collected data. We have now completed the first test of the glasses prototype with more than 30 subjects who were performing a driving task in a simulator overnight when they were tired and drowsy, and the system reminded them with a sound or with a vibration to pay attention to the road .",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/uuid/6b57bbf5-a12a-4dee-8af4-62b130f1bb65,375181,,2019-10-21T01:27:54.424Z,"['pattie@media.mit.edu', 'nkosmyna@media.mit.edu', 'camorris@media.mit.edu', 'javierhr@media.mit.edu', 'sebzepf@media.mit.edu']",True,attentivucar,2018-02-01,"['wearable-computing', 'wellbeing']",AttentivU: Towards Improved Wellbeing in the Car,PUBLIC,http://kosmina.eu,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-10-01T17:41:02.041Z,"Neha Narula and Rhys Lindmark are teaching a class on Blockchain Ethics. See our course website, syllabus, and forum here: http://blockchainethics.co ",2019-12-31,['digital-currency-initiative-dci'],,375310,,2019-10-01T19:24:03.021Z,[],False,blockchain-ethics,2019-06-01,[],Blockchain Ethics,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-10-03T18:20:24.093Z,"Oniorb is an explorable musical object that was created with the idea of stress relief in mind. It is meant to act as a stress ball that also gives auditory feedback as you squeeze it. The orb is embedded with several different kinds of sensors that collect data from user interactions, which is then mapped to sound frequencies to create a musical output. Oniorb combines aspects of both active and receptive music therapy to create a calming experience for the user, meaning that it allows people to not only experience soothing music, but also gives them a role in creating and shaping the music they are hearing. ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/10/03/IMG_5430_Fz1ihsS.png,375359,,2019-10-03T20:00:22.850Z,['hrl@media.mit.edu'],True,oniorb,2019-05-01,[],Oniorb,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-10-04T16:51:01.183Z,"Flower is a new visualization tool for in-depth study of multi-sensor recordings in the time domain. It has been released for public download as a fully functioning tool available for experimental, research, and creative use. Flower uses unsupervised machine learning to extract latent representations for time-series data (EEG in particular) and show them through different visualization settings. In particular, it adds color and thickness to time-series plots, making them easier to understand and compare. Flower aims to enable more natural intuition around data results, using machine intelligence to translate time-series data for improved understanding by the human eye.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/10/04/flower-01.png,375379,,2019-10-12T22:58:43.102Z,['mmv@media.mit.edu'],True,flower-eeg-visualization-with-the-aid-of-machine-learning,2019-08-30,"['artificial-intelligence', 'data-visualization', 'electrical-engineering', 'neural-interfacing-and-control']",Flower: EEG visualization with the aid of machine learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-04T17:25:00.379Z,"Communication skills are critical for all. We  design and develop systems to help people with communication issues, e.g., more empathic communication, more self-confident communication, and more. We also design novel assistive communication devices using state of the art small, wearable sensors and powerful machine learning technologies to create highly personalized solutions for assistive communication. ",,"['affective-computing', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2022/10/07/123.jpeg,375388,,2023-04-04T12:36:17.700Z,"['pattie@media.mit.edu', 'utkarshs@media.mit.edu', 'jnarain@media.mit.edu', 'arnavk@media.mit.edu', 'ktj@media.mit.edu']",True,theme-communication-collaboration,,[],Theme | Communication + Collaboration,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-04T18:39:51.766Z,"“Us” aims to help people develop their sense of empathy. It uses machine learning to analyze interlocutors’ signals (e.g., choice of words,  tone of voice,  facial expressions, electrodermal activity, etc.) during conversations, and feeds the emotional content back to them in real time and through multiple analyses. We have versions of the system for both in-person and online conversations.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/10/20/P1260249_02_crop.jpeg,375396,,2022-10-26T13:41:51.800Z,"['camilorq@media.mit.edu', 'pattie@media.mit.edu', 'gauravp@media.mit.edu', 'shreyak@media.mit.edu']",True,project-us,2019-06-01,"['human-computer-interaction', 'artificial-intelligence', 'banking-and-finance', 'health', 'wearable-computing', 'affective-computing', 'technology', 'industry', 'startup']",Project Us,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-08T22:15:22.790Z,"We live in a world of personified conversational assistants (agents). We interact with these agents in our daily lives such as smart speakers or personal robots at home (Alexa, Google Home, or Jibo). Some have access to our personal calendars and communication channels which enable them to provide us personalized services. Other robots or devices operate in public spaces such as Pepper in retail stores or Care-E at airports. Currently, these conversational agents do not share information with each other. As a result, such platforms do not support the continuity of interaction with their users across different agent personas or form factors.Imagine a world, where we can seamlessly interact with AI assistants such as Alexa/Siri, which migrates across various devices thereby enhancing these assistants to deliver an experience that improves trust, likability, and competence. We present ""Migratable AI,"" a platform where a personal conversational assistant (agent) could migrate across different form factors and environments to always accompany and assist its user to support a far more continuous, personalized, and collaborative experience.",2020-06-30,['personal-robots'],https://dam-prod2.media.mit.edu/x/2019/10/21/migratable-ai-plain.gif,375458,,2022-05-13T03:56:46.489Z,"['tejwanir@media.mit.edu', 'cynthiab@media.mit.edu', 'sooyeon6@media.mit.edu', 'haewon@media.mit.edu']",True,migratable-ai,2019-01-01,"['robotics', 'artificial-intelligence', 'machine-learning', 'natural-language-processing']",Migratable AI,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-09T02:06:30.957Z,"The relationship between news content and its presentation has been a long-studied problem in the communications domain. Often, channels with a relatively smaller audience create a more powerful impact by amplifying the intensity of their content through a marked (usually biased) manner of presentation. SuperGlue Kinesics is a step in the direction of the investigation of this relationship and its influence on the public at large.SuperGlue Kinesics is a media analyzer that fuses multiple modalities to create a comprehensive model for the cross-analysis of facial expressions, body gestures, posture, scene context, and other nonverbal cues in broadcast news. We explore the nature of news portrayal on different media outlets to understand how they affect their audiences and contribute to the formation of potentially dangerous ""echo chambers.""",2020-03-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/2019/10/21/Facial Expressions_sRWye9U.png,375466,,2020-06-13T18:17:15.108Z,"['tejalw@media.mit.edu', 'mhjiang@media.mit.edu', 'lip@media.mit.edu']",True,sg-kinesics,2019-09-01,"['computer-vision', 'human-computer-interaction', 'artificial-intelligence', 'communications', 'consumer-electronics', 'politics', 'social-science', 'machine-learning', 'data-science']",SuperGlue Kinesics,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-10-09T02:08:47.620Z,"NewsMaker decouples news snippets from multiple news sources, and challenges you to re-create an objective news article using them. Once you create the article, the various news sources of your snippet melange are revealed to you, making you rethink your association/disassociation with your chosen news sources.",2019-12-01,['viral-communications'],https://dam-prod2.media.mit.edu/uuid/1a1eccb6-55e9-429a-8231-8bede28f8055,375474,,2021-02-10T12:59:07.511Z,"['lip@media.mit.edu', 'anderton@media.mit.edu', 'mhjiang@media.mit.edu']",True,newsmaker,2019-06-01,[],NewsMaker,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-10-09T14:44:23.578Z,"The Gamma SENSE (Sensory Engaging Nebulised Scent Experience) is a pioneering instrumental addition to the gamma instrument series. The device delivers multisensory gamma stimulation through auditory, visual, and olfactory channels.  ""This medical/musical instrument is based upon our groundbreaking pilot studies revealing (even non-synesthetic) humans link scent to sonic pitches. Due to the steep fall-off rate once gamma stimulation concludes, this olfactory mechanism probes the possibility of slowing 40 Hz frequency attenuation in patients.""",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/10/09/_Gamma Sense .png,375483,,2020-06-12T18:45:32.467Z,['arieger@media.mit.edu'],True,gamma-sense-sensory-engaging-nebulised-scent-experience,2019-03-01,"['interfaces', 'medicine', 'neural-interfacing-and-control']",Gamma SENSE: Sensory Engaging Nebulised Scent Experience,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-10-09T14:51:32.142Z,"I am currently developing a new approach in conducting sensory examinations via the Olfactory Response Baseline Identification Test (O.R.B.I.T) in conjunction with MGH and the Aging Brain Initiative. Based upon prior research (1), findings reveal olfactory testing to be an early detector of Alzheimer’s and Dementia (2), even aiding in differentiating the conditions based on identification distance per nostril . Furthermore, (according to HMS) ""temporary loss of smell, or anosmia, is the main neurological symptom and one of the earliest and most commonly reported indicators of COVID-19.”  In spite of the fact that these scientific principals are well founded, no mechanized or automated tools have been created to collect olfactory data for these medical purposes prior to the ORBIT. Therefore, the current prototype is under evaluation with key stakeholders on the ICU frontlines at MGH, Spaulding and within the Alzheimer’s testing domain as part of the Aging Brain Initiative. This sensory test device automates a crucial portion of the MCPT (Multisensory Cognition Proficiency Test). The O.R.B.I.T. will be the first of many mechanical designs supporting research in detecting and tracking sensory indicators of neurological performance and health throughout GAMMA treatment.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/10/09/Screen Shot 2019-09-15 at 11.12.00 AM.png,375491,,2020-09-20T17:28:05.182Z,['arieger@media.mit.edu'],True,orbit-olfactory-response-baseline-identification-test,2019-07-25,"['interfaces', 'engineering', 'medicine', 'clinical-science']",ORBIT (Olfactory Response Baseline Identification Test),PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-10-10T05:07:49.511Z,"Children’s creativity—the ability to come up with novel, surprising, and valuable ideas—has been known to contribute to their learning outcomes and personal growth. Standardized ways to measure creativity and divergent thinking reported that as children enter elementary school, their creativity slumps and thinking becomes more convergent, especially around the 4th grade. One cause for this is school curricula become more structured and lose the aspect of creative play. This is especially concerning for kids growing up in the era of artificial intelligence, where mechanical and repetitive jobs that require structured thinking move to machines. To be successful in this world of intelligent agents, we must empower children not only to understand how these intelligent agents work, but also to be able to think creatively about generating new artifacts in consort with such agents, which requires imaginative, novel thought.  In this work, we explore whether a social robot’s interaction with children can be an effective way to help children think more creatively. We suggest two ways in which robots used as pedagogical tools can help children think more creatively: 1) through artificial creativity demonstration, such as showing the use of novel ideas, and 2) through offering creativity scaffolding, such as asking reflective questions, validating novel ideas, and engaging in creative conflict. We designed four collaborative game-based activities that involve child-robot interaction and afford different forms of creative expression: 1) Droodle Game, which affords verbal creativity, 2) Magic Draw, which affords figural creativity, 3) WeDo Construction with Jibo, which affords construction creativity, and 4) Escape Adventure, which affords divergent thinking and creative problem solving. We designed the behavior of the robot such that it either scaffolds the child for creative thinking, or the robot gives the appearance of creative thinking by artificially emulating human creativity. We evaluated the role of the social robot in influencing children’s creativity by running comparative studies between children playing these creativity games while interacting with the robot with creativity-inducing behaviors (creative condition), and without creativity-inducing behaviors (non-creative condition). Children who interacted with the creative robot exhibited higher levels of creativity than children who interacted with a non-creative control robot. We conclude that children can model a social robotic peer’s creative expression via social emulation. When scaffolded for creativity, children exhibited higher levels of creativity. This enabled us to develop a robot scaffolding paradigm which fosters creativity in young children. This project contributes design guidelines for child-robot interactions which promote creative thinking, and provides evidence that these creativity inducing behaviors exhibited by social robots can foster creativity in young children.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2019/10/10/IMG_6511 2_9nqy5f6.JPG,375526,,2022-07-11T16:26:13.772Z,"['safinah@media.mit.edu', 'cynthiab@media.mit.edu', 'haewon@media.mit.edu', 'ndevasia@media.mit.edu']",True,creativity-robots,2018-09-01,"['robotics', 'social-robotics']",Creativity with Social Robots,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-10T05:23:29.680Z,"Doodle Bot is a robot-based creative AI learning platform for high school students. It consists of an introduction to robotics, hardware, software programming, and machine learning. Students build, program, and train a smart drawing robot.As  a part of our broader AI education efforts, we developed the Doodle Bot toolkit that specifically aims to teach high school children about using machine learning for creative applications. We focus on generative networks and recurrent neural networks. We piloted this kit with over 30 students in Guadalajara, Mexico and Cambridge, USA.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2019/10/10/_MG_6940.JPG,375534,,2020-04-03T20:16:37.626Z,"['cynthiab@media.mit.edu', 'safinah@media.mit.edu']",True,doodle-bot,2019-03-01,"['artificial-intelligence', 'kids', 'learning-teaching', 'technology', 'covid19']",Doodle Bot - Creative AI learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2019-10-10T05:37:15.256Z,"Families with ASD often struggle with social and emotional communication. This work explores the role of an embodied agent (a social robot) as an Augmentative and Alternative Communication (AAC) tool for families with ASD. We evaluate the usefulness of a social agent in aiding i) emotional recognition, and ii) emotional expression, over using traditional Picture Exchange Communication Systems (PECS), and tablet based AAC tools. In order to evaluate emotion recognition by children, we suggest several self report methods designed for children with ASD, evaluate each of these tools, and suggest best suited measures for self report. We iteratively design a tablet based tool that enables storytelling through a social robot. ",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2019/10/10/Screen Shot 2019-10-10 at 1.38.54 AM.png,375542,,2019-10-10T14:14:50.846Z,"['picard@media.mit.edu', 'cynthiab@media.mit.edu', 'safinah@media.mit.edu']",True,social-robots-autism,2018-02-01,[],Social Robots for Children with Autism,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-10T22:51:01.255Z,"Serosa II is a hydrogel electrode waistband to measure gastrointestinal signals. We sought to create an interface can be worn for long periods of time underneath clothing, and could be accurately placed without the help of a technician or researcher. With this version, we performed a pilot test to evaluate the signal quality of the hydrogel electrodes. We found that hydrogel electrodes record signal with less power and greater motion artifacts; however, they offer an advantage of more stable impedance over time (see Publications).We also completed a user evaluation of wearability and types of feedback (visual, auditory, or haptic). Users most understood and enjoyed receiving haptic feedback (vibrations on the wrist). In the study, we asked participants to wear Serosa while completing the Iowa Gambling Card Task. During the task, participants received audio, haptic (vibrations), or visual feedback on “gut” signal, and we compared participant preferences for feedback modality for real-time tasks. (See Master's Thesis*). In the future, we see Serosa II being useful to test gut-brain computer interfaces (GBCIs) in real-world environments where the preparation and placement of electrodes by a technician is not available.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/10/10/Serosa-v2_crop.jpg,375567,,2019-11-07T15:08:06.481Z,"['pattie@media.mit.edu', 'avujic@media.mit.edu']",True,serosa-ii,2018-03-01,[],Serosa II,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-10-10T22:55:44.822Z,"Serosa III is a wearable electrode patch that records gastric activity. It reduces the size of the interface and incorporates high-resolution electrogastrography (EGG) methodology. Using feedback and results from previous studies, Serosa III moves the bioamplifier and battery to the front of the abdomen and reduces the size of the interface. In addition, it records higher resolution gastric (stomach) activity than previous versions. Below, we demonstrate how Serosa III can record gastric (stomach) waves after a 454 calorie meal. These signals are a marker of parasympathetic activation, or “rest and digest.” This “rest and digest” activation is what we seek to maintain and modulate via gut-brain computer interfacing (GBCI).",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/10/10/Angela_Serosa_spring-2019.jpg,375575,,2020-11-23T20:21:42.060Z,"['avujic@media.mit.edu', 'pattie@media.mit.edu', 'picard@media.mit.edu']",True,serosa-iii,2019-02-01,[],Serosa III,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-10-11T18:33:55.753Z,"An initial experiment with cloud-enabled audio production software.A collection of docker containers work together to stream the audio and sheet music for an 800,000 year long composition; the piece lasts for 800,000 years, or until the underlying internet infrastructure crumbles. The project probes the fragile nature of the internet, and the varying durability of music transcription formats.",2016-05-31,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/10/11/first-5-measures.jpg,375589,,2019-10-28T14:20:00.882Z,['holbrow@media.mit.edu'],True,800-000-year-composition,2016-01-01,[],"The 800,000 Year Composition",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-10-13T15:31:53.141Z,"INSPIRE: Intelligence Never Seeks Perfection, Instead Requires Effort.Stories help us expand our sense of what's possible: by hearing from others like us who have overcome personal and societal obstacles in order to become activists, spoken word poets, and game designers (for example), we become more confident and hopeful in our ability to do the same. Unfortunately, many children grow up in environments with little exposure to these stories, and subsequently pursue pathways through their education and careers that could have been very different had they received this exposure early on.INSPIRE is a program in partnership with a local middle school that seeks to expose students to stories of triumph and hope that they find personally meaningful. A key part of the program is a mobile application that delivers short video stories of professionals talking about their life journeys. Stories are sourced from our content partner Roadtrip Nation, and delivered through a virtual coach (""Jo Jo"").  Jo Jo personalizes video recommendations for students and prompts them to reflect after they watch videos. The program also includes in-person community building and engagement initiatives developed in collaboration with the school to supplement and extend what the mobile application can offer on its own.  Ultimately, the goal of INSPIRE is to help students enhance their metacognitive traits and academic outcomes in ways that help them positively and productively shape their own unfolding narratives.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2019/12/05/jojo.png,375618,,2019-12-06T04:51:04.409Z,"['ngillani@media.mit.edu', 'belen@media.mit.edu', 'snehapm@media.mit.edu', 'mhughes4@media.mit.edu', 'dkroy@media.mit.edu', 'echu@media.mit.edu']",True,inspire,2019-04-01,"['learning-teaching', 'networks', 'storytelling', 'machine-learning']",INSPIRE,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-10-16T17:54:42.007Z,"Our body and mind relate in ways which are extraordinarily enigmatic and seemingly incomprehensible. Recent findings exemplify this by showing not just that our minds can phenomenologically inhabit multiple bodies but also that our bodies can be accessed by multiple minds. As an exploration of this concept, we present Machinoia, a symbiotic augmentation that extends the user with two additional heads each of which is unique variations of the user's identity: who you once were, and who you'll eventually become. We used a generative adversarial network to synthesize life-like human faces and controlled them through artificial attitude models extracted from social media data of the wearer, thus creating an ""artificial personal intelligence'' of the wearer, bringing to life past and future versions of oneself.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/10/15/machinoia3_U4K4pLl.jpg,375654,,2022-04-09T06:18:46.801Z,"['pattie@media.mit.edu', 'patpat@media.mit.edu', 'vdanry@media.mit.edu']",True,machinoia,2019-10-01,"['design', 'art', 'artificial-intelligence', 'data', 'wearable-computing', 'technology']",Machinoia: Machine of Multiple Me,PUBLIC,http://pat.design/,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2019-10-17T21:25:42.931Z,"The world wide web was conceived 30 years ago. What will internet media look like after another 30 years? Should we expect it to be mostly the same? Or will it evolve rapidly in the coming years?To gain insight into these questions, I have been studying the evolution of other media technologies, and identifying ways that they are both similar and different from the internet.  Media Timelines packages parts of this research in an interactive application that makes it easy to observe and compare the evolution of sound recording, film, and internet media technologies.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/10/17/Screen Shot 2019-10-17 at 5.29.11 PM.png,375721,,2019-10-19T19:19:03.983Z,['holbrow@media.mit.edu'],True,media-timelines,2019-09-13,[],Media Timelines: Understanding the future by studying the past,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-10-18T15:56:26.058Z,"The Propel program was initiated by Elements of Education to address the disproportionately low rate of girls applying to the STEM-focused high schools in Tacoma Public Schools. Between 2017 and  2019,  33% of the applicants to the two STEM-focused high schools identified as female, compared with 76% of the applicants to the performing arts high school identifying as girls. Propel has been co-developed by Jaleesa Trapp (LLK) and Zach Varnell (Elements of Education) at IDEA High School in Tacoma Public Schools, working closely with high school students as co-designers and mentors of project-based activities for middle-school students. The program was first prototyped as a summer and winter program, then expanded to a full-year course.Jaleesa Trapp has guided the iterative design of the Propel program to develop equitable pathways into STEM classes and careers, building on her research as a graduate student in the Lifelong Kindergarten  group. Her work is informed by her experience growing up attending Tacoma Public Schools as well as her experience as an educator,  including serving for the past five years as a high-school Computer Science teacher in Tacoma Public Schools. In her Master's thesis (Trapp, 2019), she documents the contrast between her experience as a middle-school student attending science class—which emphasized rigidly following rules (with constant threat of disciplinary action)—to the after-school environment of the Tacoma Computer Clubhouse that she attended, which encouraged exploration, experimentation, peer learning, and mentorship. In the design of the Propel program, Trapp is building on the Clubhouse learning approach (Resnick & Rusk, 2009), adapting the mentoring model to work across middle- and high-schools in order to greatly expand opportunities for young people from non-dominant groups to become designers and innovators in STEM.The Propel program began as an in-person model, and has transitioned to online mentoring and facilitation sessions during Fall of 2020, as schools shifted to remote learning due to COVID-19. The Propel program has increased gender, racial, and ethnic diversity of students opting into STEM classes and significantly expanded students' knowledge and interest in STEM careers (Trapp, 2019). More than 70% students have started with little to no knowledge of STEM professions or connections to people in STEM-fields, and finish Propel with an expanded network of connections across computer science, engineering, chemistry, and life sciences.",2021-06-30,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2020/03/27/PropelSTEM.jpg,375730,,2023-09-24T18:17:17.692Z,['jaleesat@media.mit.edu'],True,propel-stem,2019-07-01,[],Propel STEM,PUBLIC,http://ideatacoma.org/propel-girls-stem-experience,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-10-18T18:11:48.098Z,"​The CEE (Color Expanse Examination) system is an explorative approach to evaluating human color acuity. Color perception is shaped and altered by cultural development, ocular conditions, as well as neurological health. Human ability to differentiate and identify color can also be an indicator of wellbeing ranging from emotional status to Alzheimer’s Disease progression. In Alzheimer's, some of the most significant early changes occur in color vision. In spite of this, color testing is rarely included in cognitive baseline examinations.",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/10/18/IMG_1201.JPG,375739,,2019-10-18T20:24:43.298Z,['arieger@media.mit.edu'],True,cee-color-expanse-examination,2019-10-04,[],CEE: Color Expanse Examination,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-10-20T16:31:29.013Z,"“[The wild boars] look at me like I am a visitor. It's like they’re the owners and I’m the guest. Wild animals that normally live in nature have taken over our world. I wonder if we are the ones now living in the cage.” --  Koichi Nemoto, Fukushima ResidentAt 14:46 JST on March 11, 2011, the combination of a 9.0 Mw earthquake and tsunami greatly disrupted life in Fukushima Prefecture, Japan. While the earthquake warning systems alerted managers to shut down the Fukushima Daiichi power plant, the speed of the resultant tsunami damaged the plant before the shutdown could complete. The damaged facility seeped radioactive fallout into the surrounding air, soil, and ocean -- corrupting water and food systems.The extent of the radioactive fallout following the Fukushima disaster occurs on scales that are difficult to grasp. Fukushima 2100 takes visitors on a tour of the damaged site and catalogues the flora and fauna impacted throughout the ecosystem in the form of a speculative floating natural history museum. A Safecast geiger counter, caesium mineral deposits, rice, seaweed, anchovies, and emergency iodine pills, represent the scope of the disaster and the human engagement required to navigate life in the Anthropocene in Fukushima. ---This project was developed in course 4.S23 ""Special Subject: Architecture Studies — Earth on Display: The Anthropocene in the Museum of Natural History"" taught by Dr. Rania Ghosn with TA Jaehun Woo.  It was displayed in the Harvard Museum of Natural History on November 4th. Earth on Display is one of 14 Experiments in Pedagogy organized on occasion of the 150th anniversary of Architecture education at MIT.---Below is a fictionalized ""About Us"" page for the floating natural history museum depicted in Fukushima 2100:The Fukushima Natural History Museum was established as public space to reckon with living in the epoch of human transformation of the Earth. Our mission is to deepen our collective understanding of our collective impact and our sense of collective responsibility (response-ability) in this, our troubled, time.The Anthropocene servers as our primary focus not only because our collective impact on the Earth has been so extensive, but also because we lack the space, language, and symbols to deal with the emotional and mental task we are left in its wake. The nuclear disaster at the Fukushima Daiichi power plant provides a illustrative example of the tangible objects and intangible systems through which mankind has affected our shared world. The Fukushima Natural History Museum attempts to provide a skeleton on which we can accrete this body of knowledge and relations.Our collections display representatives of the scientific knowledge on nuclear fallout and the necessity of active, participatory engagement required to gather it. In the spirit of Bruno Latour, We walk a fine line between anthropomorphizing the Earth, that is giving it the shape of something human, and phusimorphizing, that is giving it a shape based on its preceding relationships. Anthropomorphizing serves as a way to give our visitors a vantage to relate to the task at hand but, when followed to a fault, limits our capacity to view beyond a human vector. On the other hand, phusimorphizing, takes into consideration the relationships but pre-supposes our capacity to understand the web of causes, objects and subjects that from them.The Fukushima Natural History Museum attempts to transverse the gulf of commonplace thoughtless-ness. In the spirit of Donna Haraway on Hannah Arendt, we strive to make ourselves present to ourselves, present to what is not ourselves, and to what has been missing. As humans, we have lulled ourselves into a thinking that our actions are self-producing, self-defined, and confined within predictable temporal lines. However, we have collectively produced systems that extend beyond our tenuous spatial and temporal grasp and have decentralized control and information. The Fukushima Natural History Museum, for many, will begin a stream, of thinking along these channels. For others, it will deepen their understanding. For all, it will provide a harbour to moor in the wake of the Anthropocene.",,[],https://dam-prod2.media.mit.edu/x/2019/10/20/FUKUSHIMA001 Blurry Edited  copy.jpg,375799,,2019-10-20T18:12:50.633Z,['wskamau@media.mit.edu'],True,earth-on-display-fukushima-2100,,"['design', 'agriculture', 'art', 'energy', 'biology', 'public-health']",Fukushima 2100,PUBLIC,https://architecture.mit.edu/subject/fall-2018-4s23,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-10-20T18:13:18.394Z,"“[The wild boars] look at me like I am a visitor. It's like they’re the owners and I’m the guest. Wild animals that normally live in nature have taken over our world. I wonder if we are the ones now living in the cage.” —  Koichi Nemoto, Fukushima ResidentAt 14:46 JST on March 11, 2011, the combination of a 9.0 Mw earthquake and tsunami greatly disrupted life in Fukushima Prefecture, Japan. While the earthquake warning systems alerted managers to shut down the Fukushima Daiichi power plant, the speed of the resultant tsunami damaged the plant before the shutdown could complete. The damaged facility seeped radioactive fallout into the surrounding air, soil, and ocean—corrupting water and food systems.The extent of the radioactive fallout following the Fukushima disaster occurs on scales that are difficult to grasp. Fukushima 2100 takes visitors on a tour of the damaged site and catalogues the flora and fauna impacted throughout the ecosystem in the form of a speculative floating natural history museum. A Safecast geiger counter, caesium mineral deposits, rice, seaweed, anchovies, and emergency iodine pills represent the scope of the disaster and the human engagement required to navigate life in the Anthropocene in Fukushima.",2018-11-04,[],https://dam-prod2.media.mit.edu/x/2019/10/20/FUKUSHIMA001 Blurry Edited  copy_r2Aj88e.jpg,375807,,2019-11-11T23:54:43.252Z,['wskamau@media.mit.edu'],True,fukushima-2100,2018-09-03,"['design', 'agriculture', 'art', 'energy', 'biology']",Fukushima 2100,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-10-20T19:36:00.353Z,"This sonic rite consists of a listening session of the performance of the place, guided by the Para-Cantora, a sonic device that acts as a medium between worlds. It becomes a parasite of the place and through environmental sensing, real-time mapping, and six speakers, it diffuses the chant of the apparently silent agents around us. It is a transductive device that contains different environmental sensors that are mapped in real time into different sounds (synthesizers, samples, textures, words, and other voices) that create a musical performance of the place, unveiling the performativity of non-human agents on the environment and their expressive capacities. In places where the natural and technological collide there are particular disturbances that are transduced by this parasitic medium to create a sonic poem that is imprinted in our collective material memory and sensitizes us through the aural. Become a membrane and resonate like a drum. Connect with your quark-gluon-plasma origins. Shake your material existence, and let this parasitic medium guide you through a concert of a specific moment in space and time. This is an exercise of listening to emerge together, stimulate perception, confuse reality, and resonate with the sense that we are as much part of nature as we are part of culture.This experimental performance took place in different parts of the world where the natural and technological collide, as part of the Simetría Residency (CERN, ALMA and ESO). Recordings and listening sessions where hosted at  CERN in the LHC ALICE Detector in Switzerland, the ALMA Observatory OSF, in the Atacama Desert in Chile, and Paranal Observatory, ESO, in the Atacama Desert in Chile.Sensors in the sculpture: Barometric pressure, altitude, temperature, accelerometer, electromagnetic fields, anemometer, wind turbine, light, proximity, and vibrations.It has six independent channels and it works with battery, so it is absolutely self contained. ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/10/20/66053C35-CA8E-4B04-A7EA-6D3E2698970E 2.JPEG,375815,,2020-02-03T19:53:55.826Z,['nicolelh@media.mit.edu'],True,el-poema-de-la-fabrica-c-smica,2019-07-14,[],El Poema de la Fabrica Cósmica,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-10-20T22:46:42.958Z,"In this work we suggest to harness the power of knowledge-seeking positive experiences, engagement, and curiosity that objects perceived as magical carry in themselves by combining them with pop-cultural references and neuroscience in order to derive a novel intervention to foster a growth mindset in children of 8-12 years old. We created “The Thinking Cap 2.0,"" a wearable system in the form-factor of masks, tiaras, or helmets from different sci-fi universes like Star Wars or Avengers, fitted with a commercially available electroencephalography (EEG) headset or headband and a Bluetooth speaker. We adopt the form-factor towards the preferences of the child, which universe or hero is being their favorite one. We designed and conducted a first study with 50 children to investigate the effect of using “The Thinking Cap” to foster children’s mindset. In the case of our study we assessed the mindset and self-esteem of the children of 8-12 years old before and after the “intervention” using the ""Cap"" in order to see if any changes in the self-perception of the children could be observed and if the ""Cap"" helps children gain more self- confidence in their capabilities to solve math problems. The ""Cap"" uses Brain-Computer Interface (BCI) algorithms to recognize mental imagery of the child pre-trained for a 2-class choice problem. In an initial phase, the ""Cap"" is used to recognize and report on the brain patterns of the user. We believe that demonstrating such basic recognition of brain signals will lead the child to develop trust in the hat’s ability to “know them."" Thus, when the ""Cap"" in a later phase praises the child who wears it for their ability and/or effort on a task, the child is likely to listen to it and be affected by its suggestions in their subsequent performance. We hypothesized that using the ""Cap"" can thus lead to fostering growth mindset. Our results suggest that interacting with a “Cap” (limited by classification accuracy and recording factors) has a positive impact on children’s mindset as expressed through their communicated beliefs and task- based behaviors. Though our study should be considered and treated as a preliminary proof-of-concept, the results suggest a possibly provocative new kind of relationship and interaction paradigm between children and a wearable EEG system, enhanced by perceived magic and cultural references. Find out more about the first version of the system, ""The Thinking Cap"" here.Check the video presentation of this work  from The ACM Interaction Design and Children (IDC) conference 2020.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/06/14/Screen Shot 2020-06-07 at 1.26.05 PM.png,375831,,2021-04-26T13:19:40.722Z,['nkosmyna@media.mit.edu'],True,the-thinking-cap-2-0-any-universe-is-your-universe,2019-01-01,['cognitive-science'],The Thinking Cap 2.0: Fostering growth mindset of children by means of electroencephalography and perceived magic,PUBLIC,https://braini.io,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-21T01:38:57.404Z,"Looking Through the Piano is a project created to visualize all of the past projects from the Opera of the Future group. The piano acts as an interface and a timeline, allowing the user to hold down one or more keys to see and hear information about different projects throughout the group's history. ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2019/10/20/Looking_through_the_Piano_header.jpg,375839,,2019-10-21T17:17:27.653Z,"['hrl@media.mit.edu', 'manaswim@media.mit.edu', 'karstens@media.mit.edu']",True,looking-through-the-piano,2019-10-20,[],Looking Through the Piano,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-10-21T06:45:52.671Z,"FlowIO is the first fully-integrated and truly general-purpose, miniature pneumatics development platform for control, actuation, and sensing of soft robots and programmable materials. FlowIO was designed to strip barriers to entry into the growing field of soft programmable materials and enable researchers, artists, students, and makers to be able to rapidly prototype and effortlessly bring to life their soft robotics projects. Designed with wearability and reconfigurability in mind, FlowIO comes with a set of magnetically connected modules and accessories to satisfy diverse pressure, flow rate, and sensing needs, and to make it suitable for both wearable and desktop applications. To make FlowIO suitable for broad and diverse audiences, we developed a web-GUI that runs in Google Chrome, compatible with all major mobile and desktop operating systems, enabling a user to wirelessly control FlowIO from any Bluetooth-capable device. Additionally, FlowIO is fully Arduino-compatible and has multiple APIs for both Arduino and JavaScript. Additional support for Python, Unity, and Scratch is also planned.",,"['responsive-environments', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/03/21/Main.jpg,375856,,2023-09-07T23:33:01.233Z,['alims@media.mit.edu'],True,flowio,2019-05-01,[],FlowIO Platform for Soft Robotics and Programmable Materials,PUBLIC,https://www.softrobotics.io,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-21T18:34:09.451Z,"Relationship discovery between two entities is a problem that has to be addressed when constructing a Knowledge Base (KB). A solution to this problem is important because the KB built from the discovered relations can play a key role in down stream tasks, such as analogical reasoning. An example of this kind of reasoning is whether a dog desires cake: a dog is an animal, cake is food, animals desire food, therefore a dog desires cake. We constructed a system that is trained on a commonsense KB and whose inputs are pairs of concepts and its outputs are the strength of commonsense assertions between the concepts. Our approach is unique because it can handle out of vocabulary entities and can generalize commonsense to out of knowledge concepts. We utilize the system to be able to infer the answer for out of knowledge assertions such as the aforementioned whether a dog desires cake.",2020-08-01,['object-based-media'],,375868,,2022-06-10T14:09:49.380Z,['pe25171@media.mit.edu'],True,deep-relationship-discovery,2019-01-01,[],Deep Relationship Discovery,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2019-10-21T18:35:20.956Z,"Retrofitting is a technique used to move word vectors closer together or further apart in their space to reflect their relationships in a Knowledge Base (KB). However, retrofitting only works on concepts that are present in that KB. RetroGAN uses a pair of Generative Adversarial Networks (GANs) to learn a one- to-one mapping between concepts and their retrofitted counterparts. It applies that mapping (post-specializes) to handle concepts that do not appear in the original KB in a manner similar to how some natural language systems handle out-of-vocabulary entries. We test our system on three word-similarity benchmarks and a downstream sentence simplification task, and achieve the state of the art (CARD-660).  Altogether, our results demonstrate our system’s effectiveness for out-of-knowledge and rare word generalization.",2020-08-01,['object-based-media'],,375876,,2022-06-10T14:09:49.396Z,['pe25171@media.mit.edu'],True,retrogan,2019-01-01,"['artificial-intelligence', 'machine-learning']",RetroGAN,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-10-22T03:25:41.816Z,"The fourth industrial revolution has brought new meanings to surveillance in the digital age. In recent years, we have seen the rise of state and enterprise-driven surveillance – from the tracking of our digital footprint, to piloting facial recognition CCTVs, to eavesdropping on our private conversations - the panopticon is more ubiquitous than ever. Governments and corporations are increasingly data-hungry, arguing that more data beget better services, greater convenience, and higher levels of security. Convenience, however, has come at the cost of personal privacy. User data have been mined, analyzed, profiled in the name of “free services” and “personalized experiences”. Whilst researchers, artists, and activists have drawn attention to the threats posed by camera and video surveillance technologies, lesser attention has been given to audio surveillance technologies. These technologies, however, are often more covert and less visible than a CCTV. The hidden networks of audio surveillance obscures and yet fuels the reach of surveillance capitalism. In the past year alone, we have witnessed a myriad of privacy breaches and violations in the voice technology space. With the smart speaker market projected to outnumber the human population by 2021, I argue that a thorough and critical analysis of privacy, surveillance and their relationship to voice technologies is pressing.I am particularly interested in the dynamics of the seen/unseen, the visible/invisible, surface/subterrane, public/private of surveillance and counter-surveillance. Surveillance, I argue, operates in duo-realms, serving different purposes and goals. While overt surveillance acts as a means of control, covert surveillance obscures the mechanisms of the loss of privacy and agency. My research aims to uncover, unpack and expose these multi-dimensions of surveillance/counter-surveillance and their manifestations. I am interested in the aesthetic of control and subversion, asking: “what does civic resistance look/feel/sound like in the age of surveillance capitalism?” My research has 3 key contributions i) employing critical design as a medium for reflecting on the cultural, social, and ethical implications of emerging technologies; ii) making hyper-visible the aesthetics of surveillance and counter-surveillance via DressKit, a hardwear kit for voice technologies; iii) amplifying camp as a methodology of subversion. DressKit is a low to high-tech hardwear kit for voice technologies that uses ultrasonic jamming and sound masking to reclaim users’ privacy in the age of surveillance capitalism. Each kit comes with a range of visual metaphors of sound jamming and masking, employing camp as a visual aesthetic i.e. “earplugs”, “earmuffs”, “plunger”, “hat”, “phone case. The kit is a play on “wearable technologies” by questioning who (or what) is doing the wearing. I use camp as an analytical framework to make hyper-visible the invisible forces of surveillance capitalism given its historical roots in subversion and performance of heteronormativity in black queer communities. The research will culminate with 3 outputs: i) DressKit: a hardwear kit for audio surveillance technologies; ii) an exhibit of DressKit, placing surveillance on (literal) display; iii) a visual ethnography of the aesthetics of surveillance and counter-surveillance in non-Western contexts, particularly in Singapore. Success will be determined by a panel critique of the exhibit, feedback from participants in the visual ethnographic piece, and the extent to which conversations can be masked via jamming and masking. Further, I am interested in surveilling audio surveillance by conducting a short experiment investigating the extent to which Facebook (and Instagram) are listening in on our conversations. I will conduct several engineered conversations along key trigger words with a set of participants where they will self-monitor the types and number of related ads that show up on their feed after the conversation.Ultimately, my work is about the de-centering and re-centering of power; de-centering power by exposing and unveiling the aesthetics of surveillance in order to re-center and reclaiming civic privacy and agency in the age of surveillance capitalism.",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2019/10/21/Yellow_Surveillance Capitalism.png,375892,,2022-06-10T13:35:30.145Z,['rubezc@media.mit.edu'],True,anti-surveillance-technologies,2019-09-09,[],Dressing Audio Technologies: In/Visibility and Civic Resistance in the Age of Surveillance Capitalism,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-10-22T14:44:49.540Z,Proxymix is a visualization tool to understand the influence of spatial configuration on human collaboration. This agent-based model at the architectural scale enables more optimal uses of space by incorporating a human behavioral component in the design process.This project is open source and open to external contributors on this GitHub repository: https://github.com/CityScope/CS_Proxymix,,['city-science'],https://dam-prod2.media.mit.edu/x/2019/10/22/CS_MainImage_Final.gif,375900,,2020-03-31T10:05:22.277Z,"['agrignar@media.mit.edu', 'crisjf@media.mit.edu', 'kll@media.mit.edu', 'nicolasa@media.mit.edu']",True,proxymix,2019-06-01,"['human-computer-interaction', 'architecture', 'behavioral-science', 'data-visualization']",Proxymix: Influence of spatial configuration on human collaboration,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2019-10-25T00:49:23.247Z,"By Ken Nakagaki, Joanne Leong, Jordan L Tappa, Joao Wilbert, and Hiroshi IshiiOverviewProject HERMITS explores a way to greatly advance the versatility of Robotic Tangible Interfaces. Inspired by hermit crabs, we designed a modular system for table-top wheeled robots to dock to passive attachment modules, defined as ""mechanical shells."" Different types of mechanical shells can uniquely extend and convert the motion of robots with embedded mechanisms, so that, as a whole architecture, the system can offer a variety of interactive functionality by self-reconfiguration. We envision this novel interactive architecture to bring a rich application space including physical space organization, digital data physicalization, and entertainment and storytelling systems.The general approach in HERMITS expands how physical interfaces and computers in our daily life can adapt and reconfigure for user interactions with passive attachments. We are in the age where robotic systems are emerging in our living space (e.g. robotic vacuum cleaner, drones). The idea presented in HERMITS has greater implications beyond our prototype, where everyday robotic systems may gain a significant amount of functionality, expressivity, and interactivity by switching mechanical shells.",,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/72972674-fddd-4cbc-a815-2e9aeb79079a,375917,,2021-06-21T19:31:39.290Z,"['ken_n@media.mit.edu', 'joaleong@media.mit.edu', 'jwilbert@media.mit.edu', 'ishii@media.mit.edu']",True,hermits,2019-10-01,"['robotics', 'human-computer-interaction']",HERMITS: ‘Mechanical Shells’ for Robotic Tangible UIs,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-10-25T00:49:48.245Z,"OverviewDefeXtiles are thin, flexible textiles of many materials that can quickly be printed into a variety of 3D forms using an inexpensive, unmodified, 3D printer with no additional software. This is achieved by using the material properties to encode the textile form: specifically using the periodic gaps that emerge when not enough material is being used to print. The printer moves and prints the same as it would a solid, rigid sheet, but by leveraging the stringing behavior that occurs in thermoplastic filament we can encode small gaps that afford the stretchability and flexibility. In a sense, the machine and the material are co-fabricating the form: the printer—the macroscopic form, the material—the microscopic formThe simplicity of this approach is what makes it so powerful. We are able to 3D print dresses for clothing design prototyping, tough badminton shuttlecocks, full-sized garments—such as a skirt—to help users “try on” clothes before ordering online, interactive lamps that turn on/off and change brightness when deformed, and a 70m roll of fabric produced in a single uninterrupted print.",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/09/853A0321-(1).gif,375925,,2022-03-28T16:15:22.979Z,"['jackform@media.mit.edu', 'ishii@media.mit.edu']",True,defextiles,2019-09-18,"['design', 'human-computer-interaction', 'fashion', '3d-printing', 'materials', 'fabrication', 'covid19']",DefeXtiles: 3D printing quasi-woven textiles via underextrusion,PUBLIC,https://jackaforman.com/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-10-25T16:11:53.977Z,"When an automated car harms someone, who is blamed by those who hear about it?  In this project, we asked participants to consider hypothetical cases in which a pedestrian was killed by a car operated under shared control of a primary and a secondary driver, and to indicate how blame should be allocated. We find that when only one driver makes an error, that driver is blamed more, regardless of whether that driver is a machine or a human. However, when both drivers make errors in cases of human-machine shared-control vehicles, the blame attributed to the machine is reduced. This finding portends a public under-reaction to the malfunctioning AI components of automated cars and therefore has a direct policy implication: allowing the de-facto standards for shared-control vehicles to be established in courts by the jury system could fail to properly regulate the safety of those vehicles; instead, a top-down scheme (through federal laws) may be called for.",2020-06-30,['scalable-cooperation'],,375933,,2020-07-23T18:30:25.668Z,"['dsouza@media.mit.edu', 'jeanfran@media.mit.edu', 'smlevine@media.mit.edu', 'awad@media.mit.edu', 'irahwan@media.mit.edu']",True,perceptions-of-automated-cars,2017-10-01,[],Perceptions of Automated Cars,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-10-29T19:43:23.479Z,"Serosa records gastrointestinal (stomach and intestinal) signals. Serosa aims for cognitive enhancement via the mind-gut connection by aiding an individual to modulate gut signal.The gut has been shown to have a connection to emotion, memory, decision-making, and more. The recent boom in microbiome research has begun to describe this interaction on the microbial level. We are inspired by this to begin recording neuronal information from the ""gut-brain""—hundreds of millions of neurons that line the gastrointestinal tract—to study their relationship to cognitive phenomena as well.In our research, have also recorded and analyzed gut signals while participants view emotional films, play the Iowa Gambling Card Task, and from real-world pilot studies. We have iterated and tested multiple electrode types and electrode montages to contribute information to form factor and signal quality.We have seen in our studies a relationship between reported emotions and gut signals, and interoceptive abilities and gut signals, and the trade-offs between varying form factors and electrode types. We seek to show from our data and contributions how gastrointestinal signals may be used for emotion regulation and other applications for GBCIs. We aim to apply principles of non-invasive brain-computer interfacing (BCI) by using electrodes to record large-scale gut neuron activity and return feedback to the user. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/10/30/angela_members-week-fall-2019.001.jpeg,375988,,2019-11-07T15:55:31.357Z,"['avujic@media.mit.edu', 'pattie@media.mit.edu', 'picard@media.mit.edu']",True,gbci,2017-10-01,[],Serosa: Towards Gut-Brain Computer Interfacing (GBCI),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-10-30T13:26:35.725Z,"Human-computer interaction (HCI) has traditionally focused on designing and investigating interfaces that provide explicit visual, auditory, or haptic feedback. We propose a new type of user interface that uses smell as an implicit, less conscious output that still influences the person's cognition. We pair that with implicit, physiological information as the input to the system. Unlike other modalities, certain types of olfactory stimuli can be presented during sleep, without awakening the subject, as well as during wake time in a very subtle manner without distracting the subject from their primary activity. This offers novel opportunities for interfaces and applications that extend from wake to sleep time. Research has shown that olfactory stimuli during sleep can significantly increase deep sleep and sleep quality as well as improving self-reported mood and positive content of dreams. Moreover, when a scent is paired with a learned material during the day and reactivated at night (Targeted Memory Reactivation) it has been shown to improve memory consolidation. We have also shown that by pairing scent with a VR experience, we can significantly increase subjective relaxation rates and relaxation scores associated with alpha and theta brain frequencies.We developed novel wearables, concepts, software, and designs that improve considerably on state of the art olfactometers. In comparison with previous scent delivery systems used for sleep studies, our prototype is more than 40 times lighter and 700 times smaller. Current scent technologies used in sleep laboratories are not portable and require the use of nasal masks, large olfactometers, and a minimum of 22 wire attachments to track physiological information. As a result, current technologies are not suitable for mobile, day-time applications or home usage by non-technical users. We designed, built and successfully deployed scent technologies that can release scent based on physiological or context-based information and can be worn during the day and re-activated at night. We have also used these devices to perform the first home-based scientific experiments of scent-delivery for Targeted Memory Reactivation and improvement of sleep quality, and have opened up the opportunity for sleep scientists to do in the wild studies.",2020-11-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/10/30/Screen Shot 2019-10-30 at 9.31.04 AM.png,375998,,2023-03-23T18:02:36.334Z,['amores@media.mit.edu'],True,closed-loop-olfactory-interfaces,2016-11-01,"['design', 'virtual-reality', 'human-computer-interaction', 'augmented-reality', 'cognition', 'consumer-electronics', 'fashion', 'health', 'human-machine-interaction', 'perception', 'wearable-computing', 'sensors', 'interfaces', 'affective-computing', 'computer-science', 'electrical-engineering', 'engineering', 'wellbeing', 'physiology', 'human-augmentation', 'medicine']",Essence Wearables: Biometric Olfactory Interfaces for Day and Night,PUBLIC,https://www.media.mit.edu/people/amores/overview/,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2019-11-04T00:56:24.053Z,"The RGBox is an experimental game to teach fundamentals of additive color mixing theory. As opposed to the behavior of pigments that get darker once mixed together (subtractive mixing), light colors get brighter once you increase the amounts of red, green, and blue in the mix. The RGBox is intended to give users intuition with this concept via a tangible experience.The interface is composed by an LED strip and potentiometers. Within a given time, the user has to match an initial randomized color by adding or subtracting color amounts from a black pixel. Developed at MIT Media Lab and International Design Centre.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/11/03/rgbox gif_1.gif,376058,,2019-11-05T14:07:10.363Z,['gba@media.mit.edu'],True,rgbox,2019-06-01,"['learning-teaching', 'interfaces']",RGBox,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-11-04T01:26:47.252Z,"Graphics on incompletenessThe City Science Summit 2019, CITIES WITHOUT, explored a future without top-down and obsolete systems. During the two-day event, talks and workshops discussed new ways to address  issues in rapidly urbanizing cities. But  withoutness  goes further.  It is the permanent state of incompleteness and therefore constant change of everything. Deriving from this idea, a series of three short movies (each one introducing the main themes of the science talks) and the summit's visual identity was created.",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/11/04/physical_1.gif,376066,,2019-11-08T20:25:42.175Z,"['gba@media.mit.edu', 'noyman@media.mit.edu']",True,incompleteness,2019-10-01,['design'],WITHOUT Identity,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2019-11-04T19:47:03.458Z,"New generative AI technologies (such as the Generative Adversarial Network, or GAN) can allow us to imagine new species. Hidden within the neural network, there are millions of these ""ganimals"" that no one has ever seen before. These ganimals occupy a digital landscape not unlike our own, where attention is short, and engagement is necessary to survive.Meet the Ganimals is a collaborative social experiment to discover new species, breed your own, and feed the ones you love. The data you provide about the ganimals is their ""food."" Unfed ganimals are quickly forgotten, so ganimals you engage with have a genetic advantage, and breed more often. Thus, to withstand the harsh conditions of the attention economy, ganimals adapt to the crowd’s opinions and preferences. So the question remains, which ganimals do you want to thrive? We built Meet the Ganimals to showcase how artificial intelligence can not only generate images of realistic animals, but also images of believable hybrid animals. These hybrid animals are created with a Generative Adversarial Network (GAN), which we baked into their name: ganimals. When we combine a Goldfish with a Golden Retriever, the result is a combination of the most striking features of both animals: a bright orange, big eyed, mopey-eared underwater canine. What the AI features is based on the idiosyncrasies of an animal relative to the pool of animals that we have selected. For ganimals, the striking and transferrable morphological features are stripes on a zebra, eyes on dogs, the shape of a great white shark, the color of goldfish, the feathers of an owl, and many more. One could argue that the biodiversity of ganimals rivals that of the natural world. Artificial evolution is not constrained by the laws of the physical world and fitness but instead the limits of our imagination and our aesthetic tastes. We can curate ganimals by adapting the AI model or adding and removing images of species on which to train the model.The Meet the Ganimals website is an online social experiment to see what happens when we allow our aesthetic interests to guide evolution. Our goal is to direct thousands of people to the website in order to see how these new artificial life forms evolve, and what dynamics emerge. This number of people is critical for us to properly understand how aesthetic interests guide evolution. ",2024-06-26,"['affective-computing', 'human-dynamics', 'responsive-environments', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2020/04/27/Screen Shot 2020-04-27 at 10.24.39 AM.png,376081,,2023-03-13T02:30:23.304Z,"['oceane@media.mit.edu', 'groh@media.mit.edu', 'zive@media.mit.edu', 'dubeya@media.mit.edu']",True,meet-the-ganimals,2019-11-04,"['artificial-intelligence', 'environment', 'ocean']",Meet the Ganimals,PUBLIC,http://ganimals.media.mit.edu,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2019-11-08T18:37:53.444Z,"A Counting is an ongoing series of software-generated live-streamed sound and video works composed of an evolving neverending count from 1 to 100, featuring a different crowd-sourced voice and language for each number. As a vocal portrait and sonic meditation, it explores the ethnic and linguistic diversity of US cities. Starting in 2019 with New York City, we've been collaborating with museums and local communities to create ongoing site-specific editions for Boston-Cambridge (‘22–), Ogden (‘21–), Omaha (‘21–), St. Louis (‘21–), and Houston (‘20–). We invite locals to call our city hotlines and count in languages they have ethnic or cultural ties to. There are also editions for Indigenous and signed languages that are in development. Each spoken language edition includes a webpage, hotline, sound and video streams. Across all the editions, the work includes over 125 languages from over 1000 participants. All editions are free and accessible to the public within the hours of the gallery and 24/7, both over the phone and online. Current ExhibitionsA Counting: Boston-Cambridge at the MIT Museum, Cambridge, MA, October 2, 2022–May 31, 2023 A Counting: St. Louis  at the Contemporary Art Museum St. Louis, St. Louis, MO,  Ongoing A Counting: Houstonat the Contemporary Arts Museum Houston, Houston, TX, OngoingPrevious ExhibitionsA Counting: Ogden at the Mary Elizabeth Dee Shaw Gallery at Weber State University Ogden, UT,  January 29–April 22, 2022 All Together, Amongst Many: Reflections on Empathyfeaturing A Counting: Omaha and Deconstructed Anthems: Nebraska 12 (2015) at the Bemis Center For Contemporary Arts, Omaha, NE, June 5–September 19, 2021Who We Are: Visualizing NYC by the Numbers, featuring A Counting: New York City at the Museum of the City of New York, New York City, NY, November 22, 2019–October 18, 2020",,"['poetic-justice', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/03/10/AC USAll_1200x1200.gif,376146,,2023-09-07T23:15:08.630Z,['ekene@media.mit.edu'],True,a-counting,2019-11-22,"['robotics', 'design', 'art', 'data', 'music', 'politics', 'creativity', 'community', 'data-visualization', 'natural-language-processing', 'social-change', 'racial-justice', 'marginalized-communities', 'data-science', 'diversity']",A Counting,PUBLIC,http://a-counting.us,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-11-11T17:36:03.335Z,"The Grain Prism is a hybrid musical instrument that combines granular synthesis and live audio recordings. Presented in a capacitive touch interface, users are invited to create experimental sound textures with their own recordings. The interface’s touch plates are introduced within a series of obscure glyphs, instigating the player to decipher the hidden sonic messages. This way, the mysterious interface opens space to aleatoricism in the act of conjuring sound, and therefore the discovery of “happy accidents” in making electronic music. Project presented at NIME 2019 (New Interfaces for Music Expression).",,['city-science'],https://dam-prod2.media.mit.edu/x/2019/11/11/grain prism_long cut_1.gif,376156,,2023-04-29T19:50:26.420Z,"['gba@media.mit.edu', 'ddh@media.mit.edu']",True,grain-prism,2019-07-01,"['design', 'art', 'music', 'interfaces']",Grain Prism,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2019-11-13T16:46:27.126Z,"The Green Book Project is a work-in-progress series of publications and interactive installations that reimagine the Negro Motorist Green Book for “traveling while Black” in this era of “New Jim Crows.” The Negro Motorist Green Book was created by Victor Hugo Green. He started publishing it in 1936  when Jim Crow had legalized segregation and racialized spaces were highly visible. It listed “safe” places for Black travelers or as John Williams describes “[places] Negroes can stay without being embarrassed,  insulted, or worse... [Black] people have a way of disappearing on the road."" In 1966 when The Civil Rights act abolished segregation, Green thought the US was moving towards a place of “equal opportunities and privileges for Blacks” and stopped publishing his book. However, five decades later, Black Americans are still fighting for racial equality and disappearing through the “New Jim Crows” of mass incarceration (e.g., Kalief Browder) and police brutality (e.g., Sandra Bland). So, the US still isn’t where Green thought it’d be when he stopped publishing his book. In 1900, W. E. B. Du Bois mapped the state of Black life through a series of hand-drawn charts and maps. Now,  we’re mapping the state of Black mobility and safety today through a series of big data and oral history powered publications and multimedia installations, titled The Green Book Project. It will reimagine the Negro Motorist Green Book for “traveling while Black” in this era of “New Jim Crows."" Through this research, we created a course titled Black Mobility and Safety in the US.",,['poetic-justice'],,376179,,2020-10-23T15:36:14.041Z,['ekene@media.mit.edu'],True,the-green-book-project,2019-06-03,[],The Green Book Project,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-11-17T19:34:35.965Z,"PlusMinus is integrating participatory design and co-design approaches in this project funded by the Robert Wood Johnson Foundation, and in collaboration between New America, the Harvard T.H. Chan School of Public Health, (HSPH), the Broad Institute, and IDEO. We're examining how technological innovation can speed the rapid translation of scientific advances to reverse shameful health disparities in the United States, improve community public health practice and reshape local, state, and federal government public health infrastructure.The PlusMinus community is definin and testing a design and evaluation process for equitable technology development.  ",,['plusminus'],,376238,,2019-11-17T19:36:18.280Z,[],False,public-health-technology-hackathon-and-working-groups,2019-08-05,[],"Public Health ""Technology Hackathon"" and Working Groups",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2019-11-18T16:48:47.747Z,"As global interest in deep space exploration rises, new mission architectures and new dwelling solutions must be sought for to accomplish longer and safer permanence in space.Less dependency from Earth supplies, better psychological and physical conditions for the astronauts, higher safety, and lower energy and resources consumption are the main requirements for such missions, and must be matched and experimented from the very beginning of human deep space exploration. To address this same need for higher feasibility and sustainability, this research explores  a novel design for a greenhouse module that can supply 100% of the food required for a crew of four astronauts on an extended mission to Mars, while also providing physical and mental health benefits for the crew members. The module accomplishes this by maximizing space and minimizing mass with a novel spiral system within an inflatable, cylindrical shell designed to protect astronauts from harmful radiation. Crops, which supply the food for the crew, grow in modular hydroponic trays that descend from the top floor of the module along six spiral tracks. The lighting, temperature, nutrient supply, track length, and vertical separation of each spiral is matched to plants growth patterns and needs, thereby maximizing volume for growth, optimizing growing conditions, and providing isolation in case of disease.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2019/11/18/Marsboreal_ML_site.jpg,376249,,2023-02-15T20:23:06.580Z,['vsumini@media.mit.edu'],True,marsboreal-greenhouse,2018-06-01,[],Marsboreal Greenhouse,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2019-11-26T11:32:16.007Z,"O Desafio de Aprendizagem Criativa Brasil é o programa de fellowship da Rede Brasileira de Aprendizagem Criativa. Ele tem como objetivo fomentar a implementação de soluções inovadoras que ajudem a tornar a educação brasileira mais criativa, prazerosa, relevante, colaborativa e inclusiva para crianças e jovens de todo o Brasil.Com o apoio da Fundação Lemann e do MIT Media Lab, o Desafio 2020 busca propostas que desenvolvem especificamente ações aprofundadas de aprendizagem criativa em sala de aula com o apoio de múltiplos níveis decisórios no ecossistema escolar. Neste ano, estamos procurando equipes formadas por gestores, diretores e professores já empenhados em tornar as escolas de sua região mais criativas e relevante para todos.Cada equipe selecionada receberá R$10.000,00 para a realização do seu projeto, visitará o MIT Media Lab e participará de encontros presenciais dos Fellows em eventos no Brasil com todas as despesas de transporte e hospedagem custeadas pelo DAC.As inscrições começam no dia 25/11!Informações: https://www.desafioaprendizagemcriativa.com/",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2019/11/26/IMG_6360.jpg,376305,,2022-09-22T01:32:48.023Z,"['mres@media.mit.edu', 'leob@media.mit.edu']",True,desafio-aprendizagem-criativa-2020,2019-11-26,[],Desafio Aprendizagem Criativa Brasil 2020,PUBLIC,https://www.desafioaprendizagemcriativa.com/,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-12-03T14:54:43.254Z,"In recent years, there has been an increase in low-cost and open-source electronic and chemical sensors that hobbyists, concerned citizens, and grassroots organizations have installed within their local environment. These sensors provide high-quality data on air and water conditions, allowing individuals to collect longitudinal datasets by monitoring how their local ecosystems are changing. Unfortunately, biological sensing, sensing that uses biological molecules to detect a specific chemical or biological substance, has lagged behind. This is primarily due to the high costs associated with laboratory-grade testing and the need for specialized equipment. The ability to use biology to better sense in the biological dimension is key to generating  higher-resolution portraits of our ecosystems, helping us learn more and react quicker to the changes we see.This project focus on developing biological sensors which leverage the unique collateral cleavage  property Cas12a, a CRISPR nuclease, to create a novel biosensor  [1,2].  This breakthrough allows for highly sequence-specific nucleic acid-targeting biosensors (able to even detect single base changes) that provide a response on the order of minutes rather than days typically needed to culture or sequence samples. These sensors have detected targets present at the attomolar level and have successfully been freeze-dried on a variety of substrates, eliminating the need for cold chain reagent storage.",2020-12-31,"['responsive-environments', 'space-exploration', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2020/03/23/DSC06853.JPG,376386,,2023-09-07T15:43:54.053Z,['devora@media.mit.edu'],True,crispr-biosensors,2019-07-15,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'environment', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'ocean', 'chemistry']",Envirome Monitoring with CRISPR Biosensors,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2019-12-05T04:40:09.221Z,"Mexico does not own the IP of their most famous architect. What happens to an artist's ideas, artifacts, and work when they die? In an age where we can computationally generate style can we do this for deceased artists? This project seeks to speculate and generate the drawings of one of the greatest architects of all time and seeks to challenge the notion that one can steal style. ",,[],,376450,,2019-12-05T04:41:39.417Z,[],False,barragan,,[],BarraGAN,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2019-12-05T04:42:31.183Z, An interactive mirror where you can record supportive messages and answer question to get rides home.,,[],,376458,,2019-12-05T04:44:25.698Z,[],False,safety-mirror,,[],Safety Mirror,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-12-06T21:10:45.158Z,"A major concern for manned missions is space radiation. Ionizing radiation is known to pose both acute and chronic risks to many organisms, including humans. At this time, it is important to expand strategies for radiation protection, including utilizing new materials and fabrication methods that can withstand different forms of radiation. The Mediated Matter group is researching the synthesis of biological pigments, including melanins and carotenoids, for biocompatible radioprotection. Such pigments can be fabricated for a variety of applications, creating a new class of materials and coatings that can protect life on Earth, in deep space, and beyond. Here, we examine the growth and behavior of five pigment-producing microorganisms after their placement in the interior of the International Space Station for one month as part of the RIDES payload. This experiment payload will launch on the SpaceX CRS-20 on March 6, 2020.",2020-09-30,"['mediated-matter', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2019/12/06/aspergillus1.JPG,376478,,2023-09-07T15:41:02.243Z,"['rssmith@media.mit.edu', 'neri@media.mit.edu', 'nah6cz@media.mit.edu', 'ssunanda@media.mit.edu']",True,radiofungi,2019-05-01,"['space', 'biology', 'zero-gravity']",Radiofungi: Biological Pigments for Radioprotection,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2019-12-16T21:29:31.292Z,"Respite is a class project, tested only in a limited pilot study. We have learned about and experienced the subjective benefits of mindfulness. These come in the form of lowered anxiety, improved sleep, longer attention span and increased interoceptive awareness. Mindfulness cannot and should not be reduced to these practical benefits: it is not simply a means to an end, and understanding it as such is a reduction of spiritual practice to wellness intervention. Yet each of these benefits, whether it be sleep or attention related, can also be understood as a tracking mechanism—tangible trait and state changes correlated with increases in mindfulness—so that those engaging in the practice of mindfulness can better understand their progress. Understanding how mindfulness is changing measurably in each of us, as habit and behavior and personality trait, allows for more informed structuring and iterating on daily mindfulness practice. Mindfulness thus far has proven hugely difficult to operationalize, or make measurable (Quickel, Johnson, and David 2014). Many different scales exist to measure mindfulness, and these are complicated by gulfs between subjective assessment and objective assessment, expert mindfulness and novice mindfulness, different meditative practices, and trait versus state mindfulness. Yet a new paper from Richie Davidson’s lab offers a step towards a solution in the form of the first validated behavioral measure of mindfulness—breath counting. In four independent studies with over 400 total participants, they found breath counting correlated with self-reported mindfulness, differentiated long-term meditators from age-matched controls, was associated with more meta-awareness, less mind wandering, better mood, and greater non-attachment. This one skill, breath counting, is a reliable index for so many different facets of mindfulness! Yet it also is a training mechanism: Davidson et al found that 4 weeks of breath counting training improved mindfulness and decreased mind wandering relative to working memory training and no training controls. These findings provide the first evidence for a behavioral measure of mindfulness, and unsurprisingly have their roots in a breath counting practice found as far back as 430 A.D. (Levinson et al. 2014). We took this study as inspiration for translational neuroscience of mindfulness! Breath counting, if it can be done cheaply and simply, could open up doors to tracking and training mindfulness and its many facets. We built Respite as a class project for Principles of Awareness: a mobile app which integrates the elements of the Davidson paper into an iOS app, including 1) breath tracking 2) breath training 3) mindfulness trait assessment MAAS 4) habit forming assessment SRHI 5) integrated scientific explanations. With this app, people can train and track their mindfulness via breath, see how their trait mindfulness personality is changing over time with the MAAS, see how successfully they are forming a mindfulness habit over time with the SRHI, and better understand the science which elucidates a mindfulness practice. We hope this tool can help people have a clearer picture of their practice and how it changes with time. HardwareOne of the goals of this project is to make this system massively deployable. Therefore, we wanted to use off-the-shelf hardware and nothing more. We chose to use the accelerometer embedded in the iPhone. According to Apple’s official documentation, the maximum frequency at which one can request accelerometer updates is hardware-dependent, but is usually at least 100 Hz (which is what we are using). Therefore, this system should be able to work in any iPhone 6 and newer.SoftwareWe sampled at a frequency of 100 Hz. To count the number of breaths, we used 4 filtering stages and one peak detector algorithm. First we cleaned up the data removing big jumps  between samples. Given that for deep breathing one normally do a breath of at least 5 seconds, the breathing frequency is at most 0.2 Hz. Therefore, the second stage was a low-pass filter with a cut-off frequency of 0.25 Hz. This removed all the unwanted high-frequency noise. The third stage was an envelope-follower with a window size of 50 samples, meaning that the output was the highest value in the last half-second. To smooth-out this signal, we applied a second low-pass filter with the same cut-off frequency of 0.25 Hz. Finally, a peak detector algorithm was used to count the number of breathe-ins and breathe-outs. If a sample was bigger than the previous one, the point was considered positive, else, negative. If 5/9th of a window of 50 was positive, the point was considered a breathe-in, else, breathe-out. 30 or more consecutive points of the same category were considered either a breathe-in or out.InteractionThe interaction is as follows: the user fills-in both surveys. Then they go into the breathe count screen, select a session duration, press start, and place the iPhone on their stomach. After the selected time passes, the app emits a sound indicating the end of the session and prompts the user to input the number of breaths they counted. The survey scores and breathe counting accuracy is logged and graphed across time.Pilot ExperimentWe executed a pilot experiment (n=8) for our final class project wherein subjects engaged in breath counting and self-assessment of mindfulness traits and habits. Subjects counted breath for 3 minutes while the Respite iOS app counted their breaths. Both scores were compared for deltas, and measured against the ground truth of an observing experimenter counting breaths by sight. This pilot experiment is meant to 1) validate the utility of the Respite iOS app for breath counting within the context of mindfulness tracking and 2) identify any trend relationships between successful breath counting and subjective mindfulness assessments. One subject was eliminated due to experimenter error, namely poor placement of the iPhone during breath counting. ResultsWith limited subjects (n=7) it is not meaningful to make claims to statistical significance, but there is apparent success in breath tracking with the Respite app, as shown in minimal deltas between RespiteCount and ExperimenterCount in the comparison of breath counts (seen below). We have arranged a meeting with Professor Richie Davidson, after sending him information on our Respite iOS demo. We will work with him to improve app aesthetics, training video, and onboard mindfulness assessments. We aim to execute a breath counting test with a longer count period, and test this data on breath count deltas (likely more varied) against MAAS and SRHI data collected from within the app.  We plan on putting the Respite app online, free and open-source, such that his paper can have maximal impact beyond the neuroscience community. A behavioral marker of mindfulness that can be tracked and trained is exciting for the community of neuroscientists who will read his paper, but should also be a tool available to the public at large who want to navigate their own spiritual and scientific practice. This project exists within a larger framework of science/spirituality/tech interactions. Our Awareness class exists within the Media Lab, and so a vocabulary infused with scale, technology, impact and objectivity are ever-present. Yet we have come across the friction between many ways of knowing, and the spaces in between. A meditation practice may not be a wholly knowable thing in the scientific sense, and the Awareness class created space for this ambiguity and personal experience. This is part of what made many of us grow over the semester, taking opportunities in discussions and in practice to think beyond strict logical models of productivity and truth. Setting time aside to simply sit still, and then finding it hugely useful, will complicate traditional models of productivity espoused at MIT!As we move forward with this project, and intersect with principles of objectivity in science and generalizability in technology, these central themes and questions must form a key part of our design process. What is the best way to work with subtle spiritual practices, that are by nature irreducible? And what if those practices have concrete cognitive and physiological benefits, which should be spread for the greater good? And how can technologists interact effectively with those scientists and spiritual practitioners each seeking to understand a practice in different lights? And what are the ethics of condensation of something as subtle as a mindfulness practice into an app, digestible and yet incomplete? These are all questions which we are interested in framing and reframing as we work between many modes of self-understanding and aim to be effective translators.",2021-03-18,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2019/12/16/Screenshot 2019-12-16 16.28.59.png,376532,,2020-05-21T07:15:22.194Z,"['adamjhh@media.mit.edu', 'tomasero@media.mit.edu']",True,respite,2019-02-12,[],Respite,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2019-12-21T07:02:51.453Z,"Living in space could have significant physical and mental impacts on astronauts. Wearables have the potential to play a critical role in monitoring, supporting, and sustaining human life in space, lessening the need for human medical expert intervention. Lab on Body is a device that integrates laboratory functions on a single integrated device to achieve customizable multiplex health monitoring automation. We demonstrated the vision of integrating such technology with the human body to sense biological signals related to health conditions at the molecular level. We have built prototypes of ""wearable lab on body” that actively and continuously monitor human biomarkers from biological fluid. Our platform contains both digital sensors for contextual activity recognition, as well as an automated system for continuous sampling and sensing of chemical biomarkers from saliva by leveraging already existing paper-based biochemical sensors. This information collected from the body can be used to provide real-time feedback and assistance to the person in space, as well as to track their physical and mental health. The platform could aid with longitudinal studies of biomarkers and early diagnosis of diseases.",,"['fluid-interfaces', 'space-exploration']",https://dam-prod2.media.mit.edu/uuid/7f90abc7-6cfe-484c-9cd3-7d3c1a71552a,376558,,2023-10-12T13:28:44.829Z,"['patpat@media.mit.edu', 'suryaj@media.mit.edu', 'dkong@media.mit.edu', 'pattie@media.mit.edu', 'abyjain@media.mit.edu']",True,bio-digital-wearables,,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Bio-Digital Wearables for Space Health Enhancement,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-01-05T15:49:41.672Z,"Colorchives represents a continuous and interweaving research thread by Nina Lutz.Color quantization is a long studied issue in computer graphics. The idea of color quantization is utilizing decompression to represent an image in less pixels than its original format. Color quantization is also utilized to generate color palettes and information from images by converting colors into geometric color spaces, represented below.",2021-08-20,['future-sketches'],https://dam-prod2.media.mit.edu/x/2020/10/19/Graphic6.png,376588,,2021-08-20T14:53:00.803Z,"['nlutz@media.mit.edu', 'zachl@media.mit.edu']",True,colorchives,2020-01-05,[],Colorchives,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-01-07T15:11:20.040Z,"As of August 2020, Ring has active partnerships with over 1400 law enforcement agencies across the US. These partnerships allow law enforcement agencies to use the ""Neighbors Portal,"" an extension of Ring's surveillance-as-a-social-network Neighbors App. The portal gives law enforcement the ability to make posts and comment on videos that users share to the app, but it also allows them to ""submit requests for video recordings."" The partnerships also give law enforcement the ability to offer subsidized ring cameras to residents, and training on how to successfully solicit videos from Ring users.If Ring is part of ""the new neighborhood watch,"" shouldn't we know where they're looking? Beyond knowing which law enforcement agencies have partnered with Ring, researchers, policymakers, and activists have very little information about how Amazon's growing surveillance network is spreading. To answer questions about Ring's spread and its presence in neighborhoods around the US, we're using the API that Ring uses to deliver data to its Neighbors smartphone app to pull all the alerts that have been posted to their ""social network"" since the beginning of 2017.To understand what makes a neighborhood more or less likely to use Ring's surveillance service, we are using spatial modeling techniques at the county level to predict Ring camera density from demographic and crime data. To test Ring's claims that their service helps public safety, we are using point-level data to test the effects of adoption and use of Ring cameras on crime solve rates in certain cities. We are also making tools that provide policymakers and activists with crucial information about neighborhood surveillance spread to inform policymaking and direct action.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2020/01/07/map-no-title.png,376606,,2020-10-16T01:59:45.172Z,['dcalacci@media.mit.edu'],True,corporate-surveillance-amazon-ring,2019-09-01,"['design', 'human-computer-interaction', 'civic-technology', 'data', 'networks', 'privacy', 'security', 'community', 'industry', 'cities']",Corporate surveillance of the commons + Amazon Ring,PUBLIC,http://dcalacci.net,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-01-10T20:56:14.089Z,"Human activities, such as research, innovation and industry, concentrate disproportionately in large cities. The ten most innovative cities in the United States account for 23% of the national population, but for 48% of its patents and 33% of its gross domestic product. But why has human activity become increasingly concentrated? Here we use data on scientific papers, patents, employment and gross domestic product, for 353 metropolitan areas in the United States, to show that the spatial concentration of productive activities increases with their complexity. Complex economic activities, such as biotechnology, neurobiology and semiconductors, concentrate disproportionately in a few large cities compared to less—complex activities, such as apparel or paper manufacturing. We use multiple proxies to measure the complexity of activities, finding that complexity explains from 40% to 80% of the variance in urban concentration of occupations, industries, scientific fields and technologies. Using historical patent data, we show that the spatial concentration of cutting-edge technologies has increased since 1850, suggesting a reinforcing cycle between the increase in the complexity of activities and urbanization. These findings suggest that the growth of spatial inequality may be connected to the increasing complexity of the economy.",,[],https://dam-prod2.media.mit.edu/x/2020/01/13/Banner3_UkFZKHh.png,376645,,2020-01-21T16:05:06.747Z,"['balland@media.mit.edu', 'crisjf@media.mit.edu', 'hidalgo@media.mit.edu']",True,complex-economic-activities-concentrate-in-large-cities,2018-01-01,[],Complex Economic Activities Concentrate in Large Cities,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1
False,2020-01-11T21:46:19.466Z,"This is a two-part project. One part is about reality. The other part is science fiction. Both are about mobile, blockchain, and cryptographically secure voting systems in the context of US elections.Securing elections and increasing access to voting are critical for our democracy to function properly.  There has been recent focus on using mobile apps and blockchain-based systems to address these needs, but some of these systems can cause more harm than good. The Reality piece is about the innovations that can make secure and transparent elections possible, and where internet-voting, mobile apps, and blockchains fit in. The Science Fiction piece uses storytelling to explore how our voting systems can and should work. It looks ahead at two potential futures for US democracy that branch from our present. One story is a dystopia where technology is misused, resulting in a disengaged and disenfranchised public, where wealth determines voting power. The other story is a utopia where elections are celebrated holidays, where technology makes voting more secure, and social changes make voting more accessible.This work came out of the MIT Digital Currency Initiative’s Blockchain Ethics course.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/01/11/pins-wide.png,376664,,2020-01-24T00:03:12.390Z,"['aberke@media.mit.edu', 'kll@media.mit.edu']",True,crypto-voting-us-elections,2019-09-11,"['open-source', 'politics', 'security', 'blockchain']",Crypto Voting + US Elections: Reality and Science Fiction,PUBLIC,http://aberke.com,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-01-14T15:06:01.368Z,"In this work, we propose a novel fabrication method for 3D objects based on the principle of spooling. By wrapping off-the-shelf materials such as thread, ribbon, tape or wire onto a core structure, new objects can be created and existing objects can be augmented with desired aesthetic and functional qualities. Our system, WraPr, enables gesture-based modelling and controlled thread deposition. We outline and explore the design space for this approach. Various examples are fabricated to demonstrate the possibility to attain a range of physical and functional properties. The simplicity of the proposed method opens the grounds for a light-weight fabrication approach for the generation of new structures and the customization of existing objects using soft materials.",2019-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/01/14/WraPrBanner-Edited.jpg,376693,,2020-06-09T17:37:44.924Z,"['ishii@media.mit.edu', 'ken_n@media.mit.edu', 'joaleong@media.mit.edu']",True,wrapr-spool-based-fabrication-for-object-creation-and-modification,2019-03-01,"['design', 'human-computer-interaction', '3d-printing', 'materials', 'fabrication']",WraPr: Spool-Based Fabrication for Object Creation and Modification,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-01-23T00:11:33.361Z,"Hybrid Living Materials (HLMs) are formed by combining living and non-living materials such that the resulting composites take on the functional properties of both. In the 2019 paper published to Advanced Functional Materials, we detailed the HLM Platform: a method of interfacing a 3D digital design and printing platform with engineered bacteria, in such a way that we achieve programmable, replicable control of gene expression across the surface of 3D printed objects.",2021-05-21,[],https://dam-prod2.media.mit.edu/x/2020/01/22/gradients2.png,376806,,2021-05-21T14:42:38.321Z,"['rssmith@media.mit.edu', 'kolb@media.mit.edu', 'bader_ch@media.mit.edu', 'ssunanda@media.mit.edu', 'limulus@media.mit.edu', 'neri@media.mit.edu']",True,hybrid-living-materials,2020-01-23,"['design', 'manufacturing', 'synthetic-biology', '3d-printing']",Hybrid Living Materials,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-01-27T08:52:49.800Z,"By Hila Mor, Yu Tianyu, Ken Nakagaki, Benjamin Harvey Miller, Yichen Jia, Hiroshi IshiiThe Venous Materials project envisions a new way to design dynamic tangible interactions using fluidic structures. We are inspired by venous structures that are ubiquitous throughout nature and inside the human body. By changing colors, veins can inform us of internal and external physical conditions. For example, veins in leaves transmit pigments that drive color change, which informs us of the leaf’s internal condition as well as which season it is. And when we press firmly on a button, the tip of our finger changes its color—this subtle color change can be used as an indicator of the amount of applied pressure.Similarly, the displayed flow pattern and color change of the fluidic channels within Venous Materials inform the users of the motion and physical force that’s applied on the material. The users can interact with Venous Materials by deforming or applying pressure on the material, which drives the fluid flow within the internal channels to visualize the dynamic responsive display. Therefore, Venous Materials simultaneously functions as a sensor and display of tangible information. While computer chips and electronics usually require rigid and bulky components, Venous Materials is a soft and self-contained mechanism that utilizes the motion of daily activities as its energy source. Through research with microfluidics technology, we developed an approach for designing, simulating, and prototyping fluidic interactive sensors that can be embedded in, or attached to, any object. We prototyped potential applications to Venous Materials integrate in everyday scenarios. For example, embedding it in clothing can help visualize on-body motion, pressure, and balance, while attaching it to objects can enrich learning activities, augment dynamic graphics, and indicate conditions of package delivery content. Venous Materials is a vision that puts forward a new paradigm in the field of human-computer interaction—the first step towards the integration of interactive fluidic mechanisms as tangible user interfaces. We hope our work will inspire researchers, designers, and artists by laying down a foundation for the growth of additional research within the realm of Venous Materials as interactive material design.",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/09/venous gif 2.gif,376823,,2021-09-23T19:33:47.914Z,"['hilamor@media.mit.edu', 'ishii@media.mit.edu', 'ken_n@media.mit.edu']",True,prototyping-interactive-fluidic-mechanisms,2019-06-01,"['design', 'human-computer-interaction', 'manufacturing', 'interfaces', 'computer-science', 'interactive', 'materials']","Venous Materials: Towards interactive, fluidic mechanisms",PUBLIC,http://hilamor.com,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-01-31T19:19:53.947Z,"An interactive  platform to improve decision-making related to the revitalization of the Champs ÉlyséesCityScope Champs-Élysée is a tangible interface that explores the experimental articulation between the diagnosis of existing conditions and the proposed interventions of PCA-STREAM for the Champs Élysées in 2024. The project, exposed at the Pavillon de l'Arsenal until May 2020,  illustrates interventions on the use of new mobility, the strengthening of nature along the avenue and finally the creation of new dynamic and modular spaces in the city.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/02/11/CS_Champs-hero.gif,376863,,2022-03-14T17:38:25.109Z,"['kll@media.mit.edu', 'agrignar@media.mit.edu', 'alonsolp@media.mit.edu', 'noyman@media.mit.edu', 'markuse@media.mit.edu', 'mdchurch@media.mit.edu', 'nicolasa@media.mit.edu']",True,city-scope-champs-elysees,2020-02-01,['urban-planning'],CityScope Champs-Élysées,PUBLIC,https://www.pavillon-arsenal.com/fr/expositions/11463-champs-elysees.html,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2020-02-05T01:37:23.763Z,"The Biomechatronics group recently pioneered a new type of amputation surgery, the AMI amputation, that allows patients to use a bionic prosthesis with greater control and sensory feedback. After a year of seeing patients demonstrate significantly decreased pain, improved mobility, and speak about their vivid phantom limb sensations, we started wondering how the brains of these patients changed as compared to traditional amputees. This was especially interesting since traditional amputees experience quite a bit of phantom pain and irritating sensations, arising from the abnormal way in which their brain adapts to a loss of a limb. In collaboration with the MGH Martinos Center, we performed brain scanning (MRI) to look at changes between normal subjects, subjects with traditional amputations, and subjects with the AMI amputation. We found that AMI subjects had significantly less change and were receiving sensory feedback in the brain, just like the control subjects. We also learned that they rely less on their visual system to guide their limbs, unlike traditional amputees. The regions of their brain that coordinate sensorimotor activity also resembled the activity of normal brains to a greater degree. This study is significant because it reveals that a specialized surgical paradigm can go beyond improving the functionality of our limbs and external organs to also improving the mechanisms of the brain—having a profound impact on pain, sensation, and more! This project is in collaboration with the Martinos Center for Biomedical Imaging.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2020/12/07/image_AIrMkw2.png,376912,,2020-12-17T15:14:14.571Z,"['shriyas@media.mit.edu', 'samga@media.mit.edu', 'hherr@media.mit.edu', 'hngnsong@media.mit.edu']",True,neuroimaging-to-investigate-changes-due-to-modified-amputation,2019-07-01,[],Brain plasticity following amputation,PUBLIC,https://www.linkedin.com/in/shriyasrinivasan,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-02-07T23:28:50.488Z,"SensorNets: Towards Reconfigurable Multifunctional Fine-grained  Soft and Stretchable Electronic Skins.SensorNets is a bioinspired electronic skin integrated with multimodal sensor networks for interactive media applications, from wearables, self-aware objects, to intelligent environments. It is developed by connecting miniaturized flexible printed circuit boards as two-dimensional sensor arrays with stretchable interconnects. The system is embedded in between soft deformable layers, such as textiles or rubbers. The result is a soft sensate surface that can be distributed and conformally wrap and adapt to curved structures. Each node contains a microprocessor together with a collection of nine sensors and a light-emitting diode, providing multimodal data that can be used to detect various deformation, proxemic, tactile, and environmental changes. We show that the electronic skin can sense and respond to a variety of stimuli simultaneously, as well as open up a possibility for sensor-rich virtual and augmented reality-based visualization and interaction.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2022/09/28/Screen Shot 2022-09-28 at 12.17.32 PM.png,376946,,2022-09-28T16:20:08.606Z,"['irmandy@media.mit.edu', 'elenack@media.mit.edu', 'joep@media.mit.edu', 'artemd@media.mit.edu']",True,sensornets,2019-11-01,"['design', 'human-computer-interaction', 'architecture', 'augmented-reality', 'environment', 'manufacturing', 'networks', 'wearable-computing', 'sensors', 'interfaces', 'mapping', 'materials', 'fabrication']",SensorNets,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-02-10T18:26:31.147Z,"Learning through Digital Play (LtDP) is a collaboration with the LEGO Foundation to explore how characteristics of learning through play—experiences that are active, social, iterative, meaningful, joyful—can be extended to digital environments. It looks at platforms like Scratch, LEGO Mindstorms, Minecraft, and others that can afford children with a sense of agency and opportunities for guidance from others (peers, parents, teachers, etc.) while creating something shareable. We are also interested in evidence of learning outcomes (academic and life skills), and insights on related topics like enabling factors (e.g., what facilitates use in schools or community settings), along with opportunities and challenges to scaling. The primary product will be a white paper, blog post, and related publications.  ",2021-08-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2020/02/10/spike prime robot.png,376967,,2022-05-11T14:33:04.693Z,['jhgray@media.mit.edu'],True,learning-through-digital-play,2019-07-01,[],Learning through Digital Play,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2020-02-11T19:44:19.376Z,"This strand of work seeks to develop new interventions to reduce the spread of information, and deploy those interventions towards a crowdsourcing framework for detecting and evaluating misinformation on social media. Design interventionsCrowdsourcing Results ",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2020/02/11/bg copy 2.png,377006,,2020-04-24T19:34:54.271Z,['zive@media.mit.edu'],True,crowdsourcing-the-detection-and-evaluation-of-misinformation,2020-02-04,['human-computer-interaction'],Crowdsourcing the detection and evaluation of misinformation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-02-12T20:36:10.388Z,"Emotional arousal influences focus, attention, and decision-making, which are critical when driving. To help promote an optimal arousal level, this work considers a closed-loop navigation system that adapts its voice tonality based on the physiology of the driver.  The Empathic GPS system comprises three different modes: 1) ""neutral'' in which the voice of the navigation is always the same, 2) ""congruent'' in which the perceived arousal of the voice mirrored the physiological arousal of the driver, and 3) ""incongruent'' in which the voice of the navigation system mirrored the inverted arousal of the driver.Below are sound files of exemplary navigation commands for both the calm and the aroused voice created with Amazon Polly from Amazon Web Services.",,['affective-computing'],,377041,,2021-10-04T16:14:49.291Z,"['sebzepf@media.mit.edu', 'picard@media.mit.edu', 'javierhr@media.mit.edu', 'neska@media.mit.edu']",True,empathic-gps,2020-02-12,['human-computer-interaction'],Empathic GPS,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-02-13T16:43:37.960Z,"What are radically sustainable methods for knitting, making and building in the age of the Anthropocene? How can humankind and members of other species such as silkworms collaborate in the construction of objects, products, and buildings? Can we extract silk without boiling cocoons?  Standing six meters tall and five meters wide, Silk Pavilion II offers insights into these questions by combining kinetic manufacturing with biological construction, uniting the built and the grown, fusing technology and biology.",2020-05-31,['mediated-matter'],https://dam-prod2.media.mit.edu/uuid/4c7ab85a-edf9-489d-ac09-4aa73b554810,377072,,2021-05-21T14:43:15.663Z,"['neri@media.mit.edu', 'jpcosta@media.mit.edu', 'bader_ch@media.mit.edu', 'ssunanda@media.mit.edu', 'fkraemer@media.mit.edu', 'susanwil@media.mit.edu', 'jdisset@media.mit.edu']",True,silk-pavilion-ii,2017-09-01,"['design', 'architecture', 'art', 'construction', 'synthetic-biology', 'biology', 'materials', 'covid19']",Silk Pavilion II,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2020-02-28T21:30:52.543Z,"There is a continuous and ubiquitous collection of precise, timestamped, geolocation data from apps and devices, being amassed by private firms. This data collection can be considered an ongoing “population survey” and can serve many of the same uses as traditional government travel surveys. Instead, these “location based services” (LBS) datasets are used for advertisement targeting, company analytics, and other means for private profit.  This work is about leveraging LBS data to benefit the public from whom it is sourced.However, the utility of these datasets must be balanced with the privacy of individuals within them.PrivacyThis work focuses on protecting the privacy of individuals within location datasets. To do so we develop an approach with machine learning deep neural networks to produce synthetic data that has the same attributes as the original data in aggregate, but sufficiently varies at the individual level in order to protect user privacy.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/02/28/Screen Shot 2020-02-28 at 4.34.06 PM.png,377352,,2022-03-08T17:32:52.653Z,"['emoro@media.mit.edu', 'doorleyr@media.mit.edu', 'kll@media.mit.edu', 'aberke@media.mit.edu']",True,private-location-data-for-the-public-good-and-urban-understanding,2019-10-01,"['civic-technology', 'privacy', 'security', 'computer-science', 'data-science']",Private Location Data for the Public Good and Urban Understanding,PUBLIC,http://aberke.com,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-03-02T17:46:15.956Z,"We envision that a close-to-nose interface could be used as a complementary or alternative wearable method to nasal cannulas or masks,  commonly used in medicine to deliver drugs to the body.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/03/03/collage_faces_square.jpg,377380,,2020-12-05T15:15:12.678Z,"['pattie@media.mit.edu', 'amores@media.mit.edu', 'yanan@media.mit.edu']",True,on-face,2019-01-01,"['design', 'human-computer-interaction', 'consumer-electronics', 'fashion', 'health', 'interfaces', 'engineering', 'fabrication', 'medicine']",On-Face Olfactory Interfaces,PUBLIC,http://www.judithamores.com,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-03-04T19:34:40.818Z,"The Digital Currency Initiative is interested in cryptography research beyond digital currency and blockchains. The DCI and its collaborators conduct research on cryptographic primitives that may be used in conjunction with blockchain technologies—such as zero-knowledge proofs and digital signatures—and on cryptographic tools and theories related to goals advanced by blockchain-based digital currency—such as anonymity, accountability/transparency, tamper-proofness, and free and secure communication.The DCI is also interested in technology policy, especially policy issues related to cryptography and digital currency. On one hand, how does or should modern technology policy impact use and development of cryptography and blockchain-based technologies? On the other hand, when and how can cryptographic tools efficiently promote specific policy goals—and when is cryptography or blockchain technology the wrong tool to achieve a given goal?",2022-12-31,['digital-currency-initiative-dci'],,377411,,2023-03-30T19:05:58.684Z,['narula@media.mit.edu'],True,cryptography-and-policy,2018-01-01,[],Cryptography and Policy,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-03-04T19:39:48.337Z,"We investigate the goal of deploying efficient non-interactive zero-knowledge proofs for moderately complex statements.  Motivated by the desire to deploy ZK proofs in real world distributed ledger systems, we seek ZK proof systems that have fast verification time, short proof size, and no trusted setup.  However, in terms of concrete parameters, we can achieve only two out of the three. In particular, it was not known how to achieve all of the following for million-gate circuits:fast (milliseconds) verifier;short proofs (couple kilobyte-long) ; anddoes not reply on a structured reference string (e.g., trusted setup) for soundness or zero-knowledge.  We propose a new form of proof systems: zk-SHARKs (zero-knowledge Succinct Hybrid ARguments of Knowledge). These combine the fast verification of zk-SNARKs with the no-trusted-setup of some non-succinct NIZKs.A zk-SHARK has two verification modes: a prudent mode (relying on a uniform random string), and an optimistic mode (relying on a structured reference string).  Crucially, even complete corruption of the setup used by optimistic verification does not invalidate the prudent verification.  Moreover, old ""prudent proofs'' can be re-accelerated with a new optimistic mode setup (in case the old setup becomes unconvincing or compromised).  We propose a construction of zk-SHARKs, tailored for efficiency of both modes: it is competitive with both state-of-the-art SNARKs (in terms of prover and verifier time) and NIZKs (in terms of proof size). Our zk-SHARK construction acieves all three properties outlined above.  We also discuss the applicability to transaction and block verification in blockchain applications.",2022-12-31,['digital-currency-initiative-dci'],,377419,,2023-03-30T19:07:09.144Z,['madars@media.mit.edu'],True,zk-sharks,2019-01-01,[],zk-SHARKS,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-03-04T20:32:20.067Z,"The OpenSolar project and platform aims to use blockchain and IoT-based smart contracts for disintermediation and contractual automation in financial processes to drive community-owned solar projects. We seek to develop open source core technology to streamline the flow of crowd-based funds (e.g., muni bonds and equity) to finance the deployment of solar assets, and allow the end-users to own these resources in a short time frame through regular utility-like payments driven by energy generation and consumption IoT data. The payment platform is built on the Stellar blockchain, and our pilot projects are done in public schools in Puerto Rico.",2021-10-01,['digital-currency-initiative-dci'],https://dam-prod2.media.mit.edu/x/2020/03/07/OpenSolar.jpeg,377429,,2021-10-04T18:16:29.063Z,['narula@media.mit.edu'],True,opensolar,,[],OpenSolar,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-03-04T20:36:10.244Z,"The DCI houses key Bitcoin Core developers and maintainers, who conduct Bitcoin Core code review and project maintenance, maintain a long-term sustainable Bitcoin implementation, and soft-fork testing and activation improvements to the process by which proposed new consensus changes are tested. To learn more about how our team contributes to Bitcoin Core development, click here.",,['digital-currency-initiative-dci'],https://dam-prod2.media.mit.edu/x/2020/03/07/bitcoin-price-latest-gold.jpg,377437,,2023-03-30T16:25:38.753Z,['coryf@media.mit.edu'],True,bitcoin-core-development,2016-09-01,[],Bitcoin Core Development,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-03-04T20:41:01.937Z,"The Working Group Program brings DCI Member Companies (1) and students together to work on real world projects that use blockchain technology, digital currency research, and many other new and evolving technologies.Students come from MIT, Harvard, and other universities at the undergraduate and graduate level; or from continuing education/professional development programs. Students can apply for this annual program during the Fall, and the program runs February - May. Once accepted they are placed into the appropriate groups, groups are determined by student interest and skill set. During the program the students work directly with Member Companies and are guided by mentors and advisors from MIT’s DCI and Sloan communities.Working groups also attend a DCI sponsored class called Blockchain Lab, specifically designed to help educate Working Group participants on topics that are important for the completion of their project. Working groups present their projects’ progression at prestigious MIT events, produce research papers, and submit open source code depositories.This program allows students to gain real life experience working in the industry, and allows Member Companies to gather fresh new insights and perspectives from passionate students. The goal of this program is to spur meaningful research and create innovative collaboration opportunities between students and industry experts in a safe learning environment.(1) Member Companies are companies that financially contribute to the Media Lab as a whole and/or more directly to a initiative or research group within the Media lab. Find out more about the Media Labs’s Member Companies at https://www.media.mit.edu/about/funding-and-support/ or focus on the DCI Member Companies at https://dci.mit.edu/support-our-work/.",2022-12-31,['digital-currency-initiative-dci'],,377445,,2023-03-30T19:03:42.795Z,"['rhysl@media.mit.edu', 'adragos@media.mit.edu', 'madars@media.mit.edu', 'coryf@media.mit.edu', 'tdryja@media.mit.edu', 'robleh@media.mit.edu', 'gensler@media.mit.edu', 'caseymj@media.mit.edu', 'narula@media.mit.edu']",True,blockchain-labs-course,2017-09-01,[],Blockchain Labs | Working Groups,PUBLIC,https://dci.mit.edu/blockchain-labs-working-groups,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-03-04T21:55:29.411Z,"Social robots equipped with AI and designed with users’ social, emotional, and relational interactions in mind are increasingly being highlighted as a way to enable older adults to age in their homes. However, older adults are often excluded in technology development and, in general, in their discussions of needs and desires. To ensure successful and sustained adoption of technology, participatory and co-design processes are essential in social robot development  to make sure older adults feel competent and empowered. This research process focuses on empowering older adults as partners in co-design research processes designing social robots. Over a year-long co-design project, we partnered with 28 older adults to understand how social robots should be designed, specifically exploring the areas of medical adherence, emotional wellness, social connection, financial management, memory, body signal monitoring, and exercise. The study was divided in 7 parts:-  initial interviews- art-based design sessions- robot hosting- debrief of robot hosting experience- rapid robot prototyping- design guideline sessions- reflection interview with card-sorting Initial results reveal how older adults want social robots to be design and illuminate older adults’ concerns for having AI systems in their homes. This methodology applied to social robot design demonstrates how users can and should be involved in designing AI that will socially impact the world. The methodology can be expanded and applied to other demographics affected by AI and focus areas of AI, such as public health and education.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2020/03/04/8jN0trXVQGyoY8GZkoBlJg.jpg,377455,,2023-09-04T18:05:31.405Z,"['akostrow@media.mit.edu', 'cynthiab@media.mit.edu', 'haewon@media.mit.edu']",True,co-design-of-social-robots-with-older-adults,2019-02-01,"['robotics', 'design', 'human-computer-interaction', 'social-justice']",Co-Design of Social Robots with Older Adults,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0
False,2020-03-06T19:56:47.819Z,Exploring the Atacama Desert and its intrinsic relationship to the Cosmos,,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2020/03/06/Saiwas Ramadita.jpg,377503,,2020-03-06T20:40:24.771Z,"['nicolelh@media.mit.edu', 'aekblaw@media.mit.edu', 'prathima@media.mit.edu']",True,myths-of-the-cosmos,2019-01-01,[],Myths of the Cosmos,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0
False,2020-03-09T15:18:55.829Z,"One of the principal benefits of counterfactual explanations is allowing users to explore ""what-if"" scenarios through what does not and cannot exist in the data, a quality that many other mediums of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces poor quality or unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that trains a generator, a discriminator, and a concept disentangler simultaneously to overcome such challenges using little supervision. DISSECT offers a way to automatically discover a classifier's inherent notion of distinct concepts rather than rely on user-predefined concepts. We validate our approach on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that our method performs consistently well across all. We demonstrate applications of DISSECT for detecting potential biases of a classifier, investigating its alignment with expert domain knowledge, and identifying spurious artifacts that impact predictions using simulated experiments.",2021-04-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2021/05/23/shapes_qualitative_dissect_mod.jpg,377531,,2022-05-12T17:51:52.248Z,"['asma_gh@media.mit.edu', 'picard@media.mit.edu']",True,towards-automated-explainability-with-disentangled-concept-discovery,2019-09-01,"['computer-vision', 'artificial-intelligence', 'extended-intelligence', 'human-machine-interaction', 'machine-learning']",DISSECT: Disentangled Simultaneous Explanationsvia Concept Traversals,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-03-12T16:27:09.734Z,"This project uses court and demographic data to ""flip the script"" on how algorithms are used for pretrial reform. Rather than measure the risk of individuals awaiting trial, we developed a risk assessment that predicts whether a judge will unlawfully incarcerate someone pretrial.",2020-12-31,[],https://dam-prod2.media.mit.edu/x/2020/03/12/JRAT_a1Y5cnQ.png,377595,,2020-03-12T20:40:12.497Z,[],False,judge-risk-assessment,2019-08-01,[],Judge Risk Assessment,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-03-14T21:31:43.950Z,"Biological Carbon Pump (BCP), which refers to the transport of particulate organic carbon (POC) from the ocean’s surface to the bottom, directly impacts the global climate. However, predicting ocean carbon sequestration remains highly uncertain due to the complexity of the BCP, observation methodology differences, and temporal and spatial aliasing. Therefore, mass-deployable (i.e. low-cost) sensors that directly observe the transport of organic particles are crucial.We set out to build an underwater stereo-imaging system to remotely measure the sinking POC in the ocean. The hardware design is primarily restricted by the depth (pressure), particle size (resolution) and sinking rate (framerate), deployment duration (battery) and cost (mass-deployment). Upon obtaining time-lapse images of the POC, 3D particle tracking velocimetry algorithm is applied to analyze the particle sinking rate. Ultimately, this will be attached to a Lagrangian float with other sensors to reliably measure various factors that affect the BCP.",2021-05-31,"['responsive-environments', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2020/06/14/sven-scheuermeier-saGbrA6s8g0-unsplash.jpg,377607,,2021-08-04T18:47:34.292Z,['junsuj@media.mit.edu'],True,tracking-marine-snow,2019-08-31,"['environment', 'sensors', 'engineering', 'ocean']",Tracking Marine Snow,PUBLIC,https://junsujang.com,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-03-15T15:38:14.972Z,"Grounded in science, #BeatTheVirus aims to raise awareness of the behaviors we all must adopt to collectively defeat COVID-19.This effort began in early March 2020 in a conversation between members of the MIT Media Lab and public health experts. They realized the urgent need to bring together fast evolving scientific knowledge of COVID-19 with state-of-the-art social media practices and media analytics to communicate essential information to all. This led to a partnership with McKinsey and New America. Now a growing network of partners have come together to launch #BeatTheVirus, a campaign to raise awareness of the behaviors we all must adopt to collectively defeat COVID-19Learn more at https://beatthevirus.org/",,"['directors-office', 'social-machines']",https://dam-prod2.media.mit.edu/x/2020/03/15/network-3139214_1920.jpg,377616,,2020-08-03T12:11:19.312Z,"['blumenth@media.mit.edu', 'sballing@media.mit.edu', 'russell5@media.mit.edu', 'dkroy@media.mit.edu', 'dkong@media.mit.edu']",True,beatthevirus,2020-03-15,"['social-media', 'covid19']",#BeatTheVirus,PUBLIC,,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-03-16T02:24:54.975Z,"Vélotropolis is a prototype supply-demand visualization tool designed with the aim to quickly highlight the gap in the provision of bike lanes across the city. Using available origin-destination data of shared bike systems, the tool derives the theoretical shortest travel path for each journey and overlays its trajectory on the city map. The aggregated layers of cycling trajectories, in turn, form a visual “demand density” on the road network, which can then be easily compared to the presence of bike lanes based on data typically made available through city governments’ open data platforms. The user can also integrate other information, such as the locations of bike accidents as a way to further inform the prioritization of investment on bike lanes in particular areas of the city.  ",2018-12-01,['city-science'],https://dam-prod2.media.mit.edu/x/2020/03/15/Velotropolis_8sec.gif,377624,,2022-10-13T16:28:42.737Z,"['ctkai@media.mit.edu', 'mcllin@media.mit.edu', 'ptinn@media.mit.edu']",True,velotropolis-1,2017-12-01,[],Vélotropolis: a bike lane supply-demand visual analytics tool,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2020-03-16T21:08:59.265Z,"Papers from SafePaths: https://github.com/PrivateKit/PrivacyDocumentsSafe Paths is an MIT-led, free, open source technology that enables jurisdictions and individuals to maximize privacy, while also maximizing the effectiveness of contact tracing in the case of a positive diagnosis. The Safe Paths platform, currently in beta, comprises both a smartphone application, PrivateKit, and a web application, Safe Places. The PrivateKit app will enable users to match the personal diary of location data on their smartphones with anonymized, redacted, and blurred location history of infected patients. The digital contact tracing uses overlapped GPS and Bluetooth trails that allow an individual to check if they have crossed paths with someone who was later diagnosed positive for the virus. Through Safe Places, public health officials are equipped to redact location trails of diagnosed carriers and thus broadcast location information with privacy protection for both diagnosed patients and for local businesses.Context Fast containment is key to halting an epidemic outbreak. But with the long incubation period of a virus like COVID-19, it is extremely difficult to identify individuals who may have been in contact with carriers of the virus and are thus at risk of contagion. Across the globe, the use of smartphones has been tested to track location and solve this problem, raising concerns about mass surveillance.  However, with our privacy-first method, the user remains in control of their data—providing a fundamentally different approach to app-based epidemic analytics.Resilience requires citizens and organizations to self-organize so that they can predict and respond to challenges (e.g., climate change) and disruptions (e.g., COVID-19). Such orchestration would be easy if everyone involved shared data about their past activities and future intentions openly, and responded to scientific evidence in ways that supported long term resilience, fairness, inclusiveness and accountability. This is, however, is challenging due to the need to maintain privacy, consent, trade secrets and compatible incentives.The current epidemic highlights this challenge. A ""big brother"" system in some countries has made a big difference in public health intervention via contact tracing, quarantine adherence verification, health verification, as well as tools for health officials such as spread analysis, resource allocation and incentive methods.Unfortunately, network analysis of social activities leads to a surveillance state. Thus, there are several big challenges to capture, analyze and act in a closed loop: (i) population scale understanding of a fast or slow moving threat without coercing an individual to reveal anything identifiable about themselves, (ii) analyze and providing precise guidance to an individual without the orchestration system knowing to who and what message is delivered and (iii) incentivize and verify the action while maintaining a sense of agency and privacy for the individual.These seemingly impossible problems can now be addressed thanks to (i) deep penetration of smartphones and IoT which can act to capture, compute, disseminate and act on information. (ii) the data sources associated with these devices (iii) practical and scalable privacy preserving algorithms and (iv) incentive mechanisms for networks of people and agents which act to guide individuals to support not only themselves but the society as a whole.Transparent, accountable, and inclusive ecosystems that can simultaneously address the privacy and utility of data in building resilient societal systems are key to humanity's future.In the short run, digital tracing and infection spread analysis, monitoring of logistics and service chains, and simulation to help policy makers will help the current public health challenges. In the medium term, such systems will be critical in restarting socio-economic activities and get the society on track to more permFor upcoming version releases, Private Kit: Safe Paths will deploy the following capabilities: V1 - Log location historyV2 - Match personal location history with infected patient anonymous redacted trace files provided by public health officialsV3 - Match personal  location history with encrypted anonymous redacted infected patient trace files provided by city officialsAs noted, Private Kit: Safe Paths works in conjunction with the MIT-developed GIS web app, Safe Places. Safe Places will be used by public health officials to:Collect time-stamped location data from one of the three sources, Private Kit: Safe Paths, Google location history, and patient interviewsProduce partially obscured trace files that meet jurisdiction legal requirements for anonymity that can be posted openly on the web and utilized for contact tracing in Private Kit: Safe PathsBy enabling contact tracing, Private Kit: Safe Paths will help to reduce panic and ""flatten the curve"" of Coronavirus spread by enabling those who have been exposed and are showing symptoms to make more informed decisions on when to seek testing and self-quarantine—without losing individual privacy and while reducing the fear unknown exposure.",,"['camera-culture', 'sculpting-evolution']",https://dam-prod2.media.mit.edu/x/2020/04/08/bikersilhouette.jpg,377655,,2020-10-23T15:37:13.039Z,"['raskar@media.mit.edu', 'sandy@media.mit.edu', 'esvelt@media.mit.edu']",True,safepaths,2020-03-01,"['artificial-intelligence', 'communications', 'data', 'politics', 'privacy', 'security', 'computer-science', 'technology', 'public-health', 'covid19']",Safe Paths,PUBLIC,https://privatekit.mit.edu,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-03-17T19:48:42.899Z,"As the presence of artificial intelligence expands significantly in children’s lives of learning and play, it is critical that students learn to be conscientious consumers of AI from an early age. Though the need for democratizing AI education is starting to be recognized, there are no current efforts to educate school-age children on the workings and implications of generative adversarial networks (GANs), despite their potential widespread use and misuse in media. The goal of this curriculum is to teach middle school children about generative machine learning techniques and how students can partner with machines in creative expression such as art, music, poems and more. We will explore tools and techniques such as neural networks and GANs across various forms of media, such as text, images, music, and videos. We frame this curriculum as an exploration of creativity, such that children’s creative and imaginative capabilities can be enhanced by innovative technologies. Further, we aim to foster discussions throughout the workshop to highlight important ethical issues around generative AI, such as creative IP and generation of hyperrealist fake media. This course is meant to be hands-on and encourages the explorative creation of art with and without AI tools. Throughout this course, students will have discussions around open-ended questions such as:What is art? Who is the creator?What is AI? What is not AI? Who owns the art co-created with a machine?How does generative AI work? How can I use generative AI to create media?These questions will allow them to reflect on how machine art differs from human created art, as well as the role that machines play in technology-guided creation. The curriculum will be taught with a series of plugged and unplugged activities, through which students will learn about generators and discriminators, examine the impact of algorithmic bias, and explore AI-driven artistic methods such as style transfer. They will also be co-creating with pre-trained GANs, gaining valuable experience in tweaking and evaluating an intelligent system. The workshop culminates in students creating their own GAN to create something personally meaningful for them. Students will organically understand the relevance and importance of this curriculum through applications of GANs that may touch their everyday lives, including interactive collaboration tools or deep fakes. By engaging with AI in different creative contexts and exercising computational action through GAN projects, students will be able to think critically about the media they create in the age of artificial intelligence. ",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2020/03/25/creativeaiworkshop.jpg,377697,,2022-03-28T19:16:31.753Z,"['safinah@media.mit.edu', 'dipaola@media.mit.edu', 'cynthiab@media.mit.edu']",True,creative-ai-a-curriculum-around-creativity-generative-ai-and-ethics,2020-03-01,"['artificial-intelligence', 'kids', 'learning-teaching', 'technology', 'covid19']","Creative AI: A curriculum around creativity, generative AI, and ethics",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-03-18T20:05:16.635Z,"""Maka"" - Eye, ""Niu"" - CoconutMaka Niu, aka the ""CocoCam,""  is a low-cost oceanographic camera and sensor platform that enables the exploration of undersea environments using a new pressure-tolerant camera housing, built in conjunction with Allan Adams and MIT's Future Ocean Lab, and an all-natural, locally sourced, biodegradable buoyancy-control device. The coconut (Cocos nucifera ) can float for over 3,000 miles and still remain viable; as such, it makes an excellent organic platform for deploying scientific apparatus.Maka Niu is a community-driven project that allows local groups to explore and monitor their marine resources. Based around off-the-shelf single-board computers and readily available microprocessors, Maka Niu is inexpensive to produce, easy to program, and simple to use. Designed as an open platform for scientific education and exploration, it will give classrooms and communities the ability to upload video and sensor data into FathomNet, a centralized database where machine learning algorithms assist with classification of marine species, seabed substrate identification, and accumulated time-series sensor data. This data will be publically available, allowing visualization, sharing, and discussion with policy makers from the local to global levels.",2021-07-31,['open-ocean'],https://dam-prod2.media.mit.edu/x/2020/03/18/310963767_707fdd0072_o.jpg,377723,,2021-08-04T18:49:33.617Z,"['katybell@media.mit.edu', 'novysan@media.mit.edu']",True,maka-niu,2020-02-07,['ocean'],Maka Niu ~ a low-cost oceanographic camera and sensor platform,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-03-19T15:44:14.007Z,"Play is a universal human need, reducing stress and supporting mental wellbeing. Swimming, surfing, diving, and other water-based activities go as far back as we've been a species. But what if we could bring the ocean where there wasn't one? What if we could create new forms of games, sports, and activities that provided a deeper connection to the environment around us? How can we apply new materials and technologies to create playful experiences that take full advantage of the affordances of water? AquaGames is an ongoing series of workshops to explore, discuss, and design the history and future of play in, on, under, or with the water.",2021-05-31,['open-ocean'],https://dam-prod2.media.mit.edu/x/2020/03/19/javardh-WZaGfccz6Dc-unsplash.jpg,377738,,2021-08-04T18:48:17.276Z,['novysan@media.mit.edu'],True,aquagames,2018-11-08,[],AquaGames,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2020-03-20T03:48:55.402Z,"While text-editors (e.g. Google Docs, MS Word) might be useful for writing a lecture, they’re not an intuitive medium for designing experiences that are open-ended, challenging, and exciting to both students and educators. This project explores how educators might design creative learning experiences using what we describe as ""creative learning design tools."" Creative Learning Design tools are more tinkerable, more personal, and more collaborative: these tools encourage educators to tinker with their plans, sketch ideas, extend their imaginations, iterate on their work, and organize their thinking in ways that make sense to them.  In the process, educators can make more of their thinking visible to themselves and others, opening up new opportunities for collaboration. Our research explores how teaching as a form of creative work can be better empowered with creative tools. And makes the case for bringing the playful spirit of technologies for children to the design of technologies for educators. In response to COVID-19, we're also exploring ways to support online collaboration among educators and students. This includes efforts to co-design and facilitate extended project based learning using a digital medium that acts both as a tool to think with and a tool for making thinking visible.Our inspiration comes from the design world, where tools like Sketch or Figma help designers bring ideas to life.  From the creative computing movement (think Scratch, Logo, and Glitch) and HCI pioneers like Alan Kay and Bret Victor.  From organizers advocating for multiple ways of knowing and thinking (like bell hooks or Sherry Turkle). From post-industrial models for school (think Reggio and Montessori). And from the countless educators and young people we work with.",2020-08-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/uuid/3754a848-ce73-4931-af88-cc9f82b95634,377759,,2022-05-11T14:31:49.850Z,['yusufa@media.mit.edu'],True,remixed,2019-11-01,"['design', 'kids', 'learning-teaching', 'creativity', 'technology', 'covid19']",Creative Learning Design Tools,PUBLIC,http://remixed.media.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2020-03-22T23:31:28.487Z,"LILLI is an immersive, volumetric, reflective lightfield display using a hybrid of video and laser projection to visualize marine wildlife and ocean climate data, et al.",2021-05-31,"['object-based-media', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2020/06/04/Lilli3DDisplay.png,377768,,2021-08-04T18:47:54.701Z,"['tjschoep@media.mit.edu', 'novysan@media.mit.edu', 'nlutz@media.mit.edu']",True,large-interactive-laser-light-field-display,2019-09-10,"['holography', 'interfaces', 'ocean', 'data-science']",Large Interactive Laser Light-Field Installation ~ LILLI,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2020-03-24T04:17:19.793Z,"Wearable Reasoner: Towards Enhanced Human Rationality through a Wearable AI AssistantWe present ""Wearable Reasoner,"" a proof-of-concept wearable system capable of analyzing if an argument is stated with supporting evidence or not to prompt people to question and reflect on the justification of their own beliefs and the arguments of others.  In an experimental study, we explored the impact of argumentation mining and explainability of the AI feedback on the user through a verbal statement evaluation task. The results demonstrate that the device with explainable feedback is effective in enhancing rationality by helping users differentiate between statements supported by evidence and those without. When assisted by an AI system with explainable feedback, users significantly consider claims given with reasons or evidence more reasonable than those without. Qualitative interviews demonstrate users' internal processes of reflection and integration of the new information in their judgment and decision making, stating that they were happy to have a second opinion present, and emphasizing the improved evaluation of presented arguments. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2022/09/27/Screen Shot 2022-09-27 at 11.00.38 AM.png,377781,,2022-09-30T19:18:14.180Z,"['vdanry@media.mit.edu', 'patpat@media.mit.edu', 'pattie@media.mit.edu']",True,wearable-reasoner,,[],Wearable Reasoner,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-03-25T16:29:48.962Z,"Project  Captivate have been designed to serve as an easy and reliable platform  to measure physiology in the real world.  We've designed it to be a tool  for researchers, a means to understand attention and engagement, and a scalable means to control responsive ecosystems. Further information can be found here: https://captivate.media.mit.edu/A Tool for Researchers

Smartglasses are  increasingly popular because the face is an ideal location for  continuous monitoring of environmental and physiological signals.  Unfortunately, no standard smartglasses platforms offer easy access to physiological data.  That leaves researchers constrained by their own  quick—and usually bulky—designs that make people self-conscious and  uncomfortable, altering their behavior and preventing long-term  naturalistic studies.The Captivate platform addresses these concerns; it saves researchers design time  and gives them easy access to data in a form factor that doesn't stand  out.We spent a summer in Shenzhen, China, learning from eyeglass manufacturers on how best to integrate sensors that track physiology events across your face, head movements, and location in a form-factor that is similar to a traditional pair of glasses. Measuring Attention Across ContextsWe are using the  Captivate platform study a user’s cognitive state, specifically  cognitive loading and overall attention.  Many lab-based behavioral  studies fail to replicate in real-world contexts.  Because of the deep engineering work behind these glasses, we are capable of  studying people naturally—all day long, in their real lives.  Combined  with new probabilistic modeling tools, we can make much more reliable  predictions about a user's cognitive state in real scenarios.   Driving IoT Ecosystems at ScaleThe glasses also talk to  each other over a mesh network, allowing them to scale to large events  and concerts, as well as to serve as one integral part of IoT ecosystems  at home and at work.  We envision the insights Captivate glasses can capture about users in these environments as a  crucial first step towards responsive, immersive environments that  support user goals.",,"['responsive-environments', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2020/11/15/front_view_v3.jpg,377805,,2023-09-06T21:36:22.354Z,"['chwalek@media.mit.edu', 'dramsay@media.mit.edu']",True,project-captivate,2019-05-27,"['design', 'environment', 'health', 'sensors']",Project Captivate: A wearable platform for measurement and actuation across contexts,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-03-25T20:33:34.357Z,"SoapCam is a WIP Human Computer Interaction system that motivates proper hand washing routines.""Hand hygiene is the most important measure to avoid the transmission of harmful germs and prevent infections.""  (WHO)""Hand washing is one of the best ways to protect yourself and your family from getting sick."" (CDC) ",,[],https://dam-prod2.media.mit.edu/x/2020/03/25/ezgif-6-a1f98d11132e.gif,377821,,2020-04-09T13:43:56.677Z,['noyman@media.mit.edu'],True,soapcam,2019-02-03,"['human-computer-interaction', 'public-health', 'gesture-interface', 'covid19']",SoapCam,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-03-26T16:58:25.481Z,"Regular disinfection of hands and frequently touched surfaces is a critical factor in preventing the spread of infectious diseases and disorders ranging from the common cold and flu to SARS and COVID-19. This activity of frequent disinfection requires a high degree of discipline and leads to increased cognitive and physical effort involved in frequent washing of hands, or the use of a pocket sanitizer.  We present an open-source, wearable sanitizer that provides just-in-time, automatic dispensing of alcohol to the wearer’s hand or nearby objects using sensors and programmable cues. We systematically explore the design space for wearable sanitizers aiming to create a device that not only seamlessly integrates with the user’s body and behavior but also frees their physical and mental faculties for other tasks. With these insights, we demonstrate our prototype and present the design process, device characterization, and our efforts for open-sourcing this project.",,"['responsive-environments', 'fluid-interfaces']",https://dam-prod2.media.mit.edu/x/2020/08/01/Main_QWuGIcP.jpg,377839,,2020-12-17T18:00:12.340Z,"['patpat@media.mit.edu', 'alims@media.mit.edu', 'joep@media.mit.edu', 'pattie@media.mit.edu', 'glennfer@media.mit.edu', 'lijw@media.mit.edu']",True,wearable-sanitizer,,"['human-computer-interaction', 'consumer-electronics', 'open-source', 'wearable-computing', 'interfaces', 'covid19']","Wearable Sanitizer: Open-source, On-body Sanitizer",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-03-29T22:25:17.214Z,"What anonymized geolocation data can tell us about the effectiveness of social distancingThe World Health Organization has officially declared COVID-19 a pandemic. The number of new confirmed cases and deaths from coronavirus continues to grow exponentially in many regions of the world. Across the globe, nations are enacting extraordinary policies to reduce the spread of the coronavirus. In several countries, notably China, South Korea, and Singapore, these policies have been extremely effective in reducing the growth rate of the virus.  In the US, social distancing has been encouraged and implemented with school closures and strict “stay-in-place” policies. The intention of social distancing policies is to reduce the speed at which the virus spreads by reducing interpersonal contact. By reducing the immediate burden on healthcare systems, social distancing is intended to save lives.With respect to social distancing policy: what is the effect of social distancing on the spread of coronavirus? Second, how well are people practicing social distancing? Using epidemiological models and high-precision mobility data, we aim to understand how policies are affecting people's behavior and if those changes are impacting the spread of coronavirus.More info: http://curveflattening.media.mit.edu",2020-12-01,[],https://dam-prod2.media.mit.edu/x/2020/03/29/example26.png,377852,,2020-05-01T16:22:19.465Z,"['bakker@media.mit.edu', 'groh@media.mit.edu', 'aberke@media.mit.edu', 'sandy@media.mit.edu', 'emoro@media.mit.edu']",True,social-distancing-in-a-pandemic,2020-03-28,['covid19'],Social Distancing in a Pandemic,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-03-31T19:15:40.907Z,"Governments and researchers around the world are implementing digital contact tracing solutions to stem the spread of infectious disease, namely COVID-19. Many of these solutions threaten individual rights and privacy. Our goal is to break past the false dichotomy of effective versus privacy-preserving contact tracing. We offer an alternative approach to assess and communicate users' risk of exposure to an infectious disease while preserving individual privacy.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/04/30/Screen Shot 2020-04-29 at 11.15.41 AM.png,377861,,2020-04-30T18:50:40.769Z,"['vepakom@media.mit.edu', 'bakker@media.mit.edu', 'raskar@media.mit.edu', 'sandy@media.mit.edu', 'kll@media.mit.edu', 'dcalacci@media.mit.edu', 'aberke@media.mit.edu']",True,assessing-disease-exposure-risk-with-location-histories-and-protecting-privacy,2020-03-14,"['privacy', 'security', 'computer-science', 'data-science']",Assessing Disease Exposure Risk With Location Histories And Protecting Privacy,PUBLIC,http://aberke.com,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-03-31T20:34:46.020Z,"RF-EATS is a new system that can verify the authenticity of food and liquids in closed containers without opening them or requiring any contact with their contents. What kinds of food/liquids can RF-EATS  be used to verify?Below are some of the applications for which we have successfully demonstrated RF-EATS's ability to verify and detect:Fake medicine. Fake medicine is a major challenge in many developing-world countries, leading to dozens of fatalities every year. A recent incident involved fake cough medicine bottles, where 90% of the active ingredient was replaced with diethylene glycol, a compound used in making antifreeze agents. Adulterated baby formula. In 2008, the Chinese milk scandal broke out after the hospitalization of 50,000 babies due to kidney damage. Manufacturers had watered down baby formulas up to 83% and mixed them with melamine CAS NO. 108-78-1, a compound used in making plastics. The purpose of adding melamine (by manufacturers) was to conceal dilution by artificially increasing protein levels.Tainted alcohol and diluted alcohol. Tainted alcohol is an ongoing problem in many developing-world countries, including China, Indonesia, Iran, Turkey, India, and Mexico. Alcohol is tainted by mixing it with cheaper methanol, and consuming it leads to hundreds of cases of blindness and death every year.Fake extra-virgin olive oil. Recent studies have shown that 69% of US-imported extra virgin olive oil has been adulterated by mixing it with cheaper oils (e.g., peanut oil). This can lead to health hazards for consumers with (peanut) allergies. Standard adulteration levels range between 70-80%.Wine fraud. Wine fraud takes many forms. A common one involves selling consumers wine vintages that are dated to earlier years, artificially inflating their price.Counterfeit perfume. Counterfeit beauty products abound, leading Estée Lauder to confiscate over 2.6 million counterfeit items in 2016 alone. Many such products are sold online.How accurate is the system?Our results demonstrate that RF-EATS can achieve over 90% classification accuracy across a wide variety of applicationsHow does RF-EATS work?RF-EATS is the first RFID-based system that can noninvasively sense food and liquids in closed containers and operate correctly in environments it hasn't seen before. RFIDs are inexpensive, battery-less tags that are placed on billions of products worldwide, including food items. RF-EATS leverages the near-field coupling between a tag’s antenna and a container's contents to sense them noninvasively. At a high level, the RFID's signal is impacted by the dielectric of the content inside the container. So, when the content changes, its dielectric change will impact the RFID's response.The challenge, however, is that RFID signals are not just impacted by materials inside a container, but also by other objects in the environment around them, including other items, furniture, and even the human body.Our solution is a new wireless AI model that enables RF-EATS to verify food and liquids despite changes in the surrounding environment. It is based on a novel RF (radio frequency) kernel function, called multipath kernel, that can be used to simulate different wireless environments. By simulating different environments,  it can adapt to them.Our results show that the system's accuracy is indeed dependent on the content of the container, specifically on dielectric differences between authentic and fake content.To learn more about how the system works, read our paper.This project is supported by a J-WAFS seed grant.If you're interested in exploring the potential of using RF-EATS for detecting different kinds of contaminants or material properties, contact us at rfiq@media.mit.edu.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2020/04/03/Screen Shot 2020-04-03 at 12.32.27 AM.png,377869,,2021-02-22T20:43:19.394Z,"['unsoo@media.mit.edu', 'fadel@media.mit.edu', 'jleng@media.mit.edu', 'alaakh@media.mit.edu']",True,rfeats,2019-01-01,"['environment', 'food', 'health', 'sensors', 'technology', 'wellbeing', 'water']",RF-EATS: Food and liquid sensing in practical environments using RFIDs,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-04-03T17:31:48.212Z,"This Primary School AI curriculum introduces elementary school children to robotics and artificial intelligence. Throughout the curriculum, students have opportunities to reflect on their own thinking and their expectation of technology that “thinks.” We start by helping students solidify their thinking about what robots and AI are by identifying examples in their environment. Then, we develop children’s intuition about how technology works through powerful metaphors in the form of hands-on activities. Finally, we work with students to develop their understanding of how design choices impact others so that children can design their own intelligent robot solutions to various problems.Examples of workshop activities:Introduction to Artificial IntelligenceSimple MachinesElectricity and ElectronicsProgrammingMachine LearningChatbotsRobot ConstructionProject Design",2019-03-31,['personal-robots'],https://dam-prod2.media.mit.edu/x/2020/04/03/primaryAI_35.png,377915,,2020-04-03T20:17:07.185Z,['randiw12@media.mit.edu'],True,primary-ai-ed,2018-06-01,"['robotics', 'artificial-intelligence', 'kids', 'learning-teaching', 'technology', 'covid19']",Primary School AI Education,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-04-04T17:10:04.713Z,"Policymakers, practitioners, and researchers are grappling with some herculean questions regarding kids' safety online. Homemade slime with Doctor Squish, 5-minute crafts, and Minecraft videos: What constitutes “kids content” and how do content-creators convey this effectively? How does YouTube limit and minimize data collection in practice? What does a reasonable parent-child consent process look like? These conversations are often led by legal experts, technocrats, and academic researchers. To better understand children’s perspective and intuition of data privacy and collection, we organized a workshop with the end-user experts themselves: kids. This project seeks to inform youth about data and privacy topics through a series of hands-on activities. The goal is to encourage students to form their own opinions and think more critically about the platforms they use every day, instead of solely listing the dangers and the “what not to do’s.”This curriculum was piloted in January 2020 with the Girl Scouts of Eastern Massachusetts and the Edward M. Kennedy Institute.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2020/04/04/badges.jpg,377928,,2020-04-06T13:49:59.153Z,"['stnguyen@media.mit.edu', 'dipaola@media.mit.edu', 'cynthiab@media.mit.edu']",True,data-privacy-design-for-youth,2020-01-01,"['artificial-intelligence', 'data', 'kids', 'learning-teaching', 'privacy', 'technology', 'covid19']",AI + Data Privacy Activities for K-9 Students,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-04-06T13:53:48.339Z,"Let’s Talk Privacy explores how the implementation of privacy and data governance policies might impact a variety of fields.Project Let’s Talk Privacy explores how the implementation of privacy and data governance policies might impact a variety of fields. Our project title reflects our work in engaging a more diverse set of voices into conversations around privacy and data governance. How do these policies translate in practice? How they might affect us? To find out, we interviewed 41 people – including lawyers, designers, engineers, advocates, policymakers, and stewards of sensitive data (social workers, pediatricians) – about three federal draft data and privacy related bills. Our objectives were to 1) explore intersections of privacy policy and design through visual prototypes; 2) identify the challenges of translating policy to platform changes based on the specific background and industry skill set of the interviewees; and 3) understand broader perspectives of privacy and control in technology.In the short time we’ve been working on this project, privacy and data protection conversations have evolved. While our work began several months ago, the insights and questions that our research surfaces are timely and relevant to evolving technologies, even during a pandemic. Questions surrounding surveillance and intrusive tracking overlap with questions about how to strengthen privacy protections using both technical and policy implementations. The emergent conversations around government surveillance and privacy during this pandemic are mentioned here as an important reminder of why privacy is such an important and fraught space.Our research reflects only a small portion of the diverse perspectives we can bring to data governance and policy making. In particular, our research focuses only on policies in the United States. In the future, we hope to spur more conversations, reveal new insights, and prompt positive action.Some things we recommendDevelop shared privacy language across industries.This could assist in identifying overlapping processes, concerns, interests, and harms, as well as enable better collaborations between individuals from different sectors/professions.Include insights from people from marginalized communities and data stewards.It’s important and valuable to talk to those who know or have a good understanding of how they have been impacted by these systems & policies.Implement human-centered practices in the policy design process.Practices that could aid the policy design process include broadening engagement with industry practitioners, visualizing policies to prototypes when possible, exploring how to test policy and prototyping processes on a larger scale, and continuously integrating feedback.",2020-08-31,"['center-for-civic-media', 'civic-media']",https://dam-prod2.media.mit.edu/x/2020/04/06/p3-poc (1).jpg,377938,,2020-06-04T20:14:03.110Z,"['stnguyen@media.mit.edu', 'dsjen@media.mit.edu']",True,let-s-talk-privacy-translating-policy-to-prototypes,2019-09-30,"['design', 'data', 'government', 'politics', 'privacy', 'community']",Let's Talk Privacy: Exploring how privacy and data governance policies translate into practice,PUBLIC,https://letstalkprivacy.media.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-04-09T03:56:38.865Z,"Designed for mass manufacture and rapid deployment, we are building an open hardware, reusable, sterilizable, modular, and filter-media agnostic face mask that aims to hit the N95 efficacy criteria. This project started with a team of 20+ seemingly random people who had never met, but who spontaneously formed into a fast-paced, rapid deployment team—hell-bent on saving lives. We are engineers, designers, clinicians, technicians, molders, quality assurance, regulatory, business, etc. This collaboration was born out of Helpful Engineering and is now being pushed forward with overall project organization through Open Standard Respirator.The OSR Mask is now available. It is not yet certified, although it has passed preliminary tests and a NIOSH application will be filed soon. It is not a medical device. Considering the virus surge in the US and around the world we wanted to make the mask available to the public. To get access to the pre-production release check out: https://openstandardindustries.com/We created both a non-profit (osrdesign.org) for international development, and a c-corp for production in the US (openstandardindustries.com) or contact info@osindustries.co",,['biomechatronics'],https://dam-prod2.media.mit.edu/uuid/5b1d5e8a-6aa8-4169-a9e4-70e43cb272be,377967,,2020-12-23T17:38:42.066Z,"['mcarney@media.mit.edu', 'cwwang@media.mit.edu']",True,maskproject,2020-03-21,"['manufacturing', 'public-health', 'materials', 'covid19']","MASKproject: Open standard respirator, N95 alternative face mask",PUBLIC,http://www.openstandardrespirator.org,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-04-10T15:07:36.039Z,"Sunny is a Facebook messenger chatbot by Monique Davey, Jaya Narain, and Tina Quach that promotes wellbeing by strengthening connections within existing social groups.  Positive social interactions have a strong, positive influence on mental health.  Sunny facilitates the exchange of positivity between friends by encouraging and mediating the delivery of reflections, memories, and compliments.  This project began in the class MAS.S60: Artificial Intelligence and Mental Health in Spring 2019.",,[],https://dam-prod2.media.mit.edu/x/2020/04/10/chat_horizontal.png,377993,,2020-04-24T15:38:20.275Z,"['jnarain@media.mit.edu', 'quacht@media.mit.edu']",True,sunny,2019-02-01,['human-computer-interaction'],Sunny,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-04-12T13:44:07.074Z,"PCB debugging can be tricky sometimes. Normally we should check which signal we want to measure in the schematic, switch to PCB layout to see which pads are connected to that signal, and choose the appropriate one. If there is not only one signal we want to measure simultaneously, but several,  some of which may not be on the same side of the PCB, your two hands would not be enough.    Here we present the PCB Probe Tester (PCBPT), a PCB automatic probing tool that helps with your debugging. It's composed of a miniature probing machine you can put on your desk and a control program run in PC. The program will parse the board design file created in Eagle, and list all the signals you can measure in the design. After you choose the signals you want to measure, the program will choose the proper pads for the signals and calculate their coordinates, then the machine will place the probes on the pads selected. All you need to do is choose the signals you want and check the output.    This is a new and better way for the hardware designers to interact with their designs.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/04/12/IMG_20200411_122128.jpg,378018,,2022-08-10T20:09:30.240Z,['fzliu@media.mit.edu'],True,pcb-probe-tester,2020-03-10,"['robotics', 'design', 'human-computer-interaction', 'human-machine-interaction']",PCBPT,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-04-14T14:58:48.370Z,"Participate in the experiment at https://detectfakes.media.mit.eduCheck out our publications in PNAS, a workshop at IJCAI, and pre-print on arXiv. Check out a video from the Election Misinformation Symposium: Fighting Misinfo Through Fact-checking and Deepfake DetectionFind our deepfake research discussed in the news: Scientific American, BBC, WSJ, NYT, , and NPR How do you spot a DeepFake? How good are DeepFake videos? How well can ordinary people tell the difference between a video manipulated by AI and a normal, non-altered video? Rather than try to explain in words, we built the Detect Fakes website so you can see the answer for yourself. Detect Fakes is a research project designed to answer these questions and identify techniques to counteract AI-generated misinformation. It turns out there are many subtle signs that a video has been algorithmically manipulated. Some subtleties are explained in detail below.",2024-01-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/2020/04/14/Screen Shot 2020-04-14 at 11.33.54 AM.png,378044,,2023-06-06T19:26:08.177Z,"['groh@media.mit.edu', 'zive@media.mit.edu', 'picard@media.mit.edu', 'lip@media.mit.edu', 'nsingh1@media.mit.edu', 'arunas@media.mit.edu', 'dyk0518@media.mit.edu']",True,detect-fakes,2020-04-27,"['human-computer-interaction', 'artificial-intelligence', 'data', 'government', 'human-machine-interaction', 'social-media', 'social-science', 'machine-learning', 'computer-science', 'technology']",Detect DeepFakes: How to counteract misinformation created by AI,PUBLIC,https://detectfakes.media.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-04-14T20:59:58.511Z,"In April 2019, Shuguang Zhang's lab started to use the QTY code to design a class of water-soluble cytokine receptors, including interferon receptors and interleukin receptors, to overcome a major medical problem called a cytokine storm. A cytokine storm is a potentially life-threatening side effect seen in CAR-T (chimera antigen receptor T-cells) treatment of leukemia, lymphoma and perhaps other cancers. Symptoms include fever, fatigue, loss of appetite, muscle and joint pain, nausea, vomiting, diarrhea, rashes, fast breathing, rapid heartbeat, low blood pressure, seizures, headache, confusion, delirium, hallucinations, tremor, and loss of coordination. A cytokine storm is the overreaction of patients' own immune systems that can result in multi-organ failure. COVID-19 is known to cause severe and often fatal cytokine storm in patients with acute infection.One year ago, Zhang’s lab set out to design a chimera of the water-soluble QTY-variant of cytokine receptors fused with the Fc domain of Immunoglobulin G (IgG). The resulting chimeric cytokine receptors can function as ""molecular mops"" as they bind with excessive cytokines released in the cytokine storm, thus protecting the patient from multi-organ failure. The development of a broad spectrum of hydrophilic, functional cytokine receptor-IgG compounds may provide significant therapeutic benefits. Shuguang Zhang has always emphasized asking good questions and pursuing curiosity-driven research. He often quotes Francis Crick: ""If you ask big questions, you get big answers.” Such curiosity-driven research again demonstrates that it is very important to ask a big question in order to overcome a major medical problem in clinics, rather than pursuing crisis-driven, reactive research. The research initiated in April 2019 is directly relevant to the treatment of the most severely infected COVID-19 patients. Curiosity-driven, or even proactive research often leads to preparedness which is key to preventing future disasters.",,['molecular-machines'],,378066,,2020-04-17T19:55:17.868Z,"['shuguang@media.mit.edu', 'ruiqing@media.mit.edu', 'haosl@media.mit.edu']",True,design-of-water-soluble-cytokine-receptors-fused-with-fc-domain-of-igg-may-be-therapeutic-for-cytokine-storms,2020-04-16,"['biology', 'public-health', 'medicine', 'covid19']",Designer molecular mops: Water-soluble cytokine receptors fused with Fc domain of IgG may be therapeutic for cytokine storms,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2020-04-15T14:42:09.692Z,"Website: https://c19observatory.media.mit.edu/The COVID-19 pandemic is having a tremendous socioeconomic impact around the world, the scale of which is still hard to assess.A unique team of computer scientists, social scientists, and business leaders are being gathered to extract insights from massive datasets from the private sector, in order to provide live and actionable insights for other researchers, policymakers, and the general public, while respecting privacy.Our objective will be to provide a rapid assessment of the impact of social-distancing policies aimed at mitigating the spread of the COVID-19 pandemic, and to quantify the extent to which changes in mobility, social interactions, and purchasing patterns relate to health and economic outcomes at a very high resolution.Our immediate contribution will be an interactive dashboard that will produce actionable insights for policymakers. This collaboration will be guided by our key privacy principles, and will build on our team's rich experience in developing and deploying numerous privacy-preserving research projects, pipelines, and algorithms.Research GoalsSocial DistanceGiven that we do not yet have pharmaceutical solutions (e.g. vaccines) for the COVID-19 pandemic, the social engineering of our lifestyle is our only current response to the pandemic. It is therefore crucial to understand how we can improve local, national, and global policy decisions regarding how to implement social distancing to minimize infection rates. This will necessitate:Careful and precise measurements of social distance and interaction, and how these respond -- causally and empirically -- to different implementations of social distancing policies.Using these measurements, we are working with epidemiologists, public health officials and modeling experts to understand how to dynamically optimize social distancing policies for optimal health outcomes.Economic ImpactVarious local and national governments are considering a vast spectrum of economic relief decisions ranging from freezing rent payments for individuals to interest-free loans for businesses. In order to inform these policy decisions, it is critical to understand which demographics of people and what types of businesses are most at risk by studying changes in spending patterns.Using the economic data from our data partners, we will more precisely measure the impact of the pandemic on individual and business financial health.By integrating financial impact into our epidemiological models, we will more thoroughly understand the trade-off between health and economic outcomes.PrivacyOur team is highly experienced in developing and deploying numerous privacy-preserving research, pipelines and algorithms such as OPAL: Open Algorithms and Secure Multi-Party Computation using Blockchain.We are deeply committed to preserving the privacy and security of the data we will be using. Our principles are:We will only use anonymized data with no personal identifiers.We will use data at the highest level of time, space and group aggregation that still allows us to gather useful insights.Working with public health practitioners and policy makers, we will only run analysis that has a net positive benefit to society.The use of data will be in compliance with existing laws and ethical standards.We will elect not to receive data from our data partners when possible, but instead elect to run our models on our data partner's premises and we will only transfer high-level statistics outside of their data clouds.We will use state-of-the art privacy and infosec techniques and protocols.We will dedicate a significant portion of our effort into researching techniques that allow for the use of sensitive data while maintaining privacy.Website: https://c19observatory.media.mit.edu/",2020-06-30,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2020/04/15/network.jpg,378089,,2020-07-05T21:33:56.546Z,"['sandy@media.mit.edu', 'dhaval@media.mit.edu']",True,covid-19-observatory,2020-04-15,[],COVID-19 Observatory,PUBLIC,https://c19observatory.media.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-04-21T05:24:43.221Z,"When materials sciences meet HCI and e-textiles, for custom sensors and actuatorsIn this project we explore the process of polymerization to add electrical functionalities to everyday materials such as fabrics, or feathers, and transform them as sensors.Creating one’s own sensitive textiles provides a plethora of opportunities: for example, it means the freedom from constraints of store bought materials. Polymerization broadens the range of interactive objects which can be created. One might augment an existing and loved item, or create unique new materials and sensors by polymerizing items with unique properties such as tape or zippers. Existing piezoresistive materials (pressure sensitive) also act as a black box in designs; it means that entrepreneurs, tailors, researchers, and hackers are dependent on a specific manufacturer. If this manufacturer changes their production process, this might make entire designs obsolete, which can make it tricky to reproduce work or create products.PolySense, our CHI publication, highlights how—by framing polymerization as a dyeing process—polymerization is a versatile and easy-to-deploy tool for creating fabric with custom electrical properties. We also show how to use etching on existing functional fabric to customize it. The resulting custom fabrics might be used as sensors or conductors, and even whole circuits including sensing elements can be implemented in fabric only.Other publications highlight specific pieces and technologies which are implemented using the process described in PolySense. For example, we demonstrate rapid prototyping of on-skin interaction, and showcase a hybrid art piece which features polymerized feathers.CreditsThe project is a collaboration with members of Datapaulette, the Saarland University HCI Lab, and various other friends and colleagues. The materials research behind this project was conducted with Ana C Baptista of Nova University Lisbon (CENIMAT-i3N). The CHI publication was developed over a series of visits at the Saarland HCI Lab by Hannah Perner-Wilson and Cedric Honnet.MoreCounterChemists.github.io",,"['tangible-media', 'responsive-environments']",https://dam-prod2.media.mit.edu/uuid/451ad9e1-436e-49f9-9904-f774dcee0200,378131,,2021-03-09T17:54:19.478Z,"['paul_str@media.mit.edu', 'marcteys@media.mit.edu', 'honnet@media.mit.edu']",True,material-functionalization,2020-04-25,"['design', 'human-computer-interaction', 'wearable-computing', 'sensors', 'materials']",Material Functionalization,PUBLIC,http://honnet.eu,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-04-22T16:06:33.199Z,"The City Science group is collaborating on several projects in response to the COVID-19 pandemic. In addition to the project pages linked below, the team is working to adapt existing City Science themes to address resilience.~Our Changing Places researchers are exploring the future of Health and the Home using robotic architecture and sensorized spaces to enable more livable spaces and to incorporate health monitoring.~The CityScope team is working to understand the future of Autonomous Communities to provide needed amenities and institutions in a short walkable distance allowing for more livability, decreasing emissions, and also allowing those communities to be closed off more easily in case of pandemic preventing entire cities from being shut down simultaneously. ~Others are deploying agent based models to understand individual movement and minimize the number of workers in any space at a given time in a large factory setting, allowing for worker safety while still enabling production in times of pandemic. We invite the City Science Network community to further collaborate and assist with these ideas and technologies wherever possible.City Science projects, articles and events linked below.",2022-04-01,['city-science'],https://dam-prod2.media.mit.edu/uuid/01bad3fd-4d1c-40a0-9ea7-b580423f9d03,378146,,2021-09-29T16:00:26.649Z,"['mdchurch@media.mit.edu', 'kll@media.mit.edu']",True,resilient-communities-and-covid19,2020-04-01,[],Resilient Communities and COVID-19,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2020-04-24T19:01:18.693Z,"HomeLAMP demonstrates the design and prototyping of a low-cost thermal processing unit for COVID-19 RT-LAMP testing and diagnostics entirely within the home, as well as other possibilities for low-cost, widely-distributed home testing solutions. From the start of the COVID-19 pandemic in the US until the end of April, 5.5 million tests were performed in total [via the Covid Tracking Project]. In order to re-open the economy and allow safe re-entry into public life, testing will need to scale up by orders of magnitude, to reach between 1 and 10 million tests per week [various sources].HomeLAMP seeks to test whether simple tests that are performed in centralized commercial lab settings can be modified to work at home, at a lower cost and with fewer logistical challenges such as packaging, stabilizing, safe transport, handling, and processing. This project is in development and is currently in early prototyping processes; we are currently working to validate the stability of our results against those of commercial lab testing.",,[],https://dam-prod2.media.mit.edu/x/2020/04/29/IMG_5665.png,378175,,2020-05-07T18:23:38.927Z,"['camorris@media.mit.edu', 'dgoodwin@media.mit.edu']",True,homelamp,2020-04-13,['covid19'],HomeLAMP,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-04-24T21:33:23.707Z,"In recent years, the retail industry has become increasingly interested in Information and communications technology (ICT) systems for enriching the shopping experience. Such systems are increasingly being deployed in retail contexts, however common implementation methods are limited due to high costs and large required footprints. Thus it is a challenge for smaller or temporary retail to install such services. In this study, we explore the usage of a load-sensitive board to improve the retail shopping experience specifically in smaller and temporary retail settings. As a case study, we develop and examine KI/OSK, an easy-to-install modular table-top retail application using SCALE, a previously developed load sensing toolkit specifically developed for Farmers Market applications. Our study uses iterative user research including surveys with Farmer's Market managers to assess design requirements, and testing and revising through a field study in a Farmers Market in Tokyo, Japan.",2019-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/07/KIOSK_FieldStudySystem.jpg,378183,,2020-07-27T16:15:27.080Z,"['ken_n@media.mit.edu', 'taka_y@media.mit.edu', 'ishii@media.mit.edu', 'koichiy@media.mit.edu', 'sasakiy@media.mit.edu']",True,ki-osk,2019-01-20,['human-computer-interaction'],KI/OSK: Practice Study of Load Sensitive Board for Farmers Market,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2020-04-27T19:23:34.600Z,"Sound  and space are fundamentally intertwined, at both a physical and  perceptual level. Sound radiates from vibrating materials, filling space  and creating a continuous field through which a listener moves. Despite  a long history of research in spatial audio, the technology to capture  these sounds in space is currently limited. Egocentric (binaural or  ambisonic) recording can capture sound from all directions, but only  from a limited perspective. Recording individual sources and ambience is  labor-intensive, and requires manual intervention and explicit  localization.In this work I propose and  implement a new approach, where a distributed collection of microphones  captures sound and space together, resynthesizing them for a  (now-virtual) listener in a rich volumetric soundscape to explore. This  approach offers great flexibility to design new auditory experiences, as  well as giving a much more semantically-meaningful description of the  space. The research is situated at the Tidmarsh Wildlife Sanctuary, a  600-acre former cranberry farm that underwent the largest-ever  freshwater restoration in the northeast. It has been instrumented with a  large-scale (300x300m²) distributed array of 12-18 microphones which  has been operating (almost) continuously for several years.This  dissertation details methods for characterizing acoustic propagation in  a challenging high-noise environment, and introduces a new method for  correcting for clock skew between unsynchronized transmitters and  receivers. It also describes a localization method capable of locating  sound-producing wildlife within the monitored area, with experiments  validating the accuracy to within 5m.The  scale of the array provides an opportunity to investigate classical  array processing techniques in a new context, with nonstationary signals  and long interchannel delays. We propose and validate a method for  location-informed signal enhancement using a rank-1 spatial covariance  matrix approximation, achieving 11dB SDR improvements with no source  signal modeling.These components are  brought together in an end-to-end demonstration system that  resynthesizes a virtual soundscape from multichannel signals recorded in  situ, allowing users to explore the space virtually. Positive feedback  is reported in a user survey.",2020-04-30,['responsive-environments'],,378197,,2020-04-27T20:27:36.531Z,['sfr@media.mit.edu'],True,soundscape-resynthesis,2018-05-18,"['robotics', 'design', 'environment', 'health', 'music', 'sensors', 'voice', 'ocean', 'climate-change', 'physics', 'covid19']",Soundscape Resynthesis,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2020-04-27T21:23:37.386Z,Virtual CityScope Champs-Élysées is an interactive and immersive platform that explores the future of Paris’ most important street.,,['city-science'],https://dam-prod2.media.mit.edu/x/2020/05/14/1.png,378206,,2020-05-14T21:46:44.032Z,"['noyman@media.mit.edu', 'agrignar@media.mit.edu', 'kll@media.mit.edu', 'alonsolp@media.mit.edu', 'nicolasa@media.mit.edu']",True,champscope,2020-03-01,"['architecture', 'civic-technology', 'transportation', 'urban-planning']",ChampScope,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2020-05-04T18:33:38.362Z,"With(in) is a multi-stage project that includes an exhibit, installation, qualitative exploration, and visual storytelling.This is a story about connection. The MIT Media Lab City Science group presents an immersive view into the worlds of three women in three settlements: Eva in Guadalajara, Mexico; Gihan in Cairo, Egypt; and MamaG in Port Harcourt, Nigeria. Through visual storytelling we experience activities from the mundane to the surreal, from the deeply meaningful to the inconsequential. We visit the fringe neighbourhoods and community centers of Guadalajara, the vertical slums and bustling streets of Cairo, and the tiny homes and crowded markets of Port Harcourt. In the lives of each individual, we examine the micro and the macro, from the gentle care of fixing one’s hair each morning to the cultural swells of holidays, religious ceremonies, and funerals. In these places, far from our own, we learn and inquire, we gather and we listen, in the hope of better understanding the complexity of the world around us and new possibilities for how we will live together in the future. As extreme urbanisation unfolds at an astounding pace, all three locations reflect the chaos and the importance of community. One woman’s journey can be both individual and global when viewed in the context of the others. We ask ourselves: how does rapid urbanisation impact the community we seek, and how do intimate domestic activities such as food preparation and celebration reflect a larger cultural context? ",,['city-science'],https://dam-prod2.media.mit.edu/x/2021/05/18/With(in) - Immersive installation_1_1.gif,378271,,2022-05-04T19:40:05.021Z,"['gbabio@media.mit.edu', 'nicolasa@media.mit.edu', 'thomassl@media.mit.edu', 'gba@media.mit.edu', 'maitanei@media.mit.edu', 'alonsolp@media.mit.edu', 'kll@media.mit.edu', 'mdchurch@media.mit.edu']",True,with-in,2021-05-01,"['design', 'architecture', 'art', 'developing-countries', 'food', 'music', 'storytelling', 'marginalized-communities']",With(in),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-05-09T21:13:33.063Z,"Programmable CRISPR enzymes are powerful and versatile tools for genome editing. They, however, require a specific protospacer adjacent motif (PAM) flanking the target site, which constrains the accessible sequence space for position-specific genome editing applications, such as base editing and precise gene insertion. For example, the standard Cas9 from Streptococcus pyogenes (SpyCas9) requires a PAM sequence of 5'-NGG-3' downstream of its RNA-programmed target, which limits genome editing applications to around 10% of all DNA sequences.To broaden the targeting range of CRISPR, we first bioinformatically discover and characterize a highly similar SpyCas9 homolog from Streptococcus canis (ScCas9) with a more minimal 5’-NNG-3’ PAM specificity (Chatterjee, et al. Science Advances, 2018). Furthermore, we employ motifs from closely-related Streptococcus orthologs to engineer an optimized variant of ScCas9 (Sc++) that simultaneously exhibits broadened targeting capability, robust DNA cleavage activity, and minimal off-targeting propensity (Chatterjee, et al. Nature Biotechnology, 2020). Next, we recombine the PAM-interacting domain of Streptococcus macacae Cas9 (SmacCas9) with SpyCas9, and subsequently introduce enhancing mutations to generate iSpyMac with efficient and accurate 5’-NAA-3’ PAM preference (Chatterjee, et al. Nature Communications, 2020).  Together, these efforts expand the range of CRISPR nucleases to over 70% of DNA sequences, allowing for targeting of genomic loci that were previously inaccessible, including sequences within candidate genes for denser CRISPR screens and disease-related mutations that can now be fixed with genome editing architectures expressing our engineered variants.",,['molecular-machines'],https://dam-prod2.media.mit.edu/uuid/8af83cdc-ab04-4b63-9b0c-7997a781fc62,378405,,2020-05-18T14:13:59.720Z,"['pranam@media.mit.edu', 'njakimo@media.mit.edu', 'jacobson@media.mit.edu']",True,robust-genome-editing-with-broad-targeting-crispr-enzymes,2016-06-03,"['bioengineering', 'genetics', 'alumni', 'covid19']",Robust Genome Editing with Broad Targeting CRISPR Enzymes,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-05-11T08:50:42.202Z,"Agent-Based Modelling for water management: a study case of the Bac Hung Hai irrigation system in the Red River Delta, Vietnam.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/05/11/CityScope_Hanoi 1_teaser3.mov.gif,378417,,2020-05-13T15:31:17.891Z,"['agrignar@media.mit.edu', 'kll@media.mit.edu']",True,cityscope-hanoi,2019-01-20,[],CityScope Hanoi: Water management in the Red River,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2020-05-12T15:59:01.169Z,"Academic research groups, especially the ones directed by new junior faculty, are under pressure to produce high-quality work timely while constrained by limited budget, resources, and lab space. A well-organized lab is crucial for a wide range of reasons, from keeping costs low, to protecting the health and safety of users. Our focus is on developing lab-level systems that create a lean and productive space in the context of academic cleanrooms. The efforts made at implementing the 5S organizational methodology to improve cleanroom functions, including regulatory compliance, organization, and resource stewardship are discussed. The quantifiable result is that chemical consumption spending is reduced by 41%, between 2018 and 2019, whereas materials consumption spending is reduced by 52%. Meanwhile, there is a 116% increase in the number of fabricated devices, showing greater output using less resources. This management system also proved itself to be beneficial during emergency situations. The recent COVID-19 pandemic is one such instance that showcased the ability to systematically ramp-down operations within 15 min, taking a huge burden away from the research group, and thus allowing us to focus on what is most important during this time of crisis: the health and safety of the group and community.",,['conformable-decoders'],https://dam-prod2.media.mit.edu/x/2020/05/12/TOC_Figure_aisy.202000074.tif,378441,,2021-10-04T16:18:41.099Z,"['canand@media.mit.edu', 'dsadat@media.mit.edu']",True,research-resiliency-through-lean-labs,2018-01-01,"['design', 'bioengineering', 'health', 'community', 'biotechnology', 'wellbeing', 'microbiology', 'covid19', 'microfabrication']",Research resiliency through lean labs,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-05-12T21:13:38.002Z,"In this speculative project, we ""flipped the script"" on a widespread narrative regarding the use of algorithmic risk assessment for criminal justice reform. Rather than claim to predict the criminal proclivities of those who are arrested, we developed an algorithm that assigns judges a risk score based on their likelihood of failing to adhere to the Constitution by unlawfully incarcerating someone before their trial. Key Contributors:Rebekah Agwunobi, Chelsea Barabas, Colin Doyle, J.B. Rubinovitz",2020-08-01,['civic-media'],https://dam-prod2.media.mit.edu/x/2020/05/13/Screen Shot 2020-05-13 at 3.49.33 PM.png,378456,,2022-06-10T13:35:30.157Z,['cbarabas@media.mit.edu'],True,judicial-risk-assessment,2019-02-01,['artificial-intelligence'],Judicial Risk Assessment,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-05-15T11:42:50.934Z,"Staining of tissues sections using chemical and biological dyes has been used for over a century for visualizing various tissue types and morphologic changes associated with cancer and other disorders for contemporary clinical diagnosis. This staining procedure often results in loss of irreplaceable tissue specimen and delays diagnoses. Other key challenges include sampling time, which can limit the amount of tissue that can be stained owing to time and cost involved, resulting in evaluation of only three 4-μm sections of tissue to represent a 1-mm diameter core. Irreversible dye staining of tissues leads to loss of precious biopsy samples that are no longer available for biomarker testing.  Dr. Shah's lab has previously described generative computational  methods that use neural networks that  rapidly stain photographs of non-stained tissues, providing physicians timely information about the anatomy and structure of the tissue.  The lab also reported a ""computational destaining"" method that can remove dyes and stains from photographs of previously stained tissues, allowing reuse of patient samples.  However,  studies testing operational feasibility and validation of results obtained by these generative neural network models and machine learning algorithms in controlled clinical trials or hospital studies for virtual staining of whole-slide pathology images did not exist, precluding clinical  adoption and deployment of these systems.  In this study led by Dr. Shah, in collaboration with Stanford University School of Medicine and Harvard Medical School, several novel mechanistic insights and methods to facilitate benchmarking and clinical and regulatory evaluations of generative neural networks and computationally H&E stained images were reported. Specifically, high fidelity, explainable, and automated computational staining and destaining algorithms to learn mappings between pixels of nonstained cellular organelles and their stained counterparts were trained.  A novel  and robust loss function was devised for the  deep  learning algorithms to preserve tissue structure. The study communicated that virtual staining neural network models were generalizable to accurately stain previously unseen images acquired from patients and tumor grades not part of training data. Neural activation maps in response to various tumors and tissue types were generated to provide the first instance of explainability and mechanisms used by  deep learning models for virtual H&E staining and destaining.  And image processing  analytics and statistical testing were used  to benchmark the quality of generated images.  Finally, the computationally stained images were evaluated by multiple pathologists for prostate tumor diagnoses  and clinical decision-making.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/x/2020/05/19/MIT NEWS Image.png,378481,,2022-10-14T21:24:05.820Z,['pratiks@media.mit.edu'],True,use-of-deep-learning-to-develop-and-analyze-computational-hematoxylin-and-eosin-staining-of-prostate-core-biopsy-images-for-digital-biopsies,2020-05-20,"['human-computer-interaction', 'artificial-intelligence', 'bioengineering', 'data', 'extended-intelligence', 'genetics', 'health', 'computer-science', 'medicine', 'clinical-trials', 'clinical-science']",Deep Learning to Develop and Analyze Computational Hematoxylin and Eosin Staining of Tissue Images for Digital Biopsies,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-05-24T16:35:34.221Z,"In creative learning, reflection encourages learners to think critically about their experiences and helps to generate new ideas, insights, and outlooks. Maker practices recommend documentation, typically in the form of digital portfolios, as a method to encourage reflection. Documentation can serve two important roles: as a tool to support personal reflection about your learning experience and as a sharable record to showcase your skills development. Unfortunately, there are not currently any documentation tools that are optimized to support both personal reflection and skills development.Jiradi is a reflective documentation tool for makers. It’s a portfolio-based website that allows users to easily curate a portfolio to showcase work and keep track of their maker journey. Jiradi has been designed to provide the necessary scaffolding to support a process-oriented approach to learning. The website aims to enhance skills development and improve access to meaningful work.",2020-05-29,[],https://dam-prod2.media.mit.edu/x/2020/05/24/IMG_7235.jpg,378579,,2020-05-27T20:26:31.533Z,['muthui@media.mit.edu'],True,jiradi,2018-09-05,[],Jiradi,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-05-27T21:46:11.539Z,"Major political campaigns and nonprofit organizations use bespoke systems to organize, inform, and solicit members. These top-down systems often require professional IT support and are proprietary, expensive, and inaccessible to smaller organizations. CivicLink is a bottom-up system designed to work at any scale, with a particular focus on the needs of small, grassroots campaigns and community groups.  Now, it is meant to suit adhoc and emergent local groups that are forming to help neighbors in a time of need and mandated isolation.  An example is a shopping list to allow people to piggyback on a food or hardware order. The real goal is to generalize this and make it easy to add your own ideas without any programming.  Be it a phone tree, a checkin scheduler, a petition drive, or a voter registration drive, Civic Link should be there to help organize and mobilize.",2020-09-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/2020/06/01/CivicLink_9_-_24_FPS_.png,378614,,2020-10-23T18:14:35.479Z,"['dmarquez@media.mit.edu', 'smpsnr@media.mit.edu']",True,civiclink,2019-07-15,[],CivicLink 2.0,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-05-28T21:26:09.903Z,"The Creative Learning Challenge Brazil is a fellowship program of the Brazilian Creative Learning Network. Supported by the Lemann Foundation and the MIT Media Lab, the fellowship  aims to identify, connect, and support Brazilian artists, researchers, educators, technology developers, entrepreneurs, and policy makers who contribute to a more creative, hands-on and relevant public education for all children and youth throughout Brazil. ----O Desafio de Aprendizagem Criativa Brasil é o programa de fellowship da Rede Brasileira de Aprendizagem Criativa. Com o apoio da Fundação Lemann e do MIT Media Lab, o Desafio de Aprendizagem Criativa  busca identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam contribuir para tornar a educação pública  brasileira mais criativa, mão na massa e relevante para todas as crianças e jovens. ",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2020/05/28/IMG_5790.JPG,378629,,2020-06-01T13:37:31.809Z,[],True,creative-learning-challenge-brazil,,[],Creative Learning Challenge Brazil / Desafio Aprendizagem Criativa Brazil,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-06-02T20:43:15.104Z,"Wunderbolt is a new take on summer camp developed by the Lab for Social Machines in the MIT Media Lab that remotely connects children ages 8-12 with both trained volunteer coaches and other participating children over the summer. We expose children to fun, personalized enrichment activities and projects (both digital and physical) to foster curiosity and creativity in children during the pandemic. We aim to embody the feeling of summer camp—where kids are engaged, empowered, excited, and enthusiastic to play, be challenged, connect with peers, and learn through doing. At Wunderbolt, every activity and project is hand-picked and personalized for the individual child to spark creativity, fun, and wonder!This program has been registered through MIT’s Protection of Minors program.",2021-06-02,['social-machines'],https://dam-prod2.media.mit.edu/x/2020/07/21/wunderbolt-banner_ZCGTv9k.png,378716,,2020-07-23T19:02:10.307Z,"['micshen@media.mit.edu', 'isysoev@media.mit.edu', 'dkroy@media.mit.edu', 'sballing@media.mit.edu', 'jnazare@media.mit.edu', 'anneli@media.mit.edu', 'mhughes4@media.mit.edu', 'perris@media.mit.edu']",True,wunderbolt,2020-06-02,[],Wunderbolt: Make Stuff. Make Friends.,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-06-03T14:03:17.785Z,"COVID-19 and virtual commencements around the world were not what many universities had planned for the graduating class of 2020. Walking across the stage and being hooded should be a well-earned memory that lasts a lifetime. As an attempt to offer some small amount of normalcy, Media Arts and Science asked to have ""Virtual Regalia"" developed and deployed in under a week, to allow the MS and PhD candidates to appear in their degree-appropriate attire during the video-based commencement reception that normally occurs in the Media Lab's third floor atrium.  Virtual Regalia operates via motion tracking and augmented reality, compositing mortarboards, tams, and robes over the participant's clothes. Physics-based simulations are also used to animate the tassel of each mortarboard or tam. As an added surprise, PhD graduates were instructed to tap their screens at a specified time and an MIT doctoral hood magically wove itself into the proper position on their shoulders.2020 will be a year to remember and this very Media Lab solution will hopefully be one of the better memories.",2020-05-28,['object-based-media'],https://dam-prod2.media.mit.edu/x/2020/06/03/Day0053.png,378728,,2020-06-03T14:48:38.888Z,['novysan@media.mit.edu'],True,virtual-regalia,2020-05-22,['augmented-reality'],Virtual Regalia,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-06-04T19:22:06.008Z,Whispers Of The Mountain is a  sensor system mounted on snow skis.  The system allows for crowdsourcing environmental data of mountains. ,,['city-science'],https://dam-prod2.media.mit.edu/x/2020/06/04/snow_ski_8ZYGxm6.jpg,378745,,2023-01-24T22:36:35.966Z,"['csmuts@media.mit.edu', 'aricom@media.mit.edu', 'kll@media.mit.edu']",True,whispers-of-the-mountain,2019-08-15,"['artificial-intelligence', 'sensors', 'data-visualization']",Whispers Of The Mountain,PUBLIC,https://www.andresrico.xyz,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2020-06-04T23:25:17.649Z,"We introduce–aSpire–a clippable, mobile pneumatic-haptic device designed to help users regulate their breathing rate via subtle tactile feedback. aSpire can be easily clipped to a strap/belt and used to personalize tactile stimulation patterns, intensity, and frequency via its array of air pouch actuators that inflate/deflate individually. To evaluate the effectiveness of aSpire’s different tactile stimulation patterns in guiding the breathing rate of people on the move, out-of-lab environment, we conducted a user study with car passengers in a real-world commuting setting. The results show that engaging with the aSpire does not evoke extra mental stress, and helps the participants reduce their average breathing rate while keeping their perceived pleasantness and energy level high.",,[],https://dam-prod2.media.mit.edu/x/2020/06/05/head2.gif,378753,,2022-09-09T18:11:15.490Z,"['yun_choi@media.mit.edu', 'jinmo@media.mit.edu', 'neska@media.mit.edu', 'picard@media.mit.edu', 'ishii@media.mit.edu']",True,aspire,2020-03-04,"['design', 'human-computer-interaction', 'health', 'sensors', 'affective-computing', 'fabrication']","aSpire: Clippable, Mobile Pneumatic-Haptic Device for Breathing Rate Regulation via Personalizable Tactile Feedback",PUBLIC,http://www.mallcong.com,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-06-05T03:00:49.033Z,"Patchbots are sheet-form robots that are programmable on-the-fly by direct manipulation through bodily movements. The integrated soft mechanism comprises sub millimeter and millimeter scale hybrid fluidic fiber muscles actuated by miniature pumps. With multi-segmented resistive sensors patterned on each fiber module, the robots perceive their own dynamic geometry as well as the body they wrap around.  Using this dynamic geometry data, patchbots are able to respond to the user’s movements in real time and can be applied for kinesthetic learning activities,  locomoting on-body haptics and dynamically re-locating assistive technologies. ",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/09/patch_main.png,378792,,2022-04-17T10:10:39.482Z,"['ishii@media.mit.edu', 'hilamor@media.mit.edu', 'ozgun@media.mit.edu']",True,project-patchbots,2020-06-04,"['robotics', 'design', 'human-computer-interaction', 'wearable-computing', 'nonverbal-behavior']",Patchbots: Milli-fluidic Soft Robots Towards On-body Locomoting Swarm UIs,PUBLIC,http://ozgunkilic.com,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-06-05T14:30:26.436Z,"JettSen is  an open source mobile sensor system that enables citizens, urban planners, and city officials to collectively address urban issues around biking infrastructure.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/06/05/20191021_190604 (1)_DqoNBmG.jpg,378803,,2023-01-24T22:38:32.703Z,"['aricom@media.mit.edu', 'yasushis@media.mit.edu', 'nanaco@media.mit.edu', 'kll@media.mit.edu']",True,jettsen,2019-08-31,"['artificial-intelligence', 'sensors']",JettSen,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2020-06-05T18:45:49.151Z,"Since the 2016 US presidential election, news coverage of hate speech and mis/ disinformation has skyrocketed. What was once a sleepy beat led by freelancers and activists has become a central topic of coverage for almost every news organization. As the news cycle is transformed by coverage of the COVID-19 pandemic and coverage of the 2020 presidential election ramps up, this beat is again at a critical juncture.To better understand the challenges and changes associated with this inflection point, we conducted 10 in-depth interviews with prominent journalists covering this beat. These interviews underscore critical debates in the field about platform accountability, the news agenda, and news organizations’ infrastructure and support systems.",2020-06-05,['civic-media'],https://dam-prod2.media.mit.edu/x/2020/06/05/daniel-schludi-BBQLNpG9UXs-unsplash (1).jpg,378822,,2020-07-07T19:50:26.003Z,"['ethanz@media.mit.edu', 'aashka@media.mit.edu']",True,challenges-and-opportunities-when-covering-hate-speech-and-mis-disinformation,2020-01-06,[],A Moment of Change: Challenges and opportunities when covering hate speech and mis/disinformation,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-06-05T23:54:11.279Z,How to attain Tangible Telepresence—taking more aspects into consideration.T²  Tool Kit:  A Tangible Telepresence (T²) Concept Framework aiming at  Future Human Diaspora,2020-10-25,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/f3a1eba2-de97-404a-9c64-61a91ebf89e5,378831,,2021-02-02T18:01:34.729Z,"['liang_zp@media.mit.edu', 'ishii@media.mit.edu', 'ken_n@media.mit.edu']",False,t-framework-for-tangible-telepresence,2020-09-09,"['robotics', 'design', 'human-computer-interaction', 'wearable-computing', 'physiology', 'human-augmentation']",T²  Tool Kit:  A Tangible Telepresence Concept Framework,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-06-06T06:07:59.251Z,"""Telepresence'"" is raising a fundamental question about our existence. ""I can see you, I can hear you, but where are you?""The Covid-19 pandemic crisis triggered a tidal shift in our work environment from the shared central office to working from home. Teleconferencing technologies have become the new lifeline for our work and life. The current dominant model is the combination of audio-video conferencing (e.g., Zoom) and cloud-based digital information sharing. This model is lacking embodied interactions with people and objects using the tactile sensations we enjoy in our everyday physical environment. A sense of touch is critical for human beings to develop a sense of trust in other people. Additionally, abstract and ephemeral pixels on the screen cannot convey the sense of persistency found in the physical world. We believe the introduction of “tangibility” is a critical step in bringing audio/visual and digital telepresence to the next level and further engaging people. In this workshop, we are going to review the history of CSCW (computer-supported cooperative work), and discuss the future of telepresence technologies.",,[],https://dam-prod2.media.mit.edu/x/2020/06/07/MakingDigTan_32.jpg,378850,,2020-09-09T04:47:15.914Z,"['ishii@media.mit.edu', 'ken_n@media.mit.edu', 'yun_choi@media.mit.edu', 'liang_zp@media.mit.edu']",True,tangible-telepresence,,[],Tangible Telepresence: Reinventing Distancing with Tele-Tangibles,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-06-08T03:15:57.741Z,"The goal of this project is to develop techniques to remove identifying information from wearable and phone data to protect patients’ privacy, yet preserve information to predict depressive symptoms. We propose a novel method following an adversarial training framework combined with representation learning using an autoencoder.We depict the identifiability of users from the wearable and phone sensors. We show that common dimensionality reduction techniques still preserve identifying information for most users. We show that our method is successful in masking identifying information as measured by accuracy and F1 score of IC(z) while being able to retain information relevant to HAMD regression task compared to baseline.This research is supported by National Institute of Health, MIT-MGH Grand Challenge, and MIT J-Clinic.",2021-08-31,['affective-computing'],,378875,,2022-10-03T21:42:58.573Z,"['asma_gh@media.mit.edu', 'roblewis@media.mit.edu']",True,privacy-preserving-depressive-symptoms-estimation,2020-06-01,"['computer-vision', 'artificial-intelligence', 'extended-intelligence', 'human-machine-interaction', 'machine-learning', 'affective-computing', 'natural-language-processing']",Privacy-Preserving Depressive Symptoms Estimation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-06-08T19:44:20.215Z,"What we can do if the screen in videoconference rooms can turn into an interactive display? With Kinect camera and sound sensors, We explore how expanding a system’s understanding of spatially calibrated depth and audio alongside a live video stream can generate semantically rich three-dimensional pixels containing information regarding their material properties and location. Four features are implemented, which are “Talking to Focus,” “Freezing Former Frames,” “Privacy Zone,” and “Spacial Augmenting Reality.”",2011-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/08/01_01_03.jpg,378896,,2020-06-12T17:12:59.403Z,"['liningy@media.mit.edu', 'raskar@media.mit.edu', 'ishii@media.mit.edu']",True,kinected-conference-1,2011-01-01,[],Kinected Conference,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-06-08T22:28:07.615Z,"We are presenting the WorkOut Sensor Node Assistant (WOSNA), a body sensor net composed of many tiny sensor nodes that can suck on the skin. The WOSNA uses pulse sensors to monitor muscle contractions.The pulse sensor can detect the change of blood flow in the muscle caused by contraction, and send data back to the central device by Bluetooth Low Energy (BLE).The program running on the central device can use the data to monitor the performance of the workout. Including how many sets of motion you did, if the motion is standard and if you are using other muscles for compensation.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/08/image1.png,378921,,2020-11-23T16:50:39.217Z,['fzliu@media.mit.edu'],True,wosna-workout-sensor-node-assistant,2020-06-01,"['design', 'health', 'sensors']",WOSNA - WorkOut Sensor Node Assistant,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-06-09T15:44:36.008Z,"The ComTouch project explores the interpersonal communication by use of haptic technology. Audio and video have been commonly used as a medium for interpersonal communication. ComTouch uses the sense of touch to compliment the existing audio and video medium. We expect that touch as a communication medium will allow for more personal communication, and perhaps even open up remote communication to deaf blind users. We hope to develop a haptic communication device that will enable users to transmit thoughts, feelings, and concepts to each other remotely. The basic concept is a handheld device that allows the squeeze under each finger to be represented as vibration. For communication, two devices will be set up to transmit and receive to each other. When transmitting, one device registers the analog force of pressure from each finger as the object is squeezed. This force is transmitted over a wireless connection. At the receiving end, the transmitted force is represented by vibrations under each finger. Through this research, we aim to describe more accurately the language of touch-based communication. We also hope to devise a set of guidelines for designing touch-based communication devices. ",2002-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/09/15-ComTouch-Preliminary-Implementation-One-finger-of-vibrotactile-communication-is.png,378940,,2020-06-09T20:11:16.761Z,['ishii@media.mit.edu'],True,comtouch,2002-01-01,[],ComTouch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-06-09T18:15:34.633Z,"We use sticks as tools for a variety of activities, everything from conducting music to playing sports or even engage in combat. However, these experiences are inherently physical and are poorly conveyed through traditional digital mediums such as video. Linked-Stick is a shape-changing stick that can mirror the movements of another person’s stick-shape tool. We explore how this can be used to experience and learn music, sports and fiction in a more authentic manner. Our work attempts to expand the ways in which we interact with and learn to use tools.",2015-12-31,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/09/linkedstick_highres.jpg,378950,,2020-06-12T17:47:19.923Z,"['ken_n@media.mit.edu', 'inamura@media.mit.edu', 'thariq@media.mit.edu', 'ishii@media.mit.edu']",True,linked-stick,2015-01-01,[],Linked-Stick,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-06-09T18:27:35.073Z,Targeted dream incubation (TDI) is a methodology for guiding (or “incubating”) dreams towards specific themes. Please read the FAQ below to learn more about TDI.We engineered a device to carry out TDI called Dormio. We recently published a study using TDI to understand the link between dreaming and creativity.,,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/12/18/banner-dreamengineers-x.png,378960,,2023-05-19T16:44:37.109Z,"['adamjhh@media.mit.edu', 'pattie@media.mit.edu', 'igrover@media.mit.edu', 'tomasero@media.mit.edu', 'abyjain@media.mit.edu', 'pcuellar@media.mit.edu', 'eyalp@media.mit.edu', 'kaes@media.mit.edu']",True,targeted-dream-incubation,2020-01-01,"['human-computer-interaction', 'art', 'cognition', 'health', 'learning-teaching', 'neurobiology', 'wearable-computing', 'behavioral-science', 'ethics', 'creativity', 'technology', 'cognitive-science', 'industry', 'wellbeing']",Targeted Dream Incubation,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-06-11T15:44:24.322Z,"Lighting conditions in an indoor environment have been shown to affect our cognition and behaviors in a variety of ways. On average, we spend 90% of our time in indoor environments. These usually involve multiple people with diverse environmental needs sharing a space with the same lighting conditions. Additionally, besides providing us with better illumination for practical and aesthetic effects, the way we use and interact with light has not changed. To address this, we present LightCloud,  a  lighting  system  that  enables  each  user  to  create  and  control their own dynamic light source in a shared enclosed space for enhancing social  interactions  and  work  experience.  The  advances  in  smart  lighting and novel distributed system architecture will further enable novel multi-user dynamic lighting. This project aims to provide a look into the applications of dynamic lighting between multiple users in a shared environment.This work won the Best Poster award prize at the Future Technologies Conference (FTC 2020)Conference Paper",2020-12-31,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/11/fig2.PNG,379001,,2020-11-19T21:10:24.113Z,"['nanzhao@media.mit.edu', 'elenack@media.mit.edu']",True,lightcloud,2018-10-01,"['design', 'environment', 'sensors']",LightCloud: Future of Dynamic Lighting in the Shared Space,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2020-06-11T15:44:56.690Z,"TransparenSea is an AI system that uses video from tuna longline vessels as input to identify dangerous fishing practices while at sea.Commercial fishing remains one of the most dangerous jobs in the nation, with a fatality rate that is 23 times higher than for all other workers. In order to increase awareness of safe fishing practices on vessels at sea, unsafe fishing practices that are generated out of habit must first be identified.",2020-08-08,"['responsive-environments', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2020/06/11/WhatsApp Image 2020-05-08 at 12.44.52 PM.jpeg,379009,,2020-06-15T01:35:31.445Z,['oceane@media.mit.edu'],True,transparensea,2020-01-01,"['environment', 'sensors', 'ocean']",TransparenSea: AI for Sustainable Fishing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-11T22:08:17.249Z,"Doze is an IoT enabled platform for personalized scent diffusion and on-skin drug delivery. Leveraging a hydrogel technology, the Doze system is designed for the on-demand metered release of scent or liquid based on the users controls, physiological state or sleep stage.The system consists of two interlocking parts: a stretchy on-skin patch or a fabric-based  eye-mask with an embedded heating coil that is rechargeable and reusable, and a single-use thermally-activated hydrogel insert, which connects to the device and rests on the face or skin. Our design allows for the pairing of specific scent (i.e. fragrance or essential oils) and/or drug (i.e. face serum,  moisturizers, or pain-relief gels) releasing hydrogels  based on the needs or preferences of the user.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/11/dsc_9547 2.JPG,379040,,2020-11-09T15:59:35.291Z,['irmandy@media.mit.edu'],True,doze,2019-09-01,"['human-computer-interaction', 'bioengineering', 'consumer-electronics', 'pharmaceuticals', 'wearable-computing', 'materials', 'fabrication', 'wellbeing', 'physiology', 'medicine']",Doze: On-skin Scent Diffusion and Drug Delivery Patch,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-06-12T04:53:12.141Z,"The Suggestion Box offers schools a toolkit for framing decisions in two simple sentences and using these sentences to involve teachers, students, and families in iterative planning amidst the current pandemic. You can find the toolkit and examples of how schools are using it here and an introduction to the project here.Join Us!We're working with school leaders around the US to test and iterate on this toolkit. If you're interested in getting involved, send an email to schoolslearntoo@mit.edu. This work is funded with generous support from the Aspen Institute. ",2021-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2020/06/12/undraw_true_friends_c94g.png,379099,,2022-09-22T01:23:05.525Z,['yusufa@media.mit.edu'],True,the-suggestion-box,2020-05-11,"['learning-teaching', 'covid19']",The Suggestion Box,PUBLIC,http://bit.ly/SchoolsLearnToo,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-06-12T16:04:21.730Z,"Project Octopus is a new way to make collaborative Scratch projects.Based on technologies developed for the Scratch programming language, Project Octopus allows makers to capture an object from the physical world (e.g., a character made out of craft materials or a hand-drawn creature), program the object using Scratch blocks, and then send the programmed object to a collective canvas hosted on a public web server.Multiple makers can capture, code, and share simultaneously, allowing large groups of people to work on the same project together in real time.Project Octopus is designed for large scale collaborations in settings such as museums, outdoor spaces, and virtual online gatherings.",2022-05-31,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2020/06/15/llk-project-octopus-website-hero-image_8Ei5IGv.png,379150,,2022-09-30T16:27:31.671Z,"['hisean@media.mit.edu', 'khanning@media.mit.edu']",True,project-octopus,2020-01-01,[],Project Octopus,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2020-06-12T18:40:33.442Z,"We are leveraging current Wireless Sensor Network technology to develop an easily deployable, and inexpensive wireless sensor (LunarWSN) network that’s can be used for future planetary in-situ exploration. The network can operate in-situ resource detection and surface environment monitoring. The localization and communication network that is set-up by the LunarWSN can also serve as an early infrastructure to provide position/navigation and communication service for astronauts and other systems that working on the lunar surface.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/14/banner.png,379193,,2022-10-26T15:07:26.378Z,"['joep@media.mit.edu', 'fzliu@media.mit.edu']",True,lunar-sensor-net,2020-05-01,"['robotics', 'design', 'environment', 'sensors', 'space', 'technology']",Lunar Sensor Net,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-12T20:43:05.191Z,"Regenerative agriculture, or “carbon farming,” is the use of agricultural practices and crops that draw down excess atmospheric carbon through photosynthesis, and store it in biomass and soils over the course of decades. Though carbon constantly cycles through the soil due to active nutrient processes, regenerative agriculture seeks to deliver additional carbon inputs to the soil, which can result in the net accumulation and storage of carbon in soil. Practically, this net accumulation can be achieved through agricultural practices like cover cropping, no-till farming, and agroforestry. Studies have estimated that, at scale, carbon farming could pull enough carbon out of the atmosphere to offset 10% of annual anthropogenic carbon emissions. Given broad recognition of the need for significant atmospheric carbon removal in addition to aggressive emissions reductions, carbon farming is seen as a promising climate response. Furthermore, carbon farming can result in significant agricultural and ecological co-benefits, like improved soil structure, a robust soil microbiome,  and better water retention, all of which bolster agriculture’s climate change resilience.Economic, policy-based, and social programs can provide the financial incentives to motivate widespread implementation of carbon farming practices. Farmers could participate in voluntary carbon markets, selling carbon credits that represent the soil carbon accumulated through regenerative farming practices. Government policy initiatives could reward carbon farming through subsidy or grant programs. Consumers could reward “low-carbon” or “carbon-neutral” products through their purchasing decisions.However, the difficulty of getting standardized, comprehensive, and accurate soil carbon measurements is a major impediment to the wider development of these programs. Traditional soil carbon monitoring techniques are slow and invasive: they require extracting soil cores that are shipped to a lab, and analyzed using expensive analytical chemistry methods. This sensing pipeline can take days or weeks and currently makes it impractical to run soil carbon incentives initiatives at scale. Thus, our group is starting to investigate sensing techniques and systems to address the challenge of scalable soil carbon sensing. We are looking systems that use data fusion to integrate farm-based sensor data, satellite imagery, and environmental modeling in order to provide fast, cheap, and accurate carbon estimation at scale.",2020-12-31,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/12/2282705095_a8b2f639d9_o.jpg,379210,,2021-08-10T13:33:22.720Z,['cjaffe@media.mit.edu'],True,soil-carbon-sensing,2020-06-01,"['environment', 'sensors', 'climate-change']",Low Cost Soil Carbon Sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-13T19:14:16.840Z,"As developers of prosthetic technology, we’re dedicated to understanding the challenges that people with limb loss face, not just after amputation but in many cases, leading up to it.With experts in global health and development, biomedical and mechanical engineering, and remote logistics, we’re designing a better way to prevent and treat amputations, and bringing our technologies directly to communities that need them most.This project focuses on capacity-building to bridge gaps in healthcare for people with diabetes and those living with limb loss by delivering convenient, high quality services for a fraction of the cost through an innovative distributed care model.Operated by locally trained people with amputation and trusted clinicians, we use a satellite mobile health clinic to increase the capacity and geographical range of services. Guided by our epidemiological spatial analysis tools, we’re prioritizing communities across the world with the highest rates of amputation and amputation risk, paired with a limited ability to respond to this need.Our TeamThe Biomechatronics Group is led by Professor Hugh Herr. This project is developed by biomedical engineer and global health systems and policy specialist Francesca Riccio-Ackerman and incorporates the work of the digital prosthetic socket design project including researchers Dana Solav, Xingbang Yang, Aaron Jaeger, Christina Meyer, Duncan Lee, Samantha Gutierrez-Arango and Erica Israel.Dr. Nancy Oriol, MD, Faculty Associate Dean for Community Engagement in Medical Education and Lecturer on Global Health and Social Medicine at Harvard Medical School.Dr. Joel Huegel, leader of the Biomechatronics Research laboratory and tenured Professor of Mechanical Engineering at Tecnologico de Monterrey in Guadalajara, Mexico who specializes in low-cost, highly functional lower limb prostheses to meet worldwide need.Our project's supporting research team includes MIT undergraduates: Carl ""Andrew"" Seelhoff , Kaili Glasser, Aashini Shah and Liam Ackerman ; and FIU undergraduates Lina Henriquez and Lara Garcia.Preceding ProjectsThe work to produce a computational algorithm to digitally design prosthetic sockets is foundational to this current translation and implementation effort. More information about these projects is provided in the articles below.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2020/06/13/homemade prostheses.jpg,379240,,2021-11-30T15:08:23.082Z,"['cimeyer@media.mit.edu', 'xingbang@media.mit.edu', 'friccioa@media.mit.edu', 'eisrael@media.mit.edu', 'samga@media.mit.edu', 'amjaeger@media.mit.edu', 'jhuegel@media.mit.edu', 'danask@media.mit.edu', 'hherr@media.mit.edu']",True,resilient-prosthetic-and-diabetic-care,2019-06-15,"['health', 'healthcare', 'public-health', 'prosthetics', 'medicine', 'bionics', 'prosthetic-design']",Democratizing prosthetic and diabetic care: A resilient model for healthcare delivery,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-06-13T19:38:31.913Z,"Dr. Jifei Ou, who received his PhD and graduated from the Tangible Media group in 2019, started a company called OPT Industries, Inc., based on his PhD thesis project, Cilllia. He prototyped nasopharyngeal swabs based on Cilllia, a method of 3D printing hair-like fibers that Jifei developed in the Tangible Media group. He is now fabricating swabs for the testing of Covid-19 infections. This is a collaboration between Dr. Jifei Ou's OPT and Formlabs, another ML startup.",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/06/13/Screen Shot 2020-06-13 at 3.39.20 PM.png,379256,,2021-04-08T16:47:06.306Z,"['jifei@media.mit.edu', 'ishii@media.mit.edu']",True,3d-printed-nasopharyngeal-np-swabs-by,2020-03-15,"['manufacturing', '3d-printing', 'alumni', 'public-health', 'materials', 'startup', 'covid19']","3D Printed Nasopharyngeal (NP) Swabs by OPT Industries, Inc.  in response to the 2020 COVID-19 Crisis",PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-13T22:33:59.874Z,"WeScratch are online workshops for everyone who wants to learn how to create projects in Scratch in an inclusive, collaborative, and playful online environment.While many online learning experiences follow the transmissive approach typical of video lessons and webinars, the WeScratch experience is designed as a hands-on, creative workshop where participants spend most of their time making projects while they share ideas with peers.WeScratch provides a model for how to design online learning environments to be more inclusive, playful, and collaborative. Educators from all over the world have participated in our workshops. Some of them are already using strategies from WeScratch to design engaging online learning experiences for their students.Although WeScratch workshops are designed to support learning to code, we see the wider potential for applying this approach to other learning environments in order to broaden participation, build connection, and expand creative expression—especially online.",,['lifelong-kindergarten'],https://dam-prod2.media.mit.edu/x/2020/06/13/wescratch-logo-background.png,379264,,2020-06-15T16:42:46.995Z,"['tarmelop@media.mit.edu', 'rupal@media.mit.edu', 'nrusk@media.mit.edu', 'carolcr@media.mit.edu', 'gabaree@media.mit.edu']",True,wescratch,2019-01-07,[],WeScratch,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0
False,2020-06-14T23:58:30.838Z,"In this project we propose a prototype which combines an existing AR headset Hololens 2 with a Brain-Computer Interfaces (BCI) system based on our AttentivU project, and we perform several tasks to validate this concept. Application 1. Assessing Internal and External Attention in AR using Brain Computer Interfaces.Most research works featuring AR and Brain- Computer Interface (BCI) systems are not taking advantage of the opportunities to integrate the two planes of data. Additionally, AR devices that use a Head-Mounted Display (HMD) face one major problem: constant closeness to a screen makes it hard to avoid distractions within the virtual environment.  In the first application, we propose to reduce this distraction by including information about the current attentional state. We first introduce a clip-on solution for AR-BCI integration. A simple game was designed for the Microsoft HoloLens 2, which changed in real time according to the user’s state of attention measured via electroencephalography (EEG). The system only responded if the attentional orientation was classified as ""external."" Fourteen users tested the attention-aware system; we show that the augmentation of the interface improved the usability of the system. We conclude that more systems would benefit from clearly visualizing the user’s ongoing attentional state as well as further efficient integration of AR and BCI headsets.Application 2. A Pilot Study using Covert Visuospatial Attention as an EEG-based Brain Computer Interface to Enhance AR Interaction.In the second application we investigated the feasibility of using a BCI based on covert visuospatial attention (CVSA) – a process of focusing attention on different regions of the visual field without overt eye movements. We operated without relying on any stimulus- driven responses. The proof-of-concept presented in this application opens up interesting possible applications of AR EEG-BCIs which use CVSA. Inherent gaze independence of CVSA makes it a potential alternative for patients who do not display any overt eye movements. Its intuitiveness—natural attraction toward regions or objects of interest in the visual field— makes it a possible candidate for BCI-driven navigation devices (like wheelchairs or robots), as well as yes–no communication. The absence of stimulation stimuli like ERPs/SSVEPs may prove it more suitable for use over longer periods of time, as it allows a more engaging, comfortable and direct operation, and it is better adapted toward out-of-lab interactions for different user groups. Please check this presentation I gave for Society of Photo-Optical Instrumentation Engineers (SPIE) in February 2020 as well as papers at IEEE BSN 2021.Big thank you to collaborators on this project: Yujie Wang, Qiuxuan Wu, Chia-Yun Hu, who equally contributed to this work. Chia-Yun Hu and Yujie Wang designed the clip-on holder for Hololens 2  to host all the electrodes and electronics for the brain-sensing component of our system - and we will be sharing it once the papers are out. Qiuxuan Wu designed the video game applications using Unity.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/uuid/4bc49ee8-44c0-42fd-a1ad-718d9ad5fd8e,379301,,2021-10-26T14:05:02.745Z,['nkosmyna@media.mit.edu'],True,attentivu-xr,2019-11-01,['neural-interfacing-and-control'],"AttentivU+XR: Measuring Engagement, Visual, Auditory Attention, Imagery and Intent using Brain Activity",PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-06-15T04:33:54.398Z,"We imagine equipping electronic skins with a sense of attention in order to operate more efficiently in resource constrained environments. A dense sensate skin that is self-aware can transition between multiple diverse functions in service of both for humanistic and scientific aims based on local conditions and available power/compute resources.  (As of June 2020, this project is in concept / early prototype development stage. It is a new research effort, yet also serves as an extension to the SpaceSkin project)Dense sensor networks incorporated onto electronic skins enable measurements across large and conformal areas. However, scaling this type of system poses fundamental challenges with respect to channel count, channel readout latency, data volume and system power consumption. These constraints become especially pronounced in remote and resource constrained environments from deep sea to deep space. Requirements are also application specific, yet increasingly sensor networks are serving multiple functions at once. We imagine incorporating a sense of attention into the skin that can be used to dynamically and intelligently mediate data acquisition.  The attention model is driven by estimates for where interesting data is most likely to occur, as well as by the temporal and spatial resolution that is likely required in order to resolve this data. Finally, various physical architectures for the skin system will be considered in order to promote efficient resource management across a multiface skin.    As a toy example: a spacecraft interested in detecting micrometeoroid impactors from the space-facing direction may concentrate its power and compute resources towards whichever region of skin is correctly oriented, even as the spacecraft performs various attitude maneuvers.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2020/06/15/brain.jpg,379322,,2023-05-31T12:27:45.635Z,['cherston@media.mit.edu'],True,attentive-skins,2020-06-15,"['sensors', 'electrical-engineering', 'materials']","Attentive Electronic Skins For Low Power, Multifunctional Operation",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2020-06-15T13:51:18.910Z,"Contact tracing, isolation, and testing are some of the most powerful public health interventions available, but they haven't been optimized to deal with the low case identification rates and high asymptomatic transmission characteristic of the COVID-19 pandemic.Current “forward-tracing” protocols seek to identify and isolate individuals who may have been infected by the known case, preventing continued transmission. However, chains of SARS-CoV-2 transmission commonly go undetected, with even the best-performing health systems struggling to detect even 50% of cases.We reasoned that when asymptomatic carriers are common and detection rates are low, “bidirectional” contact tracing could identify and isolate undiscovered branches of the viral family tree, preventing many additional cases.To test this hypothesis, we created a mathematical model informed by a range of epidemiological parameters from the literature, which predicts than bidirectional tracing can more than double the efficacy of current protocols. Importantly, this result is robust across epidemiological scenarios, and it's actually more effective in areas that are struggling to find most cases.Next, we examined whether adding smartphone-based exposure notification could help, and found that it can triple the effectiveness... but only if nearly every smartphone participates by logging exposure events. Anything less, and the benefits are marginal, strongly suggesting that digital exposure notification should be integrated into the operating system of each smartphone with voluntary sharing of exposure events upon diagnosis.Our work is available as a preprint on medRxiv and has been submitted for publication.",,['sculpting-evolution'],https://dam-prod2.media.mit.edu/x/2020/06/15/Bidirectional_tracing_black3_6HIKvd9.png,379349,,2020-06-17T19:19:52.802Z,"['esvelt@media.mit.edu', 'ecalley@media.mit.edu']",True,bidirectional-contact-tracing,2020-04-01,['covid19'],Bidirectional Contact Tracing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-15T14:47:07.116Z,"https://everettlawson.com/biologically-encoded-augmented-reality-systems-integration-and-geometric-reform/Information floods the center of our visual field and often saturates the focus of our attention, yet there are parallel channels in the visual system constantly and unconsciously processing our environment.  There is a dormant potential to activate these channels to challenge the limits of perception.This research explores potentials in augmenting the perceived driving experience through the delivery of adaptive, context-aware stimuli.  The role of peripheral vision in self-motion estimation is far more efficient than central vision. Vection, or perceived self-motion through visual stimulus alone, is heavily influenced by peripheral cues and creates strong illusory effects to the observer.  Psychophysical stimuli are delivered as fast adaptation mechanisms to manipulate raw scene data streams in their animated motion trajectories.  These algorithmically-generated signals are subtly presented to affect the observer’s sensation of speed and rate of turn in a first-person point-of-view (POV) driving environment.  This work represents a new intersection of the fields of vision science, computational imaging, and display technologies and could challenge the way we generate media for human consumption in active environments.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2020/06/15/IMG_20190826_005534-scaled.jpg,379365,,2022-06-10T14:09:49.544Z,['elawson@media.mit.edu'],True,biologically-encoded-augmented-reality-cockpit,2019-02-01,[],Biologically encoded augmented reality: cockpit,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2020-06-15T18:47:48.679Z,"A world in which thousands of people can construct and release autonomous biological agents is unlikely to flourishProfessor Kevin Esvelt, the leader of Sculpting Evolution, is responsible for CRISPR-based gene drive, the first technology that enables a single researcher to unilaterally edit entire species and ecosystems. Thankfully, gene drive favors defense. That may not be true for other autonomous biotechnologies.Our challenge is to prevent the immense power of biotechnology from being misused. Historical pandemics killed tens of millions of people, and engineered agents could be even more destructive.Many thousands of people have the technical skills to build and release autonomous biological weapons, but only if they know what to make and can obtain a physical copy – for example, by assembling it from synthetic DNA.To protect humanity, we should screen all DNA synthesis orders for potential bioweapons, ideally without disclosing what we're looking for. A secure global screening system would:Minimize the number of people with access to bioweaponsSecurely guard against new potential bioweapons without drawing attention to themRefrain from delaying legitimate researchThe Secure DNA ProjectOur team of academic life scientists, cryptographers, and policy analysts from many nations came together in 2019 to devise a system capable of secure and universal DNA synthesis screening that would be suitable for stepwise or complete implementation by stakeholders.Current screening approaches are inaccurate and insecure. They work by looking for similarities between ordered DNA sequences and the genomes of potential bioweapons. Unfortunately, there are so many similar sequences that innocent orders are mistakenly identified as threats, requiring human experts to examine them one by one. Worse, the database of potential bioweapons is necessarily public, so newly emerging threats can't be blocked without providing rogue actors with blueprints and a credible incentive to use them.We accordingly sought to re-envision DNA synthesis screening from first principles:Necessity: we only need to block the synthesis of a few essential pieces of each bioweaponFunctionality: we should also block access to any sequences predicted to function equivalentlySecurity: automated screening should not disclose DNA orders or potential bioweaponsThe Proposed SolutionSecurely screen for random critical fragments and functional equivalents without false alarms",,"['sculpting-evolution', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2020/06/15/DNA lock.png,379440,,2023-09-07T23:35:09.866Z,['esvelt@media.mit.edu'],True,secure-dna,2019-06-01,['covid19'],Secure DNA,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2020-06-15T19:11:20.334Z,"The Global Community Bio Summit 4.0 will be held October 11-13, 2020 and will virtually convene the global citizen-led, grassroots community bio movement. We will explore the forefront of Open and Citizen Science, how to start and sustain community labs, enabling technologies, bioethics, safety, and security, education and learning experiences, bio art & design, and biotech’s cultural impact together. We will examine how this community can disrupt innovation: are community labs the next Homebrew Computer Club (the birthplace of the Personal Computer) for biology?Visit https://www.biosummit.org/ to apply!",2020-10-31,['community-bio'],https://dam-prod2.media.mit.edu/x/2020/06/15/Bio Summit 4.0 web screenshot_mqUWQev.png,379448,,2020-06-19T19:02:59.103Z,"['suryaj@media.mit.edu', 'bestrada@media.mit.edu', 'desireed@media.mit.edu', 'dkong@media.mit.edu']",True,bio-summit-4,2020-01-01,[],Bio Summit 4.0,PUBLIC,https://www.biosummit.org/,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-06-15T19:39:37.321Z,"COVID-19 has over only a few short months resulted in the deaths of >400,000 people worldwide (as of mid-June 2020) and has forced many others to suspend their livelihoods. A vaccine is unlikely to arrive until sometime next year, but a safe and effective drug could be developed, manufactured, and distributed before that. With this project, we are working to develop a therapeutic for COVID-19 that is safe, effective, and difficult for the SARS-CoV-2 virus to evolve resistance against. By taking the receptor that the virus normally uses to enter our body's cells (ACE2) and tweaking it so it can be made into a drug—making a so-called ""decoy receptor mimic""—we are diverting the virus away from the normal ACE2 receptor, leaving it unable to replicate itself in cells. To do this, we are using a combination of laboratory evolution and machine learning techniques to rapidly identify hundreds of candidates with high binding affinity to the viral spike protein and—in order to ensure safety—low predicted binding affinity with other proteins in the human body. We are hoping that this strategy will provide a blueprint that can be extended to proactively develop decoy receptors for other viruses, enabling us to get ahead of the next pandemic.",,['sculpting-evolution'],https://dam-prod2.media.mit.edu/x/2020/06/15/RBD_in_complex_with_ACE2.png,379464,,2020-06-17T19:19:03.763Z,"['buchthal@media.mit.edu', 'erikad@media.mit.edu', 'ecalley@media.mit.edu', 'bscwang@media.mit.edu', 'echory@media.mit.edu', 'esvelt@media.mit.edu']",True,evolving-decoy-ace2-receptor-mimics,,['covid19'],Evolving Decoy ACE2 Receptor Mimics As COVID-19 Therapeutics,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-15T21:01:52.964Z,"Pandemic Response CoLab will help individuals and groups work together to solve practical problems created by the COVID-19 pandemic. By leveraging an open online collaboration platform, we hope to mobilize innovators, communities, businesses, and others to develop actionable solutions to real problems.Over time, we expect the online community using Pandemic Response CoLab to help:Identify key problems related to COVID-19,Develop solutions to these problems, andRecruit people and resources to implement these solutions. Our initial activities focus on the first of these stages: identifying key problems.",,['community-bio'],https://dam-prod2.media.mit.edu/x/2020/06/16/grid.jpg,379477,,2020-07-09T15:19:47.174Z,"['desireed@media.mit.edu', 'dkong@media.mit.edu', 'bestrada@media.mit.edu']",True,pandemic-response-colab,,"['health', 'member-company', 'wellbeing', 'covid19']",Pandemic Response CoLab,PUBLIC,https://www.pandemicresponsecolab.org/,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-06-15T21:22:40.952Z,"The Pandemic Supermind Activation brings together leaders in the biosciences and beyond, harnessing the power of humans and technology to identify the critical unmet needs of the COVID-19 pandemic, addressing this main challenge; How can we develop pandemic resilience—the ability for society to recover quickly from global disease outbreaks—both in resolving the current COVID-19 pandemic and in building the public health and other infrastructure to prepare for future pandemics?",,['community-bio'],https://dam-prod2.media.mit.edu/x/2020/06/15/Screen Shot 2020-06-13 at 5.06.24 PM.png,379494,,2022-06-14T19:59:36.270Z,"['desireed@media.mit.edu', 'dkong@media.mit.edu']",True,pandemic-supermind-activation,,"['member-company', 'public-health', 'covid19']",Pandemic Supermind Activation,PUBLIC,https://www.pandemicsupermind.org/,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-06-15T21:42:01.279Z,"dementAI is an open-source platform for modeling risk stratification of Alzheimer's dementia using spontaneous speech through a mobile service powered by privacy-protected and interpretable AI.Alzheimer’s disease is estimated to affect around 50 million people worldwide and is rising rapidly, with a global economic burden of nearly a trillion dollars.  We are developing a scalable, cost-effective, and robust method for stratification of Alzheimer’s dementia using a language-agnostic mobile service. Our approach uses a multi-modal ensemble of acoustic and cognitive markers (like prosody, disfluency, tracking user's train of thoughts, and more) derived from the user’s spontaneous speech.Our uncertainty-aware and multi-modal AI system has achieved state-of-the-art results on benchmark dementia datasets, and shows the promise of mobile and non-invasive applications for risk stratifying conditions like dementia. The size and minimal design of our models allow for on-device training and inference, thus supporting data privacy. We baked interpretability into our system design to help healthcare providers with insights into dementia. The modularity of our speech platform may further support other AI-assisted speech applications such as affect recognition, monitoring depression, and more. The project's  codebase is entirely open-source to promote research around scalable, cost-effective, and robust methods using spontaneous speech for dementia and other conditions.For further information, see our paper ""Multimodal Inductive Transfer Learning for Detection of Alzheimer’s Dementia and its Severity"" accepted to the INTERSPEECH Conference, 2020.  Two additional papers ""Why have a Unified Predictive Uncertainty? Disentangling it using Deep Split Ensembles"" and ""Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of Alzheimer's Dementia"" are currently under review.",,[],https://dam-prod2.media.mit.edu/x/2020/06/15/Screen Shot 2020-06-15 at 6.02.02 PM.png,379503,,2020-12-22T17:05:08.133Z,"['utkarshs@media.mit.edu', 'pattie@media.mit.edu']",True,dementai,,"['human-computer-interaction', 'artificial-intelligence', 'health', 'interfaces', 'machine-learning', 'affective-computing', 'biomechanics', 'wellbeing']",dementAI,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-15T22:03:24.570Z,"Vida OverviewThe Space Enabled research group at the MIT Media Lab is leading a US-based team of innovators from East Carolina University (Prof David Lagomasino), Blue Raster LLC (Eric Ashcroft), and consultant Dr. Mohammad Jalali. The team is actively collaborating with leaders in Latin America, Africa, and Southeast Asia to design decision support tools to inform public policy in response to challenges such as the COVID-19 pandemic. This project creates a platform for collaboration between technologists to build the Vida Decision Support System, a customized online tool targeted for use by public sector officials at the city and regional level who are making policies regarding public health, the economy, or environmental management. The Vida Decision Support System helps city and regional leaders understand the relationships between important societal factors that are changing by creating models in the following areas: Public Health, Environment, Socio-economic Impacts, Public Sector Decision Making, and Disease Surveillance Technology Design. Vida is based on an integrated Data Visualization and Modeling Framework created by the Space Enabled research group at MIT that combines multiple data sources and creates simulations of the current or potential future states of a city or region (see Reid, Jack, Cynthia Zeng, and Danielle Wood. ""Combining Social, Environmental and Design Models to Support the Sustainable Development Goals."" In 2019 IEEE Aerospace Conference, pp. 1-13. IEEE, 2019). Please view the material below for an overview of Vida, active work underway by the US-based team and our international collaborators, and upcoming publications and presentations related to Vida.",,['space-enabled'],https://dam-prod2.media.mit.edu/x/2020/06/15/rio_oli_2015268_lrg.jpg,379513,,2021-11-04T16:43:44.234Z,['drwood@media.mit.edu'],True,vida-decision-support-system,2020-03-24,"['public-health', 'data-science', 'covid19']",Vida Decision Support System for COVID-19,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-06-15T22:05:05.291Z,"Liquid handling robots are key components of modern biotechnology labs, but they largely function to spare humans from laborious or repetitive protocols. We decided they could do more.Upgrading Hamilton robotsMany companies offer superb liquid-handling robots, but their software is narrowly designed to automate traditional hand-pipetting protocols and struggles to harness the full capabilities of robotic manipulation. To change that for our two Hamilton robots—and through open-source, all Hamilton robots—we developed Pyhamilton, an open-source Python package that eliminates these constraints, enabling experiments that could never be done by hand. We used Pyhamilton to double the speed of automated bacterial assays over current software and execute complex pipetting patterns to simulate population dynamics, and incorporated feedback-control to maintain hundreds of remotely monitored bacterial cultures in log-phase growth without user intervention. Finally, we applied these capabilities to comprehensively optimize bioreactor protein production by maintaining and monitoring fluorescent protein expression of nearly 500 different continuous cultures to explore the carbon, nitrogen, and phosphorus fitness landscape. Our results demonstrate Pyhamilton’s empowerment of existing hardware to new applications ranging from biomanufacturing to fundamental biology.",,"['sculpting-evolution', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/uuid/d02d8fe4-a589-4587-8895-c218d78ce978,379521,,2023-09-07T23:31:55.056Z,"['echory@media.mit.edu', 'dgretton@media.mit.edu', 'erikad@media.mit.edu']",True,flexible-automation-for-robotic-bioengineering,2018-06-01,[],Flexible automation for robotic bioengineering,PUBLIC,https://www.linkedin.com/in/emma-chory-a7466014/,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-15T22:29:55.552Z,"Appropriate and adaptive treatment plans are challenging in Parkinson’s disease for several reasons.  parkinsAI models drug response and disease progression via telemedicine allowing real-time symptom tracking through wearables, to facilitate dynamic and adaptive treatment plans with dose adjustments and potential insights about Parkinson’s disease. Our system models the patient’s tremor severity given his/her medication status (of dopaminergic drugs like levodopa used in PD treatment) and the IMU data of wearable smartwatch. This can be used to practice dynamic and adaptive patient care remotely using common wearables.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/06/15/parkinsAI.jpeg,379529,,2021-05-20T20:12:26.485Z,"['utkarshs@media.mit.edu', 'pattie@media.mit.edu']",True,parkinsai,,"['human-computer-interaction', 'artificial-intelligence', 'health', 'wearable-computing', 'interfaces', 'machine-learning', 'healthcare', 'wellbeing']",parkinsAI,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-06-16T12:37:20.894Z,"The full text of our paper is available here.Sometimes the thing that we want to see is hidden behind something else.  A neighboring vehicle on the road might block a motorist's view of a pedestrian that's about to cross the street.  Trees might severely hamper a drone's ability to navigate through a forest by occluding the drone's field-of-view in almost every direction.  Even the front-facing surface of any opaque object will block our view of the object's back-facing surface.While we can't view hidden objects directly, sometimes we can learn about them from more indirect cues.  In this paper we exploit the fact that hidden objects can still cast shadows onto surfaces that we can see.  Shadows are very informative about object shape, particularly if the position of the light source is also known.We use a laser pointer to project laser spots onto visible surfaces lying to one side of the hidden space.  These laser spots act as scannable, ""virtual"" point sources of light.  We use an RGB or RBG-D camera to observe the resulting shadows that hidden objects cast onto surfaces that lie on the opposite side of the hidden space.   After illuminating multiple laser spots and subsequently observing multiple sets of shadows, we use a space-carving algorithm to extract the detailed, 3D shapes of objects lying within the hidden space.",2023-05-31,['camera-culture'],https://dam-prod2.media.mit.edu/x/2020/06/16/Screen Shot 2020-06-16 at 8.39.23 AM.png,379572,,2022-10-14T16:46:14.415Z,"['co24401@media.mit.edu', 'tomotomo@media.mit.edu', 'tswedish@media.mit.edu', 'raskar@media.mit.edu']",True,seeing-around-objects,2019-10-01,"['computer-vision', 'artificial-intelligence', 'augmented-reality', 'consumer-electronics', 'imaging']",Using Shadows to See Behind Objects,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-16T12:49:54.941Z,"Acquisition of surface electromyography (sEMG) from a person with an amputated lower extremity (LE) during prosthesis-assisted walking remains a significant challenge due to the dynamic nature of the gait cycle. This project aims to develop a flexible dry electrode design for sEMG acquisition within prosthetic sockets. The design criteria and potential outcomes include  1) compatibility with a subject’s existing prosthetic socket and liners, 2) sufficient user comfort, and 3) affordable fabrication cost.",,['biomechatronics'],https://dam-prod2.media.mit.edu/uuid/ed83b101-c934-4c5f-b0df-80b312304013,379584,,2021-01-08T16:20:35.152Z,"['syeon@media.mit.edu', 'thhsieh@media.mit.edu', 'hherr@media.mit.edu', 'lfreed@media.mit.edu', 'emrogers@media.mit.edu', 'tonyshu@media.mit.edu', 'hngnsong@media.mit.edu']",True,flexible-electrodes,2020-01-02,"['robotics', 'human-machine-interaction', 'wearable-computing', 'biomechanics', 'electrical-engineering', 'human-augmentation', 'prosthetics', 'bionics']",Flexible dry electrodes for EMG acquisition within prosthetic sockets,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-06-16T21:31:34.624Z,"https://everettlawson.com/biologically-encoded-augmented-reality/The system architecture and methodology described here introduce a novel two-stage carrier signal generation approach:  first, proven psychophysical carrier signals are adapted to the environment and second, the adapted carriers are animated along contextual motion paths. After stimulus preparation, motion-modulated, adapted carrier signals integrate into a dynamic environment to augment observer experiences. ",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2020/06/16/IMG_20191016_094018.jpg,379620,,2022-06-10T14:09:49.432Z,['elawson@media.mit.edu'],True,biologically-encoded-augmented-reality-2,2019-01-01,[],Biologically encoded augmented reality: stimulus preparation,PUBLIC,http://everettlawson.com/,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-06-17T18:47:07.175Z,Spherical tissue deformation in electrostatic fields (NaCL+H20 suspension in hydrogel structure)https://everettlawson.com/biologically-adaptive-synthetics-toward-electrostatic-stimulation-and-mapping/,2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/uuid/af9c6d5d-f6ea-4e53-b476-8c1586671bfc,379660,,2022-06-10T14:09:49.498Z,['elawson@media.mit.edu'],True,electrostatic-priming-surface-mapping-and-deformation,2015-04-01,[],Electrostatic priming: surface mapping and deformation,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-06-17T18:50:47.688Z,"Proprioceptive feedback systems for kinetosis (motion sickness) treatment and preventionhttps://everettlawson.com/neural-mismatch-model-for-adaptive-sensory-remapping/Rather than treating the environment, the automobile and the various independent mechanisms of human biology that result in motion sickness as independent from one another, we can simplify the model.  Due to technological advances in the automotive industry we can posit an assumption that a single automobile and its sensing technologies may represent the vestibular occupancy of the vehicle and, rather than creating an individual intervention to mitigate car sickness across a vehicle’s population, the vehicle sensing capabilities can function as a metric of the human vestibular system allowing the reduction of parameterization of possible kinetosis outcomes to visual discordance.  My dissertation is specifically looking at low level visual mechanisms as a conduit to intervene weighted estimations of scene velocity.  This prototype reduced to practice is in the form of a standalone headrest, equipped with processing and feedback mechanisms for kinetosis mitigation and prevention.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2020/06/17/Annotation 2020-06-17 145114.png,379668,,2022-06-10T14:09:49.482Z,['elawson@media.mit.edu'],True,neural-mismatch-model-for-adaptive-sensory-remapping,2016-09-01,[],Neural mismatch model for adaptive sensory remapping,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-06-17T18:59:15.659Z,"https://everettlawson.com/gyroscopic/Angle, inertia, and velocity are a subset of moment arms that when instinctively activated are biologically hardcoded in how we navigate through space.  The complex geometries of gait motion extend significantly beyond any computational model in real-world outcomes.  We need to further explore shifts in torque differentials within the interface between the wearer and the device.Displacements in the alignment between the sagittal plane of interface and the trajectory of forward motion cause significant disruptions in gait efficiency.  This system provides a dynamic, real-time rotational inertia throughout the gait cycle axis, minimizing angular misalignment between the anatomical limb and the bionic interface.  This resultant stabilization is especially critical at the point of heel strike in early gait motion when contact between the device and the ground is small and unstable.",2020-08-01,['object-based-media'],https://dam-prod2.media.mit.edu/x/2020/06/17/Annotation 2020-06-17 150011.png,379678,,2022-06-10T14:09:49.463Z,['elawson@media.mit.edu'],True,gyroscopic-stabilization-assist-for-ambulation,2017-03-01,[],Gyroscopic stabilization assist for ambulation,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-06-19T01:42:11.988Z,"The   purpose of this study is to investigate possible neural correlates of   electrodermal activity. Electrodermal   activity (EDA) is a broad term describing the changes in the electrical   properties of the skin. Activation of the sympathetic nervous system (SNS) –   the part of the nervous system responsible for your “fight or flight”   response – induces microscopic changes in the sweat glands beneath the skin.   More simply, “you sweat when you’re stressed.” Clammy hands, “sweating   bullets” when nervous, and other euphemisms aptly describe this phenomenon.   However, one does not have to be sweating bullets for sensors to pick up the   tiny changes induced by the SNS. As a person sweats, even subtly, the   electrical resistance of the skin lowers, causing a corresponding increase in   the conductance across the skin. By placing two electrodes on the skin, we   can measure this skin conductance or electrodermal activity.  It is a measure of the body’s physiological   arousal.  Studies   have shown that increased EDA correlates strongly to feelings of stress,   anxiety, fear, high cognitive load (i.e., thinking hard), and physical   activity, but exactly how EDA is driven by brain activity remains unknown. The   participants in this study are patients diagnosed with intractable epilepsy –   that is, medicine or other interventions have been unsuccessful at controlling   their epilepsy. In order to manage their seizures, these patients have had   thin electrodes inserted deep into their brains. These deep brain probes are   able to both passively record the patient’s neural activity and, in the event   of a seizure, deliver tiny bursts of electrical stimulation with the hopes of   off-setting the seizure’s intensity and duration. This stimulation can also   be delivered in a controlled setting under the supervision of a doctor. For   this study, we intend to concurrently measure each patient’s neural activity   while measuring their EDA at several sites on their body.. We will measure the   brain signals passively, while they perform various activities, and we will also   actively stimulate their brain under the supervision of a doctor in order   precisely map the neural stimulation to skin conductance. In doing so, we   hope to better understand how brain activity drives EDA. Then, since EDA is easy to measure non-invasively using wrist-worn and other wearable sensors, this study will help us use EDA as a more effective tool to understand underlying brain activity and both cognitive and affective processes in the wild. The naturalistic and longitudinal measurement of cognitive and affective processes could help inform prediction and treatment modalities for individuals with epilepsy, PTSD, anxiety, depression, and autism, with future implications for digital health and personalized medicine.",2021-08-30,['affective-computing'],https://dam-prod2.media.mit.edu/x/2020/06/18/dbs10.png,379711,,2022-05-12T17:51:20.830Z,['ktj@media.mit.edu'],True,deepbrainbody,2019-02-01,[],DeepBrainBody,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-21T14:07:10.232Z,"In concert with the Mediated Matter group's HLM additive manufacturing research, the emergent field of Hybrid Living Fibers (HLF) or ""biohybrid"" fiber systems has derived novel techniques to encapsulate or adsorb living and bio-active agents within fiber-based constructs. The aim of these composites is to: (I) leverage fiber structures to benefit the survivability of biological agents, and (II) augment fibers with the functional properties of living systems (e.g. protein expression, material production, detection or digestion of pathogenic particles).  Further, HLF constructs are capable of being organized into sophisticated and hierarchical higher-order structures (e.g., helical twists, braids, knits, weaves). ",2021-05-21,['mediated-matter'],https://dam-prod2.media.mit.edu/x/2020/06/22/edited-IMG_8600 copy_Page_1.png,379738,,2021-05-21T14:43:39.965Z,"['neri@media.mit.edu', 'susanwil@media.mit.edu', 'rssmith@media.mit.edu']",True,hybrid-living-fibers,2020-06-21,[],Hybrid Living Fibers,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2020-06-22T19:05:51.624Z,"The COVID-19 pandemic, caused by the novel coronavirus SARS-CoV-2, has elicited a global health crisis of catastrophic proportions. With only a few vaccines approved for early or limited use, there is a critical need for effective antiviral strategies. In this study, we report a unique antiviral platform, through computational design of ACE2-derived peptides which both target the viral spike protein receptor binding domain (RBD) and recruit E3 ubiquitin ligases for subsequent intracellular degradation of SARS-CoV-2 in the proteasome. Our engineered peptide fusions demonstrate robust RBD degradation capabilities in human cells and are capable of inhibiting infection-competent viral production, thus prompting their further experimental characterization and therapeutic development.",,"['molecular-machines', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/uuid/386ce5ca-a317-4004-a388-7fddc08b8c7d,379760,,2023-09-07T23:35:44.106Z,"['pranam@media.mit.edu', 'manvitha@media.mit.edu', 'jacobson@media.mit.edu']",True,targeted-degradation-of-sars-cov-2-via-computationally-optimized-peptides,2020-04-01,"['biology', 'public-health', 'engineering', 'covid19']",Targeted intracellular degradation of SARS-CoV-2 via computationally optimized peptide fusions,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-06-24T15:27:34.725Z,"The Antiracism & Technology Design Research Seminar Series hosts speakers from around the world who center issues of race, intersectionality, design, technology, and sociotechnical systems in their work. We consider the practice of technology design across scale and process in concert with theories of antiracism and intersectionality. The seminar builds a community of scholars engaged in these topics and creates epistemic space for cross-disciplinary conversations on the intersections of technology, race, and social equity. The seminar is part of the Space Enabled Research Group's Antiracism & Technology Design Research Project, which aims to support the creation and sustenance of equity across intersectional racial groups with the development, design, and distribution of technology across scale. To apply to attend this research seminar series, please e-mail Dr. Katlyn Turner at katlyn@media.mit.edu.To nominate speakers to the series please fill out this form.",,['space-enabled'],,379807,,2020-09-30T18:00:17.777Z,"['drwood@media.mit.edu', 'katlyn@media.mit.edu']",True,antiracism-and-technology-design-research-seminar-series,,[],Antiracism & Technology Design Research Seminar Series,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-07-03T21:11:33.788Z,"Introducing the Labyrsense, the rehabilitative labyrinth with sensory feedback. Underpinned by early research explorations with MGH, the Labyrsense is a reinvention of a traditional medical intervention allowing for increased efficacy. ",,['opera-of-the-future'],https://dam-prod2.media.mit.edu/uuid/0d009174-9867-4785-a151-a981ab4cde05,379863,,2020-07-07T14:01:18.744Z,['arieger@media.mit.edu'],True,labyrsense,2019-08-01,[],Labyrsense,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2020-07-07T04:26:59.609Z,We present a stochastic scheme for splitting the client data into privatized shares that are transmitted to the server in such settings. The server performs the requested operations on these shares instead of on the raw client data at the server. The obtained intermediate results are sent back to the client where they are assembled by the client to obtain the final result. Link: https://github.com/vepakom/PrivacyDocuments/blob/master/Splintering.pdf,,['camera-culture'],https://dam-prod2.media.mit.edu/x/2020/07/07/Splinter_Example.PNG,379884,,2020-10-23T15:22:29.540Z,"['raskar@media.mit.edu', 'vepakom@media.mit.edu']",True,Splintering,2020-07-06,"['artificial-intelligence', 'privacy', 'security', 'machine-learning']",Splintering with distributions: A stochastic decoy scheme for private computation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2020-07-09T14:33:03.129Z,"8K-oi-Gan is a virtual koi pond in which koi patterns are generated by a StyleGan trained on thousands of images of koi and whose input noise values are collected ocean data. The size, patterns, health, and activities are all driven by collected ocean data such a temperature, salinity, pressure, micro plastic concentrations, or any other input we wish to explore. Gan generated koi are then stored on an encrypted, distributed ledger system allowing collectors to buy, sell, breed, and trade koi for their own personal virtual koi ponds.",2021-05-31,"['object-based-media', 'open-ocean']",https://dam-prod2.media.mit.edu/x/2020/07/09/style_image.png,379911,,2021-08-04T18:47:00.684Z,['novysan@media.mit.edu'],True,8koigan,2019-09-01,[],8KoiGan,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-07-13T02:38:25.909Z,"Orbiting is a cooperation between artist Thom Kubli and Prof. Hiroshi Ishii, featuring the Tangible Media group. It is a novel approach to pool scientific and artistic processes and to challenge physical realities while entangling with the virtual world.",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/07/12/_M7A0224.JPG,379951,,2021-06-07T14:33:12.672Z,"['ishii@media.mit.edu', 'josephk@media.mit.edu', 'jvanzak@media.mit.edu', 'yun_choi@media.mit.edu', 'paula@media.mit.edu', 'dqashat@media.mit.edu', 'hilamor@media.mit.edu', 'jday@media.mit.edu', 'vsumini@media.mit.edu', 'jwilbert@media.mit.edu']",True,orbiting,2019-01-01,[],ORBITING,PUBLIC,http://www.mallcong.com,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2020-07-17T14:27:11.583Z,"'Shells, often found on seashores, exemplifies the absence of existed lives. What if a presence dwells in to animate them?'Project HERMITS approaches the theme of ""Presence of Absence"" with such a setup where robots become the source of energy to give presences to the inert mechanisms.  While we perceive and express presence through motions, we envision a world where a robotic system giving presence to static mechanisms and objects.",,[],https://dam-prod2.media.mit.edu/uuid/1de59470-222f-4650-a7da-8e315aabc32d,380002,,2020-09-08T20:06:13.294Z,"['ken_n@media.mit.edu', 'joaleong@media.mit.edu', 'ishii@media.mit.edu', 'jwilbert@media.mit.edu']",True,hermits-ars-electronica20,2020-09-12,"['robotics', 'design']",HERMITS: Mechanical Shells for Robotic TUIs (Ars Electronica 2020),PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2020-07-21T14:21:13.314Z,"The Space Enabled research group at the MIT Media Lab is seeking participants for a paid study on the impacts of social distancing policies to the lives of individuals in Greater Boston. This study is called Invisible Variables.  Click here to apply. We want to learn about the experiences of people during the COVID-19 pandemic: how they live, work, play, and how their lives have been impacted by social distancing. If your primary source of employment and/or income is in any of the following sectors and you live in Greater Boston, you may  be eligible to participate in the study:if you are an undergraduate or graduate studentif you are informally employed (for example, a day laborer, domestic worker, or gig worker)if you are a contract worker (for example, a plumber, electrician, or contractor)if you are an essential worker (for example, a grocery store worker or an MBTA employee)if your work and pay relies heavily on tips (for example, waitstaff or a taxi or rideshare driver)if you live on a fixed income (for example, unemployment, retirement, or disability)Click here to apply. We want to learn about how people in Greater Boston are experiencing changes to their personal safety, their financial means, and their personal autonomy during the COVID-19 pandemic. How have social distancing policies affected people's safety at home, or work? How have these policies impacted the amount of autonomy people have in managing their daily lives? How are people in Greater Boston and their families coping with government-mandated social distancing policies?The study will take place over the course of 2020 and it may continue into 2022. Participants will be compensated $25 for 45-90 minute interviews. Interviews will be conducted over the phone or over a video conferencing platform, such as zoom. Interviews will ask participants how aspects of their lives such as their housing, employment, finances, healthcare, wellness, and family dynamics have been impacted by social distancing policies. Participant infoParticipation in this study is voluntary. The team will obtain informed consent before collecting data through interviews, internal documents or observation. The informed consent process will communicate the rights of the research subjects and the approaches used by the research team to protect their data and identities. These approaches include the following: no research subject will be quoted without their permission; no internal document will be shared without permission; confidential data will be securely stored using encryption and password protection; data will not be stored longer than is necessary for research purposes; data from individual subjects will be de-identified for storage and analysis. As always, participation by the research subjects is voluntary and may be terminated at any time.Click here to apply.",,['space-enabled'],https://dam-prod2.media.mit.edu/x/2020/07/23/osman-rana-xJueGJJHnWs-unsplash.jpg,380039,,2022-03-16T16:31:24.699Z,['katlyn@media.mit.edu'],True,invisible-variables,2020-07-21,['covid19'],Invisible Variables: Personal Security Among Vulnerable Populations During the COVID-19 Pandemic,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-07-30T02:04:23.830Z,"Emergence, when an entity is observed to have properties its parts do not have on their own, is an awe inducing phenomenon seen within nature, complex organization theory, physics, art, and philosophy. Humans can experience emergence or interdependence, a perhaps less potent version of emergence, when they come together, transform, connect, and grow in a way they could not alone through co-creation, playing games and sports, and telling stories. As such moments are scarce, cultures and groups ancient and new have developed technologies (in the sociological sense meaning techniques, processes, and material objects to produce goods, provide services, and connect people) to help groups reach those moments more easily, technologies I define as ancient social technologies (ASTs).Some ASTs, such as narrative coaching, circle practice, and dialogue across differences, have been developed for decades, even generations, to help groups reach emergence together in person. Now, as computational social technologies like video conferencing and social media have been ground breaking by connecting across distance, many facilitators have shown great creativity and resourcefulness as they use these platforms to implement ASTs online. Yet, computational social technologies do not scaffold or ease the implementation of key AST components that are deemed essential to the practice, making virtual AST use challenging.   I present in this thesis Keeper, a tool designed to augment virtual communication by scaffolding the use of ancient social technologies within modern computational social technologies. With a design informed by a deep investigation into four ancient social technologies, Keeper visualizes and mediates online, synchronous, audio and video conversations. Keeper challenges a traditional two dimensional interface through use of “space” and tone. The tool scaffolds ASTs with features like a talking stick and guidelines, but retains key affordances of the digital medium by incorporating private messaging and conversation data visualization. Keeper fosters socially beneficial group dynamics by making visible conversation measures to promote equitability, and it prompts reflection and learning by offering visual maps of a conversation over time. The reception of this tool through experiments and interviews is discussed, and reflections on future work offered.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2020/07/29/Screen Shot 2020-05-18 at 8.58.14 AM.png,380165,,2021-01-14T13:51:40.131Z,[],False,keeper-project,2019-09-01,[],Keeper: Online conversation support scaffolding modeled after ancient and modern social technologies,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2020-08-06T15:20:00.995Z,"Freedom Radio (in development) is an ongoing participatory artwork collecting multilingual thoughts and expressions of freedom across national borders and digital divides. We invite the public to participate by calling a toll-free hotline, answering three questions about freedom, and leaving a question for the next caller. Their recordings are added to a software-generated live-streamed sound and video work featuring callers reimaging and expanding the language of freedom. You can participate by calling 844-335-3588 (~5 mins).",,['poetic-justice'],https://dam-prod2.media.mit.edu/x/2022/03/10/FR_IGpost_03.png,380313,,2022-10-22T00:22:33.851Z,"['nlutz@media.mit.edu', 'jblinder@media.mit.edu', 'aneeshn@media.mit.edu']",True,freedom-radio,,"['art', 'data', 'creativity', 'community', 'natural-language-processing', 'social-change', 'racial-justice', 'diversity']",Freedom Radio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-08-06T15:39:06.481Z,"Real Talk Radio (in development) is a series of software-generated live-streamed sound and video works composed of an evolving and neverending mix of spoken-word intros, outros, interludes, and skits from hip-hop songs alongside shout-outs from Black creatives. As a vocal portrait and sonic meditation, it explores multi-genre and multigenerational Black thought and expression in the US. The first edition will feature the top 100 musicians with the most popular spoken-word song segments. All editions will be free and accessible to the public 24/7, both over the phone and online.",,['poetic-justice'],https://dam-prod2.media.mit.edu/x/2022/03/10/RTR Share graphic one line.png,380321,,2022-10-22T00:29:16.031Z,['ekene@media.mit.edu'],True,real-talk-radio,,[],Real Talk Radio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-08-17T15:20:50.908Z,"We study the effect of Opportunity Zones (OZs) on employment outcomes. We match zip codes with OZs to a control group of similar zip codes that do not have any OZs, and compare job postings and posted salaries across these two groups over time. We find that zip codes with OZs have fewer job postings but higher posted salaries, though both effects are very small in magnitude. We explore further the extent to which the results are driven by particular OZs. About 8700 census tracts were designated as OZs in 2018, yet it appears that only a few hundred have received investments. Those OZs for which we are able to gather data on investments are among the most economically attractive in that they already had more job postings and job growth prior to their OZ designation. When we redo our analysis on the smaller set of OZs with investment we again find fewer job postings and higher posted salaries, but the magnitudes are slightly higher than on the full matched sample.",,['city-science'],,380394,,2020-08-17T19:54:41.755Z,['crisjf@media.mit.edu'],True,job-postings-opportunity-zones,2019-10-01,[],What is the Impact of Opportunity Zones on Employment Outcomes?,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2020-08-20T14:27:36.286Z,"CabildoxLatAm: A space for digital participation in Latin AmericaIn the face of the challenges that have emerged and been exacerbated by the Covid-19 pandemic, collective responses by, with, and for communities have developed in Latin America (e.g., community food banks and open data citizen-led initiatives). Faced with this scenario, the research group Democratic Reconfigurations of the IASS Potsdam has decided to convene a Cabildo por América Latina (CabildoxLatAm), a participatory digital space to learn, strengthen, and design collective responses to the pandemic. The event will bring together civil society actors, governments, communities, and citizens from Argentina, Chile, Colombia, Guatemala, Mexico, Peru, and Puerto Rico.Strengthening forms of participatory governanceThe CabildoxLatAm will take place between September 8th and 11th through the Zoom platform, also having other means of communication for those who do not have access to computers, making it possible to participate through telephone lines and via WhatsApp. During the Cabildo there will be spaces and moments of open deliberation focused on new forms of participatory and planetary governance that, through collective action, respond to the regional challenges of food sovereignty, education, and digital governance.The mission of the Cabildo is to create a space for horizontal, inclusive, and digital participation where a roadmap for self-management and a unified petition of processes and participatory responses necessary to confront the Covid-19 pandemic will be jointly designed.A collaborative effortThis platform for dialogue and exchange was designed in conjunction with numerous partners on both sides of the ocean: Center for Civic Media at the MIT Media Lab, Planetary Health Alliance, Women Leaders for Planetary Health, TEC Monterrey, Pontificia Universidad Católica del Perú, Institut de Ciència i Tecnologia Ambientals de la Universitat Autònoma de Barcelona, LATINNO Project - WZB Berlin, Asuntos del Sur, Fundación Avina, el Centro por la Justicia y el Derecho Internacional (CEJIL), Ciudadania Inteligente, Hackeo Cultural, Instituto Cultural, Diversa, and Global Change.For more information, updates and questions:Website: www.CabildoxLatAm.orgFacebook | Instagram | Twitter",2020-09-11,['civic-media'],https://dam-prod2.media.mit.edu/x/2020/08/20/CabildoxLatAm Intro.jpeg,380448,,2020-08-24T10:22:54.319Z,['pcuellar@media.mit.edu'],True,cabildoxlatam,2020-08-20,"['civic-media', 'politics', 'civic-action', 'covid19']",CabildoxLatAm,PUBLIC,https://reynoldscuellar.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-08-20T15:39:42.205Z,"Dimensions of human movement that are absent from the eye, such as pressures, muscular tensions, and external features,  are represented by a programmable tensegrity, that records and replays a dancing body. We hypothesize that the nervous system, given abstract information in the form of a tangible dancing tensegrity, will try to compensate for the unavailable (absent) information in an endeavor to understand the whole of the human body's movement.Fluifiber approaches ""Presence of Absence""  through a vision of utilizing enabling technologies to transfer and preserve the intangible tacit knowledge; hereby of a skillful dancer's across generations and distances.",2022-04-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/08/28/tensegrity_2.gif,380458,,2021-05-02T19:13:46.321Z,"['ishii@media.mit.edu', 'ozgun@media.mit.edu']",True,fluifiber,2020-04-01,"['robotics', 'human-machine-interaction']",Fluifiber: Interweaving Dancing Bodies over Time and Space via Programmable Tensegrity,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-08-21T04:04:39.235Z,"Under the sky of this blue planet, we borrow the air from the Earth and release them again, repurpose and rearrange the element of it. We are breathing the same atoms that Leonardo da Vinci inhaled and exhaled [Stager, Wired, 2014].  When feeling being apart and being isolated, taking a breath might be a way to connects us with others, the nature and also our own self.aSpire represents one's presence by a tangible form of one's breathing pattern.  We present  the concept of 'Presence of Absence' in a form of a kinetic and interactive installation. The aSpire which is  a clippable pneumatic-tactile feedback device that helps users to regulate their breathing, is hang on a chair. It has three inflatable silicone air pouches in array that mimics the motion of human's breathing and delivers the tactile feeling from it.  When an audience takes a seat on the chair, the aSpire delivers different breathing patterns by varying its air pouch shapes to represent the other audience's existence who were sitting on the same chair.",2020-09-30,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/08/21/pofa_1.jpg,380474,,2023-06-22T16:21:26.032Z,"['yun_choi@media.mit.edu', 'ishii@media.mit.edu']",True,aspire-ars,2020-08-01,"['art', 'physiology']",aSpire: Breathing in Sync Toward Interpersonal Connectedness,PUBLIC,http://www.mallcong.com,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-08-21T07:02:32.807Z,"HelioZZZ is an opacity changing eye mask that wakes you up with natural sunlight when you want it. The mask warms your eyes to help you fall/stay asleep and is controllable via a web-app interface.Before going to bed, the user sets the alarm on their phone, and the mask becomes opaque and slightly warm to help the user fall asleep. Fifteen minutes before wake up, the mask begins the transition from a clear to an opaque state, allowing sunlight to penetrate through the mask gently waking up the user. the user can further snooze this alarm by pressing the button on the side, which causes the hydrogel to reverse back to the opaque state for 5 minutes.",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/08/21/sleepy.png,380490,,2020-10-23T16:14:23.879Z,"['ishii@media.mit.edu', 'jackform@media.mit.edu']",True,heliozzz,2019-09-18,[],HelioZZZ: a smart (material) sleep mask,PUBLIC,http://jackaforman.com,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-08-21T14:47:11.570Z,"Renee Magritte's “Treachery of Images” highlighted the fallacy in which images and words are prescribed meaning. It is obvious that a painting of a pipe is not a pipe. The absence of tangibility is a divisive barrier between what is and what is reference of. DefeXtiles explores the gray area when the “reference of” becomes tangible. If something looks, moves, and feels like fabric, does that make it fabric? What is the barrier of fidelity for acceptance, and is that barrier universal or personal? ",,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/e1ead1a1-e4c4-48bf-9216-f77dc590f3c3,380498,,2020-10-27T00:49:38.368Z,['jackform@media.mit.edu'],True,defextiles-ars-20,2019-10-13,[],DefeXtiles: 3D Printing Quasi-Textiles (ARS 2020),PUBLIC,https://jackaforman.com/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2020-08-27T15:43:02.958Z,"MIT students on a mission to tackle the education crisis created by Covid-19The US school system is struggling to support all of its students during the Covid-19 pandemic, and many are falling behind. The challenges affect low-income communities most strongly. This is not a problem MIT can solve alone, but we can make a difference. During the Fall 2020 semester we will be recruiting up to 100 MIT students as Tech Tutors, who will work with small groups (""learning pods"") of students who need extra support during Covid-19. Tech Tutors will focus on offering creative STEAM activities and general support for digital learning (help with setting up Zoom calls, etc.). The program is open to low-income families anywhere in the US as well as members of the MIT community with school-age kids enrolled in grades 3-8.  Becoming a Tech Tutor offers MIT students an experiential learning experience and opportunity to invest in positive social change in their communities. Tech Tutors is supported by the Office of Experiential Learning and the Office of the Provost and coordinated by the Media Lab Digital Learning + Collaboration Studio together with the Media Arts & Sciences Program. Update: During the fall we worked with 5 MIT students (""tech tutors"") who supported 15 children from 14 families in the MIT community. They engaged in a wide range of activities, including helping with basic school work, creating games using Scratch and python, and working through the engineering parts of a science fair project (which ended up winning an award)! We received a lot of positive feedback from parents, who appreciated their children getting some extra attention and help (while they got to take a break). We also learned some things we can improve. We also got positive feedback from the tutors, who appreciated the opportunity to put their own hectic lives on pause to help young students. All the tutors enjoyed the experience and said they would recommend the program to their friends.",2021-01-31,"['program-in-media-arts-and-sciences', 'digital-learning-studio']",https://dam-prod2.media.mit.edu/x/2020/08/27/UN-covid-19-crop.jpg,380586,,2021-02-09T00:41:29.189Z,['ps1@media.mit.edu'],True,tech-tutors,2020-09-01,['covid19'],Tech Tutors,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-08-28T03:16:26.334Z,"In this project the ephemeral and intangible aspects of human communication are represented by a soap-bubble. The shapeless, intangible, and insubstantial speech—once the speech is shouted out through the speaker's mouth, it disappears unless someone hears it immediately, or even if it is heard, the message will be forgotten as time goes—is transferred to a semi-tangible yet still fleeting bubble. ",,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/5a91ca90-c91c-460b-b597-2c56053f494c,380617,,2020-09-08T05:21:48.610Z,['yun_choi@media.mit.edu'],True,bubble-talk,2018-12-03,"['design', 'art', 'fabrication']",Bubble Talk: Translating the presence of modern digital chat into fleeting bubbles,PUBLIC,http://www.mallcong.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-09-16T15:59:36.015Z,"Walking as the most natural form of urban mobility plays a crucial role in shaping the quality of daily life. However, safety concerns often hinder people's willingness to walk and traverse public spaces. This discrepancy between perceived and actual security creates mistrust and discomfort in urban environments. To address this challenge, our study aims to delve into the complex dynamics of urban security.To tackle this issue, our team conducted fieldwork in Guadalajara, Mexico, collaborating with local communities to develop and validate urban safety metrics. By reviewing 150 peer-reviewed articles encompassing crime prevention, environmental design, and social behavior, we identified relevant metrics specifically tailored to the Guadalajara community. Through two weeks of on-site observations and assessments via focus groups and surveys, we evaluated these metrics.To prototype the interconnectedness of risk metrics and public spaces, we developed an agent-based model (ABM). This ABM enables real-time feedback on targeted urban interventions concerning mobility and safe interactions. Our study introduces novel analysis metrics that were previously unavailable, empowering decision-makers to take effective measures in enhancing urban safety.Our research demonstrates the feasibility of generating reliable results that contribute to urban safety improvement. Overall, our study emphasizes the significance of understanding both the perception and reality of security to make informed decisions regarding urban planning and public spaces. By bridging the gap between perception and reality, we pave the way for evidence-based urban planning.  By comprehending the metrics influencing safety in urban areas, we can work towards creating safer communities.An agent-based model to evaluate the perception of safety in informal settlements.",,['city-science'],https://dam-prod2.media.mit.edu/x/2023/05/18/Porfolio_April2023.gif,380807,,2023-06-02T16:28:08.447Z,"['leticiai@media.mit.edu', 'alonsolp@media.mit.edu', 'crisjf@media.mit.edu', 'agrignar@media.mit.edu', 'kll@media.mit.edu']",True,safety-perception-in-public-spaces,2021-01-05,[],Safe Cities: Urban Mobility and Security Perception via Agent-Based Modeling,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-09-23T02:34:50.697Z,"New technology for next-generation wireless + batteryless micro-implantsThese wireless batteryless micro-chips can reprogram themselves inside the body, and open up new capabilities in drug delivery, long-term monitoring, and early disease detection.We present the design, implementation, and evaluation of µmedIC, a fully-integrated wireless and batteryless micro-implanted sensor. The sensor powers up by harvesting energy from wireless signals and communicates at net-zero power. They could be used for continuous monitoring of biomarkers and tumors, ultra-long lasting drug delivery systems (e.g., for patients with Alzheimer’s or Osteoporosis), and closed-loop control systems with real-time feedback (e.g., artificial pancreas for Diabetes’ patients).We built our design on millimeter-sized integrated chips and flexible antenna substrates. Our micro-chips are:Ultra-low-power: They consume as little as 350 nanoWatts.  High-data-rate:  They can transfer data up to 6Megabits per second from inside the body. This allows us to support high data rate applications like streaming low-resolution images.Adaptive to Tissues: They can adapt to different surrounding tissues by shifting their resonance (by more than 200MHz)µmedIC has been tested in-vitro (fluids) and ex-vivo (in different multi-layer tissues with meat, fat, and bone). It builds on our earlier generation implants that have been tested in living pigs.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2020/09/22/umedic-gif.gif,380877,,2021-02-08T15:43:05.241Z,"['fadel@media.mit.edu', 'mrhamid@media.mit.edu']",True,umedic,2020-09-20,"['health', 'internet-things', 'medicine']",Self-Reconfigurable Micro-Implants,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-09-28T17:04:28.770Z,"In “normal” times, the process of voting can be confusing enough for many Americans -- from whether and where they’re registered, to absentee v. early v. mail-in voting, to what actually happens at the polls. The 2020 election has seen this confusion soar, with pandemic scrambling the process at the national, state and local levels. To lessen this confusion (and the associated misinformation that ensues) LSM has launched a voter education project aimed at addressing the following question: Can we impact exposure to reliable information through influencer interventions?In the manner of LSM’s BeatTheVirus project, LSM will apply network analytics to 1] identify communities on Twitter who are underexposed to voting information and 2] inform the development and distribution of voter education messaging to these audiences through social media influencers.Core elements of the system include:Map: Use Twitter to find and map “interest communities” based on patterns of followership.  Track: Track the exposure to voting-related content -- specifically hashtags, links and phrases -- within each of these Twitter communities. Measure: For every instance that an influencer tweets a voter education message, measure increased exposure across every community they are associated with in the system. Target: Identify for each community, through these same followership analytics, social media influencers specific to that community.Message: Voter ed organizations can use these analytics to invite influencers to tweet voter education messaging to their audiences that are then measured for impact in our system.",2020-11-30,['social-machines'],https://dam-prod2.media.mit.edu/x/2020/09/28/Voting-Booths-000005814659_1600x737.jpg,380923,,2020-10-13T19:33:28.007Z,"['sballing@media.mit.edu', 'snehapm@media.mit.edu', 'dougb5@media.mit.edu', 'wesc@media.mit.edu', 'bcroy@media.mit.edu', 'dkroy@media.mit.edu', 'russell5@media.mit.edu']",True,voter-education-through,2020-09-28,"['social-networks', 'civic-technology', 'networks', 'natural-language-processing']",Voter Education through Network Analytics,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-09-30T22:13:01.933Z,"The Space Sustainability RatingThe Space Sustainability Rating design team—comprised of the World Economic Forum, the European Space Agency, the Space Enabled research group at MIT, the University of Texas at Austin, and BryceTech—announces that the EPFL (École polytechnique fédérale de Lausanne) Space Center, which was selected as the operator for the SSR, is creating a nonprofit called the ""Space Sustainability Rating"" to operate the program. In June 2022, the EPFL Space Center formally launched the operations of the Space Sustainability Rating and issued the first rating to a paying client. Read more in the press release here.Space Enabled is also announcing that Prof. Moriba Jah has been named an MIT Martin Luther King Visiting Professor for 2023. Prof. Danielle Wood and Prof. Jah spoke about research collaboration in 2021 at a virtual event hosted by the MIT Media Lab ""Perspectives Series.""Project OverviewThe global space community is witnessing a rapid increase in creative business models and new technologies that are leading  to  plans  to  launch  thousands  of  satellites  into  Low  Earth  Orbit.  The  number  of  satellites  being  proposed  is  much greater than the historical patterns that have been seen globally to date. While the technology has the potential to bring useful societal services, in areas such as satellite communication and earth observation, there is a growing risk that the capacity of Earth orbit to accommodate such a large set of new space objects safely may be in jeopardy.The Space Sustainability Rating is an initiative that seeks to foster voluntary action by satellite operators to reduce the risk of space debris, on-orbit collisions, and unsustainable space operations. The concept for the Space Sustainability Rating was conceived by the Global Future Council on Space Technologies of the World Economic Forum through a series of workshops starting. The World Economic Forum held a competitive call for proposals in 2018 and selected a  team  composed  of  four  organizations  to design  the  Space  Sustainability  Rating; these  organizations  include  the  European Space Agency, Massachusetts Institute of Technology, University of Texas at Austin, and Bryce, Space and Technology. These four organizations formed a consortium with the World Economic Forum to define the technical and  programmatic  aspects  of  the  Space  Sustainability  Rating  during  the  period  from  2019  through  2021.  The  Consortium  brings  expertise  in  the  areas  of  modeling  and  evaluating  the  impact  of  space  debris  in  Earth  orbit, astrodynamics, characterization of space objects, technology policy, space economics and understanding of the role of emerging countries and private actors in the space sector.Read more about the project at these links:Press Release from the World Economic ForumAnnouncement from MIT Media LabPress Release from European Space AgencyAnnouncement: Prof. Jah joining Space Enabled as an Affiliated ResearcherAnnouncement: Prof. Jah and Prof. Wood present virtual talk on ""Space Environmentalism"" on June 24.",,"['space-enabled', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2020/10/23/NASA image for SSR page.jpg,380957,,2023-10-04T07:34:02.328Z,"['drwood@media.mit.edu', 'minoo@media.mit.edu']",True,ssr-space-sustainability-rating,,"['environment', 'health', 'politics', 'sensors', 'space', 'technology', 'wellbeing', 'zero-gravity', 'racial-justice', 'women']",Space Sustainability Rating,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-10-01T15:10:54.867Z,"Ever wish you could toss your folded up tent and have it self-assemble mid-air?  Well...you can. In a zero gravity environment.Self Assembling Space Frames is a system of struts, connection nodes, and elastic to enable quadrilateral mesh geometries to self-assemble from a collapsed bundle.The  software accepts meshes or lines and outputs 3d printable nodes that are then strung together with tubes and elastic.  This system can be used to design self assembling objects and spaces. The frame can accept fabric to create an enclosed private space for astronauts to temporarily deploy and occupy like a space tent.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/07/28/20210523_Steve-Boxall_PSM_MIT_Research_ZG548_7296.jpg,380979,,2023-09-07T14:55:29.959Z,['cwwang@media.mit.edu'],True,self-assembling-space-frames,2020-09-23,"['robotics', 'design', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'engineering', 'wellbeing', 'zero-gravity', 'chemistry']",Self Assembling Space Frames,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2020-10-02T01:52:29.562Z,"Here is a sampling of MS theses.It is a good idea to have models on which to base your thesis. All theses are measured both by thoroughness, references, and novelty. Also extensibility. It should teach and be a basis on which others build.In general, there are several types: engineering, science, design, art, etc. (After Bill Mitchell) An engineering thesis describes a solution to a problem and is evaluated by whether it works and improves the art. A science thesis proves a theorem. Design is evaluated by critique and iteration by experts. For example, redesigning the signage at an airport ought to show how it works better and is elegant as determined by peers in the field. Note that when choosing a model it is tempting to pick a thesis that is not the best. If you want a model, chose the best. I have supervised over 50 theses, many stand out. Sometimes the thesis is the full picture of a student's graduate career, other times it is just one component.  Discuss your model with me.The most important thing about a thesis is that it be done. The statistical mode of thesis readership is 2—you and me (A paper gets a far larger audience.) We have broad latitude in what goes into it. (A PhD thesis, on the other hand, is perfect; time is the free variable.) You have to define a hypothesis that can be shown/proven in the allotted time. ""In the fullness of time"" is not an evaluation. E.g., I don't endorse a thesis that is based on the inevitability of VR.Remember, a thesis is not a memoir. Your personal journey can be an organizing thread but not a theme. Posterity doesn't care about you—it cares about what you contributed.Finally, all of these students left a positive imprint on the lab.  Every student in the group or lab did better as a result of their contributions.",,['viral-communications'],,381009,,2020-10-05T19:44:57.101Z,['lip@media.mit.edu'],True,prior-theses,,[],Prior MS Theses,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-10-06T14:08:54.713Z,"Judging by the work of its alumni and faculty, MIT is an amazing place for learning. When I first arrived on campus, I wanted to better understand how the learning takes place, to look under the hood, and find out what the ""secret sauce"" was (and where it was made). —Philipp SchmidtOur small team started asking different groups of people on campus, and what we heard was fascinating. The stories of learning at MIT are full of surprises, diversity, and humility - and delightfully unexpected. Hint: Not many of them take place in lecture halls! In order to share them with a broader audience, we worked with a talented animator to create a short video (using the original voices from our interviews). ",2015-05-01,['digital-learning-studio'],https://dam-prod2.media.mit.edu/x/2020/10/08/Screen Shot 2020-10-08 at 14.44.17.jpeg,381041,,2023-05-25T21:57:15.069Z,"['ps1@media.mit.edu', 'kamcco@media.mit.edu']",True,how-mit-learns,2015-02-01,['learning-teaching'],How MIT Learns,PUBLIC,,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0
False,2020-10-08T16:20:03.330Z,"Building an open, shared, and collaborative future of lunar exploration.Explore the Lunar Open Architecture (LOA) at loa.mit.edu. We're building LOA because the future of lunar exploration is getting crowded: In contrast to the Apollo era, planned lunar activity now includes more actors than ever before, coming from around the world, in the public and private sector, interested in making a return to the surface of the Moon.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2020/10/08/LOA_moon_background_0tBrsSa.png,381083,,2023-09-07T15:35:56.873Z,"['msarang@media.mit.edu', 'aekblaw@media.mit.edu']",True,loa,2019-10-12,"['architecture', 'environment', 'health', 'politics', 'sensors', 'space', 'technology', 'wellbeing', 'zero-gravity', 'racial-justice', 'diversity', 'women']",Lunar Open Architecture,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-10-11T23:14:09.392Z,"Parents often select schools by relying on subjective assessments shared by other parents—which are increasingly becoming available on school ratings websites in the form of written reviews. We apply recent advances in natural language processing to analyze nearly half a million reviews posted by parents for over 50,000 publicly-funded US K-12 schools on a popular ratings website. We find: i) schools in urban areas and those serving affluent families are more likely to receive reviews; ii) review language correlates with standardized test scores—which are known to closely track race and family income—but not school effectiveness, measured by how much students improve in their test scores at the school over time; and iii) the linguistics of reviews reveal several racial and income-based disparities in K-12 education. These findings suggest that parents who reference school reviews may be accessing, and making decisions based on, biased perspectives that reinforce achievement gaps.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2020/10/11/school_reviews_cLgDR3C.png,381140,,2021-12-17T14:43:39.725Z,"['ngillani@media.mit.edu', 'echu@media.mit.edu', 'dougb5@media.mit.edu', 'dkroy@media.mit.edu']",True,school-reviews,2020-01-01,"['kids', 'learning-teaching', 'machine-learning', 'natural-language-processing']",School Reviews,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-10-15T12:18:30.469Z,OmniFiber is a reconfigurable fiber technology for movement-based interactions based on thin fluidic fiber actuators with closed-loop strain control. We explore how thinness and sensory capability of artificial muscle-based devices can be utilized by designers to seamlessly weave them into everyday tangible interactions; such as haptics-mediated skills training or actuated tangibles for remote interpersonal communication.,2027-04-20,"['tangible-media', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2021/10/16/OmniFabric_ZWFQrUh.jpg,381191,,2023-10-22T23:18:17.997Z,"['ishii@media.mit.edu', 'ozgun@media.mit.edu', 'ken_n@media.mit.edu', 'hilamor@media.mit.edu', 'jackform@media.mit.edu', 'alims@media.mit.edu', 'aishni@media.mit.edu']",True,omnifiber-millifluidic-muscle-fibers,2021-01-01,"['robotics', 'design', 'human-computer-interaction', 'art', 'artificial-intelligence', 'health', 'music', 'wearable-computing', 'interfaces', 'ethics', 'materials', 'fabrication', 'data-science']",OmniFiber: Strain-tunable Fluidic Fiber Actuators for Soft Robotic Textiles and Autonomous Garments,PUBLIC,http://www.ozgunkilic.com/,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-10-15T20:11:26.389Z,Dyadic Speech-based Affect Recognition using DAMI-P2C Parent-child Multimodal Interaction Dataset,,['personal-robots'],,381208,,2020-10-16T15:40:29.494Z,['hchen25@media.mit.edu'],True,dyadic-speech-based-affect-recognition,,[],Dyadic Speech-based Affect Recognition,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2020-10-16T01:16:53.829Z,"Delivery workers, ridehail drivers, and other gig workers are increasingly controlled through algorithmic decision-making, fueled by data about their behavior while on and off the clock. Gigbox is a collection of tools and projects aimed at helping workers collect and pool data about their experience in a privacy-preserving way to help them build power, help researchers understand the gig economy more fully, and help advocates more effectively plan for the future of work.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2020/10/15/texting-to-database-2.png,381220,,2020-10-16T15:41:49.316Z,['dcalacci@media.mit.edu'],True,gigbox,2020-09-01,"['design', 'human-computer-interaction', 'civic-technology', 'data', 'economy', 'cities']",Gigbox: a research and development toolkit centering workers in the gig economy,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-10-16T02:01:35.116Z,"We take a computational lens to the analysis of a far-right online forum, using network analysis and natural language processing to understand the development of a dangerous extremist community.So far, this project provides evidence that extremist groups may start out ideologically centered, but that their growth may be understood as a process of social bonding.  This challenges the notion that ideologies themselves need to be challenged in order to curb extremist movements, and further research must examine instead the disruption or redirection of social bonds formed as a result of these organizations. ",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2020/10/15/networks-reply-yearly-forums.png,381231,,2020-10-16T15:43:23.572Z,"['dcalacci@media.mit.edu', 'dhaval@media.mit.edu']",True,investigating-the-iron-march-forums-with-computational-tools-far-right-forum-network-and-lda-analysis,,[],Investigating the Iron March forums with computational tools: Far-Right Forum Network and LDA analysis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-10-16T12:40:48.269Z,"Open Drawing Machine is a low-cost open source drawing machine for controlling drawing instruments with motors and code, and it is oriented for beginners in media arts.This project is an exploration of programmatic and procedural drawing.  It is designed to use different drawing instruments, such as pens, brushes, and to use code to choreograph their movement over the drawing surface.",2020-12-01,['future-sketches'],https://dam-prod2.media.mit.edu/x/2020/10/23/DSCF0157.jpeg,381239,,2020-10-26T19:04:21.499Z,"['gauravp@media.mit.edu', 'velouria@media.mit.edu', 'zachl@media.mit.edu']",True,open-drawing-machine,2020-07-01,"['robotics', 'art']",Open Drawing Machine,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-10-16T15:15:51.580Z,"An open source digital instrument to check if institutions still exist, and alert when they don't exist anymore.The instrument periodically pings Wikipedia, and checks if the  article is written in present or past tense. When it changes to past  tense, the institution is no more, and the instrument emits beeping,  text, or motion.The repository includes the open source code, documentation about how it works, and instructions to build your own instrument and set it up at your home to check for any institution with an active Wikipedia article.",2020-12-31,['future-sketches'],https://dam-prod2.media.mit.edu/x/2020/10/16/Screen Shot 2020-10-16 at 11.17.51 AM.png,381250,,2020-10-23T16:20:11.742Z,"['velouria@media.mit.edu', 'zachl@media.mit.edu']",True,are-you-still-here,2020-09-01,"['art', 'internet-things']",Are you still here?,PUBLIC,https://github.com/montoyamoraga/are-you-still-here,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2020-10-16T15:30:07.088Z,"How the MIT Media Lab Learns, And How Everyone Else Can Learn This Way Too",2023-05-25,"['lifelong-kindergarten', 'digital-learning-studio']",https://dam-prod2.media.mit.edu/x/2020/10/19/Untitled-1_xTgimAi.png,381259,,2023-05-25T22:05:55.528Z,"['mres@media.mit.edu', 'nrusk@media.mit.edu', 'rupal@media.mit.edu', 'leob@media.mit.edu']",True,creative-learning,2014-01-01,[],Creative Learning,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-10-16T15:36:10.786Z,"A collection of  tutorials to introduce computer protocols for sharing data between  computers and over the internet, for artistic purposes.This project focuses on sending raw data between computers, without compression, and without taking latency into account. The data is used for drawing on screens and for making sounds on your collaborator's computers, and share an audiovisual experience.This project is an open sketchbook and a celebration of open source software for arts. It started in May 2020 and updated frequently, adn open for contributions, including the languages openFrameworks, Processing, ChucK, Pure Data, Python, and Node.js.",2020-12-31,"['opera-of-the-future', 'future-sketches']",https://dam-prod2.media.mit.edu/x/2020/10/23/Screen Shot 2020-10-16 at 11.42.55 AM.png,381267,,2022-03-16T15:32:18.865Z,"['velouria@media.mit.edu', 'zachl@media.mit.edu']",True,introduction-to-computer-networks-for-artists,2020-05-01,['art'],Introduction to computer networks for artists,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2020-10-16T18:22:55.692Z,"Amputation, muscle weakness, and nerve damage can make it difficult to perform simple daily tasks.  In this project, we are developing a minimally-invasive technology using magnetic beads to track muscle lengths in real time.  This new technology can provide natural, reliable control of prostheses, exoskeletons and stimulated muscles.",,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2020/10/16/Figure-2.png,381279,,2021-08-19T18:08:11.045Z,"['crtaylor@media.mit.edu', 'syeon@media.mit.edu', 'hherr@media.mit.edu']",True,magnetomicrometry,2017-06-01,"['robotics', 'human-computer-interaction', 'health', 'human-machine-interaction', 'wearable-computing', 'sensors', 'interfaces', 'biomechanics', 'biotechnology', 'electrical-engineering', 'mechatronics', 'human-augmentation', 'prosthetics', 'medicine', 'bionics', 'neural-interfacing-and-control', 'soft-tissue-biomechanics', 'prosthetic-design', 'assistive-technology']",Magnetomicrometry: Tissue Tracking via Implanted Magnetic Beads,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-10-19T18:42:23.578Z,"AbstractDevices that facilitate nonverbal communication typically require high computational loads or have rigid and bulky form factors that are unsuitable for use on the face or on other curvilinear body surfaces. Here, we report the design and pilot testing of an integrated system for decoding facial strains and for predicting facial kinematics. The system consists of mass-manufacturable, conformable piezoelectric thin films for strain mapping; multiphysics modelling for analysing the nonlinear mechanical interactions between the conformable device and the epidermis; and three-dimensional digital image correlation for reconstructing soft-tissue surfaces under dynamic deformations as well as for informing device design and placement. In healthy individuals and in patients with amyotrophic lateral sclerosis (ALS), we show that the piezoelectric thin films, coupled with algorithms for the real-time detection and classification of distinct skin-deformation signatures, enable the reliable decoding of facial movements. The integrated system could be adapted for use in clinical settings as a nonverbal communication technology or for use in the monitoring of neuromuscular conditions.",,['conformable-decoders'],https://dam-prod2.media.mit.edu/x/2020/10/20/AlN Device Release from Silicon Wafer (1080p resolution) copy.gif,381309,,2022-10-17T20:00:58.779Z,"['canand@media.mit.edu', 'linzh@media.mit.edu', 'farita@media.mit.edu', 'dsadat@media.mit.edu']",True,conformable-facial-code-extrapolation-sensor,2017-01-16,"['design', 'bioengineering', 'health', 'systems', 'wearable-computing', 'sensors', 'biology', 'biomechanics', 'technology', 'biotechnology', 'engineering', 'medicine', 'microbiology', 'women', 'nanoscience', 'microfabrication', 'assistive-technology']",conformable Facial Code Extrapolation Sensor (cFaCES),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-10-20T15:41:43.794Z,"An annual celebration of learning, sharing, making, creating, and debating at the MIT Media Lab.",,"['lifelong-kindergarten', 'digital-learning-studio']",https://dam-prod2.media.mit.edu/x/2020/11/10/Jerry Habraken - 013120-FOL2020-007.JPG,381345,,2023-05-25T21:55:40.289Z,"['ps1@media.mit.edu', 'kamcco@media.mit.edu', 'ave@media.mit.edu', 'sjh2021@media.mit.edu', 'lledford@media.mit.edu', 'devora@media.mit.edu']",True,festival-of-learning,2012-01-01,"['design', 'art', 'learning-teaching']",Festival of Learning,PUBLIC,http://thefestival.media.mit.edu,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2020-10-20T17:22:32.899Z,"Playful and creative online courses for member companiesMLConnect is a collection of online learning programs designed exclusively for Media Lab Member Companies. We are launching talks, workshops, and highly interactive online courses that span the spectrum of Media Lab research, ranging from antiracism and technology, to blockchain applications, to building resilient communities. The format combines live online events, breakout sessions, and collaborative activities, connecting Member Companies to the Media Lab research community, and each other.More information at https://connect.media.mit.edu ",2023-05-25,['digital-learning-studio'],https://dam-prod2.media.mit.edu/x/2020/10/24/image.jpeg,381356,,2023-05-25T21:44:11.713Z,['ps1@media.mit.edu'],True,mlconnect,2020-10-01,[],MLConnect,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2020-10-20T17:25:22.555Z,"In collaboration with graduate students from across the Media Lab, the Digital Learning + Collaboration Studio has helped to develop MAS.S90: Introduction to Media Arts and Sciences -- a course for first-year graduate students at the Media Lab. The course covers topics including navigating the Lab, learning (and engaging with) the work of various professors and researchers, and how to approach and engage in responsible research conduct. MAS.S90 is designed for students to develop a “roadmap” to help guide their path through the Media Lab. We are very proud of the diverse nature of research, classes, and initiatives but, at times this can be overwhelming. The course ensures that MAS students have the basic tools, resources, and connections they need to succeed at the Lab. Learn more about the course via the syllabus here. Planning Team: Anastasia OstrowskiDaniella DiPaolaBianca DattaAvery Normandin",2020-12-31,['digital-learning-studio'],https://dam-prod2.media.mit.edu/x/2020/10/22/Screen Shot 2020-10-22 at 12.03.40 AM.png,381364,,2023-05-25T21:55:19.923Z,"['ave@media.mit.edu', 'bdatta@media.mit.edu', 'akostrow@media.mit.edu', 'joep@media.mit.edu', 'dipaola@media.mit.edu']",True,mas-s90,2020-10-01,[],MAS.S90: Introduction to Media Arts and Sciences,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-10-20T19:19:18.079Z,"Climate change and the domino effect it creates will require coordination at scales we haven’t seen and new forms of organization to address these new challenges. One of these new forms of organization is called a Decentralized Autonomous Organization. It consists of a set encoded rules backed by a blockchain that enables for unique consensus mechanisms. At the heart of these consensus mechanisms is voting on the blockchain. In a future where the keys to these transactions are the literal keys to our future, we need a tool that enables us to safely store our keys, verify the blockchain in which the DAO lives in, and vote on the issues at hand. Viral Vote is a voting wallet that does all of that, by securely storing private keys, hosting a full Ethereum node, and having the ability to vote on the issues your organization is trying to resolve. ",2020-12-14,['viral-communications'],https://dam-prod2.media.mit.edu/x/2020/10/20/ViralVotejpg.jpg,381377,,2020-10-26T15:15:10.539Z,"['dmarquez@media.mit.edu', 'lip@media.mit.edu']",True,viral-vote-1,2020-09-08,"['human-computer-interaction', 'civic-technology', 'consumer-electronics', 'crowdsourcing', 'cryptocurrency', 'economy', 'blockchain']",Viral Vote,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-10-21T08:20:59.942Z,"Agent Based Modeling for behaviours, building occupancy loads and risk infection at the University of Guadalajara. In collaboration with the University of Guadalajara, CityScience group has been working on different strategies for the reopening of public spaces in a pandemic area. This study has been focusing on 5 different sites of the UdG campus and 6 interventions to study how the utilisation of existing spaces can be improved while minimising infection risks.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/11/06/shortUDG.mov_DQAHTdj.gif,381405,,2020-11-06T11:00:28.811Z,"['agrignar@media.mit.edu', 'kll@media.mit.edu', 'markuse@media.mit.edu', 'alonsolp@media.mit.edu']",True,abm-resilient-cities,2020-04-01,[],ABM Resilient Cities,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2020-10-21T17:46:10.835Z,"Emergence, when an entity is observed to have properties its parts do not have on their own, is an awe inducing phenomenon seen within nature, complex organization theory, physics, art, and philosophy. Humans can experience emergence or interdependence, a perhaps less potent version of emergence, when they come together, transform, connect, and grow in a way they could not alone through co-creation, playing games and sports, and telling stories. As such moments are scarce, cultures and groups ancient and new have developed technologies (in the sociological sense meaning techniques, processes, and material objects to produce goods, provide services, and connect people) to help groups reach those moments more easily, technologies I define as ancient social technologies (ASTs).Some ASTs, such as narrative coaching, circle practice, and dialogue across differences, have been developed for decades, even generations, to help groups reach emergence together in person. Now, as computational social technologies like video conferencing and social media have been ground breaking by connecting across distance, many facilitators have shown great creativity and resourcefulness as they use these platforms to implement ASTs online. Yet, computational social technologies do not scaffold or ease the implementation of key AST components that are deemed essential to the practice, making virtual AST use challenging.   I present in this thesis Keeper, a tool designed to augment virtual communication by scaffolding the use of ancient social technologies within modern computational social technologies. With a design informed by a deep investigation into four ancient social technologies, Keeper visualizes and mediates online, synchronous, audio and video conversations. Keeper challenges a traditional two dimensional interface through use of “space” and tone. The tool scaffolds ASTs with features like a talking stick and guidelines, but retains key affordances of the digital medium by incorporating private messaging and conversation data visualization. Keeper fosters socially beneficial group dynamics by making visible conversation measures to promote equitability, and it prompts reflection and learning by offering visual maps of a conversation over time. The reception of this tool through experiments and interviews is discussed, and reflections on future work offered.",2021-05-01,"['social-machines', 'center-for-constructive-communication', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2020/10/21/Keeper Screen_irHWY63.png,381416,,2023-10-19T14:08:30.531Z,"['mhughes4@media.mit.edu', 'belen@media.mit.edu', 'dkroy@media.mit.edu']",True,keeper-project-1,2019-09-01,[],Keeper: Online conversation support scaffolding modeled after ancient and modern social technologies,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-10-22T04:02:17.328Z,"RF-SCG is a new system that can capture seismocardiogram (SCG) recordings without requiring any contact with the human body. Such an unobtrusive approach would enable lay users to routinely monitor their SCG signals (e.g., on a daily basis), and may provide early warnings of cardiovascular conditions. This capability could be particularly helpful for monitoring high-risk populations – like the elderly, neonates, or patients with arrhythmia – in their everyday environments. It may also enable on-the-spot heart recordings in the event of a cardiovascular emergency. For example, if someone suspects they may be suffering from a heart attack, they could use such a system to immediately measure their SCG.What is the seismocardiogram (SCG) ?Seismocardiography was first studied in the late 1950s by scientists who were inspired by the technology used in seismology to register underground vibration and predict earthquakes. They adapted the technology to measure fluctuations of the cardiac movements using accelerometers.  The medical community has invested significant effort in studying and understanding SCG recordings since their discovery. Clinical studies have demonstrated that the SCG is more sensitive and specific than the ECG in detecting coronary heart disease during stress exercise testing. Multiple projects have been dedicated to understanding the peaks and valleys of SCG recordings and to mapping these fiducial points to micro-cardiac events. Various studies have demonstrated that SCG recordings can be used to diagnose and monitor various cardiovascular conditions including arrhythmia, myocardial infarction, ischemia, and hemorrhage.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2020/10/22/미디어1.mp4_20201022_024609.gif,381493,,2021-06-20T00:40:34.175Z,"['fadel@media.mit.edu', 'salah@media.mit.edu', 'unsoo@media.mit.edu']",True,rfscg,,[],RF-SCG: Contactless heart recording,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-10-22T04:18:07.876Z,"Therms-Up! is a DIY method of creating inflatables and prototyping interactive materials from wasted thermoplastic bags that easily found at home. We used a inexpensive FFF-type (fused filament fabrication) 3D printer, without any customization of the printer, to heat-seal and patterning different types of mono and multilayered thermoplastic bags. We characterized 8 different types of commonly-used product package’s plastic film which are mostly made of polypropylene and polyethylene, and provided 3D printer settings for re-purposing each material. In addition to heat-sealing, we explored a new design space of using a 3D printer to create embossing, origami creases, and textures on thermoplastic bags, and demonstrate examples of applying this technique to create various materials for rapid design and prototyping. To validate the durability of the inflatables, we evaluated 9 different thermoplastic air pouches’ heat-sealed bonding strength. Lastly, we show use-case scenarios of prototyping products and interface, and creating playful experience at home.Published in the 15th ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI'21). Work-in-progress.",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/10/23/gecko.gif,381501,,2021-06-11T16:41:21.883Z,"['yun_choi@media.mit.edu', 'ishii@media.mit.edu']",True,therms-up,2020-09-01,"['robotics', 'design', 'human-computer-interaction', 'art', 'construction', 'environment', 'learning-teaching', 'fabrication']",Therms-Up!: DIY Inflatables and Interactive Materials by  Upcycling Wasted Thermoplastic Bags,PUBLIC,http://www.mallcong.com,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-10-22T13:34:43.422Z,"There is a consensus that the hospital experience is stressful for children and leads to anxiety and fear.  Long term effects of stress and anxiety may affect children's physiological, social, and academic functioning. The pediatric patients experience these undesirable emotional responses particularly after separation of the parents or before anesthesia in the case of a surgery experience where they have to go through a body exam with a nurse. Different types of therapy have been explored in the past to decrease children's stress during the hospital experience. Pediatric health care professionals used, for example, animal-assisted therapy, clown caring, toy-based technologies, and ride-on cars as interventions to help reduce children's undesirable emotional responses. The purpose of this study is to examine the effect of the use of a ride-on car on physiological responses, distress, and anxiety before the physical examination inside the hospital or during the preoperative experience .",,['affective-computing'],https://dam-prod2.media.mit.edu/x/2020/10/22/EnavCar.jpg,381510,,2022-10-24T16:47:43.383Z,"['neska@media.mit.edu', 'jinmo@media.mit.edu', 'picard@media.mit.edu', 'yun_choi@media.mit.edu', 'javierhr@media.mit.edu', 'amores@media.mit.edu', 'agata@media.mit.edu']",True,emotion-navigation-for-kids,,[],Emotion Navigation for Kids,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-10-22T17:29:43.084Z,"It was believed that functionalities of G protein-coupled receptors (GPCRs) require full-length sequences which are negated by residue deletions. We demonstrate that significantly truncated nfCCR5QTY and nfCXCR4QTYstill bind their respective natural ligands. Receptor-ligand interactions were discovered from yeast-2-hybrid screening and confirmed by mating selection. Two nfCCR5QTY (SZ218a, SZ190b) and two nfCXCR4QTY(SZ158a, SZ146a) were produced in bacterial cell E. coli. Synthesized receptors exhibited a-helical structures, and bound respective ligands, albeit with reduced affinities. SZ190b and SZ158a were reconverted into non-QTY forms and expressed in HEK293T cells. Reconverted receptors localized on cell membranes and functioned as negative regulators for ligand-induced-signalling when co-expressed with full-length receptors. CCR5-SZ190b individually can carry out signalling, albeit at a reduced level with higher ligand concentration. Our findings provide insight into essential structural components for CCR5 and CXCR4 functionality, while raising the possibility that non-full-length receptors may be resulted from alternative splicing and that pseudogenes in genomes perhaps remain functional in living organisms.Implications and future studies of truncated membrane receptors.Our observation of non-full-length functional CCR5 and CXCR4 variants raises more questions than it provides answers.  For example: 1) Are there DNA sequences specifically coding for non-full-length receptors in all genomes? 2) Are they capable of performing regulatory functions in vivo at another level? 3) What are the smallest functional receptors that can exist in vivo? 4) Are they synthesized and subsequently cleared? It is plausible that there are a few means of generating non-full-length receptors and proteins in general through: i) alternative RNA splicing; ii) SINE and LINE transposon insertions and deletions; iii) frame shift mutations resulting in premature translational termination; and iv) non-AUG translation initiation.  Many gene identification bioinformatics search for receptors and proteins with AUG as the translational initiation, and most experiments probe for RNA, rather than proteins. Therefore, it is plausible that such non-full-length proteins may have been overlooked.  Our unexpected discovery of truncated membrane receptor variants in this study may thus alert us again to venture beyond current paradigms to discover, characterize, and design proteins.  Furthermore, our unexpected discovery may shed light to our knowledge blind spot, increase our understanding secrets of nature and open new opportunities to advance science in general.",,['molecular-machines'],,381539,,2020-11-20T22:11:57.969Z,"['shuguang@media.mit.edu', 'pranam@media.mit.edu', 'ruiqing@media.mit.edu']",True,non-full-length-g-protein-coupled-receptors-an-unexpected-discovery-may-lead-to-design-new-molecular-devices,2015-09-09,"['bioengineering', 'health', 'wellbeing', 'clinical-science', 'covid19']",Non-full-length G protein-coupled receptors: An unexpected discovery may lead to design new molecular devices,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2020-10-23T01:04:32.916Z,"Interpreting human electroencephalogram (EEG) is a challenging task and requires years of medical training. We present a framework for learning useful representations from EEG signals via contrastive learning. By recombining channels from multi-channel recordings and fusing multiple datasets, we increase the number of samples quadratically for self-supervised pretraining. We train a channel-wise feature extractor by extending the SimCLR framework to time-series data with a specific design choice of producing sequential representations with equal length to the input sequences. This property allows us to use the learned representations in a variety of downstream tasks that require different lengths of signals. We introduce a set of augmentations for EEG channels and study their efficacy on different classification tasks. Using 3 standard datasets, we demonstrate that the learned features improve EEG classification and reduce the amount of labeled data needed on three separate tasks: (1) Emotion Recognition (SEED), (2) Normal/Abnormal EEG classification (TUH), and (3) Sleep-stage scoring (SleepEDF). Our models show improved performance over previously reported supervised models on all three tasks with training on only 10% or less of the labels. Fine-tuning on 100% of the labels, we achieve the performance of 85.77% on Emotion Recognition and 85.12% on Sleep-stage scoring.",2021-01-01,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/10/22/figT.png,381599,,2020-12-22T04:04:57.520Z,['mmv@media.mit.edu'],True,seqclr-self-supervised-learning-of-features-for-time-series-data,2019-02-01,"['human-computer-interaction', 'artificial-intelligence', 'bioengineering', 'cognition', 'health', 'interfaces', 'machine-learning', 'biomechanics', 'data-visualization', 'electrical-engineering', 'neural-interfacing-and-control']",SeqCLR: Self-supervised learning of features for time-series data,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-10-23T01:14:24.502Z,"Skip Norm is a new and flexible building block for Deep Learning that serves as a skip connection and normalization block simultaneously. It outperforms BatchNorm, LayerNorm, and GroupNorm when used in generative models such as Autoencoders.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/10/22/skipnorm.png,381607,,2020-10-23T14:18:21.275Z,['mmv@media.mit.edu'],True,skipnorm,2020-10-01,"['artificial-intelligence', 'cognition']",SkipNorm,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2020-10-23T02:06:10.461Z,"PAB-QAM  is a new system that achieves higher-order modulation with the current PAB sensor design. PAB relies on the piezoelectric effect to enable underwater communication and sensing at near-zero power and these sensors differ from traditional communication modems in that they do not expend their own energy for transmission. Instead, they communicate by modulating the reflections of existing underwater acoustic signals. PAB-QAM builds on the design of PAB to enable higher-order modulation schemes such as QAM which will allow us to support high throughput applications like streaming images in the oceanHow does PAB-QAM work?Piezo-electric materials have electromechanical coupling which means that the backscatter reflection from the piezo depends not just on the piezo itself, but also depends on the load  that it is attached to. PAB-QAM exploits this idea of electromechanical coupling by switching between different electrical loads at the PAB node  to achieve higher order modulation. We tested PAB-QAM in a controlled environment and our results show that PAB-QAM can deliver twice the throughput compared to the traditional PAB design in high SNR scenarios. This technology has many applications in low power and low cost subsea IoT for monitoring, exploration, and marine life sensing.If you are interested in this, check out  our other ocean research projects:How to enable scalable and ultra-low power ocean IoT.Battery-free Underwater GPS.How to enable battery-free subsea IoT.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2020/10/23/wider.jpg,381618,,2020-10-23T18:00:14.110Z,"['rezagh@media.mit.edu', 'afzals@media.mit.edu', 'wakbar@media.mit.edu', 'osvyrd@media.mit.edu', 'fadel@media.mit.edu']",True,pab-qam-higher-order-modulation-for-underwater-backscatter,,[],PAB-QAM: Higher-Order Modulation for Underwater Backscatter,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-10-23T02:12:42.400Z,"EmbER (Embodied Experience Remotely) explores the use of wearable sensory devices, communicating nonverbal signals, as an additional communication channel in virtual and remote interactions.Social interaction is a critical element of our psychological well-being: Deprivation of human interaction, including experiences of social isolation and loneliness, has been shown to cause significant harm to both mental and physical health. Our current tools for virtual communication are able to simulate many elements of face-to-face social interaction, but they fall short in many other ways with potential psychological health ramifications. EmbER aims to contribute information about more emotionally rich forms of remote communication, based on research in nonverbal communication and the role of the senses on emotional perception.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2022/04/11/Ember_Mockup.png,381626,,2022-11-26T06:40:38.215Z,"['camorris@media.mit.edu', 'vdanry@media.mit.edu']",True,ember,,[],EmbER (Embodied Experience Remotely),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-10-23T03:00:51.764Z,"We present the design, implementation, and evaluation of 𝑈2𝐵, a technology that enables scalable ad ultra-low power ocean IoT.  At the core of 𝑈2𝐵’s design is a novel metamaterial inspired transducer for underwater backscatter, and algorithms that enable self-interference cancellation and FDMA-based medium access control. The design for U2B nodes alternates between active (piezoelectric) and passive (polymer) layers and it uses the resonance of each individual layer and the resonance from their coupling to demonstrate wideband behavior. Not only does this new design allow us to extend our communication range and to decode backscatter signals in the presence of strong self-interference, but it also enables us to scale underwater backscatter to multiple nodes.We fabricated 𝑈2𝐵 nodes and tested them in a river across different weather conditions, including snow and rain. Our empirical evaluation demonstrates that 𝑈2𝐵 can achieve throughputs up to 20 kbps, an operational range up to 62 m, and can scale to networks with more than 10 nodes. In comparison to the state-of-the-art system for underwater backscatter, our design achieves  5× more throughput and 6× more communication range. The design bridges recent advances in metamaterials to underwater backscatter, and demonstrates significant improvements over state-of-the-art proposals. As the research evolves, we hope that these techniques would enable truly ubiquitous subsea IoT systems that can be used for climate change monitoring, marine life sensing, and ocean exploration.If you are interested in this, check out  our other ocean research projects:Battery-free Underwater GPS.How to increase the throughput of PAB nodes using higher modulation schemes.How to enable battery-free subsea IoT.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2020/10/23/UWB.png,381635,,2020-10-25T14:59:22.172Z,"['rezagh@media.mit.edu', 'afzals@media.mit.edu', 'osvyrd@media.mit.edu', 'fadel@media.mit.edu']",True,ultra-wideband-underwater-backscatter-via-piezoelectric-metamaterials,,"['communications', 'data', 'energy', 'environment', 'networks', 'sensors', 'internet-things', 'ocean', 'water']",Scalable and Ultra-Low Power Ocean IoT,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-10-23T04:51:19.960Z,"Can we build a battery-free underwater GPS? While underwater localization is a long-studied problem,  we seek to bring it to battery-free underwater networks. These recently introduced networks communicate by simply backscattering (i.e., reflecting) acoustic signals. While such backscatter-based communication enables them to operate at net-zero power, it also introduces new and unique challenges for underwater localization. We present the design and demonstration of the first underwater backscatter localization (UBL) system. Our design explores various challenges for bringing localization to underwater backscatter, including extreme multipath, acoustic delay spread, and mobility. UBL uses an adaptive and context-aware algorithm that addresses many of these challenges and allows it to adapt to diverse underwater environments (such as deep vs shallow water, and high vs low mobility). We also present a prototype implementation and evaluation of UBL in the Charles River in Boston and highlight open problems and opportunities for underwater backscatter localization in ocean exploration, marine-life sensing, and robotics. ",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2020/10/31/samp3.gif,381645,,2020-11-30T00:04:01.946Z,"['fadel@media.mit.edu', 'osvyrd@media.mit.edu', 'afzals@media.mit.edu', 'rezagh@media.mit.edu']",True,battery-free-underwater-gps,,"['robotics', 'virtual-reality', 'artificial-intelligence', 'augmented-reality', 'environment', 'health', 'sensors', 'imaging', 'computer-science', 'technology', 'engineering', 'wellbeing', 'ocean', 'climate-change', 'autonomous-vehicles', 'water']",Battery-Free Underwater GPS,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-10-23T06:31:24.637Z,"by Ken Nakagaki, Jordan L Tappa, Yi Zheng*, Joanne Leong, Jack Forman, Sven Koenig*, and Hiroshi Ishii*Collaborators at the University of Southern CaliforniaOverview(Dis)Appearables is an approach for actuated Tangible User Interfaces (TUIs) to appear and disappear. This technique is supported by Stages: physical platforms inspired by theatrical stages. Self-propelled TUI's autonomously move between front and back stage allowing them to dynamically appear and disappear from users' attention. This platform opens up a novel interaction design space for expressive displays with dynamic physical affordances.  We demonstrate and explore this approach based on a proof-of-concept implementation using two-wheeled robots, and multiple stage design examples. We have implemented a stage design pipeline which allows users to plan and design stages that are composed of front and back stages, and transition portals such as trap doors or lifts. The pipeline includes control of the robots, which guides them on and off stage. With this proof-of-concept prototype, we demonstrated a range of applications including interactive mobility simulation, self re-configuring desktops, remote hockey, and storytelling/gaming. Inspired by theatrical stage designs, this is a new take on `controlling the existence of matter' for user experience design.",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2020/10/23/(Dis)Appearables-header.gif,381654,,2022-05-10T19:12:17.375Z,"['ken_n@media.mit.edu', 'joaleong@media.mit.edu', 'ishii@media.mit.edu']",True,dis-appearables,2020-10-01,"['robotics', 'design', 'human-computer-interaction']",(Dis)Appearables,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-10-23T14:24:04.903Z,"With Vote Purple '20, we are continuing our work of presenting a cognitive middle ground in media consumption - with a targeted focus on the 2020 U.S. Presidential Elections. We created a set of eight one minute videos that outline some aspect of a prominent social issue expressing both sides.  For each we ask where you stand.  In the US, red equal Republican, Blue equals Democrat.  For each of these we add purple:  you lie in the middle.Our work is not partisan.  It is intended to be depolarizing.  To get people to see how strongly they feel about issues and to tease apart one that may matter a lot to them from others where they may common ground with people they don’t often agree with.  For example, some people may feel very strongly about the environment but are less wedded to a particular economic idea like taxes.  The impassioned points often cloud the conversation and the decision.  But maybe they will be less of a one-issue voter if they broaden the context of their thinking.  And if they think, perhaps they will be more likely to vote.  That's what we hope will happen.",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2020/10/24/Untitled_Artwork 2.jpg,381665,,2020-10-26T15:14:17.484Z,"['lip@media.mit.edu', 'dmarquez@media.mit.edu']",True,vote-purple-20,2020-09-01,[],Vote Purple '20,PUBLIC,https://www.votepurple20.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-10-23T14:57:17.976Z,"The Brain Switch is a real-time, closed-loop brain-computer system allowing for real-time correspondence of simple user needs to a caretaker non-verbally. The Brain Switch is a lightweight, wearable and wireless system which aims on helping to restore and support communication to those with physical challenges (ALS, CP, SCI).Brain Switch consists of wearable, wireless electroencephalography system, which is comfortable to the user and can be worn over extended periods of time (no sticky electrodes, 8hours+ of active use; 2 hours of charging time). It supports different needs of the users, for example, those who require or wish to remain in bed, have limited neck muscle support, etc.  No need for any muscle activity or even eye activity is needed. In addition, two mobile applications are being provided to the family. They connect to a server, one being a notification app used by the caretaker for remote insights (e.g., what the user is in need now) and the other, used by or near the patient. The patient's app streams packets of raw electroencephalography data from a  brain computer interface back to the server. With this information, a  neural network is trained and used to classify mental states like imagery as well as attention of the user in real-time. Enabled by the cloud platform, the system is able to work anywhere and on a multitude of devices, computationally unlimited. Using Brain Switch, the caretaker can be notified remotely if the user needs something like eye drops or to be turned, can ask questions (""Do you want the music on?"" and see a response in the app in real-time), and monitor the overall well-being of the patient in real-time, all without any need for muscle or eye movement activities. Past collaborators in this project include:  Arnav Balyan, Noah Pacik-Nelson, Tobias Schmidt, Daniel Hails, Guillaume Hessel.Interested to learn more?Please contact nkosmyna AT mit DOT edu if you are interested in being part of the study and/or learning more about this system.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2020/11/12/illustration-01.png,381698,,2023-08-22T19:36:55.584Z,"['nkosmyna@media.mit.edu', 'pattie@media.mit.edu']",True,brain-switch,2020-01-15,[],Brain Switch,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-10-23T16:08:06.279Z,"Powered ankle prostheses have been proven to improve the walking economy of transtibial amputees although these powered systems are usually much heavier in weight than conventional prostheses. All commercial powered ankle prostheses that are currently available can only perform one-degree-of-freedom motion in a limited range. However, the human ankle can perform both frontal and sagittal plane motions. Studies have shown that the frontal plane motion during ambulation is associated with balancing. As more advanced neural interfaces have become available for amputees, it is possible to fully recover ankle function by combining neural signals and a robotic ankle. Accordingly, there is a need for a powered ankle prosthesis that can have active control on not only plantarflexion and dorsiflexion but also eversion and inversion.The prosthesis in this project is a second-iteration design based on its predecessor. The new design features a larger joint range of motion, a more robust and efficient transmission, and a more powerful battery module. At present, a system with such capabilities only exists as tethered. ",,['biomechatronics'],https://dam-prod2.media.mit.edu/uuid/fb90d1f6-606a-4d24-8ed3-acb0e9ee055f,381715,,2021-01-20T00:03:42.594Z,"['thhsieh@media.mit.edu', 'syeon@media.mit.edu', 'tonyshu@media.mit.edu', 'lmooney@media.mit.edu', 'mcarney@media.mit.edu', 'jfduval@media.mit.edu']",True,a-two-degree-of-freedom-powered-ankle-foot-prosthesis,,"['robotics', 'mechanical-engineering', 'mechatronics', 'bionics']",A Two-Degree-of-Freedom Powered Ankle-Foot Prosthesis,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-10-23T16:09:52.788Z,"Nina does daily sketches on Twitter @ninasketches or at the website for her 2021 sketches , where you can see them in all their glory.You can read her article about the methodology behind these daily sketches from her 2020 of doing them here.This project received an honorable mention for the 2021 Harold and Arlene Schnitzer Prize in the Visual Arts.",2021-09-01,['future-sketches'],https://dam-prod2.media.mit.edu/x/2020/10/23/EhVzGdPWAAIGdnH_t7QFQI2.png,381723,,2022-04-02T06:47:08.469Z,"['nlutz@media.mit.edu', 'zachl@media.mit.edu']",True,daily-sketches,,"['design', 'art', 'computer-science', 'code']",Daily Sketches: Nina Lutz,PUBLIC,http://www.nlutz.me,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-10-23T21:39:41.981Z,"Ever since the introduction of the first mining pool for Bitcoin in November 2010, mining pools have been used by miners to reduce the variance in rewards of Proof-of-Work cryptocurrencies. Miners connect their computational resources to a pool - and the pool is responsible for dividing the work between the miners to prevent doing duplicate work. On top of that, the pool is responsible for dividing the mining rewards amongst its participating miners. The downside to the usage of mining pools, however, is that they introduce a centralized point of control over its participating miners. Mining pools give miners (or more specifically: their hardware) direct instructions over what they need to work on. This control gives mining pools the ability to have its miners work on a different cryptocurrency, or to unwillingly assist in performing attacks such as block withholding or even 51% attacks. Mining hardware is generally optimized for computing proof-of-work functions as efficiently as possible (measured in calculations per joule), and therefore the hardware is generally not equipped with additional logic to verify the work it’s given. We built a system, called PoolDetective, that actively monitors the workstream between our mining hardware and 25 mining pools across 10 different proof-of-work cryptocurrencies from 5 geographical locations (the number of pools and currencies is still increasing). This includes the major Bitcoin mining pools, collectively responsible for over 75% of the network hashrate. On top of this, we run (official) full nodes for each of the mined currencies, as well as monitor the peer-to-peer network using unofficial nodes for the propagation of new blocks.We store all monitored data from mining, full nodes and block propagation and analyze this data for unexpected behavior by mining pools. Since launching the PoolDetective in November 2019 we have archived nearly 28 million mining jobs from the various pools, and have started analysis on this data. We have already seen evidence of mining pools sending work for different blockchains than expected, and are currently looking for other anomalies.",2022-12-31,['digital-currency-initiative-dci'],https://dam-prod2.media.mit.edu/x/2020/10/23/logo-pd.5f6899a0.png,381791,,2023-03-30T17:41:26.029Z,['narula@media.mit.edu'],True,pool-detective,2020-09-01,[],Pool Detective,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2020-10-25T01:14:38.481Z,"Resilience Across Scales is a research theme that encompasses the work of a few Media Lab Principal Investigators and their research groups. The teams review resilience at three scales, human, community and societal scale and ask the following questions:  Human: How can humans become more physically and emotionally resilient? Community: How can we increase the social and environmental resilience of the communities where we live and work? Societal: How can we improve economic and public health resilience at the global scale?In addition, the teams consider essential prerequisites for resilience at all scales:Trust: The importance of building trust in order to collectively address challenges in life.Communication: Why new ways of communicating and consensus building are essential in a polarized world. Education: The need for more effective and equitable learning platforms in order to make informed decisions in a complex world.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/10/27/resilience across scales-.gif,381860,,2020-10-30T15:08:36.981Z,"['carmencp@media.mit.edu', 'mdchurch@media.mit.edu', 'kll@media.mit.edu', 'sandy@media.mit.edu', 'pcuellar@media.mit.edu']",True,resilience-across-scales,,"['design', 'architecture', 'civic-technology', 'data']",Resilience Across Scales,PUBLIC,https://www.media.mit.edu/projects/resilience-across-scales,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-10-26T20:32:28.657Z,"The current system of scientific recognition and funding has significant bias towards work published in the top few percent of journals and by a relatively small number of research groups. In other words, a large amount of high-quality scientific research goes unnoticed due to current promotion and funding criteria. In this work, we developed a system we call DELPHI (Dynamic Early-warning by Learning to Predict High Impact) to explore whether we could use data science to find “hidden gem” research that would go on to become highly “impactful” without having the high out-of-the-box citation counts that are typical of well-known and highly-established research groups.As co-author James Weis tells MIT News, “We hope we can use this to find the most deserving research and researchers, regardless of what institutions they’re affiliated with or how connected they are.” ",,['molecular-machines'],,381908,,2021-05-21T21:12:00.006Z,['jww@media.mit.edu'],True,scaling-science,,"['artificial-intelligence', 'networks', 'machine-learning']",Scaling Science: Using Data to Find  “Hidden Gem” Research,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-10-26T21:33:41.624Z,Designing protein inhibitors for a protein target using a combination of deep learning and high throughput wetlab screens.,,['molecular-machines'],https://dam-prod2.media.mit.edu/uuid/326b95ad-24c4-4655-9359-6307a045aca2,381924,,2023-10-23T18:30:04.455Z,"['allanc@media.mit.edu', 'manvitha@media.mit.edu']",True,rational-engineering-of-proteins-with-deep-learning,,[],Rational Engineering of Proteins with Deep Learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-10-26T22:58:28.407Z,"Our journeys off-world have taught us just as much about humanity as the harsh environment of space. For the past 20 years, we have had a continuous presence aboard the International Space Station, which has expanded our technical, biological, and operational knowledge of living and working in microgravity. We have also developed a wealth of more personal insights about space from the lived experiences of the astronauts and cosmonauts who have called the ISS home.The Astronaut Ethnography Project captures and distills experiences from spacefaring humans in order to inspire future engineering, policy, and design. Through interviews with astronauts, cosmonauts, and spaceflight participants—as well as analysis of primary source accounts, video, and photographic documentation of space environments—this research endeavor aims to present embodied, human-centered insights about how humans work, play, and reflect on Earth from space. As the nature of life in space changes to include more diverse crews and missions, we can learn much from how today’s spacefarers adapt to the challenges of microgravity when imaging the future.",,['space-exploration'],https://dam-prod2.media.mit.edu/x/2020/10/26/ESA — A. Gerst — ISS Attribution-ShareAlike 2.0 Generic (CC BY-SA 2.0).jpg,381940,,2021-08-20T14:46:15.451Z,"['aekblaw@media.mit.edu', 'sands@media.mit.edu', 'sanas@media.mit.edu']",True,astronaut-ethnography,2018-09-01,"['design', 'architecture', 'social-science', 'space', 'zero-gravity']",Astronaut Ethnography,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-10-27T18:43:07.132Z,"The MIT Center for Collective Intelligence, MIT Media Lab’s Community Biotechnology Initiative, and MilliporeSigma—the life science business of Merck KGaA, Darmstadt, Germany—have come together to collaborate on multiple pandemic response programs. This series of virtual events broadens the conversation around individual technical challenge areas identified in the Pandemic Response Supermind Activation. During each session, thought leaders from across the scientific community take an in-depth look at the current state-of-affairs in the relevant topic area, share data-driven insights and discuss the latest innovations and proposed solutions to some of today’s toughest challenges in pandemic response. Each session includes live audience Q&A.   You can read more about the Pandemic Response Supermind Activation and CoLab findings here!",,['community-bio'],https://dam-prod2.media.mit.edu/x/2020/11/25/Screen Shot 2020-10-28 at 1.35.37 AM.png,381999,,2022-06-14T20:11:19.282Z,"['dkong@media.mit.edu', 'desireed@media.mit.edu']",True,pandemic-response-catalyst-conversations,,"['health', 'member-company', 'technology', 'public-health', 'wellbeing', 'covid19']",Pandemic Response Catalyst Conversations,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2020-10-29T11:07:21.017Z,"Countries and cities around the world have resorted to unprecedented mobility restrictions to combat COVID-19 transmission. Here we exploit a natural experiment whereby Colombian cities implemented varied lockdown policies based on ID number and gender to analyse the impact of these policies on urban mobility. Using mobile phone data, we find that the restrictiveness of cities’ mobility quotas (the share of residents allowed out daily according to policy advice) does not correlate with mobility reduction. Instead, we find that larger, wealthier cities with more formalized and complex industrial structure experienced greater reductions in mobility. Within cities, wealthier residents are more likely to reduce mobility, and commuters are especially more likely to stay at home when their work is located in wealthy or commercially/industrially formalized neighbourhoods. Hence, our results indicate that cities’ employment characteristics and work-from-home capabilities are the primary determinants of mobility reduction. This finding underscores the need for mitigations aimed at lower income/informal workers, and sheds light on critical dependencies between socio-economic classes in Latin American cities.",2020-12-31,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2020/10/29/figure1_PCdlVuR.png,382016,,2021-04-01T14:45:43.081Z,['isal@media.mit.edu'],True,controlling-covid-19-labor-structure-is-more-important-than-policy,2020-05-01,"['data', 'developing-countries', 'economy', 'covid19']",COVID-19 policy analysis: labour structure dictates lockdown mobility behaviour,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0
False,2020-10-29T20:12:49.149Z,"NoPeek-Infer: Preventing face reconstruction attacks in distributed inference after on-premise training (Won FG-2021 Mukh Best Paper Runner Up Award)For models trained on-premise but deployed in a distributed fashion across multiple entities, we demonstrate that minimizing distance correlation between sensitive data such as faces and intermediary representations enables prediction while preventing reconstruction attacks. Leakage (measured using distance correlation between input and intermediate representations) is the risk associated with the reconstruction of raw face data from intermediary representations that are communicated in a distributed setting. We demonstrate on face datasets that our method is resilient to reconstruction attacks during distributed inference while maintaining information required to sustain good classification accuracy. We share modular code for performing NoPeek-Infer at http://tiny.cc/nopeek along with corresponding trained models for benchmarking attack techniques.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2020/10/29/Screen Shot 2020-10-29 at 4.14.49 PM.png,382026,,2022-03-29T18:48:10.880Z,['vepakom@media.mit.edu'],True,nopeek-splitnn,2020-10-29,[],NoPeek,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2020-11-02T18:46:59.866Z,"On Earth, morning, noon, and night have their own specific meanings. The feeling of a second or a minute passing by, the shortening of the days in the winter, the use of terrestrial time zones, the importance of certain celestial events, all of these phenomena are linked to the ways in which we travel through the solar system on our spaceship Earth.As we adopt other spaceships: whether on the ISS, the Moon, Mars, or beyond, the relationships we have with time, and with each other, will fundamentally shift. A “lunar day” will be different from a “martian day”, or a day on the ISS, and our ability to relate to, communicate, and understand one another will have to reflect these new lived realities. With Zenolith, we attempt to conceptualize and represent the diversity of experiences an interplanetary species might have, grounded in the concept of spacetime.  This project proposes a novel free-flying pointing device to orient space travelers in the universe. Composed of a set of nested spheres and enclosed by a harness, an internal sphere remains fixed, pointing to a target regardless of the traveller’s location and orientation. Like a sextant, the outer shell provides an interface for the traveller to measure the angle between two bodies. The outer shell can be manipulated to aim at another body in space with the help of the side scope. The angle between the two bodies can then be determined by reading the offset angle between the inner sphere and the outer shell.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/04/23/Untitled design (2).png,382063,,2023-09-07T14:56:17.206Z,"['sands@media.mit.edu', 'cwwang@media.mit.edu', 'msarang@media.mit.edu']",True,zenolith,2020-09-01,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Zenolith - An Interplanetary Time Device,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2020-11-05T19:56:59.186Z,"The Space Exploration Initiative charters an annual ZERO-G parabolic flight for 14-15 projects and 25 researchers across MIT Media Lab, several departments at MIT (including AeroAstro, EAPS, and others), as well as outside collaborators. To help support this flight opportunity, SEI teaches a course each Fall term on project development, prototyping, and deployment readiness for parabolic flights. Much more information about the course (including current projects, recordings and other lecture material, and data from previous flights) is located on the official course website.SEI has flown five flights so far (Nov 2017, Aug 2019,  twice in May 2021, and May 2022), with the next flight planned for Spring 2024. Explore the modules below to learn more about each flight!",,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/05/27/20210520_Steve-Boxall_ZG547-MIT_6474.jpg,382098,,2023-09-06T22:29:10.378Z,[],True,microgravity-research-flights-with-the-space-exploration-intiative,2017-10-01,"['space', 'zero-gravity']",Microgravity Research Flights with the Space Exploration Intiative,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2020-11-12T14:50:35.378Z,"The environmental conditions of prolonged spaceflight pose significant psychological risks for astronauts. In particular, crews of future long-duration exploration missions will have to cope with a wide range of stressors that present significant challenges for maintaining optimal performance. Crews will have to operate under conditions of high workload, reduced sleep and circadian dysregulation, limited sensory stimulation, confinement, extended separation from family and friends, and communication delays isolating them from real-time interaction with ground support, which may be particularly critical in the event of emergencies. These factors present significant risks to optimal cognitive/behavioral functioning and performance, across individuals and teams, and such challenges will only increase in criticality as human exploration moves beyond Earth’s orbit to targets such as the Moon and Mars.In order to keep astronauts healthy, productive, and to mitigate against mission-related disruptions arising from decrements in behavioral health and performance, researchers from Massachusetts General Hospital (MGH, PI Gary Strangman), in collaboration with researchers from the Fluid Interfaces group (MIT Media Lab, research scientist Nataliya Kosmyna and Professor Pattie Maes), are developing and testing a set of just-in-time strategies and interventions to maintain and restore psychological well-being and operational effectiveness of the crew. The project aims to refine and empirically assess a platform designed to monitor and guide crew members towards optimal physiological and mental states for current or future tasks via personalized changes to the surrounding environment. The team will explore the impact of changes in audio/music, light, and haptics using a closed-loop intervention approach that enables real-time monitoring of these effects using physiological and brain sensors.The four specific aims of the project are: AIM 1: Perform a detailed risk assessment of factors that contribute to personal (and team) dysfunction, particularly in isolated, confined, and extreme environments. AIM 2: Develop a personalized performance-optimization platform (P-POP) based on closed-loop/feedback that integrates physiological sensing with augmentation of the astronaut’s local working environment (e.g., audio, haptics, light) using existing prototypes like AttentivU platform.AIM 3: Characterize the ability of P-POP to improve key performance capabilities including attention, response time, memory, cognitive control, and operationally-relevant performance. AIM 4: Assess the feasibility, acceptability, and efficacy of our proposed platform for use with individuals and teams via empirical testing during long-duration spaceflight analogs. HYPOTHESES: H1:  The novel P-POP will provide real-time physiological monitoring to enable the personalized manipulation of the local work environment—both in the lab and in HERA (Human Exploration Research Analog, a closed habitat designed to serve as an analog for isolation, confinement, and remote conditions in exploration scenarios, located at Johnson Space Center).H2:  Our targeted work environment modulations (e.g., audio, haptics, light) will generate significant improvements in individuals’ cognitive and operational performance. DELIVERABLES: Our project will generate the following deliverables: (1) a characterization of those factors that contribute to poor individual and team performance in ICE settings; (2) a novel platform technology built on existing platforms like AttentivU, capable of real-time tracking of psychological and behavioral health markers and providing targeted augmentation of the local work environment to manipulate those markers; (3) an evaluation of the feasibility, acceptability, and efficacy of the proposed platform technology, on both individual and team metrics, including testing in a spaceflight analog. Based on our findings, we will develop specific protocols and guidelines for optimal deployment of our platform, as well as providing standards recommendations. SIGNIFICANCEThis work will provide NASA with a novel and scalable platform technology for on-board behavioral health management—adapting the local working environment via feedback based biosensing. The approach is personalized and closed-loop, guiding individuals away from less-optimal states (as assessed by physiological measurements) and towards more-optimal states for the task or activity at hand. We expect the approach to help maintain and improve individual performance as well as team performance. The system does not require video displays or graphics. Importantly, however, the platform will be designed for future augmentation via other countermeasure approaches (e.g., visual, olfactory), depending on the needs and capabilities of any particular exploration mission. On Earth, such a platform could have considerable utility for optimizing human performance in a wide range of workplaces. PROJECT UPDATE, MAY 25, 2022: ZERO-G FLIGHT EXPERIENCEIn May 2022, AttentivU Platform, in a form-factor of glasses, boarded Zero-G flight to accompany several flyers. The glasses feature non-invasive brain sensing (Electroencephalography or EEG) and eye movement tracking (Electrooculography or EOG) modalities. Currently, there is paucity in the knowledge on the exact effect of microgravity on the human brain. Since space studies are limited by logistic, financial, and practical restrictions, ground-based analogues have been developed to overcome some of these problems. Most state-of-the-art (SoA) and the current knowledge of the effect of microgravity on the brain is based on results obtained from dry immersion and head-down bed rest space analogues. With this flight we tried to bridge this SoA gap by using wireless, wearable, compact  brain sensing device on multiple flyers simultaneously.  Please stay tuned for more information.",2024-11-04,"['fluid-interfaces', 'space-exploration']",https://dam-prod2.media.mit.edu/x/2022/05/25/IMG_9344.jpg,382179,,2023-10-23T17:42:52.816Z,"['nkosmyna@media.mit.edu', 'pattie@media.mit.edu']",True,attentivu-p-pop,2020-11-02,"['robotics', 'architecture', 'environment', 'health', 'wearable-computing', 'sensors', 'interfaces', 'space', 'technology', 'fabrication', 'wellbeing', 'zero-gravity', 'racial-justice', 'women']",Personalized Performance-Optimization Platform (AttentivU - P-POP),PUBLIC,http://braini.io,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2020-11-16T17:23:47.249Z,"This project was selected to represent MIT at the Microsoft Design Expo 2020 on ‘Healthier Futures.’ iNonymizeA tele-mental health platform that uses generative AI avatars for preserving a person’s privacy and identity while allowing for expressivity.Mental health concerns are growing, and the need for digital solutions are on the rise—especially in the area of tele-counseling services such as hotlines, chatrooms, online communities, and mental health apps. However, patients generally have concerns around both their privacy and their data security. This is particularly because mental health issues can be very stigmatizing. One common method for preserving patient privacy is leveraging “anonymity,” which is particularly helpful for those who feel vulnerable sharing information about their health. Anonymity can help people to avoid feelings of embarrassment or shame. It allows people to be more open and honest, knowing that their identity is secure.  However, anonymity is typically associated with a number of disadvantages as well. From a therapist’s perspective, there’s a higher chance of misunderstanding. When they cannot see the other client’s face, emotional information that would normally be put forth by their facial expressions is lost.  Additionally, there is a lower sense of empathy and trust that can develop between the two parties. As such, those surveyed about anonymity with regards to online interventions for mental health have even mentioned that the anonymous experience can feel very impersonal or dehumanizing. We present a preliminary prototype of iNonymize, a tele-mental health platform that uses generative AI avatars to preserve a user’s privacy and identity while allowing for expressivity (i.e., the transmission of emotion through facial expressions and voice). Our approach uses generative AI to allow someone to swap the look of their face and change the sound of their voice. In this way, emotional signals can be carried forward, while not being tied to the user’s identity. We also envision this pipeline being useful at different levels.  Beyond the experience of helping people connect with a counsellor, iNonymize can also be used to the facilitate and strengthen interpersonal connections in support groups, and help with community outreach.StatusApplication Concept: Generative AI Avatars for Empathetic Anonymity in Tele-Mental HealthVisual Mock-Up: Mobile app to help people anonymously connect with mental health professionalsEarly Technical Prototype:  Pre-rendered avatars generated using the  First Order Motion Model by Siarohin et al.  [1]Presentation: Microsoft Design Expo 2020 on ""Healthier Futures"" (October 29 2020)[1] Siarohin, Aliaksandr, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. ""First order motion model for image animation."" arXiv preprint arXiv:2003.00196 (2020).",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/uuid/46eae292-4548-4d12-a922-95a6b9a3f912,382203,,2021-02-04T05:24:49.321Z,"['joaleong@media.mit.edu', 'patpat@media.mit.edu', 'pattie@media.mit.edu']",True,inonymize,2020-10-14,"['human-computer-interaction', 'artificial-intelligence', 'communications', 'health', 'privacy', 'affective-computing']",iNonymize: Empathetic Anonymity in Tele-Mental Health Using AI Generated Avatars,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2020-11-19T21:18:04.531Z,"Novel Antenna-Based Controller for Musical ExpressionTune Field is a 3-dimensional tangible interface that combines and alters previously existing concepts of topographical, field sensing and capacitive touch interfaces as a method for musical expression and sound visualization. Users are invited to create experimental sound textures while modifying the topography of antennas. The interface’s touch antennas are randomly located on a box promoting exploration and discovery of gesture-to-sound relationships. This way, the interface opens space to playfully producing sound and triggering visuals; thus, converting Tune Field into a sensorial experience.There are two ways to make a classical sculpture; by addition or subtraction, so it happens with music and sound waves. Tune Field is influenced by this idea in its physical interaction, through a series of antennas that slide along the Z axis, and the sound generated, a minimalist drone characterized by its slight harmonic variations.",,[],https://dam-prod2.media.mit.edu/x/2021/04/28/img_BqG2SNh.png,382231,,2021-09-21T16:10:13.110Z,['gbabio@media.mit.edu'],True,tune-field,2019-11-01,"['design', 'human-computer-interaction', 'art', 'music', 'interactive', 'data-visualization']",Tune Field,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2020-11-24T16:16:13.438Z,"UN Habitat recognized in its Global Report on Human Settlements an effort to address informality with unconventional approaches, focusing on the up-scaling and replication of successful strategies identified in recent years. To extrapolate these strategies, it becomes mandatory to fully understand the communities and their inhabitants to create policies and technologies accountable for its citizens. Nowadays urban communities are able to solve existing problems, become more efficient, and make better decisions by using data-driving evidence based tools like the CityScope platforms that analyze, understand, and forecast human dynamics by correlating the data collected in urban ecosystems.  However, informal settlements lack the infrastructure and technological tools needed to gather this type of data. The complexity and broad scope of assessing informality has led world organizations such as UN-Habitat, to leave the data gathering and assessment task in the hands of local partners that use a diverse and very different (not normalized) set of methodologies and metrics, giving rise to unequal evaluations. As such, unified field- research inhibits potential comparisons between communities, and thus inhibits ways to extrapolate successful strategies learned in a settlement that might benefit other locations and communities.The purpose of our Taxonomy is to better understand informality, so more informed policies and technologies can be developed and deployed “by and for” the communities. In order to achieve this goal we are collaborating directly with informal communities, and building the tools that will allow us to crowdsource the data needed to create this taxonomy. Moreover, this tool will give visibility to “invisible communities” while helping researchers understand the challenges of informality.At City Science we propose a non-traditional methodology that collects and groups the different indicators and definitions of informality found in existing research and reports. By doing so, the new taxonomy will enable the development of unified standards that will allow researchers on the ground to assess informal settlements with a degree of commonality across the globe. It presents the initial steps to understand informality and infringement to Human rights globally with a unique set of variables. In parallel, this taxonomy provides the tool for unknown communities to raise awareness about their situations and will be a good excuse for making “visible” the “invisible communities""; this taxonomy will provide local leaders, citizens from the communities, and to NGOs a platform for sharing their challenges, ideas, solutions, and data in an international network of informal settlements.Although settlements might have distinctive features, there are overall characteristics that might help to identify patterns of certain typologies that are common for some of them to a diverse set of communities. This will make it easier to address problems and face challenges universally. The criteria and groups identified for the taxonomy correspond to different scales that can be parallel to the morphology of the built environment: (1) The environment scale, or site (2) The street scale, or architecture (3) At the person scale, or populace These three scales intend to understand informality as a whole while identifying common challenges and strengths/advantages of the different settlements. To gather the data from the communities we have created an online platform that surveys users through a series of qualitative and quantitative questions. Consequently, the data will be simultaneously geographically located and aggregated in the visualization part of the website.One of the main goals of this platform is to empower the communities, making them the lead source of information. This research intends to create a mutually beneficial situation, where the community helps and contributes to raise awareness by defining the problems they are facing, while allowing researchers, institutions and governments to understand the challenges that these communities are facing and to identify successful strategies that can be replicated in other informal settlements. Finally, the latest edition of the Global Sustainable Development Report proposes a global call for action that would be especially helpful for the implementation of all 17 SDGs altogether. As the report states: “organizations should facilitate the exchange of information and dissemination of lessons learned on the use of the Sustainable Development Goals framework among countries”; through the taxonomy of informality, the City Science group aims to contribute in this direction.",,['city-science'],https://dam-prod2.media.mit.edu/x/2020/11/24/Tondo_Phillipines001_GSV.png,382285,,2021-10-29T19:02:22.982Z,"['mdchurch@media.mit.edu', 'markuse@media.mit.edu', 'kll@media.mit.edu', 'alonsolp@media.mit.edu', 'maitanei@media.mit.edu', 'gbabio@media.mit.edu']",True,taxonomy-of-informality,2019-03-01,[],Taxonomy of informality,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-11-24T16:43:44.219Z,"Following The Power of Without agenda, the City Science group proposes the Technologies Toolkit. This research aims to translate the knowledge gathered through the Taxonomy collected into actionable and impactful insights for new technologies shaping existing and future communities. ",2025-06-30,['city-science'],https://dam-prod2.media.mit.edu/x/2020/12/04/DSC_1160.jpg,382293,,2023-09-15T15:03:44.402Z,"['markuse@media.mit.edu', 'mdchurch@media.mit.edu', 'kll@media.mit.edu', 'alonsolp@media.mit.edu', 'gbabio@media.mit.edu', 'maitanei@media.mit.edu']",True,toolkit,2019-05-01,"['design', 'developing-countries', 'energy']",Technologies Toolkit,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2020-12-04T20:49:28.361Z,"Voyage Viewer is an online, open source interactive tool especially designed to visualize and study human mobility and migration.",,"['human-dynamics', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",,382407,,2023-09-07T23:21:35.449Z,"['sandy@media.mit.edu', 'isal@media.mit.edu']",True,voyage-viewer,2020-03-01,"['artificial-intelligence', 'data', 'social-science', 'technology', 'visualization', 'data-visualization', 'social-change', 'data-science']",Voyage Viewer,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2020-12-17T18:24:08.659Z,"Unbreakout (beta) is a tool for participatory Zoom breakout rooms. Let your meeting attendees propose their own breakout discussion topics and join a room of their choosing using a fun, collaborative interface 😎Learn more at unbreakout.media.mit.edu.Unbreakout (beta) is made by the Digital Learning & Collaboration Studio—the same folks who brought you Unhangout! ",2023-05-25,['digital-learning-studio'],https://dam-prod2.media.mit.edu/x/2020/12/17/Screen Shot 2020-12-17 at 13.42.25_s55DPmi.png,382506,,2023-05-25T21:45:04.816Z,"['kamcco@media.mit.edu', 'ps1@media.mit.edu']",True,unbreakout,2020-10-01,"['learning-teaching', 'open-source']",Unbreakout,PUBLIC,https://unbreakout.media.mit.edu,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2020-12-23T17:02:54.337Z,"About CloverClover aims to research, design, and deploy a new pro-social media network for tweens and early teens, that promotes positive identity development and a sense of belonging while mitigating bullying, hate speech, and social comparison. We believe that environment shapes behavior and that toxic online environments are by design. This new online space will challenge harmful social media conventions, such as vanity metrics and the endless scroll. Clover will be explicitly designed as a creative, safe, “brave space” where young people can explore and try things out, fail safely, and thrive socially and emotionally.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2020/12/23/JIMENA ROQUERO_Kozuoui.jpg,382548,,2020-12-23T22:37:06.752Z,"['perris@media.mit.edu', 'sballing@media.mit.edu', 'jnazare@media.mit.edu', 'anneli@media.mit.edu']",True,clover,2020-09-01,[],Clover,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2021-01-04T06:55:44.485Z,The human body produces a variety of different electrophysiological signals from brain-waves (EEG) to the electrical activity of the heart (ECG) to muscles (EMG) and even the gastrointestinal system (EGG). Physiophone aims to serve as an auditory display for these signals with the following characteristics:Real-time: The sound is synthesized in real-time with small latency (<0.1s)High-fidelity: The sound captures the details of the signal down to 𝜇V level Interactive: The instrument can be interactively used on the body where the physician can change the position of the electrodes and the parameters of the sound-generation algorithm with ease (see the video below).Light-weight and Low-power: The instrument is light and portable and can be used for many hours of inspection and recording on a single charge.,,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/02/24/perspective copy.png,382596,,2021-10-08T03:15:12.519Z,['mmv@media.mit.edu'],True,physiophone,2020-11-01,"['human-computer-interaction', 'artificial-intelligence', 'augmented-reality', 'bioengineering', 'data-visualization', 'electrical-engineering', 'neural-interfacing-and-control']",Physiophone: Electrophysiology Sonification,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2021-01-16T21:44:33.649Z,"The sense of touch is so critical to daily functioning. Everything from balance and grip is modulated with an intricate ensemble of signals from various touch receptors in our body. With current amputation, these sensors are all destroyed, leaving patients to struggle with simple tasks like buttoning a shirt or climbing rough terrain .Coming soon is the Cutaneous Mechanoneural Interface, a new composite tissue that is surgically connected to a skin flap in the residual limb and activated through an electrical stimulation system. The interface generates and transmits nerve signals that represent touch and vibratory sensation. ",2021-02-27,['biomechatronics'],https://dam-prod2.media.mit.edu/x/2021/01/16/Picture1.jpg,382687,,2021-02-02T18:00:11.339Z,"['shriyas@media.mit.edu', 'hherr@media.mit.edu']",True,restoring-a-sense-of-touch,2019-05-01,[],Restoring a Sense of Touch,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2021-01-24T19:12:27.827Z,"EEG-based biometrics (user identification) has been explored on small datasets of no more than 157 subjects. Here we show that the accuracy of modern supervised methods falls rapidly as the number of users increases to a few thousand. Moreover, supervised methods require a large amount of labeled data for training which limits their applications in real-world scenarios where acquiring data for training should not take more than a few minutes. We show that using contrastive learning for pre-training, it is possible to maintain high accuracy on a dataset of 2130 subjects while only using a fraction of labels. We compare 5 different self-supervised tasks for pre-training of the encoder where our proposed method achieves the accuracy of 96.4%, improving the baseline supervised models by  22.75% and the competing self-supervised model by 3.93%. We also study the effects of the length of the signal and the number of channels on the accuracy of the user-identification models. Our results reveal that signals from temporal and frontal channels contain more identifying features compared to other channels.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/01/24/fig-topo-01.png,382785,,2021-01-25T17:08:01.282Z,['mmv@media.mit.edu'],True,large-scale-eeg-biometrics-through-self-supervised-learning,2020-12-01,"['artificial-intelligence', 'data-visualization', 'electrical-engineering', 'neural-interfacing-and-control']",Large-scale EEG-Based User-Identification Using Self-supervised Learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2021-01-24T19:23:06.682Z,Using affective signals to summarize 16 hours of body-cam video into 15 minutes of daily recap. See the video below.,,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/01/24/affective.png,382793,,2021-01-25T17:09:11.990Z,['mmv@media.mit.edu'],True,affective-memory-summarization,2018-09-01,[],Affective Memory Summarization,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2021-01-28T19:01:33.891Z,"Check out the project site here. The pursuit of justice in AI requires more than just tweaking the numbers. It requires us to  “study up”—to map, measure, and intervene on the ways that powerful actors maintain social inequity through their decisions and policies. Only by flipping the script  of mainstream data narratives can we  radically re-imagine the terms of our engagement with AI.This website provides an accessible introduction to key concepts like Laura Nader's call to ""study up,"" as well as case studies, demos, and a study guide intended to help students grapple with issues of power in AI.",,[],https://dam-prod2.media.mit.edu/x/2021/01/28/Screen Shot 2021-01-28 at 1.08.19 PM_qvzaMWQ.png,382849,,2021-01-28T19:44:23.455Z,['cbarabas@media.mit.edu'],True,studying-up-in-ai,2021-01-01,"['artificial-intelligence', 'civic-technology']",Curriculum: Power + AI,PUBLIC,http://www.chelsbar.com/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2021-02-02T21:04:15.437Z,"With the advent of blockchain, cryptocurrencies, and more importantly, decentralized applications (DApps), we need to find better ways to onboard people into this new ecosystem that is just about a decade old. With Crypto Bot, we've created a Telegram bot to guide people through the onboarding process. ",,['viral-communications'],,382875,,2021-02-04T16:39:16.186Z,"['dmarquez@media.mit.edu', 'lip@media.mit.edu']",False,crypto-bot,2021-01-04,[],Crypto Bot,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2021-02-09T00:44:24.425Z,"MIT students on a mission to tackle the education crisis created by Covid-19The US school system is struggling to support all of its students during the Covid-19 pandemic, and many are falling behind. The challenges affect low-income communities most strongly. This is not a problem MIT can solve alone, but we can make a difference. We have recruited a group of 18 MIT undergraduate students as Tech Tutors, who will work with small groups (""learning pods"") of students who need extra support during Covid-19. Tech Tutors will work one-on-one with students,  offering mentorship with a focus on creative STEAM activities and general support for digital learning. Tech Tutors is supported by the Office of Experiential Learning and the Office of the Provost and coordinated by the Media Lab Digital Learning + Collaboration Studio together with the Media Arts & Sciences Program. This program has been registered through MIT’s Protection of Minors program.Our ApproachTech Tutors recognizes that a lot of students are struggling to engage and get excited about learning at a distance. We want to encourage students by taking an interest in their education and expanding on their curiosities. By enrolling your child in Tech Tutors, they can look forward to: Personalized, independent, and most-importantly fun enrichment activities and projectsIndividualized support and regular connections with a mentorTech Tutor activities and projects will be both motivated by your child’s interests and will be directly supported by their mentor. Tech Tutors aims to provide parents and caregivers with visibility into all aspects of children's interactions with mentors. Tech Tutors is a 2 hour per week time commitment. A parent or guardian must supervise all mentoring sessions.",2021-05-20,['digital-learning-studio'],https://dam-prod2.media.mit.edu/x/2021/02/22/TT Background.png,382920,,2021-03-01T19:12:12.204Z,"['sjh2021@media.mit.edu', 'ps1@media.mit.edu']",True,tech-tutors-spring-2021,2021-02-01,['covid19'],Tech Tutors - Spring 2021,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-02-13T05:30:57.510Z,"In this work, we introduce FedML, an open research library and benchmark that facilitates the development of new 'federated learning algorithms' and fair performance comparisons. FedML supports three computing paradigms (distributed training, mobile on-device training, and standalone simulation) for users to conduct experiments in different system environments. We maintain the source code, documents, and user community at https://fedml.ai as well as at https://github.com/FedML-AI/FedML.  ",,['camera-culture'],https://dam-prod2.media.mit.edu/uuid/3983394d-8915-4569-b8e2-76b9f4e9267e,382969,,2021-02-18T17:03:31.722Z,"['raskar@media.mit.edu', 'abhi24@media.mit.edu', 'vepakom@media.mit.edu']",True,fedml-a-research-library-and-benchmark-for-federated-machine-learning,2020-09-28,"['human-computer-interaction', 'artificial-intelligence', 'health', 'interfaces', 'machine-learning']",FedML: A Research Library and Benchmark for Federated Machine Learning,PUBLIC,http://www.proneater.com,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-02-15T00:03:48.259Z,"Breathing Planet is an actuated expandable object that uses an iris structure as the underlying mechanism. It has a light source in the center and there is a variety of translucent or transparent petals that can be assembled on top of it to generate dynamic shadows and reflection effects as the structure expands and shrinks. We envision it could be a small object for meditation purpose, as people can inhale and exhale with the movement. Or at a larger scale, being part of an art installation.",,['city-science'],https://dam-prod2.media.mit.edu/x/2021/03/05/0030_Py3BOl3.png,382979,,2021-03-05T20:21:21.514Z,['naroa@media.mit.edu'],True,breathing-planet-transformable-structure,2020-09-01,[],Breathing Planet,PUBLIC,https://www.linkedin.com/in/naroacoretti/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-02-16T21:34:35.701Z,This project aims to explore how the Covid-19 pandemic might have      further increased social inequalities by impacting those who were      already vulnerable disproportionately .,,['city-science'],https://dam-prod2.media.mit.edu/x/2021/02/17/Screen Shot 2021-02-16 at 4.41.35 PM.png,382994,,2021-03-06T19:29:01.745Z,['naroa@media.mit.edu'],True,the-covid-pandemic-through-the-lens-of-vulnerable-youth-in-nyc,2020-09-01,['covid19'],The COVID Pandemic through the Lens of Vulnerable Youth in NYC,PUBLIC,https://www.linkedin.com/in/naroacoretti/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2021-02-19T18:27:59.029Z,"In 2020, first-year PhD students at the MIT Media Lab were tasked with a special project—to reimagine the Lab and write sci-fi stories about the MIT Media Lab in the year 2050.But, we are researchers. We don't only write fiction, we also do science!  So, we did what scientists do! We used a secret time machine under the MIT dome to go to the year 2050 and see what’s going on there! Luckily, the Media Lab still exists and we met someone…really cool!Interview of Cyber Joe, Artificial Intelligence Mentor for MIT Media Lab Students of 2050",,[],https://dam-prod2.media.mit.edu/uuid/854479f5-5da9-4bf3-a3a2-bd29599a3c79,383042,,2021-02-19T18:34:08.646Z,"['patpat@media.mit.edu', 'nsingh1@media.mit.edu', 'tayshin@media.mit.edu', 'joep@media.mit.edu', 'wskamau@media.mit.edu']",True,cyborg-joe,,"['robotics', 'entertainment', 'creativity']","Cyber Joe, Artificial Intelligence Mentor for MIT Media Lab Students of 2050",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2021-02-19T18:34:41.882Z,"This video interview is a part of Media Lab X.0: Anthology of Tomorrows.In 2020, first-year PhD students at the MIT Media Lab were tasked with a special project—to reimagine the Lab and write sci-fi stories about the MIT Media Lab in the year 2050.But, we are researchers. We don't only write fiction, we also do research.  So, we did what technologists do! We used a secret time machine under the MIT dome to go to the year 2050 and see what is happening then. Luckily, the Media Lab still exists and we met someone…really cool!",,[],https://dam-prod2.media.mit.edu/x/2021/02/21/ezgif-7-ac5d14fc64db.gif,383050,,2021-03-01T17:39:12.436Z,"['patpat@media.mit.edu', 'nsingh1@media.mit.edu', 'wskamau@media.mit.edu', 'tayshin@media.mit.edu', 'joep@media.mit.edu']",True,cyber-joe,,"['artificial-intelligence', 'learning-teaching']",Cyber Joe: AI Mentor for MIT Media Lab Students of 2050,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0
False,2021-03-02T13:02:38.297Z,"View the main City Science Andorra project profile.Country-scale analysis of high-resolution mobility patterns and infection spreadThe MIT Media Lab City Science group and the Andorra Innovation Hub have been collaborating since 2015 on research related to urban programming, new mobility systems, tourism, energy, sustainability and other urban innovations. In 2020, in response to the COVID-19 pandemic, the focus of our partnership shifted towards developing models and analysis for better understanding the relationship between government policies, mobility behavior and the spread of COVID-19.All over the world, non-pharmaceutical interventions, such as mobility restrictions, have been globally adopted as critically important strategies for curbing the spread of COVID-19. However, such interventions come with immense social and economic costs and the relative effectiveness of different mobility restrictions are not well understood.In May 2020, the Andorran government carried out a voluntary population-wide serological screening for COVID-19 antibodies (Royo-Cebrecos et al., 2020). We were provided access to the resulting serology data through our partnership with the Andorra Innovation Hub. We also have access to geolocated telecoms data for all mobile subscribers in Andorra, providing a rich source of information about how people are moving around the country. The combination of serological data and telecoms data presented a unique opportunity to better understand the spread of COVID-19 and its relationship to the mobility of the population.The aim of this work was to analyze the changes in mobility behaviors and interactions between March and October, 2020, and to test if they were correlated with changes in transmission rate.",,['city-science'],https://dam-prod2.media.mit.edu/x/2021/03/02/before_during_horiz_light_fast.gif,383149,,2022-03-03T19:33:26.835Z,"['aberke@media.mit.edu', 'alonsolp@media.mit.edu', 'noyman@media.mit.edu', 'doorleyr@media.mit.edu', 'kll@media.mit.edu']",True,mobility-and-covid-19-in-andorra,2020-09-01,"['data', 'government', 'public-health', 'data-science', 'covid19']",Mobility and COVID-19 in Andorra,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-03-14T02:16:47.368Z,"Can robots find and grasp hidden objects?Robots are not capable of handling tasks as simple as restocking grocery store shelves as they cannot perceive the environment as good as humans. What if we could give robots radio perception to search for items that are not in their sight? Such an ability will give them superhuman power to work in warehouses, stores, and our homes. We present the design, implementation, and evaluation of RF-Grasp, a robotic system that can grasp fully-occluded objects in unknown and unstructured environments. Unlike prior systems that are constrained by the line-of-sight perception of vision and infrared sensors, RF-Grasp employs RF (Radio Frequency) perception to identify and locate target objects throughocclusions, and perform efficient exploration and complex manipulation tasks in non-line-of-sight settings. RF-Grasp relies on an eye-in-hand camera and batteryless RFID tags attached to objects of interest. It introduces two main innovations: (1) an RF-visual servoing controller that uses the RFID’s location to selectively explore the environment and plan an efficient trajectory toward an occluded target, and (2) an RF- visual deep reinforcement learning network that can learn and execute efficient, complex policies for decluttering and grasping. We implemented and evaluated an end-to-end physical prototype of RF-Grasp and a state-of-the-art baseline. We demonstrate it improves success rate and efficiency by up to 40-50% in cluttered settings. We also demonstrate RF-Grasp in novel tasks such mechanical search of fully-occluded objects behind obstacles, opening up new possibilities for robotic manipulation.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2021/03/15/ezgif.com-video-to-gif.gif,383251,,2021-04-11T00:36:50.817Z,"['tarab@media.mit.edu', 'jleng@media.mit.edu', 'ijc@media.mit.edu', 'fadel@media.mit.edu']",True,rfgrasp,2021-03-13,"['robotics', 'computer-vision', 'artificial-intelligence', 'networks', 'machine-learning', 'rfid', 'mechanical-engineering']",RF-Grasp: Robotic Grasping of Hidden Objects,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-03-15T18:16:37.351Z,"A Handheld Device for Realtime Spatial Movement, Remote Control and Perspectival Orientation between Physical objects and Digital 3-Dimensional Environments. The Tangible ViewCube is a handheld interface device designed to enable naturalistic interaction with architectural models and other 3D spaces where mapping of first-person perspective to an analog reference plan is desired. This invention detects the cartesian coordinates (latitude/longitude) of a handheld device as it is moved over a model, plan or mapped representation of physical space. It then performs real-time digital reconstruction of the handheld device's rotation and bearing, and communicates with software to create a real-time 3D visualization of the physical space/object from the handheld devices point of reference. A virtual perspective or video is then shown on a display screen.",,['city-science'],https://dam-prod2.media.mit.edu/uuid/0124cb6b-110d-4483-b7f9-419d45ffe9b9,383260,,2021-06-15T18:09:12.725Z,"['csmuts@media.mit.edu', 'kll@media.mit.edu']",True,tangible-view-cube,2015-11-18,[],Tangible View Cube,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-03-16T21:46:58.549Z,"In recent years, there has been a great deal of concern about the proliferation of false and misleading news on social media . Academics and practitioners alike have asked why people share such misinformation, and sought solutions to reduce the sharing of misinformation .We find that the veracity of headlines has little effect on sharing intentions, despite having a large effect on judgments of accuracy. This dissociation suggests that sharing does not necessarily indicate belief. Nonetheless, most participants say it is important to share only accurate news. To shed light on this apparent contradiction, we carried out four survey experiments and a field experiment on Twitter; the results show that subtly shifting attention to accuracy increases the quality of news that people subsequently share. Together with additional computational analyses, these findings indicate that people often share misinformation because their attention is focused on factors other than accuracy—and therefore they fail to implement a strongly-held preference for accurate sharing. Our results challenge the popular claim that people value partisanship over accuracy , and provide evidence for scalable attention-based interventions that social media platforms could easily implement to counter misinformation online. ",2021-07-16,['human-dynamics'],https://dam-prod2.media.mit.edu/uuid/d44ad8fe-76fc-4b73-9186-7ef587f16f39,383287,,2021-03-17T17:25:33.023Z,['zive@media.mit.edu'],True,distraction-online,2021-03-09,"['social-media', 'technology']",Distraction online - subtle accuracy prompts can reduce misinformation on social media,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2021-03-26T14:32:33.279Z,"Tangible Swarm is a tool that displays relevant information about a robotics system (e.g., multi-robot, swarm, etc.) in real time while the system is physically conducting its mission. Information such as robots' IDs, sensor inputs, robot trajectories, distance between robots, battery status, communication patterns, etc. can be easily displayed and customized for different missions and scenarios.",,['city-science'],https://dam-prod2.media.mit.edu/uuid/18dd2598-829e-415d-abc2-b0102d0cc4b7,383352,,2021-08-02T13:38:25.865Z,"['kll@media.mit.edu', 'alonsolp@media.mit.edu', 'agrignar@media.mit.edu', 'ecstll@media.mit.edu']",True,tangible-swarm,2019-04-01,"['robotics', 'computer-vision', 'data', 'visualization']",Tangible Swarm: see the unseen,PUBLIC,http://www.eduardocastello.com,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2021-04-01T14:30:38.713Z,"Somos centinelas: hacia una educación sin barreras de accesoWe are Sentinels: Towards an education without access barriersLa Universidad de Guadalajara (UdeG) y The City Science Group están apoyando un proyecto educativo surgido de manera autónoma en la ""comunidad Lomas del Centinela"" (Zapopan, Guadalajara, México), que promueve un innovador modelo de gestión y transferencia tecnológica para el fomentar el desarrollo de habilidades y conocimientos de las personas que viven en la comunidad.The Guadalajara University (UdeG) and The City Science Group supports an autonomously-emerged educational project at ""Lomas del Centinela community"" (Zapopan, Guadalajara, México), by promoting a management and technology transfer model for the development of the individuals at the community skills and knowledge.Un nuevo modelo educativo ha florecido naturalmente dentro de la comunidad la al combinar la cultura y los conocimientos informales ya existentes de forma local, con las metodologías y los recursos formales ofrecidos por la academia y la industria. El objetivo es complementar las lagunas existentes en la auto-formación informal local con la tecnología y el conocimiento proveniente de la academia y la industria, de modo que se pueda obtener un ""certificado de habilidades específicas"" al final del taller. Este certificado está avalado por la UdeG y el equipo de City Science, con el fin de que los ""ciudadanos informales"" puedan encontrar ""trabajos formales"".By combining the already existing informal local culture and knowledge with the formal methodologies and sources offered by academia and the industry, a new educational model has naturally flourished in the community. The goal is to complement the gaps of the local expertise with the technology and knowledge coming from academia, and the industry, so a ""specific-skills'-certificated"" can be earned by the end of the workshop. This certificate is backed up by the UdeG and the City Science team, so ""Informal citizens"" can find ""formal jobs"".La finalidad es la de replicar este innovador modelo educativo en otros escenarios con características emergentes dentro de México, América Latina, y el mundo.We aim to replicate this new educational model in other scenarios with emerging characteristics, all over, México, Latin America, and the world.",,['city-science'],https://dam-prod2.media.mit.edu/x/2021/04/01/WhatsApp Image 2021-03-20 at 16.55.50 (4).jpeg,383396,,2023-07-25T19:26:59.103Z,"['leticiai@media.mit.edu', 'maitanei@media.mit.edu', 'gbabio@media.mit.edu', 'aricom@media.mit.edu', 'alonsolp@media.mit.edu', 'kll@media.mit.edu']",True,huertos-comunitarios-keyhole-in-lomas-del-centinela,2021-03-20,[],Somos Centinelas - We are Sentinels,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-04-09T17:29:56.614Z,A FerroMagnetic Suit For Zero Gravity // Grounding Humans without Gravity,,['tangible-media'],https://dam-prod2.media.mit.edu/uuid/cf29a52f-14db-49e9-8b23-ad665588cbe7,383454,,2021-10-15T17:47:29.210Z,['honnet@media.mit.edu'],True,ferrozuit,,"['wearable-computing', 'space']",FerroZuit,PUBLIC,http://honnet.eu,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-04-13T19:15:57.674Z,"The rapid advancement of electronic devices and fabrication technologies has further promoted the field of wearables and smart textiles. However, most of the current efforts in textile electronics focus on a single modality and cover a small area. In this work, we introduce a new platform of modular, conformable (i.e., flexible and stretchable) distributed sensor networks that can be embedded into digitally-knit textiles. This platform can be customized for various forms, sizes and functions using standard, accessible and high-throughput textile manufacturing and garment patterning techniques. Here, we have developed a tailored, electronic textile conformable suit (E-TeCS) to perform large-scale, multi-modal physiological (temperature, heart rate, and respiration) sensing in vivo. ",,"['conformable-decoders', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/uuid/ce11256c-2dd6-4a29-a6c7-52c4abae8938,383486,,2023-09-07T23:28:06.665Z,"['canand@media.mit.edu', 'irmandy@media.mit.edu']",True,a-tailored-electronic-textile-conformable-suit,2019-01-01,"['design', 'bioengineering', 'consumer-electronics', 'health', 'sports-and-fitness', 'sensors', 'space', 'biology', 'technology', 'biotechnology', 'materials', 'medicine', 'microbiology', 'nanoscience', 'covid19', 'microfabrication']","A tailored, electronic textile conformable suit for large-scale spatiotemporal physiological sensing in vivo",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2021-04-15T14:10:48.842Z,Sparking conversations through artwork  and envisioning future values for a spacefaring society,2023-07-31,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2021/04/15/5fa16aca5faaa.image.jpg,383522,,2023-09-07T22:39:08.637Z,"['k.mccall@media.mit.edu', 'reeddc@media.mit.edu', 'msarang@media.mit.edu', 'lizbethb@media.mit.edu', 'aekblaw@media.mit.edu']",True,space-freedoms,,"['government', 'politics', 'behavioral-science']",Space Freedoms,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2021-04-15T14:24:03.413Z,Studying group dynamics in space environments with virtual reality simulations to inform the design of future governance systems,2020-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/04/15/Screen Shot 2021-04-15 at 10.25.20 AM.png,383530,,2023-09-07T15:42:15.764Z,"['asimonso@media.mit.edu', 'k.mccall@media.mit.edu', 'reeddc@media.mit.edu']",True,space-for-virtual-reality,,"['government', 'politics', 'behavioral-science']",Space for Virtual Reality,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2021-04-15T15:19:58.705Z,"🚨Application to the 2022 Co-Design Experience here bit.ly/codesign2022 🚨Historically, technology transfer has been associated with efforts to construct mechanisms allowing the exporting/importing of technologies globally, more often than not, with questionable results. In this project, we co-opt this term and its historical understanding to name a nascent movement of grassroots knowledge dissemination at a regional scale in rural Colombia. This project builds upon tensions between long-standing traditions of self-determination, predominantly from Afro-Colombian farmers and Indigenous groups, and the mainstream model of venture-based businesses or dependence on imported technology within the technology development sector in Colombia. We use this space to explore alternatives considering transitioning to technology development models in harmony with nature and society locally.The project's goals are (1)  to co-create/redesign and expand, along with local collectives in Colombia, a portfolio of technologies and (2) co-develop a technology transfer strategy driven by these same collectives, to learn and exchange technological knowledge. This knowledge reciprocation is what we refer to as Regional Technology Ecosystems.As part of this initiative, we have launched two learning experiences: A Co-Design Experience: Technology Design for Coffee ProductionA Co-Design Experience: Technologies for Rural Sustainability in Colombia",2024-01-15,['space-enabled'],https://dam-prod2.media.mit.edu/x/2021/04/15/IMG_5665.jpeg,383540,,2023-05-10T20:58:00.809Z,"['pcuellar@media.mit.edu', 'drwood@media.mit.edu']",True,technology-co-creation-and-transfer-ecosystems-in-latin-america,2021-06-01,"['design', 'civic-technology', 'technology', 'marginalized-communities']",Regional Technology Co-Creation and Transfer Ecosystems in Latin America,PUBLIC,https://www.media.mit.edu/people/pcuellar/overview/,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-04-15T23:35:51.060Z,"Protegiendo el bosque del CentinelaPromover y brindar las herramientas necesarias en el comportamiento del fuego y en el manejo de incidentes, para poder preservar el bosque de Nixticuil y así crear participación en la toma de decisiones además de proporcionar experiencia para las actividades en campoProtecting the Sentinel ForestPromote and provide the necessary tools in fire behavior and incident management, in order to preserve the Nixticuil forest and thus create participation in decision-making as well as providing experience for field activitiesEl Bosque Centinela, ubicado en el kilómetro 3 de la carretera Las Cañadas, se encuentra en medio de un área de interfaz urbana, tiene una extensión de 107 hectáreas. Es el área perfecta para que se puedan realizar diferentes actividades recreativas y es un beneficio ecológico para las comunidades cercanas, sin embargo, cuenta con gran vulnerabilidad en la exposición de incendios forestales y actividades de inseguridad.The Sentinel Forest, located at kilometer 3 in the Las Cañadas highway, is located in the middle of an urban area, it has an area of 107 hectares. It is the perfect area for different recreational activities to be carried out and it provides an ecological benefit for the nearby communities, however, it has great vulnerability in the exposure of forest fires and insecurity activities.",,['city-science'],https://dam-prod2.media.mit.edu/x/2021/04/15/Screen Shot 2021-04-15 at 7.36.45 PM.png,383548,,2021-04-16T00:25:50.777Z,"['maitanei@media.mit.edu', 'alonsolp@media.mit.edu', 'leticiai@media.mit.edu', 'gbabio@media.mit.edu', 'kll@media.mit.edu']",True,brigada-forestal-femaf-femaf-s-forest-firefighters,2020-12-01,[],Brigada forestal FEMAF - FEMAF's Forest Firefighters,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-04-23T02:09:09.979Z,"We built an inexpensive, portable, micro-projector-based laptop extension the purpose of which is to engage real space in remote interactions. The goal is to expand the viewport from the 6x8 tablet or PC so that a learner or communicator can engage themselves physically and viscerally in an interaction. We want to expand the horizon and perspective. In some cases, this can be used to present a gallery on a large background while screen-sharing occurs on the small one, or vice versa. This has potential for learning, simulation and conferencing with a point of view.To get instructions on how to make your own, go to the GitHub repo.",,['viral-communications'],https://dam-prod2.media.mit.edu/x/2021/04/22/Big Screen Title_7u6UfVv.png,383600,,2021-08-20T14:44:37.489Z,"['lip@media.mit.edu', 'dmarquez@media.mit.edu']",True,big-screen,2021-02-01,[],Big Screen,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2021-04-27T19:18:43.018Z,"Spanning from the early Musical Telegraph and Electronic Sackbut, to the late EMS Synthi AKS and Moog Synthesizers, electronic music and musical controllers, particularly the keyboards, have enabled people from all walks of life and all around the world to produce and manipulate sound as a mean for creativity, expressions, and shared experience. Note that most of the expressive keyboard interfaces to date rest on a rigid and heavy structure. On the other hand, textiles are ubiquitous in our daily life. They are highly formable and palpable materials with a broad spectrum of patterns, structures, and textures, making them a great candidate for physical interfaces. Inspired by theremin's expressive controls and the soft and deformable tactile properties of knitted textiles, we have developed an interactive textile-based musical interface with a familiar layout of piano keys. ",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2021/06/14/KnittedKeyboard_5.jpg,383624,,2022-05-23T14:48:02.611Z,"['irmandy@media.mit.edu', 'mhjiang@media.mit.edu', 'joep@media.mit.edu']",True,knittedkeyboard-ii,2020-12-01,"['design', 'human-computer-interaction', 'architecture', 'art', 'consumer-electronics', 'entertainment', 'environment', 'manufacturing', 'music', 'sensors', 'interfaces', 'performance', 'interactive', 'materials', 'fabrication', 'textiles']",KnittedKeyboard II,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-04-28T16:16:22.836Z,"Video has evolved from an esoteric production to the default means of online and broadcast communication. We accept a wrong or lengthy video where we would object to poor writing. And because moving images impact us viscerally and intellectually, we are susceptible to bias, misinformation, and falsity in the video because of its presentation rather than its argument. Liquid Movies is an analytical engine built on years of research and development for the intelligent search and segmentation of image, text, and sound within a video. First, we showed how Liquid Movies applied to news, and other media (e.g., Netflix) provides improved navigation through the sea of uncertainty for typical media consumers. Now with Liquid Movies: News and Liquid Movies: Learning, the platform expands into increasingly assistive media aids that enable users to explore  across videos in a graphically and intelligent way. And then, from such explorations, users can distill their journeys into movies.",2023-06-01,['viral-communications'],https://dam-prod2.media.mit.edu/x/2022/03/21/netflix animals screenshot.png,383635,,2023-06-07T05:30:37.019Z,['js_tan@media.mit.edu'],True,liquid-movies,2021-04-28,"['computer-vision', 'human-computer-interaction', 'communications', 'natural-language-processing']",Liquid Movies,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2021-04-30T22:40:19.259Z,"Our collaboration with Ars Electronica Festival Ars Electronica is one of the world’s most renowned festivals focusing on art, science, and technology. Ever since 2018, the Space Exploration Initiative has been invited to present our various projects at the festival. Here, you can find information on our annual exhibitions and why the arts are a crucial component of the Space Exploration Initiative from our Ars Curator Xin Liu.",,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/04/30/DSC07195.JPG,383665,,2021-05-03T19:28:41.829Z,['xxxxxxin@media.mit.edu'],True,sei-at-ars-electronica,,[],Space Exploration Initiative at Ars Electronica,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2021-05-07T16:47:35.286Z,"Prolonged exposure to microgravity is known to cause various acute health risks, including muscle atrophy, bone loss, cardiovascular deconditioning, and orthostatic intolerance. Due to the absence of gravitational force, bodily fluid hydrostatic pressure gradients vanish, and blood distribution shifts from the astronaut's legs toward their upper body. Consequently, it is imperative to provide continuous medical check-ups and interventions for astronauts and crewmembers throughout their long-term journey in outer space and also after their return to Earth. PS-Suit is an active bioelectronic intra-vehicular activity spacesuit that could simultaneously perform wireless multi-modal monitoring of vital signs, including heart electrical activity, respiration, blood flow, and oxygen level and exert controlled, spatiotemporal and peristaltic pressure through five textile-based compression sensors and five pneumatic chambers integrated across the bodysuit. Integrating physiological and physical sensing and pneumatic actuation systems in the PS-Suit (1) facilitates closed-loop and timely intervention for astronauts to regulate their cardiovascular dynamics and (2) enables researchers to study the direct influence of active-dynamic compression in micro to hypergravity conditions on various cardiovascular and physiological markers. In the end, we tested and evaluated a functional prototype of the PS-Suit during a parabolic flight campaign to investigate the response of heart-rate and blood pulse arrival time to applied gradient compression and changes in gravitational force.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2021/09/15/20210523_Steve-Boxall_PSM_MIT_Research_ZG548_7494-2.jpg,383715,,2023-09-20T04:04:52.881Z,"['irmandy@media.mit.edu', 'alims@media.mit.edu', 'joep@media.mit.edu', 'dnewman@media.mit.edu']",True,peristaltic-suit,2021-01-01,"['robotics', 'data', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity']",Peristaltic Suit,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2021-05-09T21:34:44.622Z,"When imagining human life in space, how will we express our humanity and creativity in an alien environment? Can the art we make both reflect the unique qualities of space and keep us connected to Earth?Fluid Expressions incorporates science, art, and design into an exploration of art-making and place-making in outer space. The work centers around the design and craft of an object called a memory capsule, designed to function as both a traveler’s painting kit for space and an embodied, material reminder of home.",2022-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/05/09/banner.png,383737,,2023-09-07T14:57:19.790Z,['sanas@media.mit.edu'],True,fluid-expressions,,"['robotics', 'design', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'sensors', 'performance', 'space', 'biology', 'technology', 'engineering', 'wellbeing', 'zero-gravity']",Fluid Expressions - Art and Craft in Microgravity,PUBLIC,https://www.sanasharma.com/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-05-10T23:04:20.719Z,By Po-Hao ChiHow could microgravity/hypergravity shape our perceptual experience of listening with weightlessness? How can we seek to exploit this spatial aspect of sound as a distinct parameter in its own right? Exploring our understandings of all of these qualities in tandem will play an essential role in this project.,2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/07/14/20210523_Steve-Boxall_PSM_MIT_Research_ZG548_7375.jpg,383760,,2023-09-07T14:54:57.865Z,[],True,harmony-in-precarity,,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Harmony in Precarity,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-05-10T23:36:23.297Z,"By Rachel BellisleOverview:The Gravity Loading Countermeasure Skinsuit (GLCS or “Skinsuit”) is an intravehicular activity suit for astronauts that has been developed to simulate some of the effects of Earth gravity. The GLCS produces a static load from the shoulders to the feet with elastic material in the form of a skin-tight wearable suit [1],[2] (Figure 1). The GLCS is a potential spaceflight countermeasure, which aims to mitigate physiological effects of microgravity, including spinal elongation, muscle atrophy, and sensorimotor changes. This wearable system is intended to supplement exercise during future missions to the moon and Mars (where current exercise equipment may be too large and bulky for the small spacecraft) and to further attenuate microgravity-induced physiological effects in current ISS mission scenarios. Several GLCS versions have been developed over the past decade with various design modifications (Figure 2). Previous GLCS experiments, led by MIT and/or international collaborators, have included ground experiments, parabolic flights, International Space Station (ISS) Expedition 44S (Sep 2015, 10 days) and ISS Expedition 50/51 (Oct 2016 to Jun 2017) [3]. These previous studies have primarily studied operational feasibility, loading magnitude, and spinal elongation attenuation [4]. Current work at MIT aims to investigate the GLCS as a countermeasure for muscle atrophy and sensorimotor deterioration [5]. This work aims to characterize the function and physiological effects of the GLCS. Overall, the proposed work would support the goal of the GLCS project in enabling humans to adapt to multiple levels of gravity, bringing us one step closer to long-term space habitation.",,['space-exploration'],https://dam-prod2.media.mit.edu/x/2023/07/18/skinsuit.png,383768,,2023-07-19T03:41:48.711Z,[],True,gravity-loading-countermeasure-skinsuit,,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'health', 'music', 'wearable-computing', 'sensors', 'space', 'community', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",The Gravity Loading Countermeasure Skinsuit,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-05-12T16:53:54.025Z,"Increasingly, devices designed with artificial intelligence (AI), such as robots or smart speakers, are entering people’s homes, workspaces, and environments. Many home robots are made to interact with users in an interpersonal way and can be designed to be socially persuasive. This translates to an increased need for policy around these devices surrounding dimensions including autonomy, bias, social persuasion, privacy, emotional deception, and transparency. Previous works have revealed, when people explore designing these devices, they express concerns around privacy, autonomy and transparency and design to adjust for these concerns. Other work has shown that with the proper scaffolding, students are able to identify the socio-political aspects of technology. Our project builds upon these works to extend beyond participatory design of these devices and learn more about how people would design policy around these technology dimensions. Ultimately, through expanding the research to policy design, we can engage in participatory democracy in policy around these devices. The Robot Policy Design Toolkit is designed to allow any generation to design and consider how we can develop policy for social robot technologies around nine ethic topics. The toolkit is situated around the context of you in your home considering how policy around robots may affect your environment and interactions with technology and other people. In the toolkit, people select and ""pass"" laws for social robots and structure various policies with these laws around topics such as autonomy, data bias, data privacy, and social persuasion.As agents are beginning to enter people’s homes, smart speakers and robots are generating legal and policy issues that require developers to reconsider the design of these technologies and how we should design future policies around these devices. Social robots are a much more powerful piece of technology than we’ve seen before because of their ability to engage people in a natural way of interaction. We need to investigate what this means and need to be proactive when we design policies around these technologies. It is important to incorporate the voices of those who will be impacted. The investigations surrounding these topics and design of these devices must include people of all ages, allowing researchers to compare between generations. An intergenerational approach also includes children and older adults who are typically left out of the design conversation of these devices. ",,['personal-robots'],https://dam-prod2.media.mit.edu/uuid/82d87496-6f74-4294-a2c1-adc1c526eed4,383788,,2022-02-18T16:24:52.422Z,"['dipaola@media.mit.edu', 'akostrow@media.mit.edu', 'cynthiab@media.mit.edu']",True,robot-policy-design,2021-05-12,"['robotics', 'politics', 'social-robotics']",Robot Policy Design Toolkit,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2021-05-14T15:08:24.686Z,"Rapid prototyping platforms such as 3D printers used for digital fabrication are today able to manufacture custom objects for specific tasks. For example, a 3D printer deployed to the International Space Station has been able to print objects such as wrenches for astronauts on demand. However, these objects cannot be reconfigured for other tasks, and their constituent materials can generally not be recycled in order to reprint other objects for different applications. For long duration and deep space missions, digital fabrication techniques will need to adapt to allow fabricating items capable of acquiring multiple geometric configurations. This would allow finite material to be used to create items that can fulfill multiple tasks, thereby shirking reliance on a constant stream of raw material feedstock that will no longer be available.Objects capable of morphological adaptation would address many challenges associated with today’s limitations on launch mass and volume, as well as facilitating stowage during launch. Such reconfigurable objects could realize new applications including rapid prototyping, forming temporary structures to aid in spacecraft inspection and astronaut assistance, and actively changing their inertia properties, while also enabling replacement or augmentation of structures over multiple launches.This research introduces a concept for a reconfigurable structure based on pivoting cubes that achieve their pivoting maneuvers by controlling the magnetic fields of electromagnets embedded in their edges. We exploit a novel framework for controlling electromagnets to form reconfigurable structures in space by controlling their polarization in such a way as to allow the formation of temporary hinges and pivoting between adjacent modules. Supplementing laboratory experiments on an airtable, pivoting maneuvers were successfully executed in microgravity on a parabolic flight in May 2021.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/09/13/multi-face.jpg,383820,,2023-09-07T14:56:34.888Z,[],True,reconfigurable-space-structures,2020-10-07,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'electrical-engineering', 'wellbeing', 'zero-gravity', 'chemistry']",Reconfigurable space structures,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-05-14T15:19:23.819Z,"By Somayajulu Dhulipala and Manwei ChanAs humanity explores deeper into space, long-duration missions will require horticulture activities to provide food, oxygen, and psychological benefit to astronauts. The AgriFuge is a rotating plant habitat that provides simulated gravity as well as a controllable irrigation system. Simulated gravity enables certain plant functions, such as circumnutation of growing vines. Additionally, one of the biggest challenges in the development of in-space horticulture is irrigation, as fluid handling in micro-gravity is usually a challenge. On the ZeroG flight, AgriFuge will test the rotational irrigation system.Past experiments such as VEGGIE relied on passive wicking to irrigate their plants; however, this system was subject to failure modes of over or under-watering plants. The Advanced Plant Habitat (APH), was developed afterward with an automated irrigation system. For long-duration missions, the psychological stresses on astronauts are exacerbated. Caring for plants can be one source of stress alleviation in space. This specific AgriFuge experiment combines both a controllable irrigation system as well as incorporates human caretaking by allowing the habitat to be manually spun. Specifically, the ZeroG flight will be used to test the consistency of an astronaut who will manually spin the habitat. The astronaut will be aided with a visual indicator to give feedback to his or her activities.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/07/21/IMG_0496.jpg,383828,,2023-09-07T14:52:55.796Z,[],True,agrifuge,,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",AgriFuge,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-05-14T15:57:26.237Z,"How did life originate? Nobody knows. Life might not even be native to our Earth - it might have come from asteroids or the interstellar medium. While pioneering laboratory studies recently made progress for prebiotic (origin-of-life) chemistry, the question arises whether such reactions would also work in zero gravity environments.With Ø-scillation, we aim to achieve the first steps towards an answer using a proxy reaction: how does zero (and hyper-) gravity affect the reaction rates in oscillating reactions? In this study, we develop a robust, compact, and simplified version of the Briggs-Rauscher experiment, an oscillating chemistry event often called the Iodine clock, which cycles through amber and blue colors. The hypothesis to be tested is that different gravity environments do not alter the reaction rates of the involved chemistry. If this can be confirmed, we might just be able to add another piece to puzzle of life.Learn more on the external website.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/05/14/EmbeddedImage (1).png,383836,,2023-09-07T14:53:16.314Z,"['aekblaw@media.mit.edu', 'csmuts@media.mit.edu', 'xxxxxxin@media.mit.edu', 'vsumini@media.mit.edu', 'mcoblent@media.mit.edu', 'cherston@media.mit.edu', 'mcarney@media.mit.edu']",True,scillation-oscillating-chemistry-in-zero-gravity-and-beyond,,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Ø-scillation: oscillating chemistry in zero gravity and beyond,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2021-05-14T16:17:34.329Z,"Solid-state microfrabricated devices are in development to replace contemporary heavy mechanical components for use in spacecraft propellant management.By Jon MacArthurThe booming market of smaller, cheaper, and lighter CubeSats has created the need for equally small propulsion systems. Electric propulsion is a very efficient option for most large spacecraft, but small spacecraft typically lack the power needed to operate them. MIT’s Space Propulsion Lab has developed the ion Electrospray Propulsion System to address this operational regime to make CubeSats viable platforms for numerous missions that traditional chemical propulsion cannot enable. To accompany these lightweight and low-power propulsion systems, propellant control must also be low mass and require minimal power. Recent developments in microfabrication techniques have shown that electrostatic fields can effectively initiate the flow of propellant via wicking materials and capillary tubes with no need for heavy mechanical valves and pressure vessels that require more power. These devices are being built for testing in a zero-gravity environment to conclusively show their operational capabilities for future spacecraft.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/05/14/eyJidWNrZXQiOiJhc3NldHMucHVicHViLm9yZyIsImtleSI6IjhtbGNwaWh6LzcxNjA1NjYwNDk0MTQ5LnBuZyIsImVkaXRzIjp7InJlc2l6ZSI6eyJ3aWR0aCI6ODAwLCJmaXQiOiJpbnNpZGUiLCJ3aXRob3V0RW5sYXJnZW1lbnQiOnRydWV9fX0=.png,383844,,2023-09-07T14:54:12.369Z,[],True,moving-liquids-with-no-moving-parts,2020-10-07,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Moving Liquids With No Moving Parts,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-05-17T17:01:48.463Z,"The Miniature Optical Steered Antenna for Intersatellite Communications (MOSAIC)By Shreeyam KackerThe Miniature Optical Steered Antenna for Intersatellite Communications (MOSAIC) project aims to qualify liquid lens technology for use in space. The project aims to construct a prototype miniaturized optical transceiver with hemispherical pointing-and-tracking coverage, enabling wide field-of-view laser communications for Cubesats without any additional body pointing.By virtue of being liquid, the lenses sag in the presence of gravity. This causes different focusing powers across the surface of the lens, creating an aberration called coma. We are looking at investigating if this coma aberration goes away in the absence of gravity, and if there are any changes in the focusing power.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/uuid/c6d287e0-d60b-40a7-85c8-c8346cef5842,383879,,2023-09-07T14:54:42.636Z,[],True,mosaic,,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Miniature Optical Steered Antenna for Intersatellite Communication (MOSAIC),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-05-18T15:44:26.265Z,"By Nancy Valladares and Rae Yuping HsuThis project is an art performance that seeks to rethink imaginaries of multispecies survival in space exploration via the lens of symbiosis. The narrative is inspired by biologist Lynn Margulis' endosymbiotic theory of species evolution through a process which Donna Haraway describes as “critters eating critters and getting indigestion.” The flyer will wear a bio-designed flight suit to collide with a bacteria cellulose sac in microgravity and proceed to ""become one"" by inhabiting the bacteria cellulose.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/05/18/Flight suit_full.JPG,383894,,2023-09-07T14:51:49.571Z,[],True,panspermia,,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Panspermia,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2021-05-19T02:56:35.692Z,"By Larissa Zhou and Adam ZacharFood is crucial to maintaining the physical and psychological health of humans in space. Eating food provides nutrients and energy. Preparing food can promote relaxation and creativity. Sharing food improves crew cohesion. But most foods are perishable, bulky, and heavy. For deep space missions, it is not feasible to ship all the food from Earth. Instead, growing and processing crops during deep space missions “represents the greatest means of self-sufficiency for a crew habitat over the long term” [1].Food processing tools are the missing link in a sustainable food supply chain for long term space habitation. While the ISS contains a water dispenser, a food warmer, and even a test oven, there has been limited work to develop tools for in-situ processing of raw ingredients into delicious, nutritious food. Specifically, fluid-mediated cooking in microgravity poses challenges in terms of fluid and thermal management.We are designing a device, nicknamed H0TP0T, to enable liquid-mediated cooking in partial gravity conditions. We'll test the first prototype on a parabolic flight in May 2021, made possible by an exciting collaboration with the MIT Media Lab Space Exploration Initiative.[1] Perchonok, Michele H., Maya R. Cooper, and Patricia M. Catauro. ""Mission to Mars: food production and processing for the final frontier."" Annual review of food science and technology 3 (2012): 311-330. https://www.annualreviews.org/doi/abs/10.1146/annurev-food-022811-101222.",2022-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/07/08/20210523_Steve-Boxall_PSM_MIT_Research_ZG548_7346.jpg,383907,,2023-09-07T14:56:57.973Z,[],True,hotpot,,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",H0TP0T,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2021-05-19T19:06:33.933Z,"Through the ages, artists, writers, and filmmakers have always been inspired by space. Artists’ visionary depictions of space as an environment for people have influenced the scientific and engineering feats we know so well today. How will artists continue to inspire the future of space exploration? What are the opportunities and challenges in the creation of art for space, and art depicting space and space technologies? In 2019, the Space Exploration Initiative announced an open call to invite artists to submit artworks for an exciting ISS launch opportunity.  After three rounds of reviews, nine groups of artists are selected to be on board Sojourner 2020, an international art payload.Sojourner 2020 (a 1.5U size unit, 100mm x 100mm x 152.4mm ) will be launched into low Earth orbit for about 30 days. It features a three-layer telescoping structure which creates three different “gravities”: zero gravity, lunar gravity, and Martian gravity. Each layer of the structure rotates independently. The top layer remains still in weightlessness, while the middle and bottom layers spin at different speeds to produce centripetal accelerations that mimic lunar gravity and Martian gravity, respectively. Each layer carries 6 pockets that can hold projects.",2020-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/uuid/19237341-d59f-4a77-9de8-c4481abae92e,383924,,2023-09-07T15:38:37.334Z,[],True,sojourner-2020-an-international-art-payload-to-iss,,[],Sojourner 2020 | An international art payload to ISS,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2021-05-19T19:17:21.336Z,"Impenetrable darkness, extreme pressure, cold water, and disorienting equilibrium. Deep sea creatures live in a world that is closer to outer space than Earth's land. We shall listen to the callings from the ocean floor, as the legends tell our reverence to the deep. Everything from the past and the past-to-be falls to the ocean floor. The world of the deep swallows and transforms all: organism, lava, plastic.",2021-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2019/08/18/ZG492_steve-boxall-20190815_0664.jpg,383932,,2023-09-07T15:37:32.661Z,['xxxxxxin@media.mit.edu'],True,mollastica-from-deep-sea-to-deep-space,,[],Mollastica - From Deep Sea to Deep Space,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2021-05-25T14:57:05.602Z,"Gravity Proof is a performance and mission to prepare and cook bread in space. Inspired by ancient recipes and archaeological bread remains, this exploration reimagines the cosmic past, present, and future of bread. We are producing experimental methods and data to capture the materials, processes, and sensory attributes of bread as it ventures off our home planet.  An ambassador of Earth, dough breathes and dances through simulated zero gravity on a parabolic flight.  A story shared through texture and aroma, bread offers a glimpse into another space and time.This multi-part work includes an experimental performance on a parabolic flight, a video, bread recipes, and space fermentation technology. Research is ongoing. Video is on view at the Expo 2020 Dubai from October 1 - March 31, 2021.",2022-05-01,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/10/05/MicrosoftTeams-image (4).png,383992,,2021-10-06T12:46:18.505Z,['mcoblent@media.mit.edu'],True,gravity-proof,2021-05-01,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Gravity Proof: Speculating Cosmic Futures,PUBLIC,https://www.maggiecolentz.com/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2021-05-25T15:42:30.419Z,"Following The Power of Without agenda, the City Science group proposes the Informality Crowdsourced Database. This platform aims to help us understand the strength of each community in addition to the struggles.",,[],https://dam-prod2.media.mit.edu/x/2021/05/25/PoW for website_1.gif,384000,,2021-08-23T19:54:49.990Z,"['kll@media.mit.edu', 'gbabio@media.mit.edu', 'maitanei@media.mit.edu', 'alonsolp@media.mit.edu', 'mdchurch@media.mit.edu']",True,informality-database,2019-01-01,"['design', 'developing-countries', 'energy']",Informality Crowdsourced Database,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-05-26T15:56:18.791Z,"Traditional additive manufacturing processes, especially those that make use of liquid resin as the feedstock, are constrained by the gravity environment on Earth. Gravity prevents extrusion into free space without sagging, which requires the use of support material to prevent. In a microgravity environment, extrusions into free space that would normally sag can proceed unimpeded, allowing for the creation of structures that could not be produced in a gravity environment. Our original approach demonstrated this using a hybrid extrusion process we developed, which can create 3D structures (such as coils) from feedstocks of flexible metal wire and photocurable resin, where the wire is bent into a 3D shape and evenly coated with the resin. We then pivoted to creating flexible ""skins"" in the desired shape that would then be filled with liquid resin and cured with UV light through the skins. These skins are very lightweight and flexible and are incapable of holding their shape in 1g, but still form the resin into the desired geometry. These skins can be produced on Earth and transported to space compactly, before being unfurled and used to create the shapes out of resin. After the resin cures, the skins can be cut away leaving the finished part.In November 2022, this experiment was flown to the ISS aboard CRS-26, a NASA  resupply mission operated by SpaceX with their Cargo Dragon capsule atop a Falcon 9 rocket, for a 45-day stay in space. Multiple skins were filled with resin, and then cured along with static pre-filled samples.NASA chose to feature this experiment in their pre-launch coverage:NASA Press articleNASA VideoInterview with team member Martin Nisser before the Nov 22 launch attemptAdditional sample of varying geometries will be flown on an additional parabolic flight, focusing on rapid filling and curing of the skins - to fit in the short windows of microgravity that the parabolic flight affords. The increase mass and volume allowance of the parabolic flight will allow for many more samples of additional geometries to be generated, along with more powerful curing lights.",2023-12-31,['space-exploration'],https://dam-prod2.media.mit.edu/x/2022/11/29/jsc2022e083566~orig.jpg,384026,,2023-07-19T18:21:28.571Z,"['aekblaw@media.mit.edu', 'seanauff@media.mit.edu', 'cwwang@media.mit.edu']",True,microgravity-hybrid-extrusion,2021-01-01,"['robotics', 'architecture', 'art', 'consumer-electronics', 'data', 'food', 'music', 'wearable-computing', 'sensors', 'space', 'biology', 'technology', 'wellbeing', 'zero-gravity', 'chemistry']",Microgravity Extrusion,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-06-02T21:52:04.409Z,"OctoStudio is a free coding app is available at octostudio.orgOctoStudio that transforms how young people use mobile phones and tablets, enabling them to create their own stories and games anytime, anywhere. Children can take photos and record sounds, bring them to life with coding blocks, and send their projects to friends and family.Scheduled to launch on October 24, 2023, OctoStudio is developed by the Lifelong Kindergarten research group at the MIT Media Lab; the Lifelong Kindergarten group also created Scratch, the world’s most popular coding language for kids. OctoStudio is completely free of charge, without any ads or in-app purchases.",,"['lifelong-kindergarten', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/04/18/octo-headerArtboard 1_pfZcdG3.png,384065,,2023-10-23T21:55:57.329Z,"['nrusk@media.mit.edu', 'khanning@media.mit.edu', 'mres@media.mit.edu', 'shrutid@media.mit.edu', 'hisean@media.mit.edu', 'rupal@media.mit.edu', 'csadler@media.mit.edu', 'thaisx@media.mit.edu', 'eschill@media.mit.edu', 'rschamp@media.mit.edu', 'kchadha@media.mit.edu', 'gabaree@media.mit.edu', 'carolcr@media.mit.edu', 'katelynm@media.mit.edu', 'jaleesat@media.mit.edu']",True,octostudio,2020-04-15,"['robotics', 'design', 'artificial-intelligence', 'learning-teaching', 'neurobiology', 'code', 'civic-action', 'neural-interfacing-and-control', 'marginalized-communities', 'women']",OctoStudio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-06-07T18:01:11.607Z,"RoboScope is a transformable, tangible-interface developed for the CityScope platform. This interface allows for on-demand, three-dimensional, manipulation and visualization of urban form. Like other CityScope interfaces, RoboScope has been designed to facilitate user understanding of urban data and simulations in a visual and tangible format. RoboScope  allows for on-demand, three-dimensional, visualization and manipulation of urban form. The interface is composed of an array of 3D pixels organized in a grid used to abstract the urban area under investigation. Each 3D pixel can be controlled volumetrically to represent topography and urban from, as well as represent urban characteristics via  illumination. In addition, interaction through push-pull manipulation allows users to update urban charactirestics in near real time.",,['city-science'],https://dam-prod2.media.mit.edu/x/2021/06/07/FullBse_2020-Nov-10_02-17-25PM-000_CustomizedView14231733974_png_alpha-10434.png,384089,,2021-06-23T18:55:21.421Z,"['markuse@media.mit.edu', 'thomassl@media.mit.edu', 'kll@media.mit.edu']",True,roboscope,2019-12-12,[],RoboScope,PUBLIC,http://codigogenerativo.com/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2021-06-15T13:46:35.227Z,An application that drains your computer's resources at the rate of COVID-19 infections and deaths in your state. It exhausts your CPU power at the rate of local infections and dims your screen at the rate of local COVID-19- related deaths.,,[],https://dam-prod2.media.mit.edu/x/2021/06/15/pandemic-pulse_laptop_off_hi-res_MBt8D0s.png,384307,,2021-06-15T14:04:02.970Z,['jblinder@media.mit.edu'],True,pandemic-pulse,2020-09-01,"['design', 'art', 'data-visualization']",Pandemic Pulse,PUBLIC,https://justin.work,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2021-06-16T16:44:27.021Z,#ZoomADay was a year long project exploring the creation and use of synthetic characters and deep fakes for use in online telepresence and communications.,2022-08-01,[],https://dam-prod2.media.mit.edu/x/2021/06/16/Day0016_WdSzHLf.gif,384318,,2022-08-09T00:58:18.728Z,['novysan@media.mit.edu'],True,zoomaday-ai-synthesized-characters,2020-03-15,"['computer-vision', 'human-computer-interaction', 'augmented-reality', 'entertainment']",ZoomADay - AI Synthesized Characters,PUBLIC,https://novysan.com,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2021-06-22T00:56:20.326Z,"We extend previous studies on the impact of masks on COVID-19 outcomes by investigating an unprecedented breadth and depth of health outcomes, geographical resolutions, types of mask mandates, early versus later waves and controlling for other government interventions, mobility testing rate and weather. We show that mask mandates are associated with a statistically significant decrease in new cases (-3.55 per 100K), deaths (-0.13 per 100K), and the proportion of hospital admissions (-2.38 percentage points) up to 40 days after the introduction of mask mandates both at the state and county level. These effects are large, corresponding to 14% of the highest recorded number of cases, 13% of deaths, and 7% of admission proportion. We also find that mask mandates are linked to a 23.4 percentage point increase in mask adherence in four diverse states. Lastly, using a large novel survey dataset of almost half a million people in 68 countries, we introduce the novel results that community mask adherence and community attitudes towards masks are associated with a reduction in COVID-19 cases and deaths. Our results have policy implications for reinforcing the need to maintain and encourage mask-wearing by the public, especially in light of some states starting to remove their mask mandates.",2021-06-23,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2021/06/22/3 by 2 earlier_vs_later_waves.jpg,384406,,2021-06-23T16:49:33.643Z,"['dhaval@media.mit.edu', 'kdinakar@media.mit.edu', 'sandy@media.mit.edu']",True,mask-association,2020-07-11,"['public-health', 'medicine', 'covid19']","Association between COVID-19 Outcomes and Mask Mandates, Adherence, and Attitudes",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-06-29T20:02:19.296Z,"Foodborne pathogens are a major source of human morbidity, food recalls, and economic loss. Current detection methods for foodborne pathogens are culture-based and highly sensitive, but involve multiple labor-intensive steps and are available at only a handful of labs in the US. This means that food is often already in consumer hands when contamination is confirmed, leading to disease outbreaks and costly recalls. Rapid tests have emerged to address this challenge, but they typically lack sensitivity and are not necessarily compatible with assays used to identify where and how an outbreak originated. This project aims to develop a scalable, distributable, and sensitive foodborne pathogen analysis platform that returns results in 4-6 hours. The proposed method is based on affinity ligand-based whole-cell capture, concentration, and culture for sensitive pathogen detection in a format that preserves cells for further downstream testing.  The goal is to develop a food safety test that is suitable for implementation in an integrated benchtop fluidic device for use by moderately-trained technicians directly at production sites. Using this novel platform, the research team aims to demonstrate detection of Salmonella enteritidis from a variety of food matrices. This research represents a potential advance towards faster on-site foodborne pathogen testing that is complementary with regulatory diagnostic workflows, decreasing the cost of foodborne microbial contamination in terms of both human lives and economic losses.",2022-08-26,['health-0-0'],https://dam-prod2.media.mit.edu/uuid/5f18234c-53dd-428c-81bd-f83010fba0fb,384465,,2022-10-14T21:23:28.012Z,['pratiks@media.mit.edu'],True,on-site-analysis-of-foodborne-pathogens-using-density-shift-immunomagnetic-separation-and-culture,,"['food', 'health', 'public-health']",On-site Analysis of Foodborne Pathogens Using Density-Shift Immunomagnetic Separation and Culture,PUBLIC,https://www.pratiks.info/,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-06-30T15:03:24.495Z,A space architecture project for the Mechanical Artifact: Ultra Space course in partnership with the Center for Collaborative Arts and Media (CCAM) and the School of Architecture at Yale. By Yong Choi,,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/06/30/IMG_6110.jpg,384474,,2021-06-30T19:01:43.768Z,[],True,puzzle-box,,[],Puzzle Box,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2021-06-30T15:23:05.312Z,A space architecture project for the Mechanical Artifact: Ultra Space course in partnership with the Center for Collaborative Arts and Media (CCAM) and the School of Architecture at Yale. by Tyler Krebs,,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/06/30/1_EPv-uepIYiz6TmR0LL79xA.jpeg,384483,,2021-06-30T19:00:53.456Z,[],True,space-vase,,[],Space Vase,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2021-06-30T15:28:28.513Z,A space architecture project for the Mechanical Artifact: Ultra Space course in partnership with the Center for Collaborative Arts and Media (CCAM) and the School of Architecture at Yale. by Sam Landy,,['space-exploration'],https://dam-prod2.media.mit.edu/x/2021/06/30/20210520_Steve-Boxall_ZG547-MIT_6528.jpg,384491,,2021-06-30T19:00:06.703Z,[],True,wearables-for-orientation-wearables-for-communication,,[],"Wearables for Orientation, Wearables for Communication",PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-07-08T18:52:27.597Z,"As tools head out into the world, we watch carefully. https://www.sciencemag.org/news/2021/06/are-advertisers-coming-your-dreamshttps://www.popularmechanics.com/technology/a36719140/sleep-ads-dream-implantation/https://thehill.com/changing-america/well-being/mental-health/561650-neuroscientists-warn-advertisers-are-trying-tohttps://www.theguardian.com/media/2021/jul/05/advertisers-targeted-dream-incubation",2021-07-02,['fluid-interfaces'],,384553,,2021-07-12T16:21:32.125Z,[],False,dream-ethics,2021-07-08,[],Dream Engineering Ethics Initiatives,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2021-07-10T17:04:14.011Z,"Audio-visual data set containing around 21.6 hours of parent-child interaction during story reading.The DAMI-P2C dataset consists of audio-visual recordings of 2 sessions from each of the 34 parent-child dyad participating in the story reading activity. The 34 families with children between the ages of 3-7 years were recorded for two 45-minute in-lab sessions (Huili, et al., 2020). The parent and child select any books from a corpus of 30 storybooks that were digitized on a touchscreen tablet. The dyad was always a pair of one parent and one child.",,['personal-robots'],https://dam-prod2.media.mit.edu/uuid/9b08e939-993a-4515-bdb8-47bdc25ecc58,384594,,2022-08-24T01:29:30.581Z,"['xiajie@media.mit.edu', 'sharifah@media.mit.edu', 'haewon@media.mit.edu', 'hchen25@media.mit.edu']",True,dami-p2c,2021-07-10,"['robotics', 'human-computer-interaction', 'artificial-intelligence', 'cognition', 'data']",Dyadic Affect in Multimodal Interaction - Parent to Child (DAMI-P2C) Dataset,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-07-30T15:52:22.215Z," When robots, devices, or humans in an online network want to work together as peers, they must communicate with each other to reach their collective goals. If the group’s messages are not secure, outsiders might learn the goals of the group or even impersonate its members. In this work, we demonstrate the first multi-agent system able to autonomously secure its peer-to-peer communication while collaborating to achieve a common goal, using an authenticated data structure known as a Merkle tree. The demonstration of Merkle trees as a successful decentralized cooperation mechanism has significant implications for any future complex systems application in which security or privacy plays a crucial role—networks of IoT devices, multi-robot factories, healthcare automation, and well beyond.",,[],https://dam-prod2.media.mit.edu/uuid/ac390189-db85-4552-aa50-48cce2fc27f9,384751,,2021-07-30T16:04:15.170Z,[],False,secure-and-secret-cooperation-in-robot-swarms,,"['robotics', 'privacy', 'security', 'blockchain', 'data-visualization', 'data-science']",Secure and secret cooperation in robot swarms,PUBLIC,http://www.eduardocastello.com,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-07-31T14:27:26.153Z,"Ancient textiles have played a major role in the social, economic, and religious structures of communities around the world. Iconic, indigenous clothes are imbued with unique designs and patterns. Some are believed to have magical powers and designed to carry specific meanings and wishes. Often complementing traditional dance and music, these textiles amplify a sense of community, identity, and expression.Motivated by the craftsmanship and connections of cultural textiles such as Javanese Batik or Balinese Ikat to their traditional performance arts, we began to apply an artistic approach into technological textile design and merge new materials, sensing technologies, and digital fabrication with contemporary dance and music into one united and harmonious piece of object and performance.  Tapis Magique is a pressure-sensitive, knitted electronic textile carpet that generates three-dimensional sensor data based on body postures and gestures and drives an immersive sonic environment in real-time. Demonstrating an organic and expressive relationship between choreography and music has been a never-ending feat in the performance arts, as seen in previous work by Cage and Cunningham, Horst and Graham, or Stravinsky and Balanchine. Our work unveils dancers' creative, unconventional possibilities of agency, intimacy, and improvisation over the music through a textile interface.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2021/07/31/IMG_8797-2a_12x249Q.jpg,384769,,2022-06-29T19:39:14.016Z,"['irmandy@media.mit.edu', 'ddh@media.mit.edu']",True,tapis-magique,,"['design', 'human-computer-interaction', 'architecture', 'art', 'consumer-electronics', 'entertainment', 'environment', 'fashion', 'human-machine-interaction', 'manufacturing', 'music', 'sensors', 'interfaces', 'performance', 'interactive', 'community', 'materials', 'textiles']",Tapis Magique: A Choreomusical Interactive Carpet,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2021-08-11T13:50:33.397Z,"""Autonomous operations are critical for the success, safety, and crew survival of NASA deep space missions beyond low Earth orbit, including Gateway."" Building on our team’s prior work in the on-body miniaturized robotic swarm, we propose a suite of autonomous robotic swarms for spacecraft assembly and external servicing in lunar orbit.",,"['responsive-environments', 'space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/uuid/e805516f-d0b7-45d0-aca0-5fc17a576f1f,384850,,2023-09-07T22:30:22.481Z,"['aekblaw@media.mit.edu', 'seanauff@media.mit.edu', 'fzliu@media.mit.edu']",True,astroant-1,2021-01-01,"['robotics', 'sensors', 'space']",AstroAnt,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-09-12T14:46:46.545Z,Feeling Climate Crisis is a series of kinetic sculptures that visualize the ongoing climate crisis. Movements of the sculptures express data about changes in vital signs of our planet. These sculptures are made with found natural objects juxtaposed with electromechanical elements. This series attempts to create an experience that allows viewers to feel climate data.,2021-09-15,"['fluid-interfaces', 'future-sketches']",https://dam-prod2.media.mit.edu/x/2021/09/12/Scan 75 copy.jpeg,385384,,2021-09-13T15:55:07.592Z,['gauravp@media.mit.edu'],True,feeling-climate-crisis,2020-05-01,[],Feeling Climate Crisis,PUBLIC,http://gauravpatekar.in/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2021-09-14T16:14:34.829Z,"We are pleased to introduce the newest member of the City Science Network, Ho Chi Minh City (HCMC), Vietnam. As part of this collaboration, the MIT City Science team will work with the Architecture Research Center, referred to as ARC, to understand, model and predict a series of new developments in District 4. District 4 is located on the Saigon River, includes the port, and is part of the urban core of the city. The district is slated for several new developments, and has become a prime location for real estate due to the economic impact of the area and the many highly skilled workers living there. Researchers anticipate that modeling the proposed developments against the current landscape of the city can help better understand interventions and opportunities. Researchers will consider factors like mobility, housing, and access to amenities, as well as broader themes like livability, sustainability, and innovation potential. Both teams believe that the CityScope platform will play an essential role in both visualizing the current and proposed scenarios, as well as contributing to the data- enabled decision making that is essential to these new large scale projects. Teams agree that the research will support a shared goal to increase equity and resilience in the community.“HCMC is one of the most creative and vibrant cities in Asia, and rapidly becoming a center of cultural and economic exchange. I am excited that the Architectural Research Center of HCMC has become the newest member of our international network of City Science Labs, and we are proud to play a role in imagining an exciting future for the city.” — Kent Larson, Director of MIT City Science",2022-10-01,['city-science'],https://dam-prod2.media.mit.edu/x/2021/09/15/WhatsApp Image 2021-04-20 at 20.52.13.jpeg,385411,,2023-01-10T20:53:44.283Z,"['ryanz@media.mit.edu', 'agrignar@media.mit.edu', 'markuse@media.mit.edu', 'mdchurch@media.mit.edu', 'alonsolp@media.mit.edu', 'kll@media.mit.edu', 'doorleyr@media.mit.edu', 'thomassl@media.mit.edu']",True,city-science-lab-ho-chi-minh-city,2021-01-05,[],City Science Lab @ Ho Chi Minh City,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-09-14T16:48:18.420Z,"MetaSense: Integrating Sensing Capabilities into Mechanical Metamaterial. J. Gong*, O. Seow*, C. Honnet*, J. Forman, S. Mueller",,['tangible-media'],https://dam-prod2.media.mit.edu/x/2021/09/15/banner.JPG,385420,,2021-09-25T05:04:20.725Z,"['honnet@media.mit.edu', 'jackform@media.mit.edu']",True,metasense,2020-12-01,"['human-computer-interaction', 'consumer-electronics', 'entertainment']",MetaSense,PUBLIC,http://honnet.eu,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2021-09-15T17:19:14.862Z,"The superpose installation mediates continuously changing wave phenomena within a space that are visible, audible, and react to the presence of a person in the space. Connecting space and sound with one another to create a dynamic multi-sensory environment with the help of the visual elements of waves in water. Sound, an invisible stimulus moving through space, presents the audience with a connecting thread between visible space and invisible physical vibrations. superpose explores the potential of interaction and experiential design to create holistic experiences that offer a new understanding of how sound operates as a physical phenomenon within space: Do audiences understand how sound waves propagate through space? Do they have to? ",2021-10-31,['opera-of-the-future'],https://dam-prod2.media.mit.edu/x/2021/09/15/superpose_motion_1600px_fnLZP3k.jpg,385445,,2021-10-13T19:16:50.074Z,['karstens@media.mit.edu'],True,superpose,2021-04-01,"['design', 'architecture', 'art', 'music']",superpose - A connected experience of sound and space,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-09-17T02:39:19.211Z,What will be the impact of shared autonomous micro-mobility systems? Will autonomy make the micro-mobility systems even more attractive?  This work explores this and many other questions in an ad-hoc agent-based simulation that explores the fleet behavior of shared autonomous bicycles.,,['city-science'],https://dam-prod2.media.mit.edu/uuid/d44bc2f3-8e1f-464c-a566-5e86a2c18e57,385472,,2023-10-03T13:44:33.532Z,"['naroa@media.mit.edu', 'alonsolp@media.mit.edu', 'inigo@media.mit.edu']",True,autonomous-micro-mobility-fleet-simulation,2020-09-01,"['robotics', 'design', 'data', 'energy', 'environment', 'transportation', 'urban-planning', 'engineering', 'climate-change', 'autonomous-vehicles']",Autonomous micro-mobility simulation study,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-09-20T12:10:40.993Z,"E-maki is a collection of ancient, painted scrolls accompanied by a cross-reality app that give contextual information and interactive experiences to museum exhibition's audiences. We used and took inspirations from the ""Four Seasons"" hand scroll made in the Edo period by Nishikawa Sukenobu that highlights a vast majority of Japanese historical objects from tablewares, musical instruments, to clothing and textiles in multiple scenes. Information gathering, photogrammetry, and 3D modeling were performed to the various artifacts found in the MFA Boston: Arts of Japan Collections that belong within the same period as these scrolls. We then designed augmented and virtual reality apps that connect several markers spread across the scrolls to their information and 3D visual representation and provide ability for the users to explore forms and textures of the selected museum artifacts in detail. By transporting the audience into a Japanese decorated room and garden, we also created  a contextual audio-visual digital environment that immerses the users based on the artifacts, narrative, and seasonal settings of the scroll.This project is supported by the Museum of Fine Arts, Boston as a part of Computational Design Lab/Design Experience Workshop.",,['responsive-environments'],https://dam-prod2.media.mit.edu/x/2021/09/20/teahouse.png,385489,,2021-09-20T20:50:53.686Z,['irmandy@media.mit.edu'],True,e-maki,2020-04-01,"['design', 'virtual-reality', 'art', 'augmented-reality', 'interactive', 'visualization']",E-maki: XR-enabled Scroll for Interactive Museum Artifacts,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-09-28T17:53:24.159Z,WiStress is a system that can monitor a user’s stress passively using wireless signals. WiStress can be installed on a desk or near a couch to monitor a nearby user’s stress levels. It works by continuously transmitting ultra-low-power wireless signals that reflect off the human body and capturing these reflections in order to infer the person’s stress level. It is the first to demonstrate the potential of using such signals to monitor a person’s stress. ,2021-12-31,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2021/09/30/overall_jpg.jpg,385572,,2021-10-05T19:41:42.164Z,"['fadel@media.mit.edu', 'unsoo@media.mit.edu']",True,wistress,2020-06-01,"['human-computer-interaction', 'artificial-intelligence', 'bioengineering']",WiStress: Contactless Stress Monitoring,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2021-10-02T22:42:24.425Z,"RFusion is a robotic system that can search for and retrieve items in line-of-sight, non-line-of-sight, and fully occluded settings. It consists of a robotic arm that has a camera and antenna strapped around its gripper, and it uses both of them to find and retrieve target items.  The robot introduces two new primitives: RF-visual sensing and RF-visual reinforcement learning to efficiently localize, maneuver toward, and grasp target items. RFusion is very accurate. It localizes target items with centimeter-scale precision and achieves 96% success rate in retrieving fully occluded objects, even if they are under a pile. Thus, it paves the way for novel robotic retrieval tasks in complex environments such as warehouses, manufacturing plants, and smart homes.",,['signal-kinetics'],https://dam-prod2.media.mit.edu/x/2021/10/03/RFusion-gif.gif,385622,,2021-10-05T13:59:04.221Z,"['tarab@media.mit.edu', 'iperper@media.mit.edu', 'mergen@media.mit.edu', 'fadel@media.mit.edu']",True,rfusion,2021-10-03,"['robotics', 'computer-vision', 'artificial-intelligence', 'sensors', 'rfid']",RFusion: Robotic Grasping via RF-Visual Sensing and Learning,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2021-10-03T21:08:55.136Z,The Affective Computing group is collaborating with Weill Cornell Medicine to build empathetic behavior in physicians working with opioid use disorder patients. The project has two main components:Use real clinical interactions to define the affective expressions (facial) in physician-patient interactions that are most and least favorable to patientsBuild a tool to train medical students to empathetically engage with a simulated patient who has opioid use disorder. The student will be tasked with performing a standard substance use assessment through interacting with the simulation. The tool will analyze the student’s speech content and facial expressions and provide real-time feedback through the patient’s responses. It will also provide an overt breakdown of feedback after the interaction.This project is funded by the Foundation for Opioid Response Efforts (FORE) and Weill Cornell Medicine.,2023-08-01,['affective-computing'],https://dam-prod2.media.mit.edu/x/2021/10/03/training image.png,385632,,2022-08-25T16:27:31.688Z,"['ilak@media.mit.edu', 'picard@media.mit.edu', 'harrisc@media.mit.edu']",True,empathetic-behavior-training-for-physicians-working-with-patients-who-have-opioid-use-disorder,2021-08-01,[],Empathetic behavior training for physicians working with patients who have Opioid Use Disorder,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2021-10-04T18:31:06.088Z,"Bitcoin’s (un)common goodDigital Currency Initiative at the MIT Media Lab Launches New Bitcoin Software and Security Effort with Industry LeadersThanks to millions of open source developer hours over the past 12 years, and a burgeoning and supportive ecosystem, Bitcoin is no longer an obscure cryptographic toy. It is now an open-source financial network that secures on the order of $1T of value. As the use of Bitcoin grows, and as it becomes more deeply embedded into our societies, the security of the network must grow and strengthen alongside it. Yet, as a common good, there is no one single Bitcoin protector or guardian to take on this formidable task. By design, there is no central command. And while this presents significant logistical challenges, it is also the distinguishing feature perhaps most unique to Bitcoin: no central point of failure. Bitcoin's nearly-uninterrupted operation over the years is a testament to the power of decentralization.That said, a lack of central organization does not mean there is no need for thoughtful focus and coordination to continually reinforce Bitcoin’s security and preemptively shore up any vulnerabilities. Moving into the future, strategically supporting the development of Bitcoin will require understanding and proactively addressing a particular type of risk: protocol risk. Unlike traditional assets, Bitcoin is software running on a decentralized network. Bitcoin's security is predicated on the accuracy and robustness of the software and hardware running it, and the actions of those participating in the network. With this in mind, DCI has established a new Bitcoin Software and Security Effort.The four-year research and development program is designed to continue to harden the Bitcoin network and steward the industry's commitment to funding open source software. This will include contributing to Bitcoin Core development as well as rigorous long-term research, such as investigations into the stability of fee-based rewards and software to provide strong robustness and correctness guarantees. It will also include attracting domain experts in network and operating system security, compilers, programming languages, and more to join the effort.A second important goal of this effort is to help reduce bottlenecks in the development ecosystem which might lead to centralization. Crucially, effort must be taken to ensure that critical knowledge, not only around the technology itself, but also around key processes, is systematically recorded and passed down to harden security and improve resilience. The objective of DCI’s new program is to contribute neutral, expert resources to improving the robustness of the Bitcoin protocol. Bitcoin’s security is foundational to the underlying technology’s continued evolution, as well as the broad realization of the public-good promises of digital currencies.To fund the new four-year program, DCI has reached out to a cross-section of industry leaders, and we're proud to share that we’ve received $4M in charitable contributions towards our $8M goal. These resources will be used to hire open source developers and researchers, and to kick off an ecosystem-wide focus on longer-term strategies to harden the underlying protocol. Over the coming four years, DCI will focus its contributions to the Bitcoin community in four core areas of R&D: Sustaining a growing senior team of Bitcoin developers (Bitcoin Core and related infrastructure), moving from three to eight researchers and engineers, and expanding our expertise to meet new threatsBuilding up long-term defenses against layer-1 Bitcoin Core bugs by, for example supporting modularization efforts, researching safer programming paradigms and languages, and adding formal verification where possiblePreemptively investigating, monitoring, and strengthening the software against attacks, as well as researching the long-term economic security of the system  (e.g. the economic security of fees vs. inflation)Improving automation, writing new tests and security tools, and decreasing reliance on scarce expertsFounding contributors to the DCI program include Alex Morcos (MIT '98) and Suhas Daftuar (Chaincode), CoinShares, Meltem Demirors (MIT Sloan '15), Jack Dorsey, Fidelity Digital Assets, Cameron and Tyler Winklevoss (Gemini), Reid Hoffman, Michael Saylor (MicoStrategy, MIT '87), and John Pfeffer.Meltem Demirors, chair of the funding efforts, notes, “At CoinShares, we have a strong commitment to funding research and development that makes Bitcoin even more sustainable and secure. We’ve enjoyed collaborating with academic institutions like MIT, as well as hiring in-house researchers to further these goals. In the years ahead, I look forward to working with industry peers to promote a culture of sustainable open source funding in the Bitcoin community.”Michael Saylor, Chairman and CEO of MicroStrategy, adds, “Bitcoin is the most important innovation since the advent of the internet, and it is our responsibility to invest not only in the asset, but also in the underlying infrastructure that is maintained and improved by open-source developers and nonprofit institutions like MIT’s DCI.”",,"['digital-currency-initiative-dci', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/52810052-355f-4b3e-a758-b325fb5d1fc2,385643,,2023-09-07T23:15:42.207Z,['narula@media.mit.edu'],True,bitcoin-security-initiative,2021-08-01,[],Bitcoin Security Initiative,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-10-07T14:14:17.449Z,"Using OmniFiber technology, we fabricated a type of undergarment that singers can wear to monitor and play back the movement of respiratory muscles, to later provide kinesthetic feedback through the same garment to encourage optimal posture and breathing patterns for the desired vocal performance. Although this initial testing is in the context of vocal pedagogy, the same approach could be used to help athletes to learn how best to control their breathing in a given situation, based on monitoring accomplished athletes as they carry out various activities and stimulating the muscle groups that are in action. ",2022-09-01,['tangible-media'],https://dam-prod2.media.mit.edu/x/2021/10/08/Breathing Garment_ML.png,385687,,2022-03-21T07:45:20.705Z,"['ozgun@media.mit.edu', 'ishii@media.mit.edu']",True,breathing-garment,2021-07-01,"['robotics', 'design', 'human-computer-interaction', 'health', 'human-machine-interaction', 'music', 'materials', 'fabrication', 'human-augmentation']",Breathing Garment: A respiratory regulation garment for voice pedagogy,PUBLIC,http://www.ozgunkilic.com/,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2021-10-07T16:31:48.955Z,"A custom circuit board is embedded into robotic toys which makes them come alive when a sleeper begins to dream. As you slip into REM sleep, they wake into the world.A Hypnodyne EEG tracks brainwave changes and eye movement frequency. A sleep staging algorithm automatically detects REM sleep and broadcasts signal to a bluetooth receiver embedded into each animatronic animal. When wake is detected, movement stops. This art piece will be shown at the V&A Museum in London as part of their Alice: Curioser and Curioser exhibition.  This work was funded in part by the Council for the Arts at MIT.",2022-03-16,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/10/07/Butterfly.jpeg,385700,,2021-10-07T18:00:50.762Z,"['pattie@media.mit.edu', 'adamjhh@media.mit.edu', 'abyjain@media.mit.edu']",True,toy-stories,2021-10-07,[],Toy Stories,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-10-07T17:04:18.135Z,"This project is a pipeline to algorithmically generate visuals of dreams collected from large groups during the COVID pandemic. Dreams are changing as we speak, collectively. Communal traumas change communal dreams in ways we may not perceive; After  9/11 there was a systematic change in dream imagery across diverse groups of individuals, but it was not perceptible in obvious images of airplanes or urban explosions in dream reports.  Instead dream imagery intensity, a marker of emotional arousal, increased after 9/11 across all sort of dreams in a country full of all sorts of feeling. These communal dream changes were visible only at the population level, as collective cognition. So, how is the current communal trauma of COVID changing our dreams, what we can imagine, how does this fear shift our internal collective world?Cyber Key to Dreams lets us ask this question in a process of collecting, mapping, and interpreting dreams at the level of the collective.  To participate in mapping the unmappable, please submit your dream data here. Your dream will be added to a growing corpus, sorted, placed nearest its conceptual kin. You will be given an image of a dream in return for your submitted text.  Your dream will become part of collective sculptures, seen below. Your dream will be connected to other dreams in video artworks, like these. These images function as both a canvas for us to see how computers represent fundamentally uncomputable human dream cognition, and a Rorschach test for us to see how humans understand fundamentally uninterpretable computational dreams.Consider the cluster of dream analysis of 4,725 dreamers from 2020 shown below: Do these dream themes, and the link between them, resonate with you when you when you consider the past year of intensified dreaming? But then what is lost when we compute a dream, and what is found? What is lost in the patterned, computed collective, the tools with which surveillance capitalism maps our tweets, stories, connections, dreams? As we become more completely colonized and seen by computation, we must keep in mind the unmappable, illegible parts, those that resist by their ambiguous nature. We must keep in mind the inherent freedom in dreaming.These dreams and associated visuals have been combined into collective sculptures, still in process, in collaboration with the artist Agnieszka Kurant. This work is going to be shown at the V&A Museum's Alice: Curiouser and Curiouser exhibition and the Miaau (Museo intangible de artes audiovisuales) exhibition of the Bienal de Artes Mediales (BAM) Santiago, Chile.Here is Agnieszka Kurant, co-creator of Cyber Key to Dreams,  describing the artwork:""Surveillance capitalism tries to colonize every single aspect of our lives. We thought that our dreams were among the last private territories resisting colonization by corporations and governments. But today corporations have developed tools allowing for the searching and aggregating of any descriptions and recounts of our dreams left online. This actually isn’t a new endeavor to penetrate our dreams. In the 1920s, when Freud’s psychoanalysis was weaponized as a novel technique to better understand colonial subjects, anthropologist Charles Gabriel Seligman, who was a longtime adviser to colonial governments, built a database of colonial dreams within the diverse cultures under British rule. And in 1931 he used a BBC radio broadcast to solicit dreams and auto-interpretations from ordinary people in Britain. In 1945, in Jamaica, a study sponsored by the Colonial Office amassed dream reports and Rorschach inkblot test results from adults and children. Other examples of dream surveillance took place in many countries of the former Soviet Bloc. In 1949, Chris Marker and Alain Resnais did probably the first project based on crowdsourcing to TV audiences, in which they asked viewers to submit their dreams. The most interesting ones were staged and filmed by the two authors in a series of TV programs named Key to Dreams. Soon French TV considered the project too controversial and it was taken down.My project Cyber Key to Dreams is a collaboration with computer engineers and researchers: Adam Haar Horowitz, Pat Pataranutaporn, and Eyal Perry from the MIT Media Lab and MIT Dream Lab. We built a system which allows for the crowdsourcing of dream reports and a dream questionnaire. We employ a combination of several AI “text to image” algorithms, in order to analyze these various types of information and to generate sequences of dream images. The next step is to create aggregated, collective dream images on the bases of the recurring patterns, figures, objects, such as the ghosts appearing in thousands of Covid dreams. I propose to use AI machine learning to look for recurring patterns in our dreams, in our collective unconscious. In a way I am treating an entire society as one psychoanalytic patient. Perhaps the identification of these patterns can help find ways to heal the collective subjectivity. Freud talked about the phenomenon of condensation happening in our dreams, which consists of our unconscious fusing together many people we meet in our conscious life into one nonexistent person, or fusing many objects into one condensed object. A similar process happens with machine learning, which produces nonexistent cats, dogs, or human faces on the bases of millions of actual cats, dogs, or humans. In that sense the black box of the AI to some degree resembles the black box of our unconscious. "" —",2022-05-12,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/10/12/Collective Dreaming.001.jpeg,385709,,2022-01-20T14:22:02.581Z,"['patpat@media.mit.edu', 'eyalp@media.mit.edu', 'adamjhh@media.mit.edu', 'abyjain@media.mit.edu', 'igrover@media.mit.edu']",True,cyber-key-to-dreams,2021-10-07,[],Cyber Key to Dreams,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2021-10-08T04:59:58.450Z,"Situated VR: Towards a Congruent Hybrid RealityThe vision of Extended Reality (XR) systems is living in a hybrid reality or ""Metaverse"" where real and virtual elements seamlessly and contextually augment experiences of ourselves and the worlds we inhabit. While this integration promises exciting opportunities for the future of XR, it comes at the risk of experiential distortions and feelings of dissociation, especially related to virtual reality. When transitioning from a virtual world to the real, users often report experiential structures to linger on, as sort-of after images, causing disruptions in a user's daily life. In this work, we define these atypical experiences as experiential artifacts and present preliminary results from an informal survey conducted online with 76 VR users to highlight different types and their durations. Based on models of cognition as situated, we propose Situated VR, which blends the real and virtual in novel ways, as a method that can potentially help reduce the artifacts and simultaneously increase the user's sense of presence. We discuss the implications of experiential artifacts and highlight the importance of Situated VR for a future where we spend most of our time in such systems. Through examples from our own work in building hybrid experiences, we demonstrate the potential and relevance of Situated VR in the design of a future, more immersive, experiential artifact-free hybrid reality.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/10/15/svr_figure_FsvTnuH.png,385725,,2023-04-20T09:33:12.997Z,"['sra@media.mit.edu', 'pattie@media.mit.edu', 'vdanry@media.mit.edu']",True,situated-vr-towards-a-congruent-hybrid-reality,2021-06-01,"['virtual-reality', 'cognition', 'environment']",Towards a Congruent Hybrid Reality without Experiential Artifacts,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2021-10-08T11:28:11.637Z,"​We take as a starting point for our work the recent scholarship by Sasha Costanza-Chock, Design Justice, in which she described design justice as a framework for analysis of how design distributes benefits and burdens between various groups of people. In addition, design justice calls for attention to how “design reproduces and/or challenges the matrix of domination.” The project aims to develop methods for design justice audits in classrooms, in academia broadly, and in the design process in educational institutions and corporations. Our work also extends to creating guidelines to inform how design justice can be leveraged in technology design processes and policy development. Overall, this project audits institutions to understand how they engage in design justice (critique) and suggests ways to achieve design justice in a variety of spaces (solutions). This combines for a focus on both critique and design solutions.Our overarching theme is to understand how design and justice are taught, intertwined, and embodied and what techniques are successful and what aren’t for engaging with design justice in varying settings. We use MIT as an exemplar for extracting design pedagogy and justice best practices. We are specifically mindful of and intrigued by design pedagogy paradigms, encoded values, and student, instructor, and community partner experiences. This often includes identifying exemplars of how MIT currently incorporates design justice into design pedagogy, suggesting future techniques to increase design justice’s presence in the classroom, and designing new methods to support design justice in design pedagogy. Our approach can be expanded to other educational institutions and corporate settings.Our goals and outcomes include:Framing of design justice and impacts on design outcomes Considerations for what constitutes ‘positive’ design outcomes and the nature, quality, quantity, and goodness of those outcomes Recommendations for incorporating design justice pedagogy into design courses at MIT, other educational institutions, and corporationsGuidelines for how to/not to teach and incorporate design + ethics and equity + justice into design spacesOur interdisciplinary team, listed below, is broadly interested in exploring how design pedagogy and practice can center justice and inclusivity:Rima Das, Mechanical Engineering, PhD StudentAnastasia K. Ostrowski, Media Arts & Sciences, Postdoctoral AssociateJana I Saadi, Mechanical Engineering, PhD StudentAditi Verma, Assistant Professor; Nuclear Engineering & Radiological Sciences; University of MichiganSara Atwood, Dean of the School of Engineering, Mathematics, and Computer Science, Elizabethtown CollegeCatherine D’Ignazio, Assistant Professor of Urban Science & Planning, Department of Urban Studies & Planning, MITMaria Yang, Professor of Mechanical Engineering, MITCynthia Breazeal, Professor of Media Arts & Sciences, MITGillian J Roeder, Mechanical Engineering, Undergraduate StudentPrevious Collaborators:  Olivia Dias, Computer Science and Electrical Engineering, Undergraduate Student; Barbara Adjei, Cognitive Science and Linguistics, Undergraduate Student, Dartmouth College; Paige Lighthammer, Nuclear Engineering, Undergraduate Student, University of Michigan; Kimberley Kimura, Wellesley College; Shelly Ben-David, Electrical Engineering & Computer Science, Undergraduate Student; Raechel Walker, Media Arts & Sciences, Master's Student; Marina Santos, Biology/Biological Sciences, Undergraduate Student, Wellesley College;  Jennifer Zhang, Undergraduate Student; Kanokwan Tungkitkancharoen, Mechanical Engineering & Political Science, Undergraduate Student",,"['personal-robots', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-decentralized-society', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2021/10/08/Screen Shot 2021-10-08 at 12.23.38 PM.png,385733,,2023-10-03T20:45:04.054Z,"['akostrow@media.mit.edu', 'cynthiab@media.mit.edu', 'raechelw@media.mit.edu']",True,design-justice-auditing,2020-10-01,"['robotics', 'design', 'human-computer-interaction', 'learning-teaching', 'urban-planning', 'social-science', 'ethics', 'social-change', 'racial-justice', 'diversity', 'social-justice']",Design Justice Project at MIT,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2021-10-08T17:51:28.872Z,"Cryptoeconomic Systems (CES) aims to be the premier publication venue for blockchain research.Our mission is simple: collect and disseminate the best, most innovative research in the space. As blockchain grows, the need for dedicated, peer-reviewed publication is self-evident. Amazing research is released daily but is often scattered among conferences, journals, and even personal blogs. We hope to both capture all of the cutting-edge thinking occurring in blockchain, and expand the way we think about decentralized systems. To that end, CES is intentionally multidisciplinary. By bringing together technical fields such as cryptography or protocol engineering, and placing them side-by-side with fields like economics, law, philosophy, or art, we are confident amazing new insights will arise—and with them, a more mature research commons that puts blockchain first. Please visit our website for more information.The journal is published by The MIT Press using a diamond open access policy, coordinated by two Editors-in-Chief: Andrew Miller (UIUC/IC3) and Neha Narula (MIT DCI), along with Managing Editor Reuben Youngblom (MIT/Stanford). High-level guidance comes from our Advisory Board.",,['digital-currency-initiative-dci'],https://dam-prod2.media.mit.edu/x/2021/10/08/CES_dgMJjYb.jpeg,385745,,2022-10-14T16:08:52.100Z,"['narula@media.mit.edu', 'jacobsal@media.mit.edu', 'ryoungbl@media.mit.edu']",True,cryptoeconomic-systems-journal-and-conference-series,2019-10-01,[],Cryptoeconomic Systems Journal and Conference Series,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2021-10-08T21:17:28.442Z,"Words are fleeting. Moments come and go, and what we vocalize “disappears into thin air.""In this project, TangiBel Soundscapes, we propose the vision for co-located people to collaboratively capture, collect, and interact with their words and vocalizations, therein adding additional dimensions to vocal expression and enabling the collaborative creation of artifacts that can be used to represent conversations. We believe that this would allow people to better express themselves, feel heard, and pay attention to the present moment. This can aid in a variety of scenarios, including collaborative brainstorming, storytelling, and more.We aim to prototype an interactive system that enables people to capture recent sound bytes (e.g. spoken words and ephemeral sounds) and converts them into  interactive bits. These bits will be used as the basis to create  interactive visual artifacts that can be remixed and re-experienced, as well as  tangible and physical artifacts that can then serve as triggers to “re-inflate” previous virtual-scapes and allow people to re-experience their past conversations. This is a team project as part of the Snap Creative Challenge, focused on ""The Future of Co-located Social AR."" The theme of this year's challenge addresses the question ""Can technology foster, rather than detract from, in-person socialization?"" ",2021-12-31,"['fluid-interfaces', 'future-sketches']",https://dam-prod2.media.mit.edu/x/2021/10/14/Screen Shot 2021-10-14 at 2.52.35 PM.jpg,385757,,2022-10-06T15:17:38.239Z,"['joaleong@media.mit.edu', 'catfang@media.mit.edu', 'zachl@media.mit.edu', 'pattie@media.mit.edu']",True,tangibel-soundscapes,2021-08-16,"['augmented-reality', 'communications']",TangiBel Soundscapes,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2021-10-10T21:10:18.664Z,"Across most school districts in America, school attendance boundaries play an important role in determining which schools children are assigned to attend.  However, boundaries are often drawn in ways that lead students to attend the schools closest to their homes—recapitulating segregation in schools that reflects historically-rooted patterns of neighborhood segregation by race and income.  Beyond its role in perpetuating achievement gaps, segregation also threatens to prevent students from different racial and socioeconomic backgrounds from interacting with and learning from one another, both of which are important preconditions for cultivating empathy and respect for people with different life experiences—two bedrocks of a healthy pluralistic democracy.  While boundary changes could promote more diversity in schools, many parents fear how such changes might adversely affect their children’s educational experiences and outcomes, making it difficult for districts to pursue boundary changes as a strategy for promoting diversity in schools.  This project pursues a series of multidisciplinary research studies to explore how we might redraw attendance boundaries across US public school districts to increase racial and socioeconomic diversity in schools — in practical ways that students and parents would support.  Some of our research questions include:1. How might we use methods from artificial intelligence to propose alternative boundaries that can help reduce racial and socioeconomic segregation in schools — subject to constraints around increases in travel times, school size, and other factors families value?2. Which communications techniques affect how school district leaders and families respond to alternative boundaries?",,"['social-machines', 'center-for-constructive-communication']",https://dam-prod2.media.mit.edu/x/2021/10/10/image (5).png,385768,,2022-04-03T01:15:23.402Z,"['coverney@media.mit.edu', 'dougb5@media.mit.edu', 'dkroy@media.mit.edu', 'ngillani@media.mit.edu']",True,school-diversity,2021-09-01,[],School Diversity,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2021-10-12T15:21:46.878Z,We have collected data (n=133) in collaboration with Duke University Professor Paul Seli testing a non-contact Targeted Dream Incubation protocol that is generated from simple personalized audio and sleep profiles. We  tune timer and sleep stimulation protocols and incubate dreams without the need for wearable technology.  Paper and data analysis are in process.,2022-05-18,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/10/12/Screen Shot 2021-10-12 at 11.20.29 AM.png,385778,,2022-01-20T13:45:29.259Z,"['adamjhh@media.mit.edu', 'pattie@media.mit.edu']",True,contactless-dream-incubation,2021-10-12,[],Non-contact Dream Incubation,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2021-10-12T15:28:09.942Z,"We are working with MIT CAST Visiting Artist Carsten Höller to build prototypes for a Dream Hotel museum exhibition which will translate experiments from the sleep neuroscience lab into the artistic context. See documentation and description of our project at the  Center for Art, Science and Technology at MIT website here.  An excerpt from CAST MIT:""CAST Visiting Artist Carsten Höller creates participatory and relational works of art that explore logic and perception, the body and interactions, and the ways in which we experience and understand the world around us. During his residency at MIT, he worked with Professor Pattie Maes and graduate student Adam Haar of the Fluid Interfaces Group to build prototypes for a Dream Hotel: a museum exhibition with seven rooms, each housing a sleep experiment for overnight guests.Höller, who holds a PhD in agricultural science, applies his training as a scientist and his practice as an artist to reveal the alternate realities latent in daily life. As such, his work is closely aligned with current research at the Fluid Interfaces Group, which develops experiences and devices for expanding our cognitive capacities. By reframing experiment design as experience design, Höller’s work removes the implicit power dynamics of a neuroscience experiment in the lab; while the conventional hierarchies are dissolved, the rituals and theatrics are maintained, casting the participant into a state of pure play.The first phase of the Dream Hotel involved a “daydream night” at MIT Museum Studio, where guests explored recent findings from experiments designed to incubate and alter dreams. From a film to inspire flying dreams to a prototype for inflatable pajamas, the Fluid Interfaces Group revealed how technologies for muscle stimulation, bone conduction audio, scent delivery, and temperature adjustment can calibrate our levels of lucidity and disorientation.By innovating at the intersection of waking and sleep, the collaboration between Höller and the Fluid Interfaces Group has the potential to effect real change at the level of individual and communal consciousness. What if we understood rest as a form of resistance, and dreamscapes as test sites for generating new possibilities in the waking world? The Dream Hotel proposes dreaming as an essentially democratic space, where worlds overlap, objects are decontextualized, and minds merge into a flow state of collective receptivity.""This project will be hugely collaborative. Already work has begun with Media Labbers David Su, Abhinandan Jain, Irmandy Wicaksono, and with Yechen Zhu (RISD), Wendi Yan (Princeton), Seth Riskin and of course Carsten Höller + more. ",2022-08-31,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/10/12/Screen Shot 2021-10-12 at 11.30.03 AM.png,385786,,2023-02-08T16:43:51.311Z,"['adamjhh@media.mit.edu', 'abyjain@media.mit.edu', 'pattie@media.mit.edu', 'davidsu@media.mit.edu']",True,a-dream-hotel-with-carsten-h-ller,2021-10-12,[],A Dream Hotel with Carsten Höller,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-10-13T22:16:32.035Z,"Technologies designed with personalities and social interfaces are entering our homes in the form of social robots such as Jibo and Cozmo. By emulating interpersonal interactions, social robots have great potential to help us learn, be more creative, and reduce stress. Social robots also introduce potentials for harm, such as emotional manipulation for money or power. More information about the nature of long-term social relationships between social robots and children has the potential to help avoid potential harms.In artificial intelligence, transparency is one of the key tenets of ethical and responsible design. It is hypothesized that by knowing how a system works, users may be able to better use and trust robotic systems. Educators are beginning to create materials that give students both a conceptual understanding of the system (i.e. how do sensors work?) and an applied understanding of the system (i.e. program a sensor to detect when the lights are off). Children, as early as Pre-K, are capable of learning about and building features for social robots. However, we still do not know how social relationships between children and robots change when the inner workings of these systems become more transparent.First, I discuss the design of two curricula that take different approaches to educate youth (grades 4 and 5) about social robots. The Knowledge and Societal Impact curriculum teaches students about the technical and ethical topics surrounding social robots. The Programming curriculum allows students to program their own conversational skills on Jibo. These curricula represent two pedagogical approaches in the field of AI education, one focused on embedding ethics, and the other focused on students as self-driven makers.Next, I evaluated the impact of these curricula on fourth and fifth-grade students who simultaneously lived with a social robot in their home for two months. Students were assigned to one of four conditions: no education, only Knowledge and Societal Impact, only Programming, and both Knowledge and Societal Impact and Programming. I found that students were able to understand and engage with the curricula and that the curricula helped them form a more clear model of what their robot was capable of. However, I found no difference in perceived emotional relationship or usage among groups. Students in all groups found the robot as equally likable, anthropomorphic, intelligent, safe, and animated. However, students who engaged in the Knowledge and Societal Impact curriculum found their robot to be significantly less trustworthy than those in different groups.Overall, results from this study indicate that children will continue to treat robots as social partners regardless of the information that they hold about them. However, it seems that teaching students about the societal impact of robots makes students less trusting of their own robots. These results are timely and relevant given public discourse. Many have advocated for education to prevent deception or misuse of social robots, and findings from the study suggest that new approaches to education are needed.",,['personal-robots'],https://dam-prod2.media.mit.edu/x/2021/10/13/Screen Shot 2021-10-13 at 6.23.59 PM.png,385817,,2021-10-19T01:50:34.389Z,"['haewon@media.mit.edu', 'kdarling@media.mit.edu', 'cynthiab@media.mit.edu', 'dipaola@media.mit.edu']",True,education-and-robot-relationships,2021-01-01,[],Impact of Education on Child-Robot Relationships,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2021-10-14T14:25:19.925Z,"Advancements in machine learning have recently enabled the hyper-realistic synthesis of prose, images, audio and video data, in what is referred to as artificial intelligence (AI)-generated media. These techniques offer novel opportunities for creating interactions with digital portrayals of individuals that can inspire and intrigue us. AI-generated portrayals of characters can feature synthesized faces, bodies and voices of anyone, from a fictional character to a historical figure, or even a deceased family member. Although negative use cases of this technology have dominated the conversation so far,  we explore emerging positive use cases of AI-generated characters, specifically in supporting learning and well-being. We demonstrate an easy-to-use AI character generation pipeline to enable such outcomes and discuss ethical implications as well as the need for including traceability to help maintain trust in the generated media. As we look towards the future, we foresee generative media as a crucial part of the ever growing landscape of human–AI interaction. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/12/16/Paper Figure3.jpg,385826,,2022-12-01T23:43:44.976Z,"['vdanry@media.mit.edu', 'joaleong@media.mit.edu', 'pattie@media.mit.edu', 'patpat@media.mit.edu', 'sra@media.mit.edu', 'novysan@media.mit.edu']",True,ai-generated-characters,2021-10-14,"['human-computer-interaction', 'art', 'artificial-intelligence', 'communications', 'health', 'human-machine-interaction', 'learning-teaching', 'wellbeing']",AI-generated Characters for Learning and Wellbeing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2021-10-14T14:31:31.736Z,"This research presents the results of a study with 134 participants to explore the effects of learning from an AI-generated virtual instructor that resembles a person one likes or admires. We found that they can significantly improve students' motivation towards learning, foster more positive emotions, and boost their appraisal of the AI-generated instructor as serving as an effective instructor.",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2022/09/27/virtual-instructor.gif,385834,,2023-04-10T14:44:33.835Z,"['vdanry@media.mit.edu', 'patpat@media.mit.edu', 'pattie@media.mit.edu', 'joaleong@media.mit.edu', 'sra@media.mit.edu']",True,learning-from-virtual-instructors,2020-10-14,"['human-computer-interaction', 'artificial-intelligence', 'extended-intelligence', 'learning-teaching', 'wellbeing']",AI-Generated Virtual Instructors Based on Liked or Admired People Can Improve Motivation and Foster Positive Emotions for Learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2021-10-15T15:02:01.135Z,"Perceptual illusions have the power to change our abilities and behaviours. Prior research in virtual reality (VR) has shown that transporting ourselves into a new digital physical or social environment, or having an experience of piloting a virtual avatar, even if unrealistic, can be a transformative experience.  However, what happens when we see a hyper-realistic and modified version of ourselves? One that is younger or older, or plays another role? One that behaves in a different way? With advancements in generative AI, we are now able to synthesize new digital versions of ourselves that can look and sound remarkably like us. Synthesized selves can be a provoking medium for questioning what could be.  Instead of looking at our own simple reflections, synthesized reflections can beg us to question and explore ""Who am I?"" Such a probe can even shape our self-beliefs and encourage us to pursue our greatest potentials. ",,['fluid-interfaces'],https://dam-prod2.media.mit.edu/x/2021/10/15/pexels-amine-m'siouri-2050590.jpg,385876,,2023-03-21T03:43:07.591Z,"['joaleong@media.mit.edu', 'patpat@media.mit.edu', 'pattie@media.mit.edu']",True,synthetic-self-perception,2020-05-01,"['human-computer-interaction', 'artificial-intelligence', 'augmented-reality', 'cognition', 'learning-teaching', 'perception', 'creativity', 'human-augmentation']",Synthetic Self-Perception,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2021-10-15T17:28:25.448Z,"Different approaches and models have been used to detect driver’s stress and affective states, using physiology, facial expression, and self-reports. Including contextual information is expected to help improve the accuracy of the models. This project focuses on vision-based extraction of driving environmental context. Thanks to recent advances in machine learning with shared real-world datasets, we believe it is now feasible to train an automated system to predict a driving-induced state of stress from a visual scene.This work may help not only with predicting driver stress in real-time applications but also in expanding the utility of other unlabeled data sets for additional research. ",,['affective-computing'],https://dam-prod2.media.mit.edu/x/2021/10/14/ACIIGif3.gif,385897,,2022-10-24T16:59:50.595Z,"['neska@media.mit.edu', 'agata@media.mit.edu', 'picard@media.mit.edu']",True,Predicting-Driver-Stress-by-analyzing-the-road-scene,,"['human-computer-interaction', 'artificial-intelligence', 'data']",Predicting Driver Self-Reported Stress by Analyzing the Road Scene,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2021-10-17T01:32:30.696Z,"Globally, depression affects more than 264 million people of all ages, and is a leading cause of disability worldwide. Several interactive technologies for mental health have been developed to make various therapeutic services (e.g. psycho-education, health monitoring, clinical assessment, etc.) more accessible and scalable. However, most of them are designed to engage users only within the therapy/intervention tasks. In this project, we present a social robot that delivers interactive positive psychology interventions designed to improve people's psychological wellbeing, and provides other useful skills to build rapport with people over time in their homes. Our previous work showed that college students showed a significant improvement in psychological wellbeing, mood, and motivation to change after completing seven positive psychology interactions with the robot. However, we also found that students' personality traits were also shown to be associated with the intervention outcomes as well as their working alliance with the robot and their satisfaction with the interventions. Also, students' working alliance with the robot was shown to be associated with their pre-to-post change in motivation for better wellbeing. Based on these results, we are investigating how a social robot can support highly neurotic people better by delivering the mental health interventions in a non-threatening and socially supportive manner. We present a new way for social agents to deliver mental health interventions in a companion-like style, in which the agent uses gentler language to prompt users to engage in the intervention. In this new style, the robot does not instruct the human user but demonstrates the intervention activity first and invites him/her to join in, instead of using explicit directives for instructions. A eight-week home deployment study is currently conducted with people living in the U.S. in order to compare the efficacy of this new companion-like style with the traditional coach-like intervention style. Results from our study will give insight into design guidelines for personalizing interactive technologies' intervention and behaviors based on users' traits and behavioral cues for better mental health outcomes.",2022-06-30,"['personal-robots', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",,385939,,2023-09-07T23:30:53.674Z,"['sooyeon6@media.mit.edu', 'sharifah@media.mit.edu', 'haewon@media.mit.edu', 'cynthiab@media.mit.edu']",True,companion-like-robot,2021-02-01,[],Robot Companion for Better Wellbeing,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2021-10-18T17:57:28.052Z,"Urban socioeconomic segregation is an important indicator that is crucial for social, economic, and health outcomes. Segregation is inherently linked with (bonding & bridging) social capital, which is essential for community resilience to various external shocks. Equitable access to housing, community facilities, health services, and clean environment are often exacerbated by segregation, and therefore is an important metric for public investment policies.Using human mobility data collected from mobile phones before and during the COVID-19 pandemic in Boston, we are studying how various non-pharmaceutical interventions (e.g., lockdowns) affected how we interact with each other in urban environments. So far, we have found that socioeconomic segregation have substantially increased during the pandemic, even after the ease of the lockdowns, due to not only because people traveled less, but also because people's explorative behavior - both socially and spatially - has decreased compared to pre-pandemic levels. We further focus on segregation levels in public parks, which are essential public infrastructure designed for equitable, inclusive and safe environments for the citizens. In most parks, segregation levels have increased due to the pandemic, especially in the Boston metropolitan areas (see Figure). Understanding the governing factors that contribute to the increased segregation in public spaces could be used as input for investment policies aimed to improve the provision of equitable services for the citizens.",,['human-dynamics'],https://dam-prod2.media.mit.edu/x/2021/10/18/parks_map_trim.png,385976,,2021-10-18T18:21:24.511Z,[],True,urban-segregation-during-the-covid-19-pandemic,2021-09-01,"['social-networks', 'data']",Increased Urban Segregation during the COVID-19 Pandemic,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2021-10-18T21:05:02.774Z,An exploration of how  advances in deep learning and generative models can be used to help us synthesize our ideas.,,['viral-communications'],https://dam-prod2.media.mit.edu/x/2021/10/18/Screen Shot 2021-10-18 at 7.00.18 PM.png,385985,,2021-11-20T20:05:11.598Z,['dunnell@media.mit.edu'],True,tools-to-synthesize-with,2021-09-27,"['design', 'computer-vision', 'human-computer-interaction', 'art', 'artificial-intelligence', 'communications', 'data']",Computer-Aided Synthesis,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-10-18T21:11:26.295Z,"Recent deep learning models have shown remarkable performance in image classification. While these deep learning systems are getting closer to practical deployment, the common assumption made about data is that it does not carry any sensitive information. This assumption may not hold for many practical cases, especially in the domain where an individual's personal information is involved, like healthcare and facial recognition systems. We posit that selectively removing features in this latent space can protect the sensitive information and provide a better privacy-utility trade-off. Consequently, we propose DISCO which learns a dynamic and data driven pruning filter to selectively obfuscate sensitive information in the feature space. We propose diverse attack schemes for sensitive inputs \& attributes and demonstrate the effectiveness of DISCO against state-of-the-art methods through quantitative and qualitative evaluation. Finally, we also release an evaluation benchmark dataset of 1 million sensitive representations to encourage rigorous exploration of novel attack schemes.This work was presented at CVPR 2021Singh, Abhishek, Ayush Chopra, Ethan Garza, Emily Zhang, Praneeth Vepakomma, Vivek Sharma, and Ramesh Raskar. ""DISCO: Dynamic and Invariant Sensitive Channel Obfuscation for deep neural networks."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12125-12135. 2021.",,['camera-culture'],,385993,,2021-10-19T15:28:39.821Z,"['abhi24@media.mit.edu', 'raskar@media.mit.edu', 'vvsharma@media.mit.edu', 'vepakom@media.mit.edu', 'ezg@media.mit.edu', 'ayushc@media.mit.edu', 'emizhang@media.mit.edu']",True,inference-privacy,,[],DISCO: Dynamic and Invariant Sensitive Channel Obfuscation for deep neural networks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-10-18T21:46:07.889Z,"Abstract:We introduce DeepABM, a framework for agent-based modeling that leverages geometric message passing of graph neural networks for simulating action and interactions over large agent populations. Using DeepABM allows scaling simulations to large agent populations in real-time and running them efficiently on GPU architectures. To demonstrate the effectiveness of DeepABM, we build DeepABM-COVID simulator to provide support for various non-pharmaceutical interventions (quarantine, exposure notification, vaccination, testing) for the COVID-19 pandemic, and can scale to populations of representative size in real-time on a GPU. Specifically, DeepABM-COVID can model 200 million interactions (over 100,000 agents across 180 time-steps) in 90 seconds, and is made available online to help researchers with modeling and analysis of various interventions. We explain various components of the framework and discuss results from one research study to evaluate the impact of delaying the second dose of the COVID-19 vaccine in collaboration with clinical and public health experts. While we simulate COVID-19 spread, the ideas introduced in the paper are generic and can be easily extend to other forms of agent-based simulations. Furthermore, while beyond scope of this document, DeepABM enables inverse agent-based simulations which can be used to learn physical parameters in the (micro) simulations using gradient-based optimization with large-scale real-world (macro) data. We are optimistic that the current work can have interesting implications for bringing ABM and AI communities closer. Papers1.  [Research] DeepABM: Scalable, Efficient and Differentiable Agent-based Simulations via Graph Neural Networks. (Accepted at Winter Simulation Conference 2021). https://arxiv.org/pdf/2110.04421.pdf 2.  [Public Health Policy]  Public health impact of delaying second dose of BNT162b2 or mRNA-1273 covid-19 vaccine: simulation agent based modeling study. (Accepted at the British Medical Journal 2021). https://www.bmj.com/content/373/bmj.n1087Media: Some recent media reports on the project -1. Reuters: https://www.reuters.com/business/healthcare-pharmaceuticals/delaying-second-covid-19-vaccine-doses-can-help-reduce-deaths-study-2021-05-12/2. France 24:  https://www.fr24news.com/a/2021/05/delaying-second-dose-of-covid-19-vaccine-may-help-reduce-number-of-deaths-study-fr.html3. The Weather Channel: https://weather.com/en-IN/india/coronavirus/news/2021-05-13-delaying-second-covid-vaccine-dose4.  Italy 24: https://www.italy24news.com/business/51529.html5. India.com : https://www.india.com/health/covid-19-vaccination-how-delaying-second-dose-of-coronavirus-vaccine-may-reduce-deaths-4661320/",,['camera-culture'],,386009,,2021-10-19T15:32:11.813Z,['ayushc@media.mit.edu'],True,deepabm,2020-12-01,[],DeepABM: Data-driven public policy with differentiable simulators,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2021-10-18T21:52:21.476Z,"Consider a small object sitting on a desk in your living room. The object is illuminated by light sources from all directions—this includes direct sources such as the sun or overhead lights, but also indirect sources, like the foliage outside that scatters sunlight through your window. The appearance of the object and the surface that it rests upon results from the complex interaction between the incident illumination and the geometry and material properties of the object and the desk. In this paper we ask the question—if the geometry and material properties of the observed scene are known, how well can we reconstruct the incident illumination pattern?In our work we primarily make use of shadows cast by an object onto nearby surfaces. Cast shadows are particularly easy to interpret when an object is illuminated from a single direction. For example, one can immediately determine the position of the sun by looking at a sundial. Estimating the illumination incident from all directions simultaneously is more challenging, and is a linear but ill-posed inverse problem. ",2022-05-02,['camera-culture'],https://dam-prod2.media.mit.edu/x/2021/10/18/tristan_paper_picture1_wD18ZKT.png,386017,,2021-10-19T15:35:44.701Z,"['raskar@media.mit.edu', 'co24401@media.mit.edu', 'tswedish@media.mit.edu']",True,turning-objects-into-cameras,2021-01-04,"['computer-vision', 'imaging']",Turning Objects into Cameras,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2021-10-18T22:46:01.926Z,"We propose sanitizer, a framework for secure and task-agnostic data release. While releasing datasets continues to make a big impact in various applications of computer vision, its impact is mostly realized when data sharing is not inhibited by privacy concerns. We alleviate these concerns by sanitizing datasets in a two-stage process. First, we introduce a global decoupling stage for decomposing raw data into sensitive and non-sensitive latent representations. Secondly, we design a local sampling stage to synthetically generate sensitive information with differential privacy and merge it with non-sensitive latent features to create a useful representation while preserving privacy. This newly formed latent information is a task-agnostic representation of the original dataset with anonymized sensitive information. While most algorithms sanitize data in a task-dependent manner, a few task-agnostic sanitization techniques sanitize data by censoring sensitive information. In this work, we show that a better privacy-utility trade-off is achieved if sensitive information can be synthesized privately. We validate the effectiveness of the sanitizer by outperforming state-of-the-art baselines on the existing benchmark tasks and demonstrating tasks that are not possible using existing techniques. ",,"['camera-culture', 'media-lab-research-theme-life-with-ai']",,386034,,2023-09-07T23:07:01.697Z,"['abhi24@media.mit.edu', 'vepakom@media.mit.edu', 'vvsharma@media.mit.edu', 'ayushc@media.mit.edu', 'raskar@media.mit.edu', 'ezg@media.mit.edu']",True,private-data-release,,[],Synthesizing data privately for Machine Learning,PUBLIC,https://tremblerz.github.io,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-10-18T23:34:45.531Z,"Public opinion prediction with language models.Media plays an important role in shaping people's beliefs and behaviors.Mass media and social media reflect and form public opinion, which can ultimately lead to positive outcomes such as civil engagement with opposing tribes, but also negative outcomes such as non-adherence to health-beneficial social guidelines. Understanding viewpoints expressed online and the relationship between media and beliefs is increasingly pertinent today, in a world of constant connectivity and ""alternate realities"". Motivated by the effects of misinformation in the COVID-19 pandemic, we introduce a new approach around ""media diet models"", which can act as proxies for human media consumption.By probing these models, we can predict public opinion as measured by nationally representative surveys with media diet information for each respondent. We examine this approach on surveys conducted during the COVID-19 pandemic, as well as monthly consumer confidence surveys.In both settings, we find there to be predictive power in the media diet models.Particularly in the COVID-19 setting, there is strong correlation between the probed responses and survey responses, and evidence such a model can act as a leading indicator of public opinion. Finally, we show several ways to scale this approach to automatically generate insights around the heterogeneity of messaging and possible induced human beliefs.",,['social-machines'],https://dam-prod2.media.mit.edu/x/2021/10/18/Screen Shot 2021-10-18 at 7.52.12 PM.png,386050,,2021-10-19T15:38:45.808Z,"['echu@media.mit.edu', 'dkroy@media.mit.edu']",True,pollster,2020-12-01,[],Pollster,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2021-10-19T00:07:47.061Z,"Wireless channels can be inherently privacy-preserving by distorting the received signals due to channel noise, and superpositioning multiple signals over-the-air. By harnessing these natural distortions and superpositions by wireless channels, we propose a novel privacy-preserving machine learning (ML) framework at the network edge, coined over-the-air mixup ML (AirMixML). In AirMixML, multiple workers transmit analog-modulated signals of their private data samples to an edge server who trains an ML model using the received noisy-and superpositioned samples. AirMixML coincides with model training using mixup data augmentation achieving comparable accuracy to that with raw data samples. From a privacy perspective, AirMixML is a differentially private (DP) mechanism limiting the disclosure of each worker's private sample information at the server, while the worker's transmit power determines the privacy disclosure level. To this end, we develop a fractional channel-inversion power control (PC) method, {\alpha}-Dirichlet mixup PC (DirMix({\alpha})-PC), wherein for a given global power scaling factor after channel inversion, each worker's local power contribution to the superpositioned signal is controlled by the Dirichlet dispersion ratio {\alpha}. Mathematically, we derive a closed-form expression clarifying the relationship between the local and global PC factors to guarantee a target DP level. By simulations, we provide DirMix({\alpha})-PC design guidelines to improve accuracy, privacy, and energy-efficiency. Finally, AirMixML with DirMix({\alpha})-PC is shown to achieve reasonable accuracy compared to a privacy-violating baseline with neither superposition nor PC.2105.00395.pdf",,['camera-culture'],https://dam-prod2.media.mit.edu/uuid/6a09d4ef-8c37-47db-b75a-838c4fa14c4c,386059,,2021-10-19T15:40:17.848Z,['vepakom@media.mit.edu'],True,AirMixML,2021-03-01,[],AirMixML: Over-the-Air Data Mixup for Inherently Privacy-Preserving Edge Machine Learning,PUBLIC,https://praneeth.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-10-19T00:36:31.428Z,"We introduce a differentially private method to measure nonlinear correlations between sensitive data hosted across two entities. We provide utility guarantees of our private estimator. Ours is the first such private estimator of nonlinear correlations, to the best of our knowledge within a multi-party setup. The important measure of nonlinear correlation we consider is distance correlation. This work has direct applications to  private feature screening, private independence testing, private k-sample tests, private multi-party causal inference and private data synthesis in addition to exploratory data analysis. ",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2021/10/18/Screen Shot 2021-10-18 at 9.12.14 PM.png,386087,,2022-03-29T18:43:57.491Z,['vepakom@media.mit.edu'],True,private-measurement-of-nonlinear-correlations-between-multiple-parties,2021-08-01,[],Private measurement of nonlinear correlations between multiple parties,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-10-19T00:51:02.063Z,"Focus groups are a core methodology in audience research for bringing people together to discuss an issue of concern; however, it has been criticized for not enabling researchers to gain a deep understanding of the participants' lived experiences or generating in-depth personal narratives that build on those experiences. In this project, we build on the shared epistemologies of qualitative research and journalism to introduce the Generative Dialogue Framework. The Generative Dialogue Framework is grounded in the intersection of inquiry, knowledge, and storytelling to design and facilitate remote, and in-person dialogue groups. Informed by phenomenology, the philosophy of dialogue, and design thinking, and with a strong visual focus, the framework aims at surfacing participants’ lived experiences as a way of understanding their perceptions, thoughts, and perspectives, especially within the context of controversial or polarizing topics of concern. Specifically, it aims at surfacing participants' lived experiences as a way of understanding their perceptions, thoughts, and perspectives. It stimulates constructive dialogue by offering a focused framework and a structured yet flexible question guide. The result is a reflective learning dialogue in which participants jointly develop shared meanings and insights, rather than simply exchanging rationalized opinions. Drawing on insights from a case study that explores people’s perceptions of the COVID-19 vaccination, the article demonstrates the framework’s application and provides a toolkit to structure the design and implementation process. Finally, reflecting on methodological and epistemological perspectives, the author reviews the advantages and challenges of applying the framework in focus group research for both researchers and the participants.Dimitra Dimitrakopoulou (forthcoming).  Designing Generative Dialogue Spaces to Enhance Focus Group Research: A Case Study in the Context of COVID-19 Vaccination.  International Journal of Qualitative Methods.",,"['social-machines', 'center-for-constructive-communication']",https://dam-prod2.media.mit.edu/x/2021/10/18/Designing Generative Dialogue Spaces_RWJF_August 2021_SHORT (8).png,386095,,2021-10-19T15:41:53.430Z,['dimitrad@media.mit.edu'],True,designing-generative-dialogue-spaces,2020-05-01,"['design', 'human-computer-interaction', 'communications']",Designing Generative Dialogue Spaces,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2021-10-19T01:23:36.276Z,"How might we collect rich and complex data from community dialogues, analyze and make sense of that data? How might we design outputs of that sensemaking in a participatory way? How might researchers and community leaders work together to exchange knowledge and learn together? These are some of the research questions that we address in this project that builds on community-powered understanding and participatory sensemaking. We start exploring this space by inviting community members, who are often underheard, to join facilitated conversations and share their hopes, concerns, and lived experiences. Community members, community leaders, and researchers come together to facilitate conversations, produce knowledge together and learn from one another. They collaboratively engage in a rigorous process of gaining a deep understanding of people’s lived experiences shared in facilitated dialogues in order to surface their hopes and concerns and identify themes and patterns across those experiences.   Beyond people-power alone, this project is supported and heightened with the help of different tools and skills. Our process is designed to be supported and enabled by the unique collaboration between old, ancient, human technologies like facilitation, dialogue, and active listening with new, computer-enabled technology that allows us to analyze the transcribed conversations systematically and at scale. This unique collaboration allows for the depth and richness that comes from human technologies, with the scale, replicability, memory, and transparency of new tech. Our sensemaking process evolves over three interconnected stages:Listening: Deeply listen to and highlight key moments in the conversations.Analyze: Iteratively develop thematic codes and apply the codes to the highlights.Synthesize: Summarize themes identified after coding and translate themes to prompts.Our participatory sensemaking framework is currently applied in the Real Talk For Change project a new civic space that amplifies the voices of Boston's historically underheard communities to constructively shape the 2021 election cycle.",,"['social-machines', 'center-for-constructive-communication']",https://dam-prod2.media.mit.edu/x/2021/10/18/Screen Shot 2021-10-18 at 9.22.20 PM.png,386109,,2021-10-19T02:23:01.487Z,"['dimitrad@media.mit.edu', 'mhughes4@media.mit.edu']",True,participatory-sensemaking,,"['design', 'civic-technology', 'communications']",Participatory Sensemaking,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-10-19T02:33:23.295Z,"Twitter provides a space for both social and broadcast communication; a tweet by a celebrity can reach millions of users, some of whom may reply and engage in conversation with the celebrity or any other user. This bidirectional communication channel means that influencers can both speak and listen to their audiences on issues they care about. But how will an audience respond to a particular message by an influencer? In this work we are combining deep learning models of text with social network analytics to predict audience response. We are focused on influencers using their platforms for social good, with the goal of developing models of influencers, language and audiences to support positive social impact.",2023-01-01,"['social-machines', 'center-for-constructive-communication']",,386117,,2023-10-23T17:48:50.811Z,"['bridgitm@media.mit.edu', 'russell5@media.mit.edu', 'dkroy@media.mit.edu', 'bcroy@media.mit.edu']",True,influencer-analytics-twitter,2021-01-04,[],Influencer Analytics on Twitter,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2021-10-19T03:02:39.450Z,"Classes of set functions along with a choice of ground set are a bedrock to determine and develop corresponding variants of greedy algorithms to obtain efficient solutions for combinatorial optimization problems. The class of approximate constrained submodular optimization has seen huge advances at the intersection of good computational efficiency, versatility and approximation guarantees while exact solutions for unconstrained submodular optimization are NP-hard. What is an alternative to situations when submodularity does not hold? Can efficient and globally exact solutions be obtained? We introduce one such new frontier: The class of quasi-concave set functions induced as a dual class to monotone linkage functions. We provide a parallel algorithm with a time complexity over n processors of O(n ^2 g) + O(log log n) where n is the cardinality of the ground set and g is the complexity to compute the monotone linkage function that induces a corresponding quasi-concave set function via a duality. The complexity reduces to O(gn log(n)) on n^ 2 processors and to O(gn) on n^ 3 processors. Our algorithm provides a globally optimal solution to a maxi-min problem as opposed to submodular optimization which is approximate. We show a potential for widespread applications via an example of diverse feature subset selection with exact global maxi-min guarantees upon showing that a statistical dependency measure called distance correlation can be used to induce a quasi-concave set function",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2021/10/18/Screen Shot 2021-10-18 at 11.04.45 PM.png,386125,,2021-10-19T15:44:44.079Z,['vepakom@media.mit.edu'],True,parallel-combinatorial-optimization-when-submodularity-does-not-hold,2021-08-01,[],QCSF: Parallel Combinatorial Optimization without needing Submodularity,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-10-19T14:53:47.931Z,"In this project we seek to combine data sources to understand the effect of the shock of COVID-19 as well as the recovery dynamics.  We combine data from: Transactions of credit and debit cards at the aggregate level by Municipality, gender, salary, week and category of transaction.COVID-19 trends in Mexico at the municipal level.Employment trends in Mexico aggregated at the municipal, gender, salary level, and employer size, each month.Mobility data at the municipal daily levelSome questions we want to answer are: what determined which municipalities had higher shocks in terms of expenditure and employment during COVID-19. What determined the rate of recovery? What are some expected long term changes in the expenditure/ employment sectors?So far we have observed that the reduction in mobility and expenditure was mainly driven by people's capacity to stay at home. This resulted in higher reductions in expenditure from people with higher income. The worst employment shocks are observed in services such as food and tourism. Moreover, in places where more people reduced their expenditure in such services, employment in those same sectors was hit harder.  Finally, the employment shocks were harder for people with lower salaries, and working in mid-sized  (250-1000 employees) companies.",2022-06-30,[],https://dam-prod2.media.mit.edu/x/2021/10/19/IMG_20190807_230507.jpg,386166,,2021-10-19T23:28:29.046Z,"['bernard0@media.mit.edu', 'bahrami@media.mit.edu', 'sandy@media.mit.edu', 'emoro@media.mit.edu']",True,resilience-of-communities-in-mexico-during-covid-19,2020-01-01,[],Resilience of communities in Mexico during COVID-19,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-10-19T15:24:44.471Z,"Differential Privacy offers strong guarantees such as immutable privacy under any post-processing. In this work, we propose a differentially private mechanism called PrivateMail for performing supervised manifold learning. We then apply it to the use case of private image retrieval to obtain nearest matches to a client’s target image from a server’s database. PrivateMail releases the target image as part of a differentially private manifold embedding. We give bounds on the global sensitivity of the manifold learning map in order to obfuscate and release embeddings with differential privacy inducing noise. We show that PrivateMail obtains a substantially better performance in terms of the privacy-utility trade off in comparison to several baselines on various datasets. We share code for applying PrivateMail at http://tiny.cc/PrivateMail.",,['camera-culture'],https://dam-prod2.media.mit.edu/x/2021/10/19/Screen Shot 2021-10-19 at 11.25.53 AM.png,386174,,2021-10-19T16:03:41.132Z,"['vepakom@media.mit.edu', 'raskar@media.mit.edu', 'jballa@media.mit.edu']",True,privatemail-supervised-manifold-learning-of-deep-features-with-privacy-for-image-retrieval,2021-06-01,[],PrivateMail: Supervised Manifold Learning of Deep Activations With Privacy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2021-11-12T14:32:03.147Z,"🚨Applications to the 2022 Co-Design Experience are now closed🚨OverviewThe development of technology in rural environments in Colombia is as problematic as it is vibrant. On the one hand, remnants from colonialism, decades of internal conflict, and dependence from imported technology, created and continue to maintain vast inequalities in how technology is made and how to access it. On the other hand, long-standing traditions of self-determination, primarily led by Afro-Colombian populations, farmers and Indigenous groups, continue to leverage rural inventiveness as an alternatives in transitioning to models that harmonize with nature and society at large. This includes (though is not limited to) the design and transfer of locally appropriate technologies.In this learning experience, students from MIT and other universities in the Latin America region will work on (1) co-designing technology for opportunities in rural Colombia, (2) implementing technological concepts from local collectives, and (3) scaling already designed and/or prototyped rural technologies. Students will be introduced to principles of participatory design with an emphasis in the Colombian context. The experience will require students to move from design research to implementation of functional prototypes through a host of participatory methods. We invite students from all schools at MIT who have experience in technology design, prototyping and/or other technical background to apply.As an exercise on relational design, this experience departs from the traditional format of courses and/or projects, and focuses on long-term community-based relationships. We re building a community of learning, not acting in pursue of a grade. Activities will take place across ~6 months with two key moments: a 1-week field visit in rural Colombia during Spring break '22, and a field-based internship during Summer '22 also in rural Colombia*. The experience will begin with a seminar towards the end of January '22 offered in a hybrid-format (in person + activities in the New England area + virtual gatherings with community members in Colombia) and culminate with the summer internship. All activities besides field visits will follow a hybrid format with a commitment of no more than 3 hours per week with the exception of the IAP seminar.This offering follows the 'Co-Design Experience: Technology Design for Coffee Production' offered in 2019 and is part of the ‘Regional Technology Co-Creation and Transfer Ecosystems in Latin America’ project.Key datesJanuary 14 –    Deadline for application submissionJanuary 17 –  Publication of accepted applicantsJanuary 19-20 –  Info sessionsJanuary 24-28 – IAP seminarFebruary | May – Spring learning experienceMarch 19-26 (2022) –  Colombia field visitSummer '22 (dates TBD) – Technology transfer field internshipIAP ActivitiesDuring the end of the IAP period in January 2022, we will hold a seminar geared towards laying the ethical, methodological and theoretical foundations in design and social justice that will serve as a foundation for the rest of the course. We will meet for ~ 2-3 hours every other day during the week for lectures, hands-on activities and relationship building sessions with partners in Colombia. The content in this seminar will continue to be revised at a much slower pace during the spring semester. Spring Semester Activities **Spring '22 will be as a slow burn, relational experience focused on establishing a meaningful connection with the context and needs/opportunities as articulated by members of rural collectives in Colombia. Activities during this period include:Hands-on workshops on traditional Colombian manufacturing techniquesReading circles on principles, methods and practices of design for social justice (e.g. Design Justice, equityXdesign, Participatory Action Research, Indigenous methodologies)Cultural activities and field trips in the New England area focused on Colombian culture, history and politics.Networking gatheringsSketch modeling and prototyping sessionsDesign research sessionsCostsA group of MIT students will be fully funded for all activities during the Spring break field visit. Tickets and accommodation for the Summer internship will also be covered for a group of students. Traveling students will be responsible for food and local transportation expenses (~10 USD/day).Students from universities in the Latin America are welcome to apply at no registration cost. Materials for activities throughout the course should be covered by students. We will offer a limited amount of scholarships for the Summer ‘22 internship.EnrollmentIn order to be part of this experience, please fill out the following form. Due to limited capacity we encourage you to apply ONLY if you can commit to the dates above. For more information, please contact Pedro Reynolds-Cuéllar at pcuellar@mit.eduThis experience is made possible with funding from MIT J-WEL, support from, Diversa, the MIT Media Lab and the MIT D-Lab, and is part of the Regional Technology Co-Creation and Transfer Ecosystems in Latin America project______________* Dates for the Summer '22 internship will be determined according to students availability** Contents of these activities will be collectively determined between students and communities based on scoped needs/opportunities in preparation for the Summer '22 internshipFrequently Asked QuestionsIs this a traditional course?No. If you are looking for a classic graded course on technology design, this is not it. We are experimenting with longer timelines of learning, prioritizing relationships over content. Content is still great quality, we just extend it over time, diversify it across different types of activities, and center the relational aspects of technology design which are, in our opinion, the most important ones. What do you mean by relational design? In his book ""Research is Ceremony"", Shawn Wilson teaches us that ""relationships do not merely shape reality, they are reality"". We lean on these teachings and bring them into the practice of designing technology. We believe that focusing on relationships with the land and the communities that inhabit them, we are bound to create better technology. Do I need technical experience to be part of this experience?Sorta. We would like to see a diverse range of expertise in this cohort. However, most of the projects this time around require different levels of technical experience. You don't need to know how to build complex machines (though it would be useful), though you will need to know your way around various fabrication tools and materials. I have classes during the Spring, how does this experience fit my schedule? Think of this as an extracurricular. The same way you sign up for a reading/study group, a hobby club, you will be signing up to a community of learners curious about how to change the way we build technology by co-designing and learning together with non-traditional technology creators. We will work on coordinating schedules among ourselves to arrange meeting at once or twice every week for no less than 3 hours total. Most of this time will be invested in cultural activities in the New England area, and in building relationships among ourselves and our community partners. The rest will be hard fun :)Do I need permission from my advisor to take part in this program? If you are a Media Lab student, you will be required to pursue approval from your advisor in order to participate in the travel portion of this program. What is the COVID policy for traveling to Colombia?This course will adhere to the Covid-19 guidelines provided by MIT at https://now.mit.edu/, as well as MIT's travel policy https://now.mit.edu/policies/mit-travel-policy/",2022-08-31,['space-enabled'],https://dam-prod2.media.mit.edu/x/2021/11/12/IMG_5755.jpeg,386488,,2022-04-01T21:01:57.792Z,['pcuellar@media.mit.edu'],True,a-co-design-experience-technologies-for-rural-sustainability-in-colombia-2022-edition,2021-11-12,"['design', 'agriculture', 'civic-technology']",A Co-Design Experience: Technologies for Rural Sustainability in Colombia | 2022 Edition,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2021-11-12T18:52:44.261Z,"By Chelsi Cocking and Manaswi MishraCode Cypher, hosted by CAST Visiting Artist and Grammy-winning rapper Lupe Fiasco and Professor Nick Montfort, invited MIT students to develop computational artworks that play with language and rhythm. The event was hosted from 10am to 6pm on Saturday, October 9, 2021.On the morning of the event at 10am, all the participating teams at the Code Cypher were given the following project prompt:“What I saw in the forest...” Over the next few hours, teams hacked and interpreted the themes in the prompt into a diverse set of computational art performances. After being given the prompt, we hosted a quick ""yes, and...” brainstorming ideation session with each other. Thinking of what the prompt reminded us of. Riffing off of each other's ideas and sparks of inspiration.  We both became enamored by the idea of tree trunks. We thought of what we ""saw"" in a tree trunk, and we saw: a fingerprint, a music record, a clock. We saw these things represented in the grooves and contours of a tree trunk.",,"['opera-of-the-future', 'future-sketches']",https://dam-prod2.media.mit.edu/uuid/745e59ac-fb41-4d0a-a682-97cefcb6a151,386504,,2021-11-15T15:40:37.171Z,"['chelsi@media.mit.edu', 'manaswim@media.mit.edu']",True,tree-clocks,2021-10-09,"['art', 'music', 'computer-science', 'interactive', 'code']",Tree Clocks: an interactive rhythm and language work,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-11-29T20:20:20.442Z,"In this experiment, I used 5 phototransistors to locate the position of a light source on a 2D plane, and drove interactive demos via serial communication.",,['future-sketches'],https://dam-prod2.media.mit.edu/x/2021/11/29/lingdong_w08-000.png,386679,,2021-11-30T19:24:06.109Z,['lingdong@media.mit.edu'],True,searchlight,,[],Searchlight: Tracking Light in Space,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-11-29T20:44:51.870Z,An Introduction to using JavaScript as a computer-aided design tool for fabrication.,,['future-sketches'],https://dam-prod2.media.mit.edu/x/2021/11/29/lingdong_w03-016.png,386687,,2021-12-01T03:38:32.550Z,['lingdong@media.mit.edu'],True,from-cad-to-jad-javascript-aided-design,,[],From CAD to JAD (JavaScript-Aided-Design),PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2021-12-15T18:02:39.715Z,"Two Mobility Futures 0∞ is a research project that encompasses storytelling, a democratic decision making platform, a city model, and an immersive exhibit. It will be on display at the Guggenheim Museum Bilbao from April 8 through September 18, 2022 as part of an exhibit titled Motion. Autos, Art, Architecture, curated and designed by Norman Foster.",,['city-science'],https://dam-prod2.media.mit.edu/x/2022/03/21/model car part window_3.gif,386811,,2023-03-18T02:18:31.032Z,"['gba@media.mit.edu', 'maitanei@media.mit.edu', 'jblinder@media.mit.edu', 'yasushis@media.mit.edu', 'alonsolp@media.mit.edu', 'naroa@media.mit.edu', 'mdchurch@media.mit.edu', 'kll@media.mit.edu', 'thomassl@media.mit.edu', 'csmuts@media.mit.edu']",True,two-mobility-futures,2021-05-01,"['design', 'architecture', 'art', 'civic-technology', 'environment', 'storytelling']",Two Mobility Futures 0∞,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2021-12-16T13:50:08.003Z,"SynopsisThe class will cover a wide range of perspectives on deepfakes, from their historical ancestors through their philosophical underpinnings and on to modern incarnations. Topics will be divided into two factions: Theoretical and Practical. The theoretical part will include readings of seminal works on simulative media as well as invited speakers that will discuss the societal impact of deepfakes on aspects of our lives: news, politics, entertainment and arts. The practical part will include programming tasks (Python) for hands-on usage of deepfake generators, a discussion of their powers as well as their limitations. Topics covered:History, Philosophy of the Synthetic (“Simulative”) Media: Simulacra and Simulation, Hyperreality, Transhumanism, Faith in Fakes, Ultra-realistic CGI (from Photoshop to Unreal Engine v5 and The Matrix); Baudrillard, Bostrom, McLuhan, EcoDeepfake-and-X, Societal Impacts: Journalism, Politics, Activism, Social Media, Film Industry & Hollywood, Creative and artistic expression, Learning and motivationDeepfake engines: Machine and Statistical Learning basics, Generative AI models, GANs and other Decoders, hands-on deep generators.Deepfake detectors: Datasets, competitions and approaches.Recommended literatureHands-On Machine Learning (2nd ed). Aurélien Géron. O’Reilly 2020Deep Learning with Python (2nd ed). Francois Chollet. Manning 2020Generative deep learning. David Foster. O’Reilly 2019.GANs in action. J. Langr and B. Vladimir. Manning 2019.Recommended related coursesComputer Visions, MIT Media Lab, MAS.S68 F’19, Shilkrot, Bernal and MaesMachine learning, by Andrew Ng, CourseraDeeplearning.ai, CourseraGANs specializationMIT EdX Machine Learning with Python (Barzilay & Jaakkola)Deepfakery, MIT Open Documentation & WITNESS",2020-12-03,['fluid-interfaces'],https://dam-prod2.media.mit.edu/uuid/79007402-9652-40c2-8c89-f4262c815ea3,386829,,2021-12-16T13:55:18.269Z,"['patpat@media.mit.edu', 'roys@media.mit.edu', 'joaleong@media.mit.edu', 'pattie@media.mit.edu']",True,Experiments-in-Deepfakes,2020-09-03,"['human-computer-interaction', 'art', 'artificial-intelligence', 'human-machine-interaction', 'machine-learning']","Experiments in Deepfakes: Creativity, Computation and Criticism",PUBLIC,https://deepfakes.media.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2021-12-20T16:24:11.165Z,"Fronteras de Papel es una iniciativa creada por un equipo interdisciplinar de jóvenes interesados en reflexionar, analizar y comunicar problemáticas sociales a través de medios de expresión artística. Para lograrlo, se busca el constante relacionamiento con las comunidades impactadas y la implementación de metodologías sociales innovadoras.El colectivo está conformado por una diseñadora industrial, dos diseñadores de la comunicación visual, dos antropólogas, una trabajadora social y un comunicador y politólogo.",2019-05-31,['human-dynamics'],,386853,,2021-12-20T16:51:09.837Z,['isal@media.mit.edu'],True,fronteras-de-papel,2019-05-05,[],Fronteras de Papel,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1
False,2022-01-07T03:37:35.141Z,"We designed an app that uses augmented reality to make public transit in Philadelphia more accessible for people with disabilities.The design won 1st place and the Audience Choice award in the SEPTA for All: Augmenting Transit with Augmented Reality competition, which challenged teams of developers and designers to create augmented reality tools that will make transportation more accessible for people with disabilities in Philadelphia. The challenge was administered by ​​SmartCityPHL, Philadelphia’s Office of Transportation, Infrastructure, and Sustainability (OTIS) and Southeastern Pennsylvania Transportation Authority (SEPTA). Design teamIla Kumar, a graduate student in MIT Media Lab’s Affective Computing groupTarathorn Boonngamanong, a senior at Drexel University studying Game Design & Production and Virtual Reality & Immersive MediaDesignWe modified and enhanced the existing SEPTA app, using marker and location-based augmented reality technology to create an easy to use and accessible interface. New features provide riders with real-time notifications of upcoming stops based on their origin, destination, and current location. The notifications can be customized to be auditory, visual, and/or haptic to support individuals with a range of abilities.Three accessibility issues motivated by our proposal:Transit vehicles provide inaccurate stop announcements or do not provide announcements.People with disabilities have difficulty accessing information about detours and changes to trips while enroute.People with disabilities report not having enough time to disembark the vehicle.We think it’s important to highlight here that the augmented reality in the app is simple, and we believe that’s a big part of what makes it accessible and valuable for people. They can use it flexibly with any assistive technology, and can rely on the fact that it will always be accurate and up to date. Prototype testingDuring the design process we prioritized learning from people with lived experience, conducting paid prototype-testing interviews with people with disabilities in Philadelphia.Overall, we heard very positive feedback about the app. Below are some highlights.“ … Maybe I slept off during the time that I was meant to be taking my visuals, I’ll just get the alert and I’ll just be reminded of where to stop. It’s a beautiful concept.”“... Right now bus drivers have the option to turn off the speech in the bus and a lot of times they do. And sometimes if you ask a driver to let you know when such and such stop is, well you know, they’re driving. They can’t always be expected to remember everything. So I think this would be really good.”“ It sounds like this would be very helpful for someone who is totally blind or my buddies who have really poor vision…”“ … I would love to use it regularly whenever I am traveling.”“ This is far better than the previous Septa app…”We also received some helpful actionable feedback, which led us to make the following modifications to our design. Allow people to create alerts before getting on the vehicle. This helps people differentiate between their bus and other buses that arrive at their stop and helps people know when to head over to the bus stop.Allow people to “favorite” trips in the app. This allows people to set up routes in advance, so they don’t have to worry about doing it while in transit. It also saves work for people who tend to take the same route again and again.Integrate the app with SEPTA Key. This saves people the effort of telling the app their starting stop and route - all users need to do is tap their key card when they enter the bus and then put in their destination stop to get alerts.ImplementationNow that we have been awarded 1st place in the SEPTA for All challenge, we are excited to pilot and iterate on our design with SEPTA and Philadelphia residents who have disabilities. Stay tuned as we progress further. We hope that this project is able to help SEPTA become more accessible to those who need it most.",2022-06-14,"['affective-computing', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/01/28/Promo_3 (2).gif,386939,,2023-09-07T22:10:13.622Z,['ilak@media.mit.edu'],True,septa-alerts,2021-06-14,[],SEPTA Alerts: Improving the accessibility of public transportation in Philadelphia for people with disabilities,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-01-16T05:10:29.305Z,"MotivationResearch studies from Dr. Shah's lab have described several deep learning methods and models to assist clinical decision-making using medical images. Reliable segmentation of cells or tissues from medical images continues to remain a critical need for generalizable deep learning. Random initialization or transfer of model weights from natural-world images, gradient-based heatmaps, and manifold learning have been used to provide insights for image classification tasks. An automated workflow based on statistical reasoning that achieves reproducible medical image segmentation is lacking. The toolkit reported in this study led by Dr. Shah and published in Cell Reports Methods identifies training and validation data splits, automates the selection of medical images segmented with high accuracy, and describes an algorithm for visualization and computation of real-world performance of deep-learning models.Technical summaryGeneralizability of deep-learning (DL) model performance is not well understood and uses anecdotal assumptions for increasing training data to improve segmentation of medical images. We report statistical methods for visual interpretation of DL models trained using ImageNet initialization with natural-world (TII) and supervised learning with medical images (LMI) for binary segmentation of skin cancer, prostate tumors, and kidneys. An algorithm for computation of Dice scores from union and intersections of individual output masks was developed for synergistic segmentation by TII and LMI models. Stress testing with non-Gaussian distributions of infrequent clinical labels and images showed that sparsity of natural-world and domain medical images can counterintuitively reduce type I and type II errors of DL models. A toolkit of 30 TII and LMI models, code, and visual outputs of 59,967 images is shared to identify the target and non-target medical image pixels and clinical labels to explain the performance of DL models.",2022-08-26,"['health-0-0', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/01/16/1-s2.0-S2667237521001661-gr3.jpg,387015,,2023-09-07T20:07:11.506Z,['pratiks@media.mit.edu'],True,toolkit-for-real-world-performance-evaluation-deep-learning-models,2021-11-19,[],Toolkit for real-world performance evaluation of medical grade deep learning models,PUBLIC,https://www.pratiks.info/,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-01-20T13:46:15.752Z,"The ""dream"" has historically been conceived in the West as internal, entirely private, impossibly personal. This is at odds with much of the world, where dreamers and dream workers see the dream either as a parallel and continuously existing landscape (i.e. Aboriginal Australian Pintupi) or see the dream as internal spaces which must be aligned and made continuous for the creation of a cohesive society (i.e. Ongees of the Andaman Islands). Dream data in the US is typically used to separate us, as another kind of data for categorizing and capturing; this is what people dream when they are young, or old, or blind, or Republican. There is great interest in this sort of data, which has shown sensitivity to things which subjects might otherwise want kept secret, by powers like the British Colonial Office centuries ago or Bell Labs just this year. This project is interested instead in the creation of a continuous dream. Of dream walking. It is an automated, algorithmic negotiation between stories, a shuffle so that one end meets another's beginning. Given 1000 dreams, it will reorder them such that they create one continuous dream. Given my dream and yours, whoever you may be, it will create a path that walks, hops lily pad to lily pad, dream to dream, from one story to another until I arrive in your dreamscape. Interpolating between dreams in such a manner requires establishing meaningful relationships between a multitude of dreams. We utilize the BERT framework, a powerful transformer with an extensive understanding of natural language, to align thousands of dreams according to common topics. This topical clustering happens beyond simple pattern recognition. BERT’s deep learning approach to natural language processing makes it capable of understanding complex semantics. The result is a high-dimensional cloud of dreams where spatial proximity between data points translates to semantic relatedness. Dreams with comparable content are packed together, forming small and big communities of common themes. Moving through its axis allows us to explore these communities.In order to make this cloud easily accessible to humans, its dimensions are reduced to two or three via Universal Manifold Approximation and Projection, or UMAP, before a final clustering process through HDBSCAN. The resulting point cloud can be almost treated as a geographic entity. To interlink two dreams, one can simply walk a path in-between and inspect the dreams situated along the way. To do so, we developed a path-finding algorithm designed for a balance between locally and globally optimal paths. After selecting a desired start and end dream, the system yields a list of real dreams which semantically approach each other. Following this path of dreams does not only offer us a better understanding of the cognitive and emotional distance between two dreamers but also a potential guide for connecting their mental spaces. Where we once seem impossibly distant, each in our dreamt spaces a world apart, here we see a path composed of small, familiar steps, connecting us.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/01/20/Screen Shot 2022-01-20 at 08.50.25_j2CiTie.png,387054,,2023-09-06T20:50:55.125Z,"['eyalp@media.mit.edu', 'igrover@media.mit.edu', 'patpat@media.mit.edu', 'abyjain@media.mit.edu']",True,we-walk-the-line,2022-01-20,[],We Walk The Line,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-01-21T15:11:35.129Z,"Climate change presents an existential threat to human civilization, and the IPCC report of August 2021 sounds like “a death knell for coal and fossil fuels before they destroy our planet.""  With cities generating more than 70% of current global CO2 emissions, and with 90% of future population growth occurring in urban areas, it is a societal imperative that cities rapidly transition to a low-carbon future.This workshop was a rapid-fire, high-level exploration of how to model urban interventions that could enable low-carbon (ultimately zero-carbon) cities, using the MIT-Kendall Square district as the case study.  We focused on two questions:- What would be required for MIT-Kendall Square to achieve zero-carbon in 20 years?- Can social performance be simultaneously increased to create a model entrepreneurship community?Projects:",2021-12-31,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/01/21/PHOTO-2021-09-08-14-21-03.jpg,387076,,2023-09-06T20:50:01.248Z,[],True,mas-552-projects-fall-2021,2021-09-01,[],MAS.552 Modeling Zero-Carbon Cities - Students projects - Fall 2021,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-01-27T15:52:02.409Z,"Three-dimensional Digital Knitting of Intelligent Textile Sensor for Activity Recognition and Biomechanical MonitoringWe present an approach to develop seamless and scalable piezo-resistive matrix-based intelligent textile using digital flat-bed and circular knitting machines. By combining and customizing functional and common yarns, we can design the aesthetics and architecture and engineer both the electrical and mechanical properties of a sensing textile. We propose a method to shape and personalize three-dimensional piezo-resistive textile  that can conform to the human body through thermoforming principles with melting yarns. It results in a robust textile structure and intimate interfacing, suppressing sensor drifts and maximizing accuracy while ensuring comfortability. The digital knitting approach enables the fabrication of 2D to 3D pressure-sensitive textile interiors and wearables, including a 45 x 45 cm intelligent mat with 256 pressure-sensing pixels, and a circularly-knitted, form-fitted shoe with 96 sensing pixels across its 3D surface.  Furthermore, we have designed a visualization tool and a framework that treats the spatial sensor data as image frames.  Our personalized convolutional neural network (CNN) models are able to classify 7 basic activities and exercises and 7 yoga poses in-real time with 99.6% and 98.7% accuracy respectively. Further, we demonstrate our technology for a variety of applications ranging from rehabilitation and sport science, to wearables and gaming interfaces.",,"['responsive-environments', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/07/01/3dknits2.jpg,387162,,2023-09-07T19:58:09.184Z,"['irmandy@media.mit.edu', 'joep@media.mit.edu']",True,3dknits,2021-12-31,"['design', 'human-computer-interaction', 'art', 'artificial-intelligence', 'consumer-electronics', 'entertainment', 'health', 'music', 'sports-and-fitness', 'wearable-computing', 'sensors', 'interfaces', 'machine-learning', 'ethics', 'computer-science', 'performance', 'interactive', 'biomechanics', 'technology', 'data-visualization', 'materials', 'fabrication', 'prosthetic-design', 'data-science', 'textiles']",3DKnITS: Three-dimensional Knitted Intelligent Textile Sensor,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-01-28T03:53:40.057Z,"Black Forest (in development), is a series of monuments, vocal portraits and publications for BIPOC who died from COVID-19.",,[],,387174,,2022-01-28T03:56:40.029Z,[],False,black-forest,,[],Black Forest,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-01-28T03:57:53.306Z,"Black Forest is a participatory public artwork collecting the names and stories of Black lives lost to COVID-19 in the U.S. and planting trees for them. As a living monument and story archive for the over 100,000 Black lives lost, it will include plant-ins and tree giveaways, a phone hotline, a web map, sound and video artworks, and booklets on topics ranging from Black urban forestry to Black feminist ecology.Nationwide, throughout much of the pandemic, COVID-19 disproportionately infected and killed Black people. Many Black neighborhoods have also been affected by poor air quality, fewer trees, and higher temperatures. Tree planting has long been used by Black communities to upend violent histories and uplift Black life, from Hattie Carthan and Neighborhood Tree Corps planting 1,500 trees in Brooklyn to Wangari Maathai and the Green Belt Movement planting 30 million trees in Kenya. So in the Black tradition of storytelling and tree planting, we’re recording the name and stories of those lost via our hotline and planting trees for them in their neighborhoods. Each tree will have a QR code that links to each person’s stories on our website. We’re organizing tree plantings in Black neighborhoods, especially those disproportionately affected by COVID-19. Starting November 5th in Detroit, where COVID-19 deaths were about 90% Black, we’ll be planting 60 trees with The Greening of Detroit in partnership with American Forests and sponsorship from Eddie Bauer and the Detroit Zoo. This will be the first of many “plant-ins” throughout Detroit and other cities across the US in the spring and fall planting seasons. As trees are gradually planted, dedications will be evenly distributed across the number of planted trees. For example, if only 4 trees can be planted for 100 people, each tree will be dedicated to 25 people. Black Forest honors and remembers Black life through new growth and breath as future evidence of the resilience and perseverance of Black communities. We invite the public nationwide to sign up to participate at black-forest.us.",,"['poetic-justice', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-decentralized-society', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/11/22/BF_CTA_NoNumber_1_NoAF_bFoggHX.jpg,387182,,2023-09-07T23:15:55.680Z,['ekene@media.mit.edu'],True,blackforest,2022-01-27,"['design', 'art', 'politics', 'social-change', 'racial-justice']",Black Forest,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-02-03T20:59:56.999Z,"ResearchTechnical Paper | Executive Summary | GitHubAboutIntroductionOpenCBDC is an open source project to engage in collaborative technical research to understand the space of designs for potential central bank digital currencies (CBDC). The first phase of Project Hamilton with the Federal Reserve Bank of Boston led to the release of OpenCBDC-tx, a research codebase under a larger open source project called OpenCBDC. The goal of this global open source codebase is to help central bankers, policymakers, academics, and others make decisions on how this technology might be developed while promoting neutral, collaborative global research. Through OpenCBDC we also hope to engage a global community in research and development to learn how digital currency systems can be designed to best advance privacy, user agency, innovation, and financial equity.frequently asked questionsCentral Bank CollaborationsThe DCI is engaged in collaborative research projects with central banks around the world to explore the technical feasibility of CBDC and to help policymakers understand the implications and tradeoffs of various design choices. Learn more about our collaborators at the Federal Reserve Bank of Boston and about Project Hamilton, a multi-year, collaborative research project between the MIT Digital Currency Initiative and the Federal Reserve Bank of Boston. The goal of Project Hamilton is to investigate the technical feasibility of a general purpose CBDC that could support a payment economy at the scale of the United States, as well as to gain a hands-on understanding of a hypothetical CBDC’s technical challenges, opportunities, risks, and tradeoffs.Learn more about our collaboration with the Bank of England, a twelve-month exploratory research project that supports and builds on DCI’s ongoing research into CBDC, while also contributing to the Bank of England’s wider research and exploration of central bank digital currencies.Learn more about our collaboration with the Bank of Canada, a twelve-month research project that investigating and experimenting with potential CBDC technology designs and approaches, and evaluate key tradeoffs, opportunities, and risks.Contributing to OpenCBDCWe’re actively seeking contributions to our open-source codebase, as well as new working group members.OpenCBDC is an open source project and we welcome pull requests to the code and other contributions. Collaborations in OpenCBDC will be structured around open technical working groups. Each themed working group (e.g. Privacy, Architecture, Interoperability, User Experience, etc.) will advance research with technologists, user researchers, central bankers, private sector engineers, and academics (including students). We’re actively seeking contributions to the open-source codebase as well as new working group members. To learn more or to get involved, please engage with the project on Github or Zulip, or reach out directly to our team by filling out our OpenCBDC interest form to receive monthly notifications from the OpenCBDC team.OpenCBDC Working GroupsCentral banks have traditionally played an integral role in supporting safe, low-cost, and accessible payment systems. With the rise of digital payments, cryptocurrency, and various payment innovations enabled by blockchain technology, the world is rethinking how money is (and should be) designed. Central banks are currently considering the potential utility of Central Bank Digital Currencies (CBDCs). Some central banks have already started issuing CBDCs, and many others have begun experiments. This project is an opportunity for programmers worldwide to help tackle the most pressing problems and answer the most difficult questions surrounding the development and implementation of CBDCs.Working Group MeetingsWe recently hosted a technical webinar geared towards developers to explain what we built. A recording and a Q&A can be found here. You can also find a relevant talk from our collaborator at the Federal Reserve Bank of Boston Jim Cunha here.Technical SpecificationsOpenCBDC-tx is a modular, extensible transaction processor for a hypothetical CBDC that implements two architectures. The first, the atomizer, can process 170,000 transactions per second. The second, using two-phase commit, can process up to 1.7M transactions per second.Core FeaturesA central transaction processor run by a trusted operator (such as a Central Bank)Digital signatures to authorize payments using UTXOs (unspent funds are stored as cryptographic hashes)A modular design supporting experimentation with models where intermediaries could take on a variety of different roles and serve different purposes, including non-custodial or self-custodyCurrently, the design does not directly support intermediaries, fees, or identities outside of public keys, however, we are interested in exploring and benchmarking new features.Key ConceptsDecoupling transaction validation and executionSecure and flexible transaction formatsEfficiency of transaction executionTwo Different Architectures: Atomizer and 2 Phase Commit",,"['digital-currency-initiative-dci', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/03/24/opencbdc copy.jpg,387227,,2023-09-07T15:33:39.466Z,"['narula@media.mit.edu', 'coryf@media.mit.edu', 'madars@media.mit.edu', 'stuewe@media.mit.edu', 'alexjung@media.mit.edu', 'tdryja@media.mit.edu']",True,opencbdc,2022-02-03,"['banking-and-finance', 'civic-technology', 'economy', 'blockchain', 'computer-science']",OpenCBDC,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2022-02-24T20:23:11.818Z,"City Science researchers are engaging in an exploratory process to understand, develop, and model a platform with a focus on the 15-minute city.  This project is a collaboration with NTT DATA, a Media Lab member company, with NTT DATA employees embedded on the research team to add their insight and expertise. Together the teams hope to develop an intuitive and interactive user interface to study land use and mobility scenarios with simulations to estimate the impact of proposed scenarios. This research will result in an urban performance dashboard based on the MSCI ESG Sustainable Impact Metrics which will drive visualizations and reveal benefits to the community. This research began in the fall of 2021. In the coming year, the teams will focus on a specific research area, develop simulations, and further outline  a 15-minute city for the European context.  The teams hope to better understand land use and mobility patterns. This will allow them both to understand the current context and to anticipate future uses. Furthermore, this information can help to outline the urban characteristics that are essential to the 15-minute city and offer insights that can enable a more livable, equitable, and resilient community.",,"['space-exploration', 'city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/02/24/5E5DA8E0-AAC9-4FF8-B8A0-8F5ADCE743CE.JPG,387373,,2023-09-07T20:07:47.680Z,"['doorleyr@media.mit.edu', 'markuse@media.mit.edu', 'alonsolp@media.mit.edu', 'mdchurch@media.mit.edu', 'kll@media.mit.edu', 'ohsugi@media.mit.edu', 'eijiy@media.mit.edu', 'dicostap@media.mit.edu', 'diegoan@media.mit.edu']",True,understanding-the-15-minute-city,2021-11-01,"['design', 'human-computer-interaction', 'civic-technology', 'data']",Understanding the 15 Minute City,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-03-01T02:11:09.788Z,"Social deduction or deception games are games in which a player or team of players actively deceives other players who are trying to discover hidden roles as a part of the win condition. Included in this category are games like One Night Werewolf, Avalon, and Mafia. In this pilot study (N=24), we examined how the addition of visual displays of heart rate (HR) signals affected players’ gameplay in a six player version of Mafia in online and in-person settings. We also examined moments of synchrony in HR data during critical moments of gameplay. We find that seeing signals did affect players’ strategies and influenced their gameplay, and that there were moments of HR synchrony during vital game events. These results suggest that HR, when available, is used by players in making game decisions, and that players’ HRs can be a measure of like-minded player decisions. Future work can explore how other biosignals are utilized by players of social deception games, and how those signals may undergo unconscious synchrony.To be published at ACM CHI EA 2022",,"['affective-computing', 'tangible-media', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/02/28/synchrony_inperson_HR.png,387437,,2023-09-06T20:36:29.867Z,"['picard@media.mit.edu', 'neska@media.mit.edu', 'catfang@media.mit.edu']",True,cardiac-arrest,2022-02-28,[],Cardiac Arrest: Evaluating the Role of Biosignals in Gameplay Strategies and  Players' Physiological Synchrony in Social Deception,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-03-03T00:17:52.233Z,"This project was produced as part of the   2021 Snap Creative Challenge, focused on ""The Future of Co-located Social AR.""Wemoji Exploring complementary channels of expression using augmented reality Group conversations can often be dominated by an individual speaker, while others feel uncomfortable to speak up.   However, it is important to create an environment where people feel safe to express their diverse  thoughts and opinions. We explore the use of Augmented Reality (AR) to empower people to use spatial and visual overlays to emote and react to others.",,"['fluid-interfaces', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/03/03/wemoji-banner-01.jpg,387458,,2023-09-07T20:50:54.961Z,"['pattie@media.mit.edu', 'catfang@media.mit.edu', 'joaleong@media.mit.edu']",True,wemoji,2021-09-01,"['human-computer-interaction', 'augmented-reality', 'communications']",Wemoji,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-03-03T16:58:35.804Z,"This project is a subset of the larger Dream Hotel project ongoing with CAST Visiting Artist Carsten Höller. The Room for Flying Dreams is one of the proposed rooms in the proposed hotel, where visitors will experience sleep science protocols as art pieces in the museum. This work is inspired by the Nielsen labs work on stimulation of Flying Dreams (2020). Bedtime begins with a trip on Carsten Höller's Two Flying Machines (2015). Afterwards, a viewing of our co-created video, Flying Mushrooms, a collaboration with Wendi Yan.Bedtime reading consists of a book of 1, 488 Flying Dreams, which we have algorithmically shuffled using the pipeline Titus Ebbecke created for We Walk the Line to be one continuous dream of flight from many minds across many years. A tea of Wild Red Asparagus, the Flying Herb,  is a last step before sleep. Dream of flight.",2022-11-10,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/03/03/mush.png,387500,,2023-09-06T22:15:31.831Z,['adamjhh@media.mit.edu'],True,flying-dreams-2,2022-03-01,[],Flying Dreams,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-03-04T18:04:23.052Z,"A revolutionary new piece of civic infrastructure for more inclusive public input.Real Talk for Change is a collaboration between MIT’s Center for Constructive Communication, Cortico, and a growing network of local community groups in Boston. Together, we launched a new civic initiative designed to amplify the voices of Boston community members who are often underheard by current civic processes.For our pilot program, we trained trusted community leaders to facilitate over 60 small group conversations with residents across the city in the lead-up to the 2021 Boston Mayoral and city council elections. Once captured, our Real Talk Fellows used machine-powered tools to mine over 3,000 minutes of conversation for themes and powerful audio to support them. This analysis was published to an interactive online portal that was made available to the public, in addition to candidates for Mayor and city council. We then surface community voices and perspectives in the media and electoral debate to constructively shape the 2021 Boston election cycle. Motivating ContextReal Talk for Change was inspired by the historic 2021 Boston Mayoral election. We recognized that in order to build a better, more equitable Boston, we needed not only bold, new leaders, but also bold, new systems for public input that were more attentive and accessible to those experiencing the most pressing issues. In a deliberate departure from opinion polls and town halls, Real Talk for Change has been facilitating deep and substantive small group conversations with residents across the city. Our ultimate goal is to build a lasting piece of civic infrastructure in which the voice of the public is based on people sharing their experiences of living in Boston. More specifically, we want to amplify the voices of folks who are too often underheard by current civic processes.ApproachReal Talk for Change’s approach brings together age-old methods of facilitated small group dialogue, and community organizing with modern machine learning, data visualization, and social network design. Our technology enables communities to organize conversations across the full diversity of their population, identify patterns of what people share across these conversations, and channel these voices to influence decision making in a more grounded and direct way than ever before. In addition to promoting a more inclusive democracy, this system is designed to encourage cross-community listening as well as to provide improved visibility and accountability between the public officials and the people. A key role of the technology is to provide transparency to all participants. Conversation participants can see how their voices are being used and – through public forums such as media interviews and public debates – see that community concerns are influencing politics. We believe over time this kind of visible action will encourage new voters and build trust between historically underheard communities, local media, and local institutions.",,"['social-machines', 'center-for-constructive-communication', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/uuid/d56bd11c-4c45-46c6-a8cf-94402a576339,387513,,2023-09-07T22:06:58.152Z,"['wesc@media.mit.edu', 'mhughes4@media.mit.edu', 'dsjen@media.mit.edu', 'dimitrad@media.mit.edu', 'aluk@media.mit.edu', 'hac@media.mit.edu', 'ltovar@media.mit.edu', 'dkroy@media.mit.edu', 'ceasar@media.mit.edu', 'sballing@media.mit.edu', 'ondina@media.mit.edu', 'aheyward@media.mit.edu', 'thanhmai@media.mit.edu', 'hoffman@media.mit.edu', 'wjkang@media.mit.edu', 'starget@media.mit.edu']",True,realtalk-for-change-boston,2021-07-01,"['artificial-intelligence', 'civic-media', 'civic-technology', 'government', 'politics', 'community', 'civic-action', 'marginalized-communities']",RealTalk for Change Boston,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-03-08T16:01:53.609Z,"Identity spans multiple dimensions; however, the relative salience of a dimension of identity can vary markedly from person to person.  Furthermore, there is often a difference between one’s internal identity (a personal view on the relative saliences of different aspects of one’s identity) and external identity (the external world’s view on these relative saliences). We attempt to capture the internal and external saliences of different dimensions of identity for influential users (“influencers”) on Twitter using the follow graph. We consider an influencer’s “ego-centric” profile, which is determined by their personal following patterns and is largely in their direct control, and their “audience-centric” profile, which is determined by the following patterns of their audience and is outside of their direct control. We find that relative to their audiences, influencers exhibit more salience in race and gender in their ego-centric profiles and less in religion and politics. Additionally, we investigate clusters of influencers with different patterns of divergence in salience from their audiences. We discover one cluster in which the influencers exhibit a significantly stronger salience in the LGBTQIA+ dimension of identity. This suggests a large divergence between their internal LGBTQIA+ salience and their external one, as they follow many  more LGBTQIA+ users than their audience. On a practical level this finding, and other findings like it, could be useful in identifying users that can act as a bridge between traditionally underheard communities and audiences that they do not directly interact with.Below is an example of how an influential user's ego and audience centric profiles are calculated. The teal arrows indicate the follows of the influential user, while the red are his audience's follows. These two following patterns determine the ego and audience centric profiles. We find that influencers tend to exhibit more salience on race and gender and less salience on religion and politics relative to their audiences",,"['social-machines', 'center-for-constructive-communication', 'media-lab-research-theme-decentralized-society']",,387575,,2023-09-06T20:35:04.897Z,['sfulay@media.mit.edu'],True,divergences-in-following-patterns-between-influential-twitter-users-and-their-audiences-across-dimensions-of-identity,2022-03-08,"['social-networks', 'social-media']",Divergences in Following Patterns between Influential Twitter Users and their Audiences,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2022-03-08T17:04:55.245Z,We have a lot of experience with being productive remotely but far less experience with how people can be creative. Increasingly we risk becoming isolated both physically and intellectually. Telecreativity is a suite of inventions and experiments to restore collaborative creative activities through remote connections.,,"['viral-communications', 'media-lab-research-theme-cultivating-creativity']",,387583,,2023-09-06T20:34:18.251Z,['lip@media.mit.edu'],True,theme-telecreativity,2022-03-08,[],Theme | Telecreativity,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0
False,2022-03-11T01:08:11.088Z,"Many tasks are not easily defined and/or too complex for supervised machine learning approaches. For these reasons, a technique known as reinforcement learning has grown in popularity. With this technique, agents learn from experience and eventually gain the ability to effectively perform a given task. However, even reinforcement learning has it's limits when applied to tasks with high dimensionality, thus large action and/or observation spaces.Deep reinforcement learning (DRL), a version of reinforcement learning which utilizes deep neural networks is able to address the more complex tasks that standard RL can not. An excellent usecase of such a task is an UAV autonomously navigating through the center of a racing gate. For this project, Open AI's popular Baselines DRL library was integrated with Unreal Engine. Neural network hyperparameters are directly configurable through the user interface, allowing engineers to quickly iterate and test out various configurations. All of OpenAI's standarly available DRL networks are available for use with this Unreal Engine integration. Trained models are automatically saved to a dedicated directory on the machine running the simulator, and can easily loaded within the editor. The DRL AUAV system's performance was tested using the autonomous gate navigation task, with the resultant model able to reach a 98.3% success rate during exploitation after approximately 10,000 episodes of training/exploration.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/uuid/2ff70c0c-017f-424e-a5d0-d6121223af2d,387650,,2023-09-06T20:33:23.444Z,['kaputsos@media.mit.edu'],True,drl_auav,2022-03-10,"['robotics', 'virtual-reality', 'computer-vision', 'artificial-intelligence', 'augmented-reality']",Deep Reinforcement Learning for Autonomous UAVs (AUAVs),PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-11T02:03:35.639Z,"Navigation for autonomous UAVS (unmanned aerial vehicles) is a complex problem and physical field testing of associated tasks introduces a great deal of risk. Furthermore, task performance exhibited in traditional virtual simulations often does not directly transfer to the physical world. This results in additional system modifications being needed, which greatly increase development complexity and time-to-deployment.In order to address this issue, a virtual UAV simulator (using Unreal Engine) was built around the machine learning technique of domain randomization. The simulation, tailored to autonomous UAV navigation, randomizes environmental components such as lighting, materials, and object pose on each new lap of a racing circuit. As the virtual UAV collects data it is being saved into a dedicated directory, so that the images may be used for training a computer vision (CV) model such as a CNN. The CNN is to be used for detecting racing gates and calculating state, so that appropriate navigation commands may be sent.This randomization technique increases the object detection model's robustness. Variation in the training data's environmental materials and lighting enable the model to adapt and reliably navigate when faced with this uncertainty.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/10/image (3)_lEjI8EF.png,387658,,2023-09-06T20:33:00.587Z,['kaputsos@media.mit.edu'],True,domain-randomization-synthetic-data-auav,2022-03-10,"['robotics', 'virtual-reality', 'computer-vision', 'artificial-intelligence', 'augmented-reality']",Domain Randomization & Synthetic Data Generation for AUAVs - Reducing Perceptual Uncertainty of Sim2Real Navigation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-11T02:12:46.324Z,"Depending on the operational environment of an autonomous system, a great deal of perceptual uncertainty may be introduced to an object detection model. For instance, changes in environmental lighting, reflections, lens flares, and other factors may require a system to be incredibly robust in order to avoid failure. Unfortunately, many machine learning techniques require a great deal of resources to obtain robustness, and this may not be feasible for some engineering teams.Fiducial marker-based (or ArUco) detection offers a means of obtaining reliable performance without the need of machine learning techniques. However, in more complex and dynamic environments this technique may not be sufficient. For this project, a joint marker detection, state estimation, and control algorithm was developed for AUAVs. * Marker Detection: The algorithm first detects ArUco markers in the incoming visual data stream* State Estimation: The relative transforms of the AUAV with respect to the marker is calculated* Control: The output of the state estimation (relative transform) is utilized to generate a trajectory for aligning the UAV with the center of marker, and performing a subsequent gate maneuver.",,"['personal-robots', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/10/262719144_10158710336788922_2419997503878813594_n.jpg,387666,,2023-09-06T20:32:34.955Z,['kaputsos@media.mit.edu'],True,fiducial-nav-auav,2022-03-10,"['robotics', 'virtual-reality', 'computer-vision', 'artificial-intelligence']",Fiducial Marker-Based Navigation for Autonomous UAVs (AUAVs),PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-11T02:39:00.479Z,"With the advent of real-time photorealism (RTPR), virtual environments are now able to achieve higher degrees of engagement than ever before. Unfortunately, in order to power these high-fidelity worlds, a powerful desktop machine on a pro-gaming or virtual production level is needed.  This limits most of society's exposure to these highly engaging experiences, and ultimately the applications they make take on. This web-based simulation system was made in order to bring RTPR into the hands of users without a high-end PC. The simulator runs on a remote machine/server and streams simulation frames via the internet browser. Controls for the simulator are provided in the webpage and send appropriate input to the simulator.",,"['personal-robots', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/03/10/PXL_20210811_224005209 (1)_MLJcRM7.jpg,387674,,2023-09-06T20:24:23.459Z,['kaputsos@media.mit.edu'],True,cloud-uav-sim,2022-03-10,"['robotics', 'virtual-reality', 'artificial-intelligence', 'augmented-reality']",Unmanned Aerial Vehicle (UAV) Pilot Simulator,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-03-14T18:12:00.740Z,"Photorythms: a computational art-based inquiry of portrait photographyPhotorythms investigates whether computational methods such as facial detection, computer vision, and generative forms can be utilized to create more expressive and artistic works of portraiture and the face. Giving a new take on portrait photography and new life to images through computation.Can portrait photos and photos of people be more expressive than they are today?Can computational methods assist this?",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/9a4c0d93-890a-4c3c-86ab-86ee2e129c25,387710,,2023-09-08T21:14:06.096Z,['chelsi@media.mit.edu'],True,photorythms,2022-01-01,"['design', 'computer-vision', 'art', 'computer-science', 'creativity']",Photorythms,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-03-15T18:46:39.758Z,"3 Voices is an artistic research project that looks beyond conventional approaches of listening to music, and instead probes at the deeper listening potential of bodies as conductive and resonant objects in their own right.As a testing ground for the project, we reimagined Morton Feldman’s 1982 vocal work Three Voices. In this project, we have reframed the voices in Feldman’s piece, making them tangible, touchable and transferable through the use of our soft robotic fabrics that provide nuanced, multimodal haptic feedback to the body that resembles a singer’s upper body thoracic expansions, contractions and laryngeal vibrations. Our dreams for expanding this research work encompass further explorations in how on-body or in-body sensing systems could capture the somatic experience of singing, or dancing bodies, and transfer this experience onto another person’s body through actuation.",2023-12-01,"['tangible-media', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/04/06/Screen Shot 2022-04-06 at 10.47.09_9Ypwj7t.png,387733,,2023-09-07T20:06:10.533Z,['ozgun@media.mit.edu'],True,3-voices-a-soft-robotic-opera,2021-12-01,"['robotics', 'design', 'human-computer-interaction', 'art', 'machine-learning']",3 Voices: A Soft-Robotic Opera Transposing Singing Experience Across the Audience,PUBLIC,http://www.ozgunkilic.com/,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-03-16T20:30:39.604Z,"Autonomous bicycles have recently been proposed as a new and more efficient approach to bicycle-sharing systems, but how do they compare from an environmental perspective?",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/10/24/MLWebsiteGIF_Cr7XV6f.gif,387758,,2023-09-07T20:50:18.477Z,"['naroa@media.mit.edu', 'alonsolp@media.mit.edu', 'kll@media.mit.edu']",True,autonomous-shared-bikes-env-impact,2021-09-06,"['environment', 'transportation', 'urban-planning', 'autonomous-vehicles']",Can autonomy make bike-sharing more sustainable?,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-20T16:26:15.620Z,"We explore how AI-systems can effectively assist human reasoning processes by building and evaluating combined human+AI information processing and decision making systems.Increasingly, seminal decisions affecting personal and societal futures are made by people assisted by Artificial Intelligence (AI) systems.  While the focus in the research community has been on how to make AI systems more accurate, less biased, and able to provide explanations, what ultimately matters is how the combined human+AI system performs and whether the decisions of that combined system are fair, accurate, and efficient. Insufficient attention has been given to the study of the interaction between an AI system and a human and how the way an AI is integrated into human decision making affects the performance of the combined human-AI decision system in the short and long-term. Research by some others and our team show that this question is of seminal importance to the future of humanity. People too readily adopt advice from an AI, even if that AI is malicious or simply wrong. There is also a risk of over-reliance on AI systems over time, with people forgetting relevant skills or expertise themselves.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/08/22/ai-co-01.png,387829,,2023-09-06T22:13:45.929Z,"['vdanry@media.mit.edu', 'pattie@media.mit.edu', 'patpat@media.mit.edu', 'lancelot@media.mit.edu']",True,human-ai-co-reasoning,2022-03-20,['artificial-intelligence'],Theme | Augmented Reasoning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2022-03-20T18:35:10.244Z,"This extended abstract was presented at the International Conference on Computational Social Science (IC2S2) 2022.In the past few years, there has been an increase in AI-based disinformation campaigns, which are attempts to spread misinformation online for strategic reasons. How AI-systems explain how they arrive at their classifications can be deceptive, in that they can be manipulated to make the system appear more reliable than it is. For example, a bot may claim to be human in order to evade detection, or a machine learning system may falsely claim a piece of information to be true when it is not. While previous work has shown that AI-explanations help people determine the veracity of information online and change people’s beliefs, little is known about how susceptible people are to deceptive AI systems.This project investigates how people's discernment varies when AI systems are perceived as either human fact-checkers or AI fact-checking systems, and when explanations provided by those fact-checkers are either deceptive (i.e. the AI system falsely generating explanations for why a true headline is false or why a false headline is true) or honest (i.e. the AI system accurately generating explanations for why a true headline is true or why a false headline is false). In a pilot study, we generated a dataset with honest and deceptive explanations for why a news headline was either true or false  by prompting the state-of-the-art text-generation model GPT-3.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/08/22/deceptive-ai-wide.png,387838,,2023-09-06T20:22:49.949Z,"['vdanry@media.mit.edu', 'patpat@media.mit.edu', 'zive@media.mit.edu', 'groh@media.mit.edu', 'pattie@media.mit.edu']",True,deceptive-ai-systems,2022-03-20,[],Deceptive AI systems in Human Information Processing,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2022-03-21T20:38:14.236Z,"Aguahoja III expands the capabilities of a broad library of sustainable, organic 3D materials with generative patterns that countervail structural forces.",2023-05-26,"['mediated-matter', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/03/22/AguaIIIBack_2 copy.jpg,387864,,2023-09-07T22:13:14.651Z,"['nah6cz@media.mit.edu', 'jpcosta@media.mit.edu', 'fkraemer@media.mit.edu', 'neri@media.mit.edu']",True,aguahoja-iii,2021-04-01,"['design', 'architecture', 'art', 'ecology', 'biology', 'materials']",Aguahoja III,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2022-03-22T16:40:01.775Z,"Aguahoja II builds upon the platform technology of its predecessor, enabling large-scale multi-material 3D printing with abundant, organic materials.",2025-01-01,"['mediated-matter', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/03/22/002_AguahojaII_Cpqvsgh.jpg,387911,,2023-09-07T22:27:26.142Z,"['nah6cz@media.mit.edu', 'reweber@media.mit.edu', 'j_duro@media.mit.edu', 'josephk@media.mit.edu', 'jvanzak@media.mit.edu', 'neri@media.mit.edu']",True,aguahoja-ii,2018-01-01,"['design', 'art', 'biology', 'materials']",Aguahoja II,PUBLIC,https://www.niclee.co/,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2022-03-22T21:09:28.199Z,"As part of its recommendations to increase transparency, the Aspen Institute’s Commission on Information Disorder recommended – in its final report – that “Congress should implement protections for researchers and journalists who violate platform terms of service by responsibly conducting research on public data of civic interest.”Our early-stage research project aims to shed light on public health information ""super spreaders,” demonstrating what is possible on all social platforms if the Aspen Commission's transparency recommendations enable third parties to access messages that are ""too loud to ignore."" We analyze the overarching structure of Twitter dialogue surrounding the pandemic over the last two years, its protagonists, narratives, and evolution. Our methodology highlights the main information super spreaders by quantifying their impact and describing their audiences. In partnership with a public health agency, we plan to also evaluate the veracity of super spreader narratives, and the implicit claims they perpetuate. Our primary research questions are:In the topic of COVID public health, who are the Twitter actors with greatest influence and engagement?What is the veracity of the public health messages, and their implicit claims, spread by the most prominent actors?  What other properties characterize (a) these actors, (b) their public health messages, (c) their reach, and (d) their audiences?",,"['social-machines', 'center-for-constructive-communication', 'media-lab-research-theme-decentralized-society']",,387929,,2023-09-06T20:22:09.756Z,"['hopes@media.mit.edu', 'slongpre@media.mit.edu']",True,superspreaders,2022-03-22,['social-networks'],Super Spreaders,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2022-03-22T22:03:10.541Z,A tool for exploring the Media Lab’s research and synthesizing new project ideas.,,"['viral-communications', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/9698f077-ddc9-43b9-bc7c-f2366e06ed46,387937,,2023-10-16T19:39:40.102Z,"['dunnell@media.mit.edu', 'lip@media.mit.edu']",True,latent-lab-legacy,2022-02-01,"['design', 'human-computer-interaction', 'artificial-intelligence', 'data']",Latent Lab: Legacy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2022-03-23T23:43:05.555Z,"We developed an automatic code analysis tool, LevelUp, to support educators and learners and built it into a block-based programming platform. LevelUp gives users continuous feedback on their text classification projects, showing them what they have done well and how they can improve. We evaluated the tool with a crossover user study where participants constructed two text classification projects, once with the LevelUp and once without it. To measure the tool's impact on participants' understanding of text classification, we used pre-post assessments and graded both of their projects against LevelUp's rubric. We saw a significant improvement in the quality of participants' projects after they used the tool. We also solicited participants' feedback using a questionnaire. Overall, they thought LevelUp was useful and intuitive. Our investigation of this novel automatic assessment tool can inform the design of future code analysis tools for AI education. ",2022-09-30,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/23/progress_tab_interface.png,387975,,2023-09-07T20:58:15.116Z,['randiw12@media.mit.edu'],True,ai-progress-tab,2021-09-01,[],Automatic Assessment of Block-Based Machine Learning Projects,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-03-24T15:06:02.034Z,"What will it take for decentralized networks to realize our vision of a radically improved financial system that protects user privacy and increases user agency? In addition to DCI’s focus on open source development and neutral research into the privacy, security, and scalability of decentralized networks, we also believe it will require a robust awareness of the people inside and outside the traditional financial system, their needs, and their perspectives on digital currency’s potential. That’s why we are collaborating with Maiden, a non-profit user-research lab, to surface user insights that can inform our work as technologists as well as contribute to the public policy dialogues surrounding digital currencies. We recently completed a US-based user-research study, addressing some of the riskiest and most-common assumptions about prospective digital currency users and use cases.",2023-01-31,"['digital-currency-initiative-dci', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/03/24/Maiden_ReportCover_V2.jpg,387998,,2023-09-07T20:06:55.019Z,['narula@media.mit.edu'],True,cbdc-user-research-collaborations,2021-12-01,[],CBDC User Research Collaborations,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-03-25T19:11:25.729Z,"Our Oceans IoT technologies enable new applications in climate and ecological monitoring, aquaculture, energy, and robotic navigation. Bringing massive connectivity to low-cost, low-power ocean sensors is important for numerous oceanographic applications across climate/weather modeling, marine biology, aquaculture, and defense. However, standard IoT technologies (e.g, Bluetooth, WiFi, GPS) cannot operate underwater, which has left 70% of our planet (the ocean) beyond their reach.  Our research is changing this reality by inventing IoT technologies that are inherently designed for the ocean. By rethinking the entire IoT technology stack in the context of oceans, we introduced low-cost (< $100), net-zero-power, scalable connectivity technologies that seamlessly operate underwater and pave the way for massive underwater sensing, networking, localization, imaging, and machine learning.How can we enable a batteryless ocean IoT?Underwater BackscatterWe developed Piezo-Acoustic Backscatter (PAB), the first technology that enables backscatter networking in underwater environments. PAB relies on the piezoelectric effect to enable underwater communication and sensing at near-zero power. Check out the video below and our paper to see how it works.",,"['signal-kinetics', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/28/backscatter-gopro-photo.jpg,388028,,2023-09-06T20:20:29.868Z,"['fadel@media.mit.edu', 'alineeid@media.mit.edu', 'wakbar@media.mit.edu', 'afzals@media.mit.edu', 'nazishn@media.mit.edu', 'osvyrd@media.mit.edu', 'jradema@media.mit.edu', 'allam@media.mit.edu']",True,oceans-internet-of-things,2022-03-25,"['virtual-reality', 'artificial-intelligence', 'augmented-reality', 'data', 'energy', 'environment', 'sensors', 'imaging', 'mapping', 'technology', 'internet-things', 'ocean', 'climate-change']",Oceans Internet of Things,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-25T21:44:18.449Z,"Effective community engagement is critical in diverse organizations, such as school districts. However, current methods, including surveys, public hearings, and online forums, tend to lack transparency, feel like an afterthought, and do not reflect the actual demographics of the community. Civic technology has helped increase transparency, accessibility, and accountability between local communities and governments. In this case study, we describe the process of creating a seven-month-long community engagement process within Charlotte-Mecklenburg Schools (CMS) to inform the design of new magnet programs or theme-based learning experiences. We applied concepts from in-the-wild HCI research to build novel civic engagement technologies ranging from interactive portals to facilitated small-group conversations. Over 400 participants provided input during the different stages of the process, which involved 48 small-group conversations and user interviews. A total of 961 stories and suggestions were gathered, resulting in 8 magnet programs. In our paper, we provide an overview of the community engagement process and our key reflections relating to trust, accessibility, and reaching historically underrepresented communities. We hope our learnings could be helpful for HCI researchers and practitioners who deploy community engagement initiatives. A paper on this work was recently submitted to CHI 2023 as a case study.",2022-08-23,"['center-for-constructive-communication', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/25/CMS Banner Image-01_iuSQDuo.png,388037,,2023-09-07T16:45:13.237Z,"['coverney@media.mit.edu', 'ngillani@media.mit.edu', 'dkroy@media.mit.edu']",True,future-cms-schools,2022-01-07,"['design', 'human-computer-interaction', 'civic-technology']",Voice Your Choice: Creating a Community Engagement Process for New Magnet Programs in Charlotte Mecklenburg Schools,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-03-25T22:15:15.689Z,"During facilitated small-group conversations,  facilitators have to manage many tasks, including handling the technology, managing time, following the script, and more. Facilimate is a digital support tool that helps facilitators handle the mechanics of facilitating a conversation (i.e. managing time and following a conversation guide).  We applied a user-centered design approach to create Facilimate, which involved getting feedback from facilitators while brainstorming, concept sketching, wireframing and implementing.",,"['center-for-constructive-communication', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/25/Facilimate2.PNG,388045,,2023-10-06T00:18:05.735Z,"['coverney@media.mit.edu', 'dkroy@media.mit.edu']",True,facilimate,2021-09-16,"['design', 'human-computer-interaction']",Facilimate,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2022-03-27T21:22:21.723Z,"With the emergence of social robots in people’s daily lives, their interactions with people need to consider not only engaging with individuals, but also with a group of people (e.g. families or colleagues). However, most human-robot interaction work has focused on interacting with a single user at a time, due to challenges in understanding social cues from multiple people at the same time as well as designing interactions with a group of people. Designing for group interactions is vastly different from designing interaction with individuals. Because in a group interaction users are also interacting with each other and not just with the robot, understanding engagement cues require deeper contextualized interpretation, for example. Not only that, conversing with multiple users at the same time requires both significant advancements in sensor technology and dialogue systems. Conversation is central to interactions including in a museum guide setting that this project is targeting. In this project, we aim to design contextualized and personalized conversation experience between a small group of users (2–6 people) and a museum tour-guide robot. The focus of the work is not in conversing with multiple users at the same time. Rather, we propose understanding and tracking individual engagement in a group setting and using this information to understand and strategize timing for an agent to initiate a conversation with a user. When selecting a user to converse with, the agent learns the most effective policy, i.e., optimizes to choose a user whom by increasing the selected user’s engagement best improves others and the overall group engagement. The dialogue experience between the user and the agent will be supported by MRF-Chat, a novel probabilistic graphical model using Markov Random Fields that improves the prediction accuracy of existing deep learning methods by making assumptions grounded in modeling of mutual knowledge between the user and the agent.The aforementioned goals are divided into the following approaches: (1) contextual multimodal verbal and nonverbal sentiment and engagement detection, (2) personalized dialogue understanding and generation based on mutual knowledge modeling, and (3) optimized interaction policy for conversation partner selection to improve group engagement. This final report states the methodology and the findings from this project. We report on experiments on modeling of sentiment and engagement behaviors using verbal and nonverbal temporal cues, where we achieve up to 94% balanced accuracy on the testing set. We present MRF-Chat approaches and evaluation results that have been published in the proceedings of EMNLP-2021. MRF-chat significantly boosts KV Memory and Poly Encoder performances. Even though our results seem promising, we identified few additional approaches to improve the results for generalization and real-time prototyping in the remaining project timeline.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/27/multi-group.png,388066,,2023-09-07T22:20:57.673Z,"['pe25171@media.mit.edu', 'sharifah@media.mit.edu', 'haewon@media.mit.edu', 'igrover@media.mit.edu']",True,multi-party-human-robot-conversation-interactions,2021-01-01,"['human-computer-interaction', 'artificial-intelligence']",Multi-Party Human-Robot Conversation Interactions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2022-03-27T21:50:57.619Z,"Instructed learning is ever-present throughout our lives. An element of it, How-To questions, (e.g., “How do I cook rice?”, “How do I write a check?”, or “How do I send pictures to my family from my iPhone?”) is one of the most common queries for search engines [1] and presumably of conversational agents as well. Answers to How-To questions are generally in the form of a procedure, step-by-step instructions that users perform in sequence. However, people find reading instructions cognitively demanding and often prefer that another person guide them through a procedure [2]. Prior work in automating procedural guidance either concentrates on how to communicate instructions or how to reason about procedural knowledge to extract states of entities.  To the best of my knowledge, research attempts have not worked on an end-to-end procedural voice guidance system that would be capable of automatically understanding, generating, and presenting a procedure through a conversational agent. To implement such an agent, I believe that three large gaps need to be overcome: generating a contextual knowledge graph (KG) of a procedure, reasoning on that KG to extract and order necessary information, and constructing a  system that takes the necessary/ordered information and handles conversion and delivery of it into something conversational and easily followed by an end-user.  A system like the one I propose could enhance conversational agents’ ability to interactively respond to How-To questions. This approach would improve upon the existing state-of-affairs where conversational agents hand off the interaction to a web search.  Furthermore, for smart device-related procedures, this kind of mechanism could show inexperienced users how to utilize their devices and could even enable end-user voice programming. Lastly, the intermediate contextual KG representation that this work can generate from a text could enable conversational agents to partake in explainable, informed conversational experiences.  With my prior and current work, along with surveys and analyses in this area, I believe that I can achieve the system proposed in this work and bridge the gap between modern conversational agents and modern procedural understanding systems to build dynamic conversational agents for procedures.",2023-05-23,"['personal-robots', 'media-lab-research-theme-life-with-ai']",,388074,,2023-09-06T22:22:06.129Z,['pe25171@media.mit.edu'],True,a-conversational-agent-for-dynamic-procedural-interactions,2022-03-01,[],A Conversational Agent for Dynamic Procedural Interactions,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-03-28T18:47:31.332Z,"Drawing++ Workshop: using drawing as a means of understanding computationIn this workshop participants explore computational concepts such as encoding and decoding information, rule-based drawing, generative design, and algorithmic expression through the familiar lens of drawing.  Can we teach coding without code? Can we teach computational thinking through drawing?",2023-08-31,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/a6605d97-e2a4-4d85-aca3-50de9c9af981,388104,,2023-09-06T22:14:03.679Z,"['chelsi@media.mit.edu', 'zachl@media.mit.edu']",True,drawing-plus-plus-workshop,2022-03-20,"['design', 'art', 'learning-teaching', 'computer-science', 'creativity']",Drawing++ Workshop,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-03-28T18:51:59.720Z,"Today's children will live in a world saturated with artificial intelligence. Understanding the key idea behind the functioning of intelligent systems will help them to have more agency and choice in that world. This is why we, together with a network of STEM preschools Astar Explorer, are interested in developing materials that introduce AI concepts to kids starting at the age of five.We think that robots (real or virtual ones) are excellent for introducing AI concepts, because children relate to them as characters. This allows to introduce all sorts of analogies  between AI and the child's own natural intelligence. It also brings a socio-emotional component to our designs.We are inspired by constructionist approach to learning, embodied in such designs as Scratch and Scratch Jr, where children learn in process of working on their own ideas, following their own passions. But we are also aware of the necessity of scaffolding (guidance), particularly with young learners. In this project, we experiment with delivering scaffolding via a story. Similar to the Microworlds project, we use the idea of ""close-started, open-ended"": the initial interaction is driven by a story, but in the end we hope to guide children towards open-ended interaction with our medium, so that they can use it to create stories of their own.Below is a prototype story that we developed introducing children to block-based programming. We intend to use the same format to explain such concepts as internal representations, reasoning, learning and artificial creativity.",,"['personal-robots', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/03/28/doodlebot-story.jpg,388112,,2023-09-07T23:30:20.262Z,"['isysoev@media.mit.edu', 'cynthiab@media.mit.edu', 'safinah@media.mit.edu', 'pmalacho@media.mit.edu']",True,interactive-stories-for-learning-ai-and-programming-in-preschool,2021-06-01,[],Interactive Stories for Learning AI and Programming in Preschool,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-03-29T01:15:45.968Z,"Despite technology's ability to connect people across geographic divides and give them dedicated platforms for self-expression, today's social platforms have been shown to increase loneliness and depression and decrease overall life satisfaction. This is a crisis that particularly affects young adult populations, with approximately 90% of teens using social media today.In this project, we aim to design social platforms that increase connectedness and peer support for teens and young adults. We focus particularly on young adults who can benefit most from this technology -- youth who do not receive enough social support in their lives, such as teens who are transitioning out of the foster care system or have families struggling with substance use, mental illness, housing insecurity,  and/or food insecurity.Our methodologies draw on therapeutic best practices, trauma-informed design, and human-centered design frameworks - centering the perspectives and experiences of young people who need the most support, prioritizing reciprocity and safety, and following an iterative prototyping process.We are excited to be doing this work in collaboration with Friends of the Children Boston, Communities for People, Think of Us' Virtual Support Services, Stepping Forward LA, and the JRI Foster Care Program. This project has been generously funded by The Shah Family Foundation and other donors.",2026-08-31,"['affective-computing', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/06/04/DSC02617.jpg,388121,,2023-10-23T18:00:47.009Z,"['fergusoc@media.mit.edu', 'picard@media.mit.edu', 'ilak@media.mit.edu']",True,designing-healing-centered-technology-to-support-young-people-who-have-experienced-foster-care,2022-01-01,"['design', 'human-computer-interaction']",Technology for Resilient Youth: Fostering connectedness and peer support amongst at-risk young adults,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-03-29T02:01:03.048Z,"Acu.ation is an intervention that aims to mediate the urge to smoke by pairing a wearable device that delivers transcutaneous electric acupoint stimulation (TEAS) with a mobile application that provides Mindfulness-Based Relapse Prevention (MBRP). Given the pervasiveness of cigarette smoking, high rates of relapse, and limitations of existing treatments, it is critical to explore new methods of relapse prevention. Our solution draws from acupuncture’s global use in treating addiction and recent evidence that non-invasive TEAS can reduce the urge to smoke after exposure to a drug-related cue. Specifically, we have designed a device formed to four acupoints (LI4, PC8, PC6, and TH5) located on the wrist and hand, and programmed to deliver 5-15mA stimulation. Through a paired mobile application, individuals are simultaneously guided through an MBRP intervention to help them take control of their response to the trigger. Together, this system provides real-time relief from the urge to smoke so individuals can better engage with cognitive-behavioral relapse prevention strategies in high-risk moments.To be published at ACM CHI EA 2022.ContributorsIla Kumar, a graduate student in MIT Media Lab's Affective Computing groupKatarina Richter-Lunn, a doctoral candidate at Harvard University's Graduate School of Design",2021-12-31,"['affective-computing', 'media-lab-research-theme-connected-body-mind']","https://dam-prod2.media.mit.edu/x/2022/03/29/Photo Dec 07, 4 21 19 PM3.jpg",388129,,2023-10-23T18:02:50.104Z,['ilak@media.mit.edu'],True,acuation-real-time-acupoint-stimulation-to-mediate-the-urge-to-smoke,2021-09-07,['design'],Acuation: Real-Time Acupoint Stimulation To Mediate The Urge To Smoke,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-03-29T16:57:11.811Z,"We tackle the question of how to benchmark reconstruction of inputs from deep neural networks representations. This inverse problem is of great importance in the privacy community where obfuscation of features has been proposed as a technique for privacy-preserving machine learning inference. In this benchmark, we characterize different obfuscation techniques and design different attack models. We propose multiple reconstruction techniques based upon distinct background knowledge of the adversary. We develop a modular platform that integrates different obfuscation techniques, reconstruction algorithms, and evaluation metrics under a common framework. Using our platform, we benchmark various obfuscation and reconstruction techniques for evaluating their privacy-utility trade-off. Finally, we release a dataset of obfuscated representations to foster research in this area.",,"['camera-culture', 'media-lab-research-theme-life-with-ai']",,388139,,2023-09-07T22:12:22.233Z,"['abhi24@media.mit.edu', 'mosej24@media.mit.edu', 'raskar@media.mit.edu', 'vvsharma@media.mit.edu']",True,simba,2021-06-10,[],Benchmarking Privacy in Machine Learning Prediction,PUBLIC,https://tremblerz.github.io,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-29T18:39:24.935Z,"Split learning (SL) is a promising distributed learning framework that enables to utilize the huge data and parallel computing resources of mobile devices. SL is built upon a model-split architecture, wherein a server stores an upper model segment that is shared by different mobile clients storing its lower model segments. Without exchanging raw data, SL achieves high accuracy and fast convergence by only uploading smashed data from clients and downloading global gradients from the server. Nonetheless, the original implementation of SL sequentially serves multiple clients, incurring high latency with many clients. A parallel implementation of SL has great potential in reducing latency, yet existing parallel SL algorithms resort to compromising scalability and/or convergence speed. Motivated by this, the goal of this article is to develop a scalable parallel SL algorithm with fast convergence and low latency. As a first step, we identify that the fundamental bottleneck of existing parallel SL comes from the model-split and parallel computing architectures, under which the server-client model updates are often imbalanced, and the client models are prone to detach from the server’s model. To fix this problem, by carefully integrating local parallelism, federated learning, and mixup augmentation techniques, we propose a novel parallel SL framework, coined LocFedMix-SL. Simulation results corroborate that LocFedMix-FL achieves improved scalability, convergence speed, and latency, compared to sequential SL as well as the state-of-the-art parallel SL algorithms such as SplitFed and LocSplitFed. ",2022-03-29,"['camera-culture', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/03/29/Screen Shot 2022-03-29 at 2.39.44 PM.png,388148,,2023-09-07T19:58:37.740Z,['vepakom@media.mit.edu'],True,locfedmix-sl,2021-12-31,[],LocFedMix-SL,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2022-03-29T19:24:23.801Z,"The Atlas of Opportunity is a platform that allows communities to derive insights from their own data. The tool was created in collaboration with business and government stakeholders in Adelaide, Australia, to combine open government data with commercial data to allow insights to be shared with individuals using open-source tools developed in Human Dynamics and Connection Science.The Atlas showcases recent research revealing that understanding community movement patterns is crucial for understanding economic growth and mobility. Places with more diverse movement patterns are more likely to have higher near-future economic growth. This Atlas can inform a new generation of data-informed strategies and advanced methods that are used to understand how human behavior (e.g. movements, spending) shapes economic prosperity. The current map intends to provide insights for small business owners and help them make informed decisions about the location for starting their new business or subsidiary branch. Users can find a wide variety of information about each neighborhood. The data include information about sociodemographic, economic, spending, and mobility data. Difference indices and prediction measures are provided that could be useful for business owners to make better decisions.The Atlas of Opportunity has begun expanding out from the collaboration in Australia to elsewhere in the world including Brazil, Italy, Turkey, and the U.S.",,"['human-dynamics', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/03/29/Picture1.png,388157,,2023-09-06T20:18:45.629Z,"['tsouth@media.mit.edu', 'sandy@media.mit.edu', 'bahrami@media.mit.edu']",True,atlas-of-opportunity,2022-03-29,[],The Atlas of Opportunity,PUBLIC,https://opportunity.mit.edu,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-03-29T20:09:47.228Z,"We present the ""MicroPET"" mission, consisting of an autonomous payload for enzymatic reactions and microbial cultivation with fully programmable serial passaging and sample preservation. The payload is a compact, modular bioreactor system that allows for automatic media transfers and precise data monitoring from integrated sensors (e.g., temperature, optical density, etc.). We apply this system to study the biological upcycling of the commonly used polymer polyethylene terephthalate (PET) in space in a two-step system in which PET film is enzymatically degraded and microbially upcycled.The payload system was launched to the International Space Station aboard SpaceX CRS-26 on November 26th, 2022.",,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/11/26/p-1-90813751-seed-health-bacteria.jpg,388165,,2023-09-07T22:21:10.136Z,"['seanauff@media.mit.edu', 'aekblaw@media.mit.edu', 'xxxxxxin@media.mit.edu', 'patpat@media.mit.edu', 'chwalek@media.mit.edu', 'ssunanda@media.mit.edu']",True,micropet,2021-01-01,"['bioengineering', 'environment', 'synthetic-biology', 'space']",MicroPET: Investigation of Biodegradation of PET Plastics in Spaceflight,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-29T20:40:50.105Z,"Point clouds are an increasingly ubiquitous input modality and the raw signal can be efficiently processed with recent progress in deep learning. This signal may, often inadvertently, capture sensitive information that can leak semantic and geometric properties of the scene which the data owner does not want to share. The goal of this work is to protect sensitive information when learning from point clouds; by censoring the sensitive information before the point cloud is released for downstream tasks. Specifically, we focus on preserving utility for perception tasks while mitigating attribute leakage attacks. The key motivating insight is to leverage the localized saliency of perception tasks on point clouds to provide good privacy-utility trade-offs. We realise this through a mechanism called Censoring by Noisy Sampling (CBNS), which is composed of two modules: i) Invariant Sampler: a differentiable point-cloud sampler which learns to remove points invariant to utility and ii) Noisy Distorter: which learns to distort sampled points to decouple the sensitive information from utility, and mitigate privacy leakage. We validate the effectiveness of CBNS through extensive comparisons with state-of-the-art baselines and sensitivity analyses of key design choices. Results show that CBNS achieves superior privacy-utility trade-offs on multiple datasets. ",,"['camera-culture', 'media-lab-research-theme-life-with-ai']",,388186,,2023-09-06T20:14:07.268Z,"['ayushc@media.mit.edu', 'raskar@media.mit.edu', 'abhi24@media.mit.edu']",True,cbns,2022-03-29,[],Private Machine Learning on Point Clouds,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-29T20:43:30.554Z,"Image-based virtual try-on involves synthesizing perceptually convincing images of a model wearing a particular garment and has garnered significant research interest due to its immense practical applicability. Recent methods involve a two stage process: i) warping of the garment to align with the model ii) texture fusion of the warped garment and target model to generate the try-on output. Issues arise due to the non-rigid nature of garments and the lack of geometric information about the model or the garment. It often results in improper rendering of granular details. We propose ZFlow, an end-to-end framework, which seeks to alleviate these concerns regarding geometric and textural integrity (such as pose, depth-ordering, skin and neckline reproduction) through a combination of gated aggregation of hierarchical flow estimates termed Gated Appearance Flow, and dense structural priors at various stage of the network. ZFlow achieves state-of-the-art results as observed qualitatively, and on quantitative benchmarks of image quality (PSNR, SSIM, and FID). The paper presents extensive comparisons with other existing solutions including a detailed user study and ablation studies to gauge the effect of each of our contributions on multiple datasets.""ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors"". International Conference on Computer Vision (ICCV) 2021",,"['camera-culture', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",,388194,,2023-09-06T20:13:10.838Z,['ayushc@media.mit.edu'],True,zflow,2022-03-29,[],Image-based Virtual Try-On,PUBLIC,https://openaccess.thecvf.com/content/ICCV2021/papers/Chopra_ZFlow_Gated_Appearance_Flow-Based_Virtual_Try-On_With_3D_Priors_ICCV_2021_paper.pdf,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-03-29T22:05:15.666Z,"Although lidar is widely used for mapping the 3D geometry of surfaces, the technology has historically been challenged by specular, or mirror-like, surfaces that typically scatter very little light directly back to the receiver.  This inability to detect and localize specular surfaces can result in the failure to detect navigational obstacles like mirrors and windows, or hazards such as wet or icy patches on the ground.  It can also result in incomplete scans of cityscapes or man-made interior environments in which glass and metal surfaces are relatively common, and in the complete inability to digitize artifacts that are made of glass or that present a polished metal or chrome finish.",,"['camera-culture', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/0d572135-4b4b-4121-836b-0646d07dea48,388212,,2023-09-07T23:18:04.284Z,"['co24401@media.mit.edu', 'hanklin@media.mit.edu']",True,detecting-and-mapping-transparent-or-mirror-like-surfaces-with-lidar,2021-08-01,"['robotics', 'computer-vision', 'augmented-reality', 'autonomous-vehicles']",Detecting and Mapping Transparent or Mirror-like Surfaces with Lidar,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-03-29T23:27:31.118Z,"By predicting sparse shadow cues, our physics-inspired machine learning algorithm can reconstruct the underlying 3D scene. Abstract. We present a method that learns neural scene representations from only shadows present in the scene. While traditional shape-from-shadow (SfS) algorithms reconstruct geometry from shadows, they assume a fixed scanning setup and fail to generalize to complex scenes. Neural rendering algorithms, on the other hand, rely on photometric consistency between RGB images but largely ignore physical cues such as shadows, which have been shown to provide valuable information about the scene. We observe that shadows are a powerful cue that can constrain neural scene representations to learn SfS, and even outperform NeRF to reconstruct otherwise hidden geometry. We propose a graphics-inspired differentiable approach to render accurate shadows with volumetric rendering, predicting a shadow map that can be compared to the ground truth shadow. Even with just binary shadow maps, we show that neural rendering can localize the object and estimate coarse geometry. Our approach reveals that sparse cues in images can be used to estimate geometry using differentiable volumetric rendering. Moreover, our framework is highly generalizable and can work alongside existing 3D reconstruction techniques that otherwise only use photometric consistency. Our code is made available in our supplementary materials.",,"['camera-culture', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/29/teaser.png,388228,,2023-09-06T20:12:26.903Z,"['raskar@media.mit.edu', 'tzofi@media.mit.edu', 'ktiwary@media.mit.edu']",True,towards-learning-neural-representations-from-shadows,2022-03-29,"['robotics', 'computer-vision', 'artificial-intelligence']",What can we learn from predicting shadows?,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2022-03-30T18:32:54.019Z,"In photography, illuminating a subject directly using an on-camera flash can result in unflattering photos.  To avoid this, photographers often choose to illuminate their subject indirectly by bouncing light off of a diffusely reflecting surface such as the ceiling or a nearby wall. This technique is known as bounce flash.  Inspired by this technique, we introduce a new variation on flash lidar imaging that we refer to as bounce-flash lidar.  In our method, we use a focused laser source to illuminate a spot on a diffusely reflecting surface.  Light that scatters from this spot will subsequently illuminate other points in the scene.  We ascertain the position of the laser spot using the time of flight of photons that are scattered directly backwards towards the camera.  The first signal that returns from the indirectly illuminated points will have scattered exactly twice.  We use the two-bounce travel-time, along with our knowledge of the laser spot's position, to compute the position of each indirectly illuminated point.  In effect, a bounce-flash lidar is equivalent to a bistatic flash lidar that has a transmitter embedded within the scene itself, at the position of the laser spot. ",2022-03-21,"['camera-culture', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/03/30/members_week_bf_cartoons.png,388239,,2023-09-07T22:21:28.401Z,['co24401@media.mit.edu'],True,bounce-flash-lidar,2021-01-01,"['robotics', 'computer-vision', 'augmented-reality', 'autonomous-vehicles']",Bounce-Flash Lidar,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-03-31T16:32:15.577Z,"Website of the project: https://www.neurafutures.com The installation is currently open to public starting September 25, 2023 at Cambridge Public Library. NeuraFutures art installation features devices known as Brain-Computer Interfaces (BCIs). BCIs access the user’s brain via sensors that are worn on or implanted inside of the head. In today’s research, they are used to measure, support, and augment cognitive states such as fatigue, attention, and visualization. In the next few decades, the ubiquity of BCIs will be equivalent to that of mobile phones today, which emphasizes the urgent need to discuss the ethics of interacting with such systems. While this type of work is primarily conducted through public discussions among experts in computing fields, science fiction media provides another avenue of inspiration that we believe is equally important to the understanding of BCIs. We call science fiction media featuring BCIs - BCI-fis.  BCI-fi is important to the societal acceptance of BCIs because it is the primary source of the public’s understanding of these technologies before they become real. As a result, BCI-fi has a lot of influence over our perception of the future. Our findings emphasize the necessity to properly prepare for our future of information and digital interactions; as we improve the speed and depth of computing, we risk the loss of simplicity, identity, and altruism—of everything that makes us human. We hope this installation exposes viewers to the everyday influences of the business of ubiquity and in turn sparks the questioning of their technological morals.NeuraFutures as a whole is a large explorative project about the representation of brain-computer interfaces (BCIs) in science fiction media. We designed an immersive art installation based on these ideas and our data, which is open to the general public: for our installation, we ask the attendees to face these senses and awaken themselves to their own sociotechnological morals.We have fabricated 35 real-size physical prototypes, or props, of the neural devices featured in different BCI-fi, each of which represents different permutations of answers to these senses and morals — essentially, different futures. These props are organized around the space, and this space is open for exploration. The public is also able to take a “FuturesTest” in which they make a series of choices about the future they hope or predict to see, the results of which is the prop in the space that is the most representative of their morals. The test requires users to consider new aspects of their identity and understand the ethics of AI and human-enhancement by asking questions about utopian vs. dystopian worlds, invasive vs. non-invasive technologies, and when they believe different neuratechnological events will happen. You may also take this test here: NeuraFutures Test.Please check the project’s website, http://neurafutures.com, to learn more about the scientific advancements of brain sensing research in real life.Installation lead:  Dr. Nataliya Kosmyna.Installation team: Mina Terzioglu, Huu Khiem Nguyen.Past Contributors: Damien Socia, Anna Demko, Sophia Rim, Alexandra Li, Gabriella Nugent, Louis Hand, Zoe Wu, Yuran Ding, Marshall Wang.Past members: Cassandra Scheirer,  Rinako Sonobe, Nina Cragg, Shannon Murphy, Samuel Fromovitz, Jackie Sabillon, Tatiana Zhizhimontova.This project is funded by The Council for the Arts at MIT (CAMIT): https://arts.mit.edu/camit/Image Credit: Marshall Wang, Yuran Ding, Nataliya Kosmyna",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/10/04/hero.png,388252,,2023-09-29T12:05:51.204Z,['nkosmyna@media.mit.edu'],True,neurafutures,2019-02-01,"['robotics', 'artificial-intelligence', 'learning-teaching', 'neurobiology', 'neural-interfacing-and-control', 'women']",Art Installation: NeuraFutures,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2022-04-02T11:48:00.833Z,"Finger Talk is a small experiment Future Sketches has been working on related to gestural computation. What does it mean to have code you can control with your body? For these sketches we built a hand tracking software, using coreML, and experimented with a simple gesture—pinching your fingers. We used openFrameworks to build a series of experiments to explore different ways of navigating information—moving through text, changing the size of a hamburger drawing, scaling and rotating an image, etc. The end goal is simple, playful interactions that help explore how the body and computational systems can interact.",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/04/02/Screen Shot 2022-04-02 at 7.52.10 AM.png,388276,,2023-09-06T22:04:21.083Z,"['zachl@media.mit.edu', 'chelsi@media.mit.edu', 'lingdong@media.mit.edu']",True,finger-talk,2022-04-02,[],Finger Talk,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-04-03T03:22:43.977Z," An Exploration of Drawing as Programming Language, Featuring Ideas from Lambda Calculus",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/57bcaaaf-6442-42a1-87a7-cba33a98b46d,388285,,2023-09-06T20:11:52.814Z,['lingdong@media.mit.edu'],True,2d-an-exploration-of-drawing-as-programming-language-featuring-ideas-from-lambda-calculus,2022-04-02,[],"λ-2D: An Exploration of Drawing as Programming Language, Featuring Ideas from Lambda Calculus",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-04-06T00:44:56.946Z,"Spoken language is an information-rich medium that combines words with various paralinguistic information such as emotion and prosody. In discourse, this allows for maintaining a human element that is lacking in many other channels, such as writing or social media. However, voice is a distinct biomarker, and there are many situations in which a speaker may want to hide their identity, such as if they are sharing sensitive content or want to protect personal information such as geographical background or ethnicity.In this project, we develop a system for voice anonymization using a voice conversion (VC) approach, in which we convert the vocal identity of an utterance to sound like another person without changing the linguistic or prosodic content. Using a state-of-the-art deep neural network VC model, we are able to transform any speech utterance to sound like any target speaker given a sample of the target speaker’s speech. We further explore how listening to speech anonymized in this way affects peoples’ perception of the content that is conveyed, both from the point of view of the listener and the original speaker.",,"['social-machines', 'center-for-constructive-communication', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/04/05/Screen Shot 2022-04-05 at 10.14.58 PM.png,388319,,2023-09-18T18:29:43.764Z,"['wjkang@media.mit.edu', 'dkroy@media.mit.edu']",True,voice-anonymization,2022-04-05,"['artificial-intelligence', 'machine-learning']",Voice Anonymization,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-04-06T20:37:37.214Z,"The KALM project consists of a multimodal dataset and a wearable system for managing anxiety.  With this set of tools we aim to make novel contributions in the area of anxiety management via physiological signals. Background and MotivationAnxiety and depression have become one of the largest causes of disability worldwide. EEG can offer quantifiable information on aspects of anxiety and depression. Meanwhile, EEG-based brain-computer interfaces (BCIs) have become available in wearable, compact and cheap form factors. Their new accessibility and ease of use offers a huge opportunity for everyday individuals to be able to track, understand, and manage their mental health without requiring expert knowledge. As a part of KALM we are studying multimodal physiological data that can be integrated with EEG in real-world environments for everyday users seeking to manage aspects of their mental health.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/04/06/Screen Shot 2021-07-01 at 1.00.10 PM 2.JPG,388334,,2023-09-07T20:48:42.943Z,"['avujic@media.mit.edu', 'pattie@media.mit.edu', 'snisal@media.mit.edu']",True,kalm-bci,2021-09-13,"['human-computer-interaction', 'cognition']",KALM: BCI Systems for Managing Anxiety and Stress,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-04-09T12:13:11.166Z,"We are building an intelligent musical partner that play duets with you and teaches music.  This research teaches us how to design more intimate human-AI symbiotic systems, innovate on machine learning models and architectures, as well as add a ""spark of life"" to automata.  It evolves an initial corpus as you use it. This research bridges the gap between generative machine learning models for music that are unresponsive to real time input and real time interactive music systems that typically struggle to generate high-fidelity phrases that span a larger coherent movement or gesture.  Music is a test case of the more general issue of how we explore ideas and make decisions in partnership with a machine that is increasingly viewed as “creative.” ",,"['viral-communications', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']","https://dam-prod2.media.mit.edu/x/2022/10/03/DALL·E 2022-10-03 19.25.49 - a robot playing saxophone with a human pianist, impressionist.png",388397,,2023-09-07T15:37:00.807Z,"['mhjiang@media.mit.edu', 'lip@media.mit.edu']",True,music-ai-companion,2022-02-01,[],Neural Notes,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-04-11T02:03:38.195Z,"Energy consumption of artificial intelligence (AI) systems are projected to grow at an alarming rate over the next two decades. A recent study estimates that, at current growth rate of AI systems, by 2040 total energy spent on computation will reach  1e27 J, which is far greater than the total energy that humans may be able to generate by then. Moreover, global Information Technology (IT) flourishing over internet of things and artificial intelligence paradigms, is rapidly emerging as a major consumer of world’s primary electricity, which is still the second largest contributor to greenhouse gases emissions worldwide. Thus, there is a critical need to find solutions at the material, devices, and architecture level to reduce energy consumption of computing hardware. A way forward is to replace the traditional von-Neumann computing hardware with technologies like neuromorphic and stochastic computing which are better suited for AI applications. Neuromorphic devices and architectures mimic the biological brain, the extremely energy-efficient neural network, so that memory and logic operations can be performed locally. Thus, energy losses (and latency) associated with billions of data retrieval and storage cycles in a neural network can be eliminated. Here, we are developing 2D magnetic material-based devices to form the building blocks of neuromorphic and stochastic computing architectures. Use of correlated systems like ferromagnets provides a way towards low energy device switching, while 2D nature of the materials allows ultimate scalability, and tunability of magnetic and electrical transport properties. Our benchmarking results show that our 2D neuromorphic devices based neural network can lead to more than 10,000X reduction in energy compared to that based on CMOS for performing machine learning tasks. Thus, our technology can address the energy crisis of computing industry, lead to massive reduction in greenhouse Gases helping to combat climate change and enable environmentally sustainable ""Green” AI.Our project is highlighted at the MIT AI Hardware program.Our archived paper on the first demonstration of deterministic switching in van der Waal's ferromagnet",,"['nano-cybernetic-biotrek', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/04/10/AI.jpeg,388408,,2023-09-07T23:27:51.283Z,['deblina@media.mit.edu'],True,2d-neuromorphic-devices-for-sustainable-artificial-intelligence,2022-04-10,[],2D Neuromorphic Devices for Sustainable Artificial Intelligence,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-04-11T17:23:02.713Z,"The increasing prevalence of large-scale labor aggregation platforms, worker analytics, and algorithmic decision-making by management raises the question of whether workers can use similar technologies to advocate for their own goals. Yet, there are inherent challenges in building worker-centric tools that collect, aggregate, and share data in responsible and ethical ways.This project is a tool developed in collaboration with nonprofit worker groups that allows app-based delivery workers to track and share aggregate data about their pay, increasing wage transparency. We used the Calculator to audit Shipt's shift to a black-box pay model using data contributed by over 100 workers in the summer of 2020, finding that although the average pay per-order increased under the new payment model, almost half of workers experienced an unannounced pay cut during the shift, and many workers worked shifts that paid under their state's minimum wage.",,"['human-dynamics', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/04/11/1*nkgvnDdf9--amb8mFu34ig.png,388419,,2023-09-06T22:04:39.808Z,['dcalacci@media.mit.edu'],True,the-shipt-calculator-crowdsourcing-gig-worker-pay-data-to-audit-algorithmic-management,2022-04-01,"['design', 'social-networks', 'human-computer-interaction', 'civic-technology', 'data', 'economy', 'ethics', 'industry', 'cities']",The Shipt Calculator: Crowdsourcing Gig Worker Pay Data to Audit Algorithmic Management,PUBLIC,http://dcalacci.net,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-04-12T05:01:31.437Z,"The underrepresentation of minoritized groups, particularly African Americans, is the longstanding reality of computing fields. Computing has the opportunity to change the world and is increasingly being incorporated into our daily lives. Computing classes discuss computing as abstract, neutral, utopian and unable to cause harm. While everyone needs to be part of the process of ending a multi-layered system of barriers, we focus specifically on why this goal is of particular relevance to African American students. We highlight El-Amin’s “liberation tools,” which state how a sound racial identity, critical consciousness, liberation centered achievement identity, collective obligation, along with activism skills are essential to preparing African Americans to “fight for” racial liberation. Given that computing classes teach students critical thinking skills to solve complicated problems, we argue that computing is well-positioned to incorporate “liberation tools”. They teach students how to think in terms of systems, which is essential for racial liberation. By expanding the liberation tools, we coin the term, “liberatory computing”, to reveal how computing curricula can motivate and provide African American students with practical skills to address the racism embedded in society.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",,388437,,2023-09-07T15:37:25.249Z,['raechelw@media.mit.edu'],True,liberatory-computing-for-african-american-students,2022-02-01,"['data', 'kids', 'learning-teaching', 'civic-action']",Liberatory Computing for African American Students,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-04-12T06:52:28.021Z,"The underrepresentation of minoritized groups, particularly African Americans, is the longstanding reality of computing fields. Computing has the opportunity to change the world and is increasingly being incorporated into our daily lives. Computing classes discuss computing as abstract, neutral, utopian and unable to cause harm. While everyone needs to be part of the process of ending a multi-layered system of barriers, we focus specifically on why this goal is of particular relevance to African American students. We highlight El-Amin’s “liberation tools” which state how a sound racial identity, critical consciousness, liberation centered achievement identity, collective obligation, along with activism skills are essential to preparing African Americans to “fight for” racial liberation. Given that computing classes teach students critical thinking skills to solve complicated problems, we argue that computing is well-positioned to incorporate “liberation tools”. They teach students how to think in terms of systems, which is essential for racial liberation. By expanding the liberation tools, we coin the term, “liberatory computing”, to reveal how computing curricula can motivate and provide African American students with practical skills to address the racism embedded in society.",,[],,388445,,2022-08-15T20:50:02.017Z,[],False,liberatory-computing-for-african-american-students-1,2022-02-01,[],Liberatory Computing for African American Students,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-04-14T19:53:19.705Z,Chameleon is a machine learning enhanced sensor system that uses CO2 and PIR sensors to classify room occupancy and use.,,"['city-science', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2021/03/25/Termites_mount_c smuts.jpg,388468,,2023-09-07T22:22:52.736Z,"['aricom@media.mit.edu', 'csmuts@media.mit.edu', 'kll@media.mit.edu']",True,chameleon,2020-08-10,"['artificial-intelligence', 'sensors']",Chameleon,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2022-04-25T13:49:35.232Z,"By exploring intersectionality as a concept and as a practice, students use data storytelling to understand their lives and experiences. Our self-paced curriculum is a three-part activity that explores intersectionality as a concept and as a practice. We apply data storytelling to help students understand their lives and experiences. In the first part, we introduce Michelle, an African American girl in high school, via an interactive Scratch animation. Through learning about the idea of intersectionality, Michelle finds hope in hearing how amazing people who look like her are computing pioneers and discovers her love for data science. She invites students to join her. In the second part of our curriculum, we explore data storytelling. Intersectionality data storytelling is visualizing identities and the structures of dominance through adding an emotional resonance to information. Students are introduced to the concept of data science  through historical examples, such as DuBois’ data portraits of Black America in the early 1900s. Finally, we invite students to take what they have learned from Michelle’s story about intersectionality and data science to apply it to their own lives through projects. They can choose to discuss intersectionality further with friends and family, explore careers in computing, or learn more about the stories of the women in part 1. We also offer multiple pathways to get started including making data visualizations with at home crafts, creating with Scratch, and using Python programming. Black girls will be empowered to see themselves in this curriculum, positively impact their communities, and have a new language to describe their lived experiences.This project was developed in Fall 2021 as a collaboration in Harvard Graduate School of Education's Designing for Learning by Creating (T550) course led by Media Lab alum Karen Brennan.",2022-05-31,"['lifelong-kindergarten', 'personal-robots', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/06/13/Screen Shot 2022-06-13 at 2.04.55 PM.png,388529,,2023-09-07T20:58:01.988Z,"['raechelw@media.mit.edu', 'csadler@media.mit.edu']",True,intersectionality-data-storytelling,2021-09-01,[],Intersectionality + Data Storytelling,PUBLIC,https://tinyurl.com/identity-data,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-05-06T15:02:24.881Z,Space architecture projects for the Mechanical Artifact: Ultra Space course in partnership with the Center for Collaborative Arts and Media (CCAM) and the School of Architecture at Yale. ,,"['space-exploration', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/05/06/20210520_Steve-Boxall_ZG547-MIT_6528.jpg.1400x1400_ZL87qC4.jpg,388622,,2023-09-06T20:03:22.926Z,[],True,yale-mit-collaboration-ultraspace,2022-05-06,[],Yale-MIT Collaboration: UltraSpace,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2022-05-12T16:01:01.634Z,"We are living in a time of extraordinary urban changes. Research has shown that cities can bring economic wealth and improved quality of life by fostering diverse economies, dense knowledge exchanges, and efficient district performance. However, it is also true that scientists have associated cities with crowding, segregation, environmental degradation, and other significant challenges. Sensors, Data, and Artificial Intelligence can lead to a better understanding of urban settings and their challenges by providing opportunities for insight into their social and environmental performance. Many of these sensing initiatives are carried out in a top-down fashion. Top-down sensing generates datasets that capture large-scale patterns across populations. This data could be complemented by bottom-up community-based approaches that capture more granular information emerging from the specific needs of individuals. Through a series of case studies, this thesis illustrates how to use a variety of community-scale sensor and machine intelligence implementations to measure aspects of socio-environmental cycles that emerge in different urban and environmental contexts. These studies explore possibilities for providing communities with access to localized information about socio-environmental systems that, if fully deployed, could enable bottom-up transformation of collective behavior, policies, and infrastructure to address the great challenges that future cities will face. ",,['city-science'],https://dam-prod2.media.mit.edu/x/2022/05/12/coversketch.jpeg,388657,,2022-05-12T20:27:00.571Z,[],False,socio-environmental-sensor-networks-for-community-sensing,2020-08-01,[],Socio-Environmental Sensor Networks for Community Sensing,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-05-12T20:40:06.768Z,"Low-cost, modular, open source, and portable bio-reactor for urban bio-sensor development.",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/6a800607-a76a-4a5e-b990-162b5fab4c49,388666,,2023-09-07T22:24:09.929Z,"['eyalp@media.mit.edu', 'kll@media.mit.edu', 'dkong@media.mit.edu', 'aricom@media.mit.edu']",True,maia,2020-08-10,"['artificial-intelligence', 'sensors']",MAIA,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-05-12T21:23:15.268Z,Community-scale profiling sensor node for  ocean monitoring and community coordination of fishing activities. ,,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/05/12/20211124_115344.jpeg,388674,,2023-09-06T22:01:54.422Z,"['fadel@media.mit.edu', 'afzals@media.mit.edu', 'kll@media.mit.edu', 'aricom@media.mit.edu']",True,ocean-logger,2022-05-12,"['artificial-intelligence', 'sensors']",Ocean Logger: A Community-scale Ocean Profiling Sensor Node,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-05-13T13:57:47.884Z,"The City Science group collaborated with the Ford company from 2019 to 2020 on modeling the transformation of Corktown, Detroit into a high performance, entrepreneurial, mobility innovation district. Ford purchased the historic 30-acre site anchored by iconic Michigan Central Station as part of its plan to reshape the future of global mobility while co-creating a walkable neighborhood of the future.The aim of the collaboration between City Science and Ford was to develop an interactive model allowing stakeholders in Ford to explore potential designs and interventions and their impacts on the community. The interventions considered included the built area design—the locations and densities of land uses such as Office, Residential, Light Industrial etc—and mobility innovations—addition of greenways, shuttle systems, fleet electrification and shared micro-mobility. The model was developed using the CityScope platform—a human-centered, urban modeling, simulation and decision-making platform, developed in the City Science group. The Corktown CityScope model allowed stakeholders to experiment with the land-use and mobility interventions, see how these changes impacted mobility behaviors and the impacts on a range of key performance indicators.The project involved the development of three main modules: the simulation model of land-use and mobility behavior, the urban indicator computations and the web-based interactive tool.",2020-06-30,"['city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/09/01/csjs.gif,388685,,2023-09-07T22:01:00.935Z,"['doorleyr@media.mit.edu', 'crisjf@media.mit.edu', 'agrignar@media.mit.edu', 'canwang@media.mit.edu', 'yasushis@media.mit.edu', 'alonsolp@media.mit.edu', 'noyman@media.mit.edu']",True,corktown-urban-performance,2019-07-01,[],Corktown Urban Performance,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2022-05-17T20:38:29.518Z,"By Michelle LinIn microgravity, the proprioceptive system undergoes adaptations due to the lack of constant gravitational cues [1]. Photo-video evidence and informal accounts of microgravity exposures demonstrate a shift in the quality of movements and the self-awareness of one’s body.  Utilizing a framework  inspired by ballet and freediving, we propose using fluidity as a metric to measure proprioceptive adaptation. The effects of the microgravity environment on the proprioceptive system will be validated with a wearable sensor system garment user-tested by a participant in a parabolic flight.",2022-12-31,"['space-exploration', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/05/17/_S7A8334_Original.jpg,388706,,2023-09-07T15:00:20.013Z,['shuyulin@media.mit.edu'],True,physical-instinct-in-microgravity,2022-05-17,[],Physical Instinct In Microgravity,PUBLIC,https://www.michthemartian.com/,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-05-19T17:11:45.719Z,"Cody Paige, MIT AeroAstro Contributors: Ferrous Ward, MIT AeroAstro; Don Derek Haddad, ResEnv; Jess Todd, MIT AeroAstro ",,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/12/02/DSCF7146_XKHXMLw.jpg,388733,,2023-10-17T22:46:01.515Z,"['cpaige@media.mit.edu', 'ddh@media.mit.edu']",True,capturing-the-moon,2022-05-20,[],Capturing the Moon: Assessing virtual reality for remote Lunar geological fieldwork,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2022-05-20T04:32:17.667Z,"Project Overview The Space Grip Shoe (SGS) is an iconic concept presented across the history of space fiction, from 2001: A Space Odyssey to The Expanse television series. Full Commercialization of spaceflight and space exploration is on an ever-nearing horizon, as both companies and countries are pursuing spaceflight in full force. Researchers in the fields of human-computer interaction (HCI) are contributing to this quickly evolving scene by manifesting concepts first visualized in science from science fiction novels through many years of work in wearable garments and ubiquitous computing capabilities. The SGS contributes to this field by presenting a proof-of-concept mobility device using electromagnetic force. The future versions of the shoe will be “smart”, allowing the device to detect the users gait and automatically actuate the magnets to enable a walking gait.  The SGS is an easy-to-use magnetic actuating system, with the electromagnets in each shoe operated by a toggle switch hand control. With over 40 lbs of holding force per shoe, the space grip shoe allows a novice to the microgravity environment to be able to move and perform actions as confidently and competently as they would in a one-G environment.The experimental design for this flight was developed over the fall 2021 semester, extending into the spring 2022 semester, with measurables being user reported stability and intuition of the actuator interface. During the flight, the user will perform movements in both zero, Martian, and lunar gravity gradients to evaluate posture control and stability with the magnetic shoe garment.This work aims to conceptually demonstrate the functionality and utility of a magnetic mobility assistant across gravity fields and inform future developments for mobility aiding garments in low gravity environments, further developing the repertoire of technology for long term operations and habitability in the space environment.The Skinsuit targets multiple physiological systems, aiming to mitigate spaceflight-induced musculoskeletal adaptations, such as spinal elongation. Additionally, the Skinsuit may provide benefits to the sensorimotor system, which have not been tested in previous studies. The sensorimotor effects of microgravity are difficult to simulate on Earth, even in bed rest analogs or body-weight suspension, due to the constant force of gravity on the body and body-load receptors. The goal of this project is to use the microgravity afforded by a parabolic flight to explore a research question: Can the Skinsuit restore sensorimotor functions that are typically altered in microgravity?",2022-12-31,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/05/20/image001_TsSEt59.png,388742,,2023-09-08T14:49:09.977Z,['ddh@media.mit.edu'],True,magnetic-space-grip-shoe,2022-02-01,[],Space Grip Shoe,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-05-26T22:44:08.809Z,"The Microsoft Azure Kinect is a commercial-off-the-shelf (COTS) time-of-flight (ToF) depth camera which can provide high resolution near-field depth data. The MIT team has selected to modify the camera for a technology demonstration mission on the Lunar surface in 2023.  The collected data will be used to construct a high resolution, near-field virtual environment of the lunar surface for scientific applications. The ToF camera is mechanically robust; with no baseline calibration requirement, high effective depth image resolution and low computational cost (Bamji et al., 2018). The Azure Kinect combines ToF data with RGB video and an IMU sensor to give a complete picture of the area immediately surrounding the device. By using a COTS part with integrated ToF and RGB imaging we reduce the processing required to align different camera view fields and positions when rendering the image as well as reducing development costs. We are currently working on a set of risk-reduction experiments and hardware modifications in preparation for a 2023 CLPS flight mission in 2023. We have designed an experiment to test the functionality of the camera in Lunar dust conditions that was flown on a Zero-G flight with the MIT Space Exploration Initiative in May 2022.  We are also testing the capabilities in south pole lighting conditions through benchtop testing and field tests in Svalbard, Norway. The unit will be modified for integration with a CLPS payload with several weight reduction modifications, removal of plastics incompatible with spaceflight, and as housing that integrates the camera as a payload on the Lunar Outpost MAPP rover.",2023-07-31,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/06/16/eyJidWNrZXQiOiJhc3NldHMucHVicHViLm9yZyIsImtleSI6Im9kMXIwdDhuLzIxNjMzNDQzOTU0NzE2LnBuZyIsImVkaXRzIjp7InJlc2l6ZSI6eyJ3aWR0aCI6ODAwLCJmaXQiOiJpbnNpZGUiLCJ3aXRob3V0RW5sYXJnZW1lbnQiOnRydWV9fX0=.png,388850,,2023-09-07T19:55:27.193Z,['seanauff@media.mit.edu'],True,microsoft-azure-kinect-for-a-lunar-environment,2022-01-01,[],Microsoft Azure Kinect for a Lunar Environment,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-05-26T22:54:32.165Z,"The objective of this project is to experimentally asses the impact of  gravity reference frame on the experience of Virtual Reality (VR), in  terms of both subjective response and psyschophysiological reactivity.  This work, spearheaded by graduate student Ryann Hee of the MIT  Department of Aeronautics and Astronautics, stems from a larger  NASA-funded research effort in the MGH/Harvard Medical School’s Human Performance Lab,  led by PI Dr. Aleksandra Stankovic, which seeks to optimize the  configuration of VR experiences and modalities for use in long-duration  spaceflight. While VR has shown promising potential for addressing  behavioral health and performance risks associated with the prolonged  isolation, confinement, and limited sensory environments that astronauts  face in long-duration spaceflight, many questions still remain for how  best to operationally deploy this technology. One such question, and the  focus of Ryann’s graduate work on this parabolic flight, is whether  mis-matches in gravity reference frames between the visual cues  presented in VR and the gravity state of the surrounding environment  (like for example, viewing a 1G visual scene set on Earth while  experiencing 0G weightlessness of parabolic flight) might provoke  unpleasant vestibular system responses or lead to aversive subjective  experiences such as motion sickness and disorientation. Using a  combination of questionnaire and body-worn sensor data, Ryann will  assess how subjects react to matched and mis-matched gravity frames  while viewing VR scenes in parabolic flight, with the goal of informing  the design of future VR systems for spaceflight.",2022-06-30,"['space-exploration', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/05/27/Image(1).jpeg,388858,,2023-09-06T22:02:23.112Z,[],True,virtual-reality-based-sensory-stimulation,2022-05-01,[],Testing Virtual Reality Use for Spaceflight,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-05-29T11:01:03.839Z,"A Conversation with the Sun is an artistic exploration by acclaimed Thai auteur and artist Apichatpong Weerasethakul in collaboration with DuckUnit and Pat Pataranutaporn, manifests Weerasethakul’s long-standing interests in moving images, theatricality, performance, technology, and their coalescence. Interrogating the possibilities of an existence without his personal memories, A Conversation with the Sun draws on selected self-documented footage over several years—the means in which the artist has chosen to record his life since he embarked on filmmaking. Existing as if a personal memory archive, these images together with a published conversations between the artist and artificial intelligence (AI)-generated cognitive specters of individuals and entities such as Jiddu Krishnamurti, Salvador Dali, the Sun, and others.These conversations, generated through GPT-3—an AI platform that can be used to build conversations—were developed in collaboration with Pat Pataranutaporn, a technologist and a researcher at the Massachusetts Institute of Technology who explores the intersection of synthetic virtual humans and synthetic biology. Sparked by Weerasethakul’s contemplations of the Sun as he embarked on long walks amidst nature, the artist has pushed and guided the AI to generate increasingly coherent and interesting conversations to manifest new fictions.The artist’s established motif of the fabric-curtain-backdrop present in his past projects such as Fever Room (2015, theatrical version), Blue (2018), and Constellations (2018), makes a re-incarnation in this new project. Animated and mechanized in collaboration with DuckUnit, the material’s employment—simultaneously corporeal and apparitional—alludes to its multifarious functions in both cinema and theater. Be it establishing context as a backdrop or shielding content as a curtain, its movements now parallel a performance and a reminder of the fantastical memories of the supernatural that the material embodies.A Conversation with the Sun is a project that offers up the auteur’s own introspections, as opposed to his prior reflections of the socio-cultural milieu. Akin to the Sun’s position as a life-sustaining force of nature, the light from the projector is recognized by Weerasethakul as the filmmaker’s Sun. And just as how the light from the Sun illuminates, the light from the projector casts those memories that have been captured.In addition to the exhibition, on May 28 at the BANGKOK CITYCITY GALLERY, there will also be the launch of the book A Conversation with the Sun. The book includes a conversation between Apichatpong Weerasethakul and the Sun, joined by artists, writers, and thinkers, that is generated by the artificial intelligence platform GPT-3; and a conversation with Pat Pataranutaporn on the issues of human-technology relationships and similarities between directing and commanding to artificial intelligence, as well as the idea behind this collaboration.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/05/29/FT6tnnpaUAAdHPx.jpg,388901,,2023-09-06T22:01:06.671Z,['patpat@media.mit.edu'],True,a-conversation-with-the-sun,2022-05-28,['artificial-intelligence'],A Conversation with the Sun,PUBLIC,https://bangkokcitycity.com/activity/a-conversation-with-the-sun-in-collaboration-with-duck-unit-and-pat-pataranutaporn/,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2022-05-31T17:13:47.531Z,"Climate change presents an existential threat to human civilization, and the IPCC report of August 2021 sounds “a death knell for coal and fossil fuels before they destroy our planet."" With cities generating more than 70% of current global CO2 emissions, and with 90% of future population growth occurring in urban areas, it is a societal imperative that cities rapidly transition to a low-carbon future.  In addition, a rapid transition to a hybrid form of work that emphasizes entrepreneurship will impact how we conceive of central business districts, office buildings, housing, public spaces, and services.This workshop will be a rapid-fire, high-level exploration of how to model socio-economic-environmental interventions that could enable low-carbon (ultimately zero-carbon), entrepreneurial cities, using the MIT-Kendall Square district as the case study.  We will focus on two questions:• What would be required for MIT-Kendall Square to achieve zero-carbon in 20 years?• Can social performance be simultaneously increased to create a model entrepreneurship community?Projects:",2022-05-31,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/05/31/IMG_0526.JPG,388913,,2023-09-06T19:51:48.447Z,[],True,mas-552-modeling-low-carbon-spring-2022,2022-02-01,[],MAS.552 Modeling low-carbon entrepreneurial cities - Students projects - Spring 2022,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-06-06T20:05:14.707Z, Semi-Realistic Rotary Experiences is a set of six simulations of real life scenarios involving the gesture of rotating things. Those who wish to experience them need to perform such rotations on one to four physical knobs in an exacting and skillful manner.,,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/06/08/Snip20220608_11.png,388981,,2023-09-06T21:59:24.356Z,['lingdong@media.mit.edu'],True,semi-realistic-rotary-experiences,2022-06-06,[],Semi-Realistic Rotary Experiences,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-06-08T13:38:47.572Z," Do you work with graphics? Do you often find that an earlier version of your sketch looked more appealing? But then there's no way to go back because you made too many changes since? You didn't think you need a version control but now regret it? Do you like taking screenshots?Then maybe srcsnap is a tool for you.Every time you take a screenshot of your project, srcsnap takes a snapshot of all your code too, and associate it with the screenshot. It organizes your screenshots in a nice list in the menubar: Your code can ""time travel"" back to the very moment when you took each of the screenshots.",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/06/09/bafybeidefuk2pve5yg77cvywkxh7oqmgxs4vh22sub3xpzh5e2yx5bglty.ipfs.infura-ipfs.io.jpeg,389104,,2023-09-06T03:48:25.726Z,['lingdong@media.mit.edu'],True,srcsnap-screenshot-driven-version-tracking,2022-06-08,[],Srcsnap — Screenshot-Driven Version Tracking,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-06-15T23:51:08.328Z,"The City Science group uses simulations and visualizations to explain to stakeholders, including citizens, the various ""what if"" scenarios involved in urban planning. The knowledge and indicators obtained from these simulations are often greatly influenced by the site (neighborhood) and the subject matter. A community that holds various discussions around the simulation with a wide range of possible scenarios is necessary to discuss our environment. At this point, communication between the ""experts"" who create the simulations and the people who receive them is crucial.",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/06/21/meisai_RLYPLlG.gif,389216,,2023-09-06T03:47:01.602Z,"['yasushis@media.mit.edu', 'kosukek@media.mit.edu', 'kll@media.mit.edu']",True,namaph,2022-06-15,[],Namaph,PUBLIC,http://yasushisakai.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-06-16T18:47:36.518Z,"By Martin NisserThe ability to deploy large space structures is key to enabling long-duration and long-distance space missions, supporting permanent habitation, large scale science experiments and solar power generation and transmission. However, launching and assembling large structures today remains a challenge; structures launched to space must be designed to fit within the confines of a rocket fairing; during transport, they must be built to withstand the rigors of launch; and once in orbit, they need assembling by a team of highly trained astronauts in some of the harshest conditions we know of. The engineering of modular structures capable of self-assembly would address many of these challenges; such structures could be partitioned across multiple launch vehicles and transported sequentially; once in orbit, self-assembly would void the danger, life-support overhead and expertise required for astronaut assembly; and once assembled, such structures could be engineered to reconfigure to acquire new configurations in order to adapt to new loads and use cases. This project outlines a method to discretize a desired 3D structure into cubic building blocks, and introduces a method to magnetically ""program"" these cubes to stochastically self-assemble from a random arrangement into a target structure without explicit control. We accomplish this by encoding re-programmable magnetic signatures onto the faces of the cubic building blocks using a custom-built magnetic plotter. Key to enabling self-assembly is the design of unique magnetic signatures that ensures that the face of every cube can only mate with particular other faces, and further, can only do so in particular configurations in translation and rotation. Importantly, the corollary of this feature is that every face remains magnetically agnostic to all cube faces to which it is not intended to bond. Given some initial perturbation, the cubes can thereby self-assemble over time via random collisions, bonding to their intended mates while colliding elastically with unintended mates. These magnetic signatures and the final 3D prototype are therefore programmable, reusable, and not power-consuming during operation. On this parabolic flight, we demonstrate the utility of this system by encoding a self-sorting behavior that can be performed within the 15 seconds of a single parabola. We magnetically program a set of 150 cubes into two compatible sets indicated by blue and yellow coatings.  Starting from a random initial distribution in each parabola, the cubes are able to stochastically self-assemble into two meaningfully distinct groups, one blue and one yellow, within seconds of a perturbation.",2022-12-31,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/06/16/20220520_Steve-Boxall_PSM_ZG623_MIT_Research_6787.jpg,389225,,2023-09-07T14:58:22.397Z,[],True,stochastic-self-assembly-via-magnetically-programmed-materials,2022-06-16,[],Stochastic Self-Assembly via Magnetically Programmed Materials,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-06-28T16:15:46.429Z,"Dreaming through Code Designing creative learning workshops for local organizations committed to introducing technology to kids in a non-intimidating way“Dreaming through Code” is a workshop series designed for introducing computer science in an accessible and meaningful manner. The project is designed to be an introduction to technology and coding that uplifts and amplifies how deep, transformative learning happens in all places and not only in schools. Designed to allow for multiple touchpoints of exposure and engagement with computer science, we organized a series of workshops focused on the idea of dreaming to allow young people to learn through creating and connecting. The workshops utilize three different computational tools for making: Scratch, a programming language and online community; OctoPlay, a new mobile application that allows young people to create and send animations, messages, and stories; and Makey Makey, a programmable controller that allows young people to transform everyday objects into computing interfaces. With dreaming at the center, workshop themes revolve around topics like community, role models, superheroes, music-making, and future aspirations. The workshop series was run over the course of ~1-2 months in collaboration with two organizations focused on serving their local communities:blackyard, located in Cambridge, Massachusetts, USA, and, Freedom Skatepark Jamaica, located in Bull Bay, St. Andrew, Jamaica.",,"['lifelong-kindergarten', 'future-sketches', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/9f24dbe4-176f-463d-a17e-97e26ad77e9b,389315,,2023-09-06T03:37:09.394Z,"['csadler@media.mit.edu', 'chelsi@media.mit.edu']",True,dreaming-through-code,2022-06-28,"['civic-technology', 'learning-teaching', 'computer-science', 'creativity', 'technology']",Dreaming through Code,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-06-29T17:53:42.751Z,"FuseBot is a robotic system that can efficiently find and retrieve both RFID tagged and untagged target objects in line-of-sight, non-line-of-sight and occluded settings using RF-Visual perception. The robot fuses both RF (radio frequency) and Visual information from the antenna and camera, mounted on the wrist of the robot, respectively to locate and retrieve a target item. The system introduces two key innovations: RF-Visual Mapping and RF-Visual Extraction to accurately localize and efficiently extract the item of interest.FuseBot achieves a success rate of 95% in retrieving untagged items, demonstrating for the first time that the benefits of RF perception extend beyond tagged objects in the mechanical search problem. Our experimental results demonstrate that FuseBot outperforms state of the art vision based system’s efficiency by more than 40% in terms of the number of actions required for successful mechanical search.",,"['signal-kinetics', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/06/29/ezgif.com-gif-maker_fh1ZJCj.gif,389339,,2023-09-06T03:36:10.252Z,"['tarab@media.mit.edu', 'ldodds@media.mit.edu', 'nazishn@media.mit.edu', 'fadel@media.mit.edu']",True,fusebot,2022-06-29,"['robotics', 'computer-vision', 'health', 'sensors', 'computer-science', 'rfid', 'public-health', 'electrical-engineering', 'climate-change']",FuseBot: RF-Visual Mechanical Search,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-07-05T17:18:07.279Z,"The Mechanical Artifact Ultra Space students at the Yale School of Architecture designed the Nomad Suit, a holistic and performative suit that questioned the lived experience of space through materiality, and movement. The suit considered the body's relationship  to space, across time, dimension, and sensory feedback. Constructed from  a layered system of dyneema fabric, woven cotton, found textiles, and a  deconstructed dickies coverall jumpsuit, Nomad Suit, through its form and design, asks us to challenge the new in off-planet life and consider design for off-planet life  through interconnected historical and poetic depictions of the universe  through literature, science-fiction, and artifacts. Nomad Suit  features an amulet that can only be opened in a zero-g environment, showing memories, an inflatable sensory sleeve for proprioceptive  communication between  two humans, an analog telescopic puller to activate the movement of the  layering of the suit, and a moss-clad silicon headpiece to provide a  surface to connect the flyer to the earth through biological material.  The suit as a whole embodied an inversion of  the vacuum of space and extends a terrestrial aesthetic as conduit to  how humans have been depicting the cosmos for thousands of years. ",2022-12-31,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/07/06/Nomadic Space Suit.jpg,389401,,2023-09-07T14:58:00.962Z,[],True,mechanical-artifact-ultra-space-nomad-suit-1,2022-07-05,[],Mechanical Artifact Ultra Space Nomad Suit,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2022-07-05T20:35:17.667Z,"This project seeks to examine the determinants of economic growth by using large-scale urban mobility networks. Leveraging the fine-grained phone trajectory data, the project constructs a microscopic mobility network to describe the connectivity between economic activities in the complex urban environment. The research found that the mobility networks are highly predictive for economic outcomes, such as income and property values, across the metropolitan areas in the US. Although researchers hypothesized that human dynamics can predict economic growth, this project is one of the first that corroborates this intuitive hypothesis with concrete empirical support. ",2022-12-31,"['human-dynamics', 'media-lab-research-theme-life-with-ai']",,389410,,2023-09-07T22:22:01.092Z,"['sandy@media.mit.edu', 'tyabe@media.mit.edu', 'emoro@media.mit.edu', 'shenhao@media.mit.edu']",True,economic-growth-with-urban-mobility,2020-12-01,[],Economic growth with mobility network,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0
False,2022-07-18T12:41:05.032Z,"Yourfeed is a modular tool for ecologically valid social media research. Designed to mirror the design of a social media newsfeed, Yourfeed provides researchers with the ability to conduct studies on browsing behavior. Yourfeed tracks and surfaces the dwell time of each post, a key metric from industry that is hard to measure in other survey platforms like Qualtrics. It is designed to be modular and interoperable, allowing researchers to upload their own components, such as stimulus/entity sets, ranking algorithms, or design interventions. To learn more about Yourfeed, check out our white paper on arXiv or our first paper using Yourfeed data. ",2024-02-28,"['human-dynamics', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/07/18/yourfeed copy 2_kZPNErx.png,389524,,2023-09-06T21:58:37.245Z,['zive@media.mit.edu'],True,yourfeed,2022-07-18,[],Yourfeed,PUBLIC,http://www.zive.info,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-07-30T14:31:06.055Z,"Mental Machine: Labour in the Self Economy, 2022, is a live performance by Kawita Vatanajyankur made in collaboration with Pat Pataranutaporn from the Fluid Interfaces group at the MIT Media Lab. Commissioned by the Art Gallery of Western Australia and Simon Lee Foundation Institute of Contemporary Asian Art, Mental Machine explores the concept of labour in our current era of accelerated technological enhancement.  In Mental Machine, Vatanajyankur becomes a human-machine hybrid as she embodies two cybernetic alter egos—each uniquely programmed with different beliefs and mindsets. Pataranutaporn created these cybernetic selves through the poetic use of an AI-generated character pipeline using the artist’s personal data. In doing so, Vatanajyankur’s mind and body are guided or manipulated by her cybernetic selves to thread a massive graph-like textile pattern across the floor of the Gallery. The performance is a response to rapid developments in the fields of artificial intelligence (AI) and machine learning, which are training machines to think, talk and work like humans. By then positioning a human being as a ""mental machine,""  the artists ask many interconnected questions that shape our shared futures: as we teach machines to become human are we recreating ourselves to become more like a machine?; if AI algorithms can influence human behaviour by exploiting human biases and manipulating our emotions then are we truly free to choose?; is it possible to break the cycle of unconscious self-objectification and dehumanization that is driving society?",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/07/30/cover_photo.jpg,389644,,2023-09-06T21:51:32.045Z,['patpat@media.mit.edu'],True,mental-machine,2022-07-22,"['virtual-reality', 'human-computer-interaction', 'art', 'artificial-intelligence', 'data', 'economy', 'extended-intelligence']",Mental Machine: Labour in the Self Economy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2022-08-19T16:25:32.214Z,"As we work to improve prosthetic care in the US and in countries around the world, we are committed to learning from our most valuable asset: people living with limb loss. In striving for inclusivity, not only is our Group comprised many individuals with disabilities, but we work to document the diverse experiences of Disabled people who could benefit from our work, and their challenges in accessing fair and appropriate prosthetic care—both in the MIT Media Lab and in the field.Below we present a selection of footage from:1) Amputee Camps and prosthetic clinics in Sierra Leone, West Africa with consultant on building resiliency in low-resource environments, Dara Dotz.2) remote indigenous villages and prosthetic clinics in Sonora, Mexico attended by Dr. Elizabeth Pettit, MIT Media Lab Director's Fellow,3) a prosthetic clinic north of Amman, Jordan, where a man takes his first steps in seven years-- documented by researcher Francesca Riccio-Ackerman,",,"['biomechatronics', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/uuid/c9ab9e4d-c040-4881-9269-3a6f82f9b801,389850,,2023-09-07T22:24:51.462Z,"['e.pettit@media.mit.edu', 'friccioa@media.mit.edu', 'jhuegel@media.mit.edu', 'dsengeh@media.mit.edu']",True,in-the-field-equity-prosthetics,2019-09-01,"['health', 'public-health', 'prosthetics', 'point-care']",In the field: improving health equity in prosthetic care,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-09-07T20:54:11.592Z,"SEI Analog Environments StudyProject OverviewThe way humans engage with space has changed rapidly over the past 60 years, moving from a model built around survival in extreme environments to prolonged stays and routine operations in those same conditions. Recent developments in commercial spaceflight suggest a new model of human interaction with space – one made up of crews of individuals with varying backgrounds, agendas, roles, and expertise. How might we prepare for this near- to mid-term future of large, diverse groups of people living and working in space? By examining the experiences of both new (SEI) researchers and more seasoned explorers during SEI’s arctic analog expedition, we have the opportunity to leverage the experiences of terrestrial extreme environments explorers and residents to envision a more human-centered approach to space.How do people live and work in extreme conditions, and what are strategies and heuristics that we can learn from one environment and apply to another?How does the experience of visiting and working in an extreme environment affect the individual — prior to expedition, during expedition, and upon return from the field?How do different “populations” of people interact in extreme environments? How are these communities defined (e.g. by role, duration spent in environment, familiarity & expertise, or cultural background)?To examine these questions, we utilize a series of qualitative research methods that can be used to learn from both the SEI expedition team and the broader population of the region to derive insights on topics from environmental design to crew psychology: A journal study, piloted by the researchers in 2021 during a trial expedition to Svalbard, will be used to examine how the SEI expedition adapts and acclimates to a new extreme environment.Participants will also be asked to provide responses to a periodic questionnaire to supplement the journals with quantitative data on their perceptions. Finally, participants will participate in periodic interviews — before, during, and after the expedition — to provide researchers with the opportunity to probe for context and depth.These three research instruments will allow us to examine how participants’ perceptions change over time as they adapt and then re-acclimatize after returning to the field. These journals will also allow us to examine, at a small scale, how different people’s reactions to extreme conditions can be, and how that impacts their experiences.In addition to the journal study of SEI participants, this research aims to utilize the opportunity to accompany the expedition through the following specific research methods: participant observation of SEI research teams as they deploy their research in Svalbard, and on-site interviews with both the SEI team and local participants. These field-specific activities will be further developed through relationship building between the researchers and communities in Svalbard.",2023-07-31,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/09/07/pc-Eirik-Hodne.jpg,390269,,2023-09-07T15:40:18.488Z,"['sanas@media.mit.edu', 'mcoblent@media.mit.edu', 'aekblaw@media.mit.edu']",True,sei-analog-environments-study,2022-09-07,"['design', 'environment', 'social-science', 'space', 'wellbeing']","SEI Analog Environments Study: Adaptation, Collaboration, and Resilience in Extreme Environments",PUBLIC,https://www.sanasharma.com/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0
False,2022-09-09T02:46:27.956Z,"OverviewMost SMBs make pricing decisions using intuition and inefficient methods (the old school way), yielding sub-par sales. However, with recent advances in machine learning, such decisions can now be made optimally using data to maximize sales. Problem and SolutionOne of the most important decisions a business owner makes is setting the right prices for their products. Through our primary market research (PMR), we found that most business owners are commonly asking questions such as: (i) How much are customers willing to pay? (ii) When should I increase prices? (iii) Is it a good idea to discount prices and get the inventory moving? However, current solutions in the market are inefficient. 74% of Ecommerce SMBs in the US use the most basic method of pricing: cost-plus pricing- simply charging consumers the cost of goods sold plus a predetermined amount of profit, embodying the basic “outdated” idea behind doing business. While familiar and easy to implement, these prices bear no relation to the amount that consumers are willing to pay. As a result, profits are left on the table daily. To this end, the main problem we solve is allowing businesses to quickly react to market forces and set optimal prices for their products, leading to more efficient sales. In essence, we try different prices to learn patterns and correlations between different external factors (inventory levels, ad spend, seasonality, location, weather, time of day, etc) and revenue. We use mathematical models to minimize price iterations while maximizing profit to arrive at the optimal price. If you're interested in collaborating, please reach out to: hello@catalan.aiVisionCatalan.ai: bringing machine-learning based efficient pricing to worldwide eCommerce.InspirationOur inspiration started as a problem, grew into curiosity and culminated into catalan. We wanted to eat at our favorite restaurant, but it was too expensive. We questioned why they couldn’t reduce prices during down times. Figured out they and most others didn’t have the know-how to price. We did. State of the ProjectCatalan is currently running 4 experiments, automating prices for Shopify stores in different industry verticals. In their first experiment, they increased the store’s average order value by 8.7% and profit by 7%!TeamIshaan Grover (MAS PhD ‘23) and Andres Garza (MIT Sloan ‘22) met one year ago, in Fall 2021, while taking the Media Lab’s: MAS.665 Global Ventures: AI for Impact. Taught by Prof. Sandy Pentland and Prof. Ramesh Raskar, the class inspired them to think big with the class project prompt being: “come up with a business idea that in 10 years will reach 1 billion users and achieve $1 trillion in revenue”. Galvanized, Ishaan and Andres conducted initial PMR with stores on Central Square and identified a huge and untapped problem for SMBs: suboptimal pricing methods. During IAP, they participated in MIT Fuse where they identified the beachhead market (Shopify SMBs that make between $1M-$10M in yearly revenue). In the Spring, they took Prof. Paul Cheek’s 15.378 Venture Creation Tactics, where they commenced their first experiment with a Shopify store: Rootz.mx. This summer, they participated in MIT Delta V along with Divij Lankalapalli (MIT Math / Computer Science ‘24) where they built out the tech stack, designed and launched the consumer-facing web app, and onboarded 4 more stores to conduct experiments.",,['media-lab-research-theme-decentralized-society'],,390313,,2023-09-06T21:59:52.822Z,['igrover@media.mit.edu'],True,catalan-ai,2022-06-06,[],Catalan.ai,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2022-09-22T19:40:23.637Z,"Tap To Remember is a wearable memory prosthesis.  We are testing whether laboratory techniques for enhancing memory—such as targeted memory reactivation, theta wave entrainment, and spaced repetition—can be unintrusively integrated into a smartwatch and used to improve memory. Our goal is to facilitate learning in healthy people, and improve memory function and quality of life in people suffering from memory disorders.Currently, we are beginning a study to test whether Targeted Memory Reactivation (TMR) with smartwatches can be used to improve memory for everyday events. In this study, participants watch a movie while wearing a smartwatch that periodically vibrates in a specific pattern.  When participants go to bed,  an algorithm in the smartwatches detects deep sleep and replays the same sequence of vibrations, inducing reactivation of the memory in sleep. This technique has previously been shown to improve memory in laboratory tests; our experiment aims to show that the technique can be used outside the lab to enhance memory for everyday events.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/05/smartwatch2.jpg,390440,,2023-09-06T21:33:51.178Z,"['nathanww@media.mit.edu', 'pattie@media.mit.edu']",True,tap-to-remember,2022-09-01,[],Tap To Remember,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-09-26T16:17:22.318Z,"More than 95% of the ocean has never been observed by humans, even though the ocean plays the largest role in the world's climate system, has so many undiscovered species, and regulates the world's weather.We invented the world's first battery-free wireless underwater camera.  The camera is powered by underwater sound, captures color images at ultra-low-power, and transmits them wirelessly to a remote receiver. Our electronic design consumes 100,000 times less power than state-of-the-art low-power underwater imaging systems, allowing us to power it entirely based on the harvested sound energy. This technology paves the way for massive, continuous, and long-term ocean deployments with many applications including marine life discovery, submarine surveillance,  climatology, maritime archeology, geology, space exploration, aquaculture farming, and underwater climate change monitoring.",,"['signal-kinetics', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/09/26/video-shotcut-draft7-with-backgr_2.gif,390499,,2023-09-06T21:29:42.506Z,"['afzals@media.mit.edu', 'wakbar@media.mit.edu', 'osvyrd@media.mit.edu', 'mdoumet@media.mit.edu', 'unsoo@media.mit.edu', 'rezagh@media.mit.edu', 'fadel@media.mit.edu']",True,battery-free-wireless-underwater-camera,2022-09-26,"['robotics', 'virtual-reality', 'artificial-intelligence', 'augmented-reality', 'ecology', 'energy', 'environment', 'health', 'sensors', 'imaging', 'computer-science', 'technology', 'public-health', 'electrical-engineering', 'internet-things', 'ocean', 'climate-change']",Battery-free wireless underwater camera,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-09-27T15:12:27.068Z,"Advancements in machine learning have recently enabled the hyper-realistic synthesis of prose, images, audio and video data, in what is referred to as artificial intelligence (AI)-generated media. These techniques offer novel opportunities for creating interactions with digital portrayals of individuals that can inspire and intrigue us. AI-generated portrayals of characters can feature synthesized faces, bodies and voices of anyone, from a fictional character to a historical figure, or even a deceased family member. Although negative use cases of this technology have dominated the conversation so far, in this Perspective we highlight emerging positive use cases of AI-generated characters, specifically in supporting learning and well-being. We demonstrate an easy-to-use AI character generation pipeline to enable such outcomes and discuss ethical implications as well as the need for including traceability to help maintain trust in the generated media. As we look towards the future, we foresee generative media as a crucial part of the ever growing landscape of human–AI interaction. ",,"['media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/09/28/ai-generated-characters-cover_rHwfK6O.gif,390522,,2023-10-19T19:10:00.189Z,"['vdanry@media.mit.edu', 'joaleong@media.mit.edu', 'patpat@media.mit.edu', 'pattie@media.mit.edu']",True,theme-virtual-humans,2022-09-27,[],Theme | Human-AI Interaction,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2022-09-27T18:43:00.873Z,"Joe Kennedy – Harvard University: Graduate School of Design, DDes; Morgan Ip – Cambridge University: Scott Polar Research Institute, Post-Doc; Ramon Weber – MIT: Building Technology, PhDSvalbard is one of the locations most dramatically impacted by climate change in the world, with the annual temperature rising 4 degrees Celsius in the last half century. Due to these rapidly changing and extreme environmental conditions, the settlement of Longyearbyen has had to adapt to shifting foundations and landslides that has resulted in the destruction of homes and the displacement of residents due to avalanche risk. In response to this contextual uncertainty, we explore how dynamic architectural solutions for mobile shelter can accommodate dramatically shifting temperatures, wind and topographical conditions. By combining advanced digital manufacturing with computational design and environmental simulation, we intend to prototype a lightweight portable living capsule in harsh environments both for future space pioneers and climate refugees who will require new forms of housing and shelter that can adapt to impending environmental extremes.Svalbard’s remoteness, its barren geologic topography, as well as the angle and cycle of sunlight exposure roughly approximate some of the environmental conditions that would be present in the southern pole of the moon, an area that has been considered for a future space settlement.",2023-07-31,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/12/20/_M7A6745_xgwfVxh.JPG,390535,,2023-09-07T15:40:02.831Z,[],True,micro-mobile,2022-09-27,[],Micro Mobile,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2022-09-28T01:43:22.673Z,"How are our phones and email devices altering our ability to engage deeply in our daily tasks?  How important is it to spend time deeply engaged and undistracted?  Research shows that the quality of our attention during our day is a better predictor of our happiness than what we're actually doing.  Unfortunately, studying the subtle influence of our environments and our tools on the quality of our experiences is notoriously tricky.With this project, the Responsive Environments group aims to understand and study mental experience by probabilistically combining physiological and behavioral data from everyday life.  That data includes information from the award-winning Captivates smartglasses platform built in the group alongside two other custom wearables which measure, throughout daily life, how easy it is to interrupt a person with a tap and how substantially people lose track of time.  All of these devices provide unique insight into a user's cognitive state.Combining these measurements with interventions that users can take home and live with is a powerful way to sculpt long-term, realistic, successful and individualized solutions to the challenges of modern life, and to support more time spent deeply engaged in meaningful pursuits.",,"['responsive-environments', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/09/27/flow_img_2.jpg,390566,,2023-09-06T21:35:55.760Z,['dramsay@media.mit.edu'],True,capturing-flow-in-the-wild,2022-09-01,[],Capturing Flow in the Wild,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-09-28T15:29:43.514Z,"Contributors: RESOURCE (SSERVI grant), SEIAssociated Groups: Aero/Astro Human Systems Lab (C. Paige, F. Ward), Media Lab Responsive Environments (D. D. Haddad), WHOI (J. Todd)Project overview & mission objectivesAs part of MIT’s Resource Exploration and Science of our Cosmic Environment (RESOURCE) project we will be testing both the scientific and operational usefulness of a virtual reality platform for local, small-scale (< 5 m) geological analysis for Lunar rover exploration missions.  Specifically, we will be testing the use of a commercial off-the-shelf LiDAR camera combined with RGB imagery displayed in a VR platform developed in Unity for the Oculus Quest 2. Additionally, we will incorporate local environmental data such as temperature, luminosity, humidity, and multi-spectral data. These instruments will be a compact, modular instrument package which can be deployed at each site for continuous data collection. We will collect data from a minimum of three distinct locations, render the data in VR and, using specialized tools developed in the platform, will have mission scientists answer questions about the relevant local geology, namely 1) identify warm- or cold-based glacial till, 2) identify minerology from lichen classification and 3) confirm small-scale, local permafrost activity. Having a geologist on-site we will be able to provide ground-truth and will compare the VR assessments to a screen-based platform and traditional geological field methods to better understand the relevance of VR for science in remote analysis.Related project pageFrom Zero-G class: https://zero-gravity.pubpub.org/pub/erybwm0y/release/6",,['space-exploration'],https://dam-prod2.media.mit.edu/x/2022/09/28/unnamed.png,390579,,2022-10-14T17:23:52.764Z,[],False,capturing-the-moon-assessing-virtual-reality-for-remote-lunar-geological-fieldwork,,[],Capturing the Moon: Assessing virtual reality for remote Lunar geological fieldwork,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2022-09-28T15:50:17.819Z,"Smartglasses are increasingly popular because the face is an ideal location for continuously monitoring environmental and physiological signals. Unfortunately, no standard smartglasses platforms offer easy access to physiological data.  That leaves researchers constrained by their own quick—and usually bulky—designs that make people self-conscious and uncomfortable, altering their behavior and preventing long-term naturalistic studies. We spent a summer in Shenzhen, China, learning from eyeglass manufacturers on how best to integrate sensors that track physiological events across your face, head movements, and location in a form factor that is similar to a traditional pair of glasses. From this work, we have created a set of smart eyewear platforms tailored for different applications. Project Captivate: a tool for researchers, a means to understand attention and engagement, and a scalable means to control responsive ecosystems.AirSpecs: a tool to characterize environments across contexts, estimate the constituents of pollutants in the air that users inhale, and measure how well-ventilated the area around the user is. ",,"['responsive-environments', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/09/28/DSCF0861_edit.png,390587,,2023-09-07T23:35:29.429Z,"['chwalek@media.mit.edu', 'dramsay@media.mit.edu']",True,smart-eyewear,2019-05-01,[],Smart Eyewear Research Platforms,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0
False,2022-09-28T15:56:18.936Z,"The quality of the air we inhale affects our physical well-being, our mental performance, and our emotional state. Yet, most of us are unaware of what we are breathing in, and how it impacts us.Our collective experience with COVID-19 has stressed the importance of maintaining healthy and comfortable environments where we live, work, and socialize. We now live in a time when parents equip their children with air quality sensors to assess the wellbeing of the spaces they occupy and to intervene when extraordinary conditions are sensed. It’s common knowledge that extreme levels of particulates and foreign gasses in the air are hazardous to our health. However, far less is understood about how milder levels of these constituents affect our long-term health, cognitive functions, and comfort. That is why we created AirSpecs, an eyewear platform that incorporates environmental sensing into an innovative eyewear platform for characterizing spaces across contexts. AirSpecs is designed to estimate the composition of inhaled gases for a more accurate understanding of inhaled pollutants and a more thorough assessment of how well-ventilated the area around the wearer is. AirSpecs is also designed with several other sensor modalities to characterize the spaces individuals spend time in to offer greater insight into what environmental characteristics users prefer and what changes they notice. The platform also has several features that allow researchers to extend the native sensing capabilities to allow their research to extend past controlled lab environments.",,"['responsive-environments', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/09/28/DALL·E 2022-09-28 12.01.07 - A person sitting at a desk while wearing electronic eyeglasses and wearing brain electrodes while surrounded by a cloud of pollutants .png,390595,,2023-10-17T17:54:09.201Z,['chwalek@media.mit.edu'],True,airspecs,2022-05-01,[],AirSpecs,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-09-28T16:50:12.153Z,"Somayajulu Dhulipala, Manwei Chan, Carlos Portela, MIT SEDSOverviewAs humanity inches closer to setting up colonies on the Moon and Mars, locally grown food will be a big part of making this process sustainable. One risk that plants face in such harsh climates is the possibility of hull breach (i.e. system failure) leading to instantaneous death of the plants due to low temperature, pressure, and exposure to radiation. In this project, we developed an emergency preparedness system for plants in space. Such a system would also find application for growing plants on Earth in low-resource harsh environments such as deserts and polar regions.",2023-03-31,"['space-exploration', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/10/13/DSCF7622.jpg,390603,,2023-09-07T15:39:28.398Z,[],True,agrithrive,2023-09-28,[],AgriThrive: Emergency preparedness for Plant Growth in Martian/Lunar environments,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-09-29T01:14:56.431Z,"Current methods to assess depression and select appropriate treatments have many limitations and need improvement. Diagnosis is usually based on having a clinician interview the patient, a method developed in the 1960s. The main drawbacks of most assessment methods today are lack of objectivity, being symptom-based and not preventative, and requiring accurate communication of lengthy information in a short amount of time. This work explores new technology to assess depression, including its increase or decrease in symptoms, in an automatic, more objective, pre- and post-symptomatic, and cost-effective way using wearable sensors and smart phones.  These can provide up to 24/7 monitoring of different personal parameters such as physiological data, voice characteristics, sleep, and social interaction, ""seeing"" things that may not usually be visible, which may enable earlier detection and prevention. We aim to enable more accurate subtyping of depression, prevention of depression, assessment of depression for people who cannot communicate, better assignment of an efficacious treatment, early detection of treatment remission and response, and anticipation of post-treatment relapse or recovery.",,"['affective-computing', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/09/29/20220829_110434-mountains-and-dunes.jpg,390638,,2023-09-07T23:03:18.847Z,"['sfedor@media.mit.edu', 'ncjones@media.mit.edu', 'boyuz@media.mit.edu', 'roblewis@media.mit.edu']",True,objective-assessment-of-depression,2018-02-10,[],Objective assessment of depression,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-09-29T14:43:32.182Z,"The MIT City Science team will work with the Cámara Chilena de la Construcción (CChC), Corporación Ciudades, and the Biobío regional government on a new collaboration, to create a City Science Lab in Biobío, Chile. Teams will research development and innovation related to design and technology for communities, with an emphasis on building a scalable platform for data analysis, data visualization, and simulations of urban interventions, to improve the living standard of the citizens.Team will also work with ChileMass, as the articulator between the needs of Concepción and the work carried out by the MIT City Science Lab.Cámara Chilena de la Construcción A.G. (CChC) is a non-profit trade association that aims to promote the development, strength and growth of construction activity as a fundamental driver for Chile’s development. CChC’s mission is to improve the quality of life of people, committed to the sustainable development of the construction industry. Corporación Ciudades, is a non-profit organization that aims to promote equity and territorial welfare, reducing urban segregation and promoting long-term integrated territorial planning. In Greater Concepción, the City Laboratory will be implemented in partnership with the Massachusetts Institute of Technology",,"['city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/08/17/FOTOS CONCE RRSS (232) (1).jpg,390655,,2023-09-07T13:58:31.733Z,"['alonsolp@media.mit.edu', 'kll@media.mit.edu', 'markuse@media.mit.edu', 'doorleyr@media.mit.edu', 'mdchurch@media.mit.edu']",True,city-science-lab-bio-bio,2022-10-01,[],City Science Lab @ Biobío,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-09-30T13:22:18.951Z,"Continuing the work of Liquid Movies, we introduce Liquid Learning, an extension for not simply recommending videos but encouraging people to efficiently explore the sea or reservoirs of educational video content.Currently, there are an overrepresented amount of computational decision-making systems in service of providing recommendations to keep people returning and engaged on some given platform. However, at this unprecedented time where the content on these platforms far exceeds our ability to find the information most relevant to us, we become susceptible to our incomplete intuitions and assumptions about the world, which demonstrably leads us astray in assessing truth and distilling knowledge.To meet this challenge, Liquid Learning scans the web for visual material related to a topic and organizes it so one can compose a movie on demand or travel through a knowledge space exploring diverse topic presentations. For example, the Bayes theorem can be described through examples, with equations (Bayes formula), graphically (tree diagrams), or geometrically (e.g., Venn diagrams). Unfortunately, it is also connected to typical human biases and reasoning flaws. Liquid Learning finds these explanations, analyzes them, parses them, and provides an interactive space to explore a topic that matches one’s backgrounds, learning styles, and intentions.",,"['viral-communications', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/10/17/liquid_base_many_symm_i.jpeg,390684,,2023-09-06T21:28:02.371Z,['erick000@media.mit.edu'],True,liquid-movies-learning,2022-09-30,"['design', 'computer-vision', 'interfaces', 'physics']",Liquid Learning,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-09-30T15:43:12.942Z,"CommunityLM is a research program from MIT Center for Constructive Communication (CCC) and MIT Media Lab. We are devoted to developing socially-aware language models for community understanding and constructive dialogue.  Check out our Demo!PublicationsHang Jiang, Doug Beeferman, Brandon Roy, and Deb Roy. 2022. CommunityLM: Probing Partisan Worldviews from Language Models. In Proceedings of the 29th International Conference on Computational Linguistics (COLING), pages 6818–6826, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. [Paper][arXiv][Code][Video][Slides][Poster][Models]",,"['social-machines', 'center-for-constructive-communication', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/fb7d773a-7c57-4647-8721-4909304f528f,390692,,2023-09-07T23:16:40.751Z,['hjian42@media.mit.edu'],True,communitylm,2022-02-17,"['social-networks', 'artificial-intelligence', 'civic-media', 'communications', 'politics', 'social-media', 'natural-language-processing']",CommunityLM,PUBLIC,https://hjian42.github.io/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-09-30T16:26:08.155Z,"X-AR is an augmented reality (AR) system that gives humans ""X-Ray Vision"" X-AR is a new AR headset that enables users to see things that are hidden. This new headset helps users find items that are inside closed boxes, under piles, or behind occlusions.  The headset uses holograms to guide users toward desired items and verify when they have picked them up. ",,"['signal-kinetics', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/02/22/IMG_7368.JPG,390700,,2023-09-06T21:05:14.725Z,"['tarab@media.mit.edu', 'mllam@media.mit.edu', 'ldodds@media.mit.edu', 'alineeid@media.mit.edu', 'fadel@media.mit.edu']",True,augmented-reality-with-x-ray-vision,2023-02-24,"['virtual-reality', 'computer-vision', 'artificial-intelligence', 'augmented-reality', 'consumer-electronics', 'environment', 'sensors', 'imaging', 'technology', 'rfid', 'human-augmentation', 'ocean', 'climate-change']",Augmented Reality with X-Ray Vision,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-09-30T19:41:43.215Z,"@artbhot is a Twitter bot that leverages stable diffusion technology to translate user tweets into images and animations.  Access to text-to-image networks has generally been restricted to Google Colab notebooks and interactions with the command line , until recently when implementations such as Midjourney and DALLE.2 were made available with access to more user friendly features.  Embedding this generative deep learning capability amongst social media however provides a unique opportunity to gain insight into how people interact with image and video synthesis algorithms in a social environment. At a time of unprecedented growth and usage of these generative models, and given the pervasiveness of the generated media already online, it is salient to gauge the role this technology could play moving forward.",2025-09-01,"['viral-communications', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/09/30/bhotskull.jpeg,390710,,2023-09-06T21:36:42.266Z,['amy01@media.mit.edu'],True,artbhot,2022-09-01,"['social-networks', 'human-computer-interaction', 'art', 'artificial-intelligence']",'@artbhot' - Towards a Creative Partner on Social Media,PUBLIC,https://twitter.com/artbhot,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2022-10-02T13:47:53.762Z,"CyberBiome is a programmable bio-digital organ, a wearable biosynthesis platform for personalized, on-demand production of therapeutics and biomolecules in space. The system contains 3D printed the bioreactors containing engineered living cells as the units of production. CyberBiome deploys optogenetics, which is alight-activated genetic circuit circuit allowing an astronaut to digitally control what biomolecules and therapeutics are produced, to respond to the their current health. Further, the compounds can be produced and administered in smaller quantities and possibly more continuously based on in-the-moment need, as well as during activities such as space excursions when traditional methods for responding to health issues may not be available. We envision this technology to enable continuous monitoring and intervention of astronaut health in a closed-loop, cutting down the time for health evaluations and allowing crew members to focus on theirmission while aspects of astronaut health are being continuously monitored and addressed.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/02/PTPfTvzo.jpg,390731,,2023-10-17T23:40:00.207Z,"['pattie@media.mit.edu', 'dkong@media.mit.edu', 'patpat@media.mit.edu']",True,cyber-biome,2022-10-02,"['bioengineering', 'wearable-computing', 'biology', 'biotechnology']",CyberBiome,PUBLIC,http://pat.design/,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-10-03T14:41:26.539Z,"The landscape in which society interacts with news has evolved due to the advent of the internet and modern communication platforms. Although this evolution has led to greater diversity and accessibility of news media, it has also created challenges regarding selective news coverage, bias, and fake news. This work proposes a novel news platform called Liquid News that aims to enhance people’s understanding of news by leveraging machine-learning-based analysis and semantic navigational aids. BackgroundOver decades, news and how we interact with information have significantly evolved. This evolution has led to greater accessibility and connectivity with the rise of the internet. Still, it also has strengthened or introduced negative factors such as bias and fake news. This has culminated in a world where the ability to access and share information has become readily available; however, truly understanding the news and global events has become far more obscured due to the rampant rise of bias and fake news.The intended end product of Liquid Movies News is an interface that allows users to parse news via a semantic-relational model that leverages the latent connection between news segments to garner a better understanding of the news at hand. For example, Queen Elizabeth II's death was heavily covered in the news/media. The information was focused on her death and related topics such as royal success, British history, the monarchy's wealth, etc. These topics relate to the Queen's death on a latent semantic-relational level and are essential to understanding her death's significance. However, these topics were covered across multiple news mediums and at varying depths, making it hard to identify and understand these latent connections. Liquid Movies aims to build an interface that uses machine learning to identify the key topics and parse, group, and relate news segments from many news sources, hopefully uncovering these latent relationships and promoting a greater understanding of the news and media around us. System Overview",,"['viral-communications', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/12/07/mediawall_0Jcw8D7.jpeg,390756,,2023-09-07T23:33:59.463Z,"['dagmawi@media.mit.edu', 'lip@media.mit.edu']",True,liquid-movies-news,2022-09-07,"['artificial-intelligence', 'communications', 'entertainment']",Liquid News,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-10-03T19:05:26.174Z,"Existing upper extremity prosthesis controllers have limited functionality and place high cognitive demands on users, both of which contribute to the high rates of device abandonment seen in this group.This project investigates methods of translating the intentions of  someone with upper extremity amputation into the resulting movement of  their prosthetic device. In recipients of both the AMI procedure and traditional amputations, signals generated by the residual limb musculature can be used as control signals. Using these physiological signals for control allows for the development of subject specific controllers that grant intuitive control of multiple degrees of freedom. Ultimately, sufficiently advanced controllers will grant control of a prosthesis with native biomechanics.",,"['biomechatronics', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/21/LUKEarmPiano.jpg,390781,,2023-09-06T03:16:25.859Z,"['mfernan@media.mit.edu', 'hherr@media.mit.edu']",True,neural-control-of-upper-extremity-prostheses,2022-10-03,"['bioengineering', 'biomechanics', 'prosthetics', 'bionics', 'neural-interfacing-and-control', 'prosthetic-design']",Neural Control of Upper Extremity Prostheses,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2022-10-03T19:39:06.960Z,"As of 2020, approximately 40 million adults, or 18.1% percent of the adult population, in the United States suffer from anxiety disorders, making them the country’s most common mental health condition.Nonetheless, only 36.9% of this population receive treatment [1].  Traditionally, interventions such as pharmaceuticals, meditation and biofeedback have been used to treat anxiety disorders. However, recently, music has emerged as a powerful tool for controlling negative affective states, including stress and anxiety, due to its portability, accessibility, and unique ability to modulate mood [3]. As a new potential therapy, there is still much to be explored in the realm of music-centered anxiety research. In particular, the majority of research appears to bias towards either self-reported data or physiological measurement, and rarely assesses these measures together. To address this deficiency, we designed our study using two methods of data collection: self-reported survey data and  participants’ heart rate (HR) and electrodermal activity (EDA). We sought to investigate whether we could more effectively shift users’ affective state from anxious and stimulated to calm and relaxed by modulating musical parameters, including, tempo, rhythm, and instrumentation, of a provided music piece [6], [7].In order to do so, we created an interactive musical interface, populated with fourteen 30-45 second long musical pieces specifically written for this experiment. Each musical fragment led into the next due to gradual changes in tempo, rhythm, and instrumentation. Following a stress-inducing task, we recorded HR and EDA, while participants in the intervention group were asked to navigate through the interface and participants in the control group were asked to remain seated, quiet and still. Physiological data was supported with periodic self-reported questionnaires pertaining to stress and anxiety. Despite the fact that participants' pre-study stress was reportedly higher in the control group, there was no significant difference in reported stress between both groups after the stressor task. At the end of the experiment, participants in the intervention group reported significantly less stress than participants in the control group. Physiological results were promising as well, though not as clear-cut, and must be considered alongside a discussion of our experimental design.",,"['affective-computing', 'opera-of-the-future', 'biomechatronics', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/10/03/306445AF-6512-4180-8775-0B6402017D5D.JPG,390790,,2023-09-07T20:44:42.760Z,"['maddae@media.mit.edu', 'klecamwa@media.mit.edu', 'samga@media.mit.edu', 'neska@media.mit.edu', 'nsingh1@media.mit.edu']",True,pharmamusicology,2021-10-01,"['human-computer-interaction', 'art', 'cognition', 'entertainment']",Pharmamusicology: Exploring the Impact of Music on the Physiology and Psychology of Mental Health,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0
False,2022-10-03T19:53:00.546Z,"@artbhot is a Twitter bot that leverages stable diffusion technology to translate user tweets into images and animations. Access to text-to-image networks has generally been restricted to Google Colab notebooks and command line prompts, until recently when implementations such as Midjourney and DALLE.2 were made available with access to more user-friendly features.  Embedding this generative deep learning capability amongst social media however provides a unique opportunity to gain insight into how people interact with image and video synthesis algorithms in a social environment. At a time of unprecedented growth and usage of these generative models, and given the pervasiveness of the generated media already online, it is salient to gauge the role this technology could play moving forward.",,[],https://dam-prod2.media.mit.edu/x/2022/10/03/bot.jpeg,390798,,2022-10-03T20:03:47.296Z,[],False,artbhot-towards-a-creative-partner-on-social-media,2022-09-01,[],@artbhot – Towards a Creative Partner on Social Media,PUBLIC,https://twitter.com/artbhot,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2022-10-04T17:56:46.619Z,"Over the last few decades, numerous scholars have documented the fact that in general, people of color and other socio-economically marginalized groups in the United States experience a disproportionate burden of environmental challenges such as exposure to air pollution, contaminated water, habitat loss, and disrupted livelihood due to natural hazards and climate change - this pattern is referred to as environmental justice (EJ).  Satellite Earth observations can be used to monitor air quality, water quality, extreme weather and other quantities relevant to EJ. We explore applications of satellite data for EJ through a case study of prison landscapes in the United States. In recent years, researchers and journalists have elevated a pattern of prison landscapes being exposed to environmental hazards such as air pollution, poor water quality, proximity to hazardous waste facilities, and inadequate mitigation in the face of extreme weather conditions—a pattern frequently referred to as “prison ecology.” Community organizers, lawyers and other environmental justice activists have needs for empirical evidence of the environmental injustices occurring in and around prison landscapes in order to advocate for the rights of incarcerated peoples to live in conditions that are free of life-threatening conditions. Geospatial data combined with sociodemographic information about prison populations can elucidate spatial patterns of vulnerability to environmental hazards and support decision-making workflows to advance equity and environmental justice (EEJ) in these underserved landscapes of primarily low-income people of color. To this end, this project pursues an objective to co-design and prototype an operational geographic information system that responds to data needs in decision-making workflows for prison ecology activist organizations. This objective is part of an ongoing partnership with community activists organizing resistance at the intersection of incarceration and the environment.The outcomes of this project will enhance the capability of prison environmental justice activists to apply satellite-based remote sensing data to support community level management to advance EEJ for underserved populations in prison landscapes. In the government domain, the research will consider how remote sensing could improve screening tools or legal processes to prevent or remedy prison ecology issues. In the public domain, the research will consider how remote sensing could inform strategy or media campaigns to support real-time grassroots organizing for just outcomes. This work is supported by grants from both NASA's Applied Sciences Program and MIT's IDSS Initiative on Combatting Systemic Racism.",,"['space-enabled', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/03/29/cancer_alley.png,390830,,2023-09-07T19:55:48.636Z,['ufuoma@media.mit.edu'],True,applying-the-evdt-integrated-modeling-framework-for-environmental-justice-applications,2022-01-01,[],Satellite Data for Environmental Justice,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-10-04T18:07:53.709Z,"The work of the EVDT community is based on the publication that introduced the framework in 2019:Reid, Jack, Cynthia Zeng, and Danielle Wood. ""Combining Social, Environmental and Design Models to Support the Sustainable Development Goals."" In 2019 IEEE Aerospace Conference, pp. 1-13. IEEE, 2019. As the abstract of that paper describes, the project is a research agenda to apply environmental modeling, complex systems modeling, and model-based systems engineering to inform the design of space systems in support of the Sustainable Development Goals. This work builds on previous research in the following areas: 1) physics-based environmental modeling; 2) complex systems modeling to simulate human decision making using agent-based models; and 3) model based systems engineering to inform the architecture of satellites or space-enabled data systems.",,['space-enabled'],https://dam-prod2.media.mit.edu/uuid/12e89126-da56-4dae-9ba5-ab6c3eb47a2c,390838,,2022-10-04T19:07:26.730Z,[],False,environment-vulnerability-decision-technology-evdt,2022-01-01,[],Environment-Vulnerability-Decision-Technology (EVDT),PUBLIC,https://jackreid.github.io/,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-10-04T18:16:02.824Z,"Zero Robotics is an educational program that hosts tournaments to teach middle and high school students to write code and program with robots on the International Space Station (ISS). Professor Danielle Wood, Director of the Space Enabled Research Group, took over leadership of the Zero Robotics Educational Outreach Program in 2020 at the invitation of the founders. The program was founded over a decade ago by MIT’s Alvar Saenz-Otero, Jacob Katz and David Miller and co-led by a team from Innovation Learning Center and Aurora Flight Sciences, The Zero Robotics program originally allowed students to use robotics called SPHERES (Synchronized Position Hold Engage and Reorient Experimental Satellites); this system was developed by the MIT Space Systems Laboratory, in conjunction with NASA, DARPA, and Aurora Flight Sciences, to provide a safe and reusable zero gravity platform to test sensor, control, and autonomy technologies for use in satellites. In 2022, the Zero Robotics summer program used the new free-flying robotic system Astrobee for the first time. Astrobee is a system of robots built and operated by NASA; the Astrobee project builds on the legacy of SPHERES and took over the tasks as SPHERES retired in 2019. The Astrobee system was designed and built at NASA’s Ames Research Center for use inside the ISS. Astrobee is a testbed to explore the capabilities of robotic systems in space to perform tasks, test satellite maneuvers and assist astronauts. MIT collaborates with the Innovation Learning Center to implement Zero Robotics and serve students and educators. Sponsors for the program have included NASA, Aerospace Corporation, Aurora Flight Sciences, and the ISS National Laboratory which is operated by the Center for the Advancement of Science in Space.In summer 2022, Zero Robotics held the first tournament with new robots called Astrobee. The game challenge for 2022 year was called “The Great Astro-Spelling-Bee.” Students were asked to use the trajectories of Astrobee to spell out a password needed to synchronously reboot the ISS power system during an EVA. The students need to select letters with complexity and length constraints and design the trajectories that can effectively and efficiently convey the required message. For 2022, 20 teams and 180 middle school students participated in the middle school competition. During the event, MIT was also able to hold a field day for 3 local teams in Boston area. In-person activities related to the game challenge and a hands-on workshop on innovative coding are provided to students. The final competition was live streamed from the ISS on August 3rd, with students both in-person at local hosting locations and watching virtually. For 2023, Space Enabled is leading a team to host a middle school program for US students and a special edition of Zero Robotics serving university students in the United Arab Emirates (UAE). The UAE Zero Robotics Programming Challenge is celebrating the collaboration between NASA and the UAE space program as the second astronaut from the UAE, Sultan Al Neyadi, performs a mission on the International Space Station.",,"['space-enabled', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/10/17/zr_background.png,390846,,2023-09-07T23:13:26.336Z,"['yiyunz@media.mit.edu', 'drwood@media.mit.edu']",True,zero-robotics,2021-09-01,"['robotics', 'space']",Zero Robotics,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-10-05T14:49:20.736Z,"Project Contributors: Jessica Todd, associated groups: Human Systems Lab, AeroAstro, WHOI Autonomous Robotics and Perception Lab, Woods Hole Oceanographic Institutesea2space is a project focused on developing adaptive mapping algorithms for both underwater and planetary science robotics. Many phenomena scientists want to observe in the natural world are governed by various geological and biological processes at a range of scales. We aim to use computer vision techniques to identify different terrain and habitat types at the macro scale (e.g. coral reefs, rock types) to help target exploration of phenomena at the microscope (e.g. locating subsurface Martian water or localising diseased coral). In Svalbard we are gathering invaluable datasets on planetary-like environments to be used for the development of these algorithms.",,['space-exploration'],,390879,,2022-10-14T17:23:17.758Z,[],False,sea2space,,[],Sea2Space,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2022-10-05T18:08:44.500Z,"Dive is a project to explore and induce lucid dreams using mobile devices.Lucid dreams are dreams in which  you are aware that you're dreaming, and can control the setting and plot of the dream.  Lucid dreaming has the potential to serve as a ""personal holodeck"" for practicing skills, exploring scenarios, treating mental health disorders, and exploring an inner world.The Dive project aims to make lucid dreaming more accessible and easier. In collaboration with the Cognitive Neuroscience Lab at  Northwestern, we've developed an Android app which adapts a technique previously used to induce  lucid dreams in a sleep lab by presenting sounds during REM sleep.A particular challenge in using this technique is detecting  REM sleep without specialized lab equipment. We are testing whether algorithms using data from a smartwatch, or even just the motion sensors in a phone placed on the bed, can  be used to detect REM and deliver the sounds at the correct time.If you have an Android phone, you can try Dive here!",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/05/dive3.jpg,390888,,2023-09-06T03:15:38.297Z,"['pattie@media.mit.edu', 'nathanww@media.mit.edu']",True,dive-1,2022-10-05,[],Dive,PUBLIC,https://play.google.com/store/apps/details?id=com.neurelectrics.dive&hl=en_US&gl=US,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-10-06T18:46:28.368Z,"Imagine a personal AI agent that you can converse with naturally about your friends and family, helping you remember previous topics you’ve discussed, suggesting topics for future conversations, reminding you of prior commitments made with others, and encouraging you to stay connected with your relations. To engineer such a system, one would need to consider how to process conversational data to distill important information, how to identify which threads of conversation are related to which relation of interest, and how to integrate this information into natural conversation with a human user. I hope to develop this system, in hopes of both advancing the area of organic, fluid human-AI interaction, and in hopes of addressing the rising issue of loneliness. This system could be especially useful for older adults, who are becoming increasingly socially isolated due to various factors including disability-related barriers and their lower digital literacy in using social media and other forms of communication technology [1]. Loneliness, the subjective experience of isolation, has subsequently become more prevalent; this state has been shown to lead to poor health outcomes comparable to well-known health risk factors such as smoking and alcohol consumption [2]. In my research I explore whether conversational AI characters could offer a solution for  this social epidemic—specifically, by strengthening an individual’s social circle through AI agents.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/18/Companion (1).png,390922,,2023-09-06T21:40:41.835Z,"['rliu34@media.mit.edu', 'pattie@media.mit.edu']",True,conversational-agents-for-enhancing-social-connections,2022-09-01,"['human-computer-interaction', 'artificial-intelligence']",Conversational Agents for Supporting Social Connectivity,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-10-06T18:52:33.950Z,"Al·che·my is the process of transformation, creation, and combination. It is in this conceptual space that we frame the convergence of human and computational creativity within generative deep learning technologies, specifically text-to-image AI architectures. Mystical traditions like alchemy, mediumship, and astrology provide stylized frameworks for interpreting black-boxed complex systems. By turning to these methodologies for generative AI, AI Alchemy reifies the magic and unknowability of these machines, yet still centers agency on human creators to manifest artifacts in line with their vibes. This series of talks, interactive events and research aims to encapsulate these ideas through practice, but also through learning from others via talks and healthy debate. ",2023-12-14,"['human-dynamics', 'center-for-constructive-communication', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/10/06/splash.jpg,390930,,2023-09-06T21:27:14.101Z,"['zive@media.mit.edu', 'hopes@media.mit.edu']",True,ai-alchemy,2022-10-06,"['art', 'artificial-intelligence', 'environment', 'ethics', 'creativity', 'law', 'neural-interfacing-and-control']",AI Alchemy Lab,PUBLIC,http://aialchemy.media.mit.edu/,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2022-10-06T19:34:54.091Z,"In collaboration with the Lurie Center for Autism, Massachusetts General Hospital, and MIT Lincoln Lab, the aim is to study, characterize and help communication for minimally verbal individuals diagnosed with Autism Spectrum Disorder (ASD).  Communication impairments have a big impact on the quality of life and impose increased health risks and a significant burden on patients, caregivers, families, and healthcare institutions. Physiological signals give rich information about bodily functions which can be used to provide assistance to people when they need them. People with ASD are known to have a rich inner world of language but the intelligibility and vocalization of speech are affected. We are working on using electromyography (EMG) signals, picked up using AlterEgo,  to decode what the individual is trying to say through machine learning.  Additionally, we are interested in predicting engagement and disengagement of a person when performing different speech tasks using AttentivU, an electroencephalography (EEG) and electrooculography (EOG)-enabled sensing device.The objective of the project is to eventually improve speech intelligibility using auditory, haptic, visual, and attention feedback mechanisms.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/03/20/assistive_communication.png,390939,,2023-09-07T22:25:04.499Z,"['wazeer@media.mit.edu', 'nkosmyna@media.mit.edu']",True,assisting-communication,2019-08-01,[],Assisting Communication for Minimally Verbal Individuals,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-10-06T22:58:36.766Z,"Engaging politically diverse audiences on social mediaNews Bridge studied how the language that a media outlet – PBS’ Frontline – uses to promote its content influences the political diversity of its audience. In this project we tracked user engagement with tweets posted by Frontline over three years and built models that, given the tweet text, predict the political diversity of the audience. We then integrated the models into a web app that helped Frontline craft tweets engaging to a politically diverse audience, guided by the model predictions. While studies of political polarization on social media typically investigate the behaviors of individual users, New Bridge focused on the media outlets’ impact on audience fragmentation and developed tools that can help them reduce it. We believe this approach can be further developed and generalized into tools to help communicators (e.g., in public health) engage audiences across political, social, and/or cultural boundaries.Motivating ContextThe U.S. news media is more politically fragmented than ever, with Americans of different political identities inhabiting divergent media worlds, and with ever more separation between the sources of information that they engage with and trust. Media outlets have an opportunity to counteract this polarization by actively promoting their content in a way that brings in a more politically diverse audience. In this context, News Bridge was motivated by two research questions:1. Can we build models that predict the political diversity of an audience that will engage with a tweet given the tweet text?2. Can we use such predictive models to help media outlets select tweets that are more likely to engage a more politically diverse audience?ApproachWith News Bridge, we explored how political polarization is reflected in the social media posts used by media outlets to promote their content online. In particular, we tracked the Twitter posts of several media outlets over the course of more than three years (566K tweets), and the engagement with these tweets from other users (104M retweets), modeling the relationship between the tweet text and the political diversity of the audience. We then built a tool that integrated our model and could help a media outlet or journalist craft tweets that are engaging to a politically diverse audience, guided by the model predictions. To test the real-world impact of the tool, we partnered with the PBS documentary series Frontline and ran a series of advertising experiments on Twitter. We found that in seven out of the ten experiments, the tweets selected by our model were indeed engaging to a more politically diverse audience, illustrating the effectiveness of our approach.",2020-09-15,"['social-machines', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/10/06/Screen Shot 2022-10-06 at 7.00.20 PM.png,390948,,2023-09-07T23:19:29.072Z,['msaveski@media.mit.edu'],True,news-bridge,2018-01-20,[],News Bridge,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-10-07T14:36:27.880Z,"Optimizing environmental, social, and economic performance of cities.Cities play a central role in the climate crisis. According to the latest Intergovernmental Panel on Climate Change (IPCC) report, cities contribute 67-72% of global carbon emissions. The IPCC also highlights that ‘How new cities and towns are designed, constructed, managed, and powered will lock-in behavior, lifestyles, and future GHG emissions’. Given that cities represent such a large portion of carbon emissions and are continuing to expand, there is an opportunity to address climate change by making interventions that foster a more sustainable urban life.",,"['city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/10/07/ML gif_bubble_decrease 2.gif,390962,,2023-09-06T22:05:17.373Z,"['jblinder@media.mit.edu', 'gba@media.mit.edu', 'jiajie@media.mit.edu', 'jmugicag@media.mit.edu', 'aberke@media.mit.edu', 'naroa@media.mit.edu', 'ainhoa3@media.mit.edu', 'maitanei@media.mit.edu', 'leticiai@media.mit.edu', 'aricom@media.mit.edu', 'markuse@media.mit.edu', 'diegoan@media.mit.edu', 'doorleyr@media.mit.edu', 'alonsolp@media.mit.edu', 'pablobb@media.mit.edu']",True,hyper-local-solutions-to-global-challenges,2022-04-01,[],Hyper-LOCAL Solutions to GLOBAL Challenges,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-10-07T18:41:57.321Z,"Many community organizations want to engage in conversations with their constituents but lack the support they need to analyze feedback. Most of the time, these organizations either outsource the analysis, which is expensive and distances decision-making from community members, complete a superficial reading of the data, or let the data collect dust and never uplift the stories that people shared. We first became aware of this problem while collaborating with Charlotte-Mecklenburg Schools (CMS), a school district in North Carolina, to gather people’s stories and feedback around two new magnet schools. One of the major challenges we faced was when we tried to analyze all of the data. Our partners in CMS were passionate about creating magnet programs grounded in community voices but had no experience in qualitative data analysis (QDA), or sensemaking. With help from expert sensemakers, we were able to examine the data, which involved creating a codebook, or a list of themes, from a subset of our data and then applying the codebook to the rest of our data through a process called qualitative coding. Not all organizations can work closely with expert sensemakers for every community engagement initiative they launch. As a result, we aim to create accessible entry points into sensemaking for people with no prior experience. We plan to apply machine learning and human-centered design methods to create, and then evaluate, a platform that helps non-expert sensemakers conduct qualitative coding, or apply a codebook to their entire dataset. We focus on this phase of sensemaking because it is one of the most time-consuming and tedious parts of the analysis process. Our goal is to reduce the amount of time it takes to code a sample of community conversations while improving the overall coding reliability. ",2023-05-19,"['center-for-constructive-communication', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/10/24/SenseMate Pitch 2.png,390978,,2023-09-25T21:03:52.952Z,"['coverney@media.mit.edu', 'belen@media.mit.edu', 'dimitrad@media.mit.edu', 'dkroy@media.mit.edu']",True,sensemate,2022-09-01,"['design', 'human-computer-interaction', 'artificial-intelligence']",SenseMate: an AI-based platform to support qualitative coding,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-10-08T20:48:41.611Z,"The prosthetic interface consists of a compliant liner and rigid socket, and conventionally has been made through a very hands-on artisinal process. This project has developed a prosthetic interface design and manufacturing pipeline that uses a novel computational algorithm to create subject-specific transtibial liner and socket components that can be additively manufactured at low cost. The novel interfaces are compared to conventional counterparts through a clinical trial that involves kinematic gait, pressure, and thermal metrics as well as a qualitative feedback questionnaire.The design process consists of the following. The residual limb is imaged using a magnetic resonance imaging (MRI) device, and the image set is segmented into a three-dimensional model. This approach is superior to other 3D-modeling prosthetic interface techniques as it is able to capture bone geometries and soft tissue depths of the residuum. A more accurate topology of the skin is captured using digital image correlation (DIC), and this mesh is used in replacement of the MRI skin. The socket is divided into four distinct pressure regions, and the nominal pressure applied at each region can be adjusted to be patient-specific. Finite element analysis is run to simulate liner donning and bodyweight loading upon the interface to generate the final pressure map and liner-socket geometries. Manual modifications to the mesh can be made based on subject feedback. The final model is then sent for fabrication via 3D printing.Clinical evaluation involves several steps. The subject conducts a 5 minute walking trial on one randomly selected prosthetic interface (conventional or novel), during which three data captures are taken. Gait parameters are extracted from these collections. Immediately after walking, the subject doffs the interface and thermal images are taken. The subject is then allowed to rest for 10-15 min and the process is repeated on the other interface. After these tests, the pressure test is conducted by taping sensors to the residuum and having the subject stand on their affected leg. The final step is the completion of the questionnaire, which asks the subject to compare the comfort of the novel interface to their conventional.The findings and results of this project have many beneficial applications in the prosthetics industry. The pipeline reduces the amount of required in-person time from the patient, as design can be done remotely once the image set is obtained. This will help those who do not have the time or means to travel to a prosthetic clinic often. The design algorithm also retains a memory of subject-specific liner and socket preferences, so that future sockets built on the algorithm are more likely to be comfortable on the first try. This will reduce repetition in the interface design process, shortening the lead times for comfortable sockets and allowing more patients to be seen. 3D printing from a digital model shortens the time and reduces the cost for check sockets, and by printing multiple check socket variations a patient will have the opportunity to directly compare different socket designs. We hope that the host of benefits from this design method will enable better prosthetic comfort and care for all people with amputation, and will have a profound effect on those in developing countries.",,"['biomechatronics', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/08/liner+socket_pressures_bgD3uDq.png,390997,,2023-09-06T03:13:52.212Z,"['danask@media.mit.edu', 'hherr@media.mit.edu', 'xingbang@media.mit.edu', 'drclee@media.mit.edu', 'friccioa@media.mit.edu']",True,digital-design-and-manufacture-of-a-transtibial-prosthetic-interface,2022-10-08,"['design', 'biomechanics', 'mechanical-engineering', 'prosthetics', 'bionics']",Design and Clinical Evaluation of a Digital Transtibial Prosthetic Interface,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-10-09T20:19:17.916Z,"Sleep++ aims to improve sleep quality by stimulating the brain to enter deep sleep. Currently, insufficient and poor quality sleep represents a major health burden. Poor sleep is associated with a wide range of diseases,  as well as increased risk of accidents like car crashes. Current approaches to improve sleep in otherwise healthy adults include medication and lifestyle change--but these methods have significant drawbacks that limit their usefulness.We are testing whether a new method called slow wave entrainment can can improve sleep quality. In this method, sensory stimuli like sounds are played about once per second during deep sleep. These stimuli prompt the brain to produce stronger slow waves, the brainwaves associated with deep and restorative sleep.Anyone with a smartphone can try Sleep++! If you are interested in  participating in our experiment testing whether Sleep++ improves sleep  quality, you can sign up  at bit.ly/sleepplusplusPrevious experiments in sleep laboratories have demonstrated that slow wave entertainment can improve biological measurements of sleep quality.  Based on these results, we are developing software for performing slow-wave entrainment using mobile and wearable devices. We then plan to test whether slow wave entrainment with these devices can improve sleep quality and mitigate the problems caused by poor sleep.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/09/swe.jpg,391006,,2023-09-05T21:24:17.585Z,"['pattie@media.mit.edu', 'nathanww@media.mit.edu']",True,sleep-entrainment,2022-10-09,[],Sleep++,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-10-11T01:41:02.802Z,"CosMorph is a new classification framework of modern form design from the perspective of design morphology. This paper proposes combining three form-design methods with Eastern philosophy.There are three basic questions in Eastern philosophical thinking: what is it, where it comes from, and where will it go, which lead to the three elements of form in this framework, respectively: classification method, starting point, and iteration driving force. Based on our classification framework we can reexamine, reclassify, and rediscuss hundreds of modern design schools of thought. In order to help artists, designers, and engineers establish a design thinking of form, we also propose three commonly used critical form design methods: Naturalistic Observation, Mathematical Analysis, and Deductive Reasoning.The advent of the pandemic has transformed the way people interact with the world, affected the tendency of universal values, and changed the mainstream direction of the development of the Sci-tech tree. At the same time, the continuation of the epidemic has created a new normal of inclusiveness and tenacity. The CosMorph framework enables us to predict the future of the development direction and dynamic trends of form design, which provides a new thinking way and theoretical method for form design in the post-pandemic era and even in the long-term future.",2024-01-31,"['tangible-media', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/10/17/CosMorph_En.jpg,391019,,2023-09-07T21:45:19.746Z,['yang0129@media.mit.edu'],True,cosmorph,2021-09-01,[],CosMorph: A Modern Form Design Methodology Inspired By Eastern Philosophy,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1
False,2022-10-11T10:04:55.229Z,"The Community Dialogue Data Trust (CODIDT) is an interactive digital platform designed to empower participants in facilitated dialogues with tools for safeguarding data privacy and digital identity. The platform's tools are oriented towards establishing and upholding a foundation of data trust. By utilizing CODIDT, participants can actively secure their conversational data (voices), individual and communal identity information, and community values data. Underlying this platform is a data trust model that empowers individuals to exercise authority and oversight over their own conversational data. This empowerment is facilitated through direct accessibility and control mechanisms, augmented by a feature that fosters transparency, enabling participants to offer constructive input on the interpretation of their conversational data. In essence, CODIDT establishes personal data ownership, fosters transparency, ensures accountability, and reinforces the dependability of dialogue conversation data. These principles are embedded throughout the entire lifecycle of dialogue interactions, encompassing recording, interpretation, and dissemination of conversation highlights for broader community engagement.",,"['center-for-constructive-communication', 'media-lab-research-theme-decentralized-society']",,391027,,2023-09-05T21:23:41.379Z,['vibbi@media.mit.edu'],True,community-dialogue-data-trust-codidt,2022-10-11,"['design', 'civic-technology', 'data']",Community Dialogue Data Trust (CODIDT),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-10-12T02:16:58.556Z,"This work will be presented  at the upcoming ACM CHI 2023 conference. By Joanne Leong, Florian Perteneder, Muhender Raj Rajvee, and Pattie Maes. The fear of public speaking (FOPS) is considered to be the most common lifetime social fear. To tackle anxiety, one common piece of advice is to picture the audience in a new light, using your mind’s eye.  With Augmented Reality (AR) face filters, it becomes possible to literally change how one sees oneself or others.  In this project, we explore the potential for privately applied AR filters to reduce public speaking anxiety in online video-calling. Private in this case, means that the AR effects are only visible to the speaker applying them.  To investigate this space, we conducted an online survey with 100 respondents to gather a diverse set of initial impressions and possible boundaries surrounding this controversial concept. Following this, we built a prototype of a private AR web-based video-calling application, and pilot-tested it with 16 participants to gain more in-depth insights. Our main findings comprise key user perspectives and opportunities for the private application of augmented reality filters in the context of public speaking scenarios:An overview of the general public's perception of the acceptability of different AR filter types, as applied to one's self and others. Early evidence that AR face filter can be helpful for combating public-speaking anxiety (based on a user study with 16 people with FOPS, who tested our custom application in a simulated online speaking scenario An outline of different strategies that we observed people taking with AR face filters to manipulate their perception of the audience, themselves or the situation to modify their public-speaking anxiety",2023-05-31,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/10/17/_chi2023-picturetheaudience3.gif,391046,,2023-09-06T22:00:12.025Z,"['joaleong@media.mit.edu', 'pattie@media.mit.edu']",True,picture-the-audience,2022-06-01,"['human-computer-interaction', 'augmented-reality', 'perception']","""Picture the Audience"" Exploring Private AR Face Filters for Online Public Speaking",PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-10-12T18:57:23.221Z,"We've designed a health system strengthening model  to support and expand the prosthetic and orthotic sector that cares for people with disabilities.Partnering with the Ministry of Health, local clinics and disabled communities, our goal is to improve capacity, multiply production of prosthetic and orthotic devices, and deliver care to patients in hard to reach areas across the Republic of Sierra Leone in our first four years.Due to a civil war ending in 2001, many Sierra Leoneans were injured or became disabled from violence, displacement and inability to access safe and healthy living environments. Immediately after the conflict, Sierra Leone received an influx of international support to rehabilitate and care for people needing prosthetic and orthotic devices. However, as time progressed, many short-term aid and relief efforts shifted their focus to emergencies elsewhere, and the dearth of orthotic and prosthetic service capabilities grew.In the present day, the orthotic and prosthetic sector in Sierra Leone is equipped with just one fully-trained clinician, an unpredictable flow of supplies,  and with clinics lacking adequate power and tools. Sierra Leone is home to over fifty-thousand persons with disabilities who need medical orthotic braces and prosthetic limbs, but care is severely limited.Enabled by the K. Lisa Yang Center for Bionics, our team  has taken on the challenge of using problem-solving, design thinking, global health knowledge and engineering to work together with partners from the Ministry of Health and from communities of disabled Sierra Leoneans to strengthen this sector. Our goal is to catalyze change in the prosthetic and orthotic sector to permanently improve prosthetic and orthotic care for persons with disabilities across Sierra Leone.Our efforts span six major categories: data collection and quality improvement, inclusive education and training, supply chain and logistics, investment in infrastructure, translation of new technologies, and mobile delivery of services.Core TeamDr. Hugh Herr, Principal Investigator of the Biomechatronics Group; Francesca Riccio-Ackerman, Graduate Student Lead, Researcher at Biomechatronics Group and Global Health System Designer; Dara Dotz, Humanitarian Designer and Low-Resource Fabrication Expert; Ellen Clarrissimeaux, Biomechatronics Group Engineer and Project Manager; Leila Abdelrahman; Software Engineer; Adikalie Kamara, Researcher and Community LiasonCollaborators in Sierra LeoneDr. Austin Demby, Minister of Health and Sanitation; Dr. David Sengeh, Chief Innovation Officer and Minister of Primary Education (Biomechatronics and MIT Graduate); Dr. Santigie Sesay, Director of Non-Communicable Diseases; Dr. Ismaila Kebbie, Director of Rehabilitation & Physiotherapy;  Abdulrahman Dumbaya, Head Prosthetist at the National Rehabilitation CenterCollaborators at MIT & HarvardDr. Amos Winter, Principal Investigator of the MIT GEAR Lab, and Low-Cost Prosthetic Design Expert; Dr. Nancy Oriol, Associate Dean of Harvard Medical School and Advisor on Mobile Health Delivery; Guillermo Herrera-Arcos, Researcher at Biomechatronics Group and Expert on Orthotic Design and Fabrication; Christina Meyer, Researcher at Biomechatronics Group and Designer of Assistive Technologies; Urvaksh Danesh Irani, Researcher at MIT GEAR Lab and Low-Cost Prosthesis Designer;  Kaili Glasser, Biomechatronics Group Undergraduate ResearcherCollaborating OrganizationsPartners in Health, Global Health Organization; the Human Study School of Rehabilitation Sciences, Education Organization for Prosthetist-Orthotists",,"['biomechatronics', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/uuid/21fa6f82-4674-44e8-b61d-7bb8ac395c78,391055,,2023-09-07T21:50:06.141Z,"['friccioa@media.mit.edu', 'cimeyer@media.mit.edu', 'eclx@media.mit.edu', 'hherr@media.mit.edu', 'ddotz@media.mit.edu', 'gherrera@media.mit.edu']",True,sierra-leone-prosthetics-orthotics,2021-09-01,"['health', 'biomechanics', 'healthcare', 'prosthetics', 'bionics', 'prosthetic-design', 'orthotic-design']",The Sierra Leone Orthotics & Prosthetics Program,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-10-14T05:12:54.312Z,"Computer Vision applications are making a remarkable impact on society and advancing progress in Machine Learning. Our project aims to design Computer Vision algorithms that can learn and infer from private data. This will significantly empower industries where data can be sensitive - digital health, smart cameras, etc.This year at ECCV'22, we present two research works in this direction -1. Sanitizer: Protecting sensitive information in task-agnostic data releaseIn
 Sanitizer, we develop techniques that enable sharing of sensitive 
images for health imaging, forensics, etc. without compromising 
individual privacy",,"['camera-culture', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/10/14/cameras.png,391126,,2023-09-05T21:12:08.678Z,"['abhi24@media.mit.edu', 'ayushc@media.mit.edu', 'vepakom@media.mit.edu', 'vvsharma@media.mit.edu', 'raskar@media.mit.edu']",True,private-imaging,2022-10-14,[],Computational Privacy for Computer Vision,PUBLIC,https://tremblerz.github.io,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-10-14T13:48:46.604Z,"SleepStim is a framework for automatically recording sleep physiology  and presenting sensory stimuli in sleep, using consumer smartphones and smartwatches.Sounds played during specific phases of sleep can be used to augment cognition. For example,  in Targeted Memory Reactivation (TMR), sounds are used to induce replay of specific memories in sleep, leading to stronger memory and less forgetting. A related technique uses sounds to enhance sleep-related oscillations in the brain, which may benefit sleep quality. However these techniques usually require a staffed sleep lab, which limits their utility.SleepStim is designed to automate sound stimulation during sleep, so that people can use it in their own home. We accomplish this using a smartwatch to monitor the participant while sleeping, and a novel algorithm which uses the smartwatch data to calculate the participant's sleep stage in real time. Sounds can then be played when the participant enters deep (N3) or REM sleep.SleepStim also enables researchers to collect raw sleep data while participants use the device at home. Currently, SleepStim is being used for a number of research projects involving stroke treatment, treatment of memory problems in older adults, and inducing lucid dreams.SleepStim was originally developed at the Cognitive Neuroscience Lab  at Northwestern Unviersity and is now supported by Nathan Whitmore in Fluid Interfaces.For more details, please see the Github page: https://github.com/nathanww/home-tmrPublications:Whitmore, N. W., Harris, J. C., Kovach, T. & Paller, K. A. Improving Memory via Automated Targeted Memory Reactivation during Sleep. Journal of Sleep Research 2022.06.28.497977 (2022) doi:10.1101/2022.06.28.497977.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/15/sleepstim1.jpg,391134,,2023-09-05T21:10:47.836Z,"['nathanww@media.mit.edu', 'pattie@media.mit.edu']",True,sleepstim,2022-10-14,[],SleepStim,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-10-14T16:41:11.976Z,"Abstract Since Bitcoin and the Unspent Transaction Output (UTXO) model were introduced by Satoshi Nakamoto over a decade ago, there have been many important issues identified with the UTXO model; the most important being that it is hard to extend the model to accommodate more complex use cases, such as those related to decentralized finance. Currently, Ethereum has many decentralized exchanges which allow users to seamlessly make trades. Performing a trade on chain on Bitcoin is quite difficult; currently, the most elegant way is to set up a Discreet Log Contract (DLC) between you and your counter-party. However, this currently have many downsides; for example they are not transferable (i.e. once Alice and Bob sign up for the DLC, they are stuck in the DLC until settlement or they both interactively agree to leave). We fix this by introducing the Transformable Discreet Log Contract (TDLC), which allows a third party, Carol, to swap in for either Alice or Bob midway through the contract with reduced interaction and the Truly Transformable Discreet Log Contract (TTDLC), which allows multiple parties to seamlessly trade the contract around between them. With both the TDLC and the TTDLC, the party swapping into the contract only has to interact with the single party swapping out. The end goal for the work presented in this thesis is to help improve the usability of Bitcoin for advanced use cases such as those relevant to decentralized finance.",2022-12-31,"['digital-currency-initiative-dci', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/b49ca2fa-9092-435b-bf63-f8b29890fd8f,391150,,2023-09-05T21:09:56.045Z,['narula@media.mit.edu'],True,transformable-discreet-log-contracts,2022-10-14,[],Transformable Discreet Log Contracts (TDLC),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-10-14T18:51:20.250Z,"According to WHO, epilepsy is the fourth most common neurological disease globally. CDC statistics show that 1 in 26 individuals in the U.S. will be diagnosed with epilepsy over a lifetime. While there are 36 epilepsy drugs available on the market, 1 in 3 adult patients and 20 to 25% of child patients have drug-resistant epilepsy When having a seizure, patients with epilepsy (PWE) may suffer from temporary loss of consciousness, sensation, and motor control. The notorious nature of generalized tonic-clonic seizure (GTCS) has been known for a long time, especially the correlation between high-frequency GTCS and sudden unexpected death in epilepsy (SUDEP). Based on patient surveys, the seemingly random timing of seizures is one of the worst aspects of epilepsy. Unexpected episodes are disruptive, significantly hindering their daily routines. Sometimes, there may be severe secondary damage, such as having an attack while driving or sporting. Historically, patients have often reported sleep deficiency as triggers for seizures. However, these claims lacked substantiated and objective evidence. We analyzed wearable data from over 2,000 patients with epilepsy and uncovered correlations between sleep parameters, including sleep deprivation, regularity, efficiency, and fragmentation, and the risk of next-day generalized tonic-clonic seizure (GTCS). Further, We investigated the circadian and multi-day rhythms in wrist electrodermal activity (EDA) using wavelet methods, identifying these rhythms as potential prognostic biomarkers for GTCS episodes. These findings can pave the way for better patient guidance and possible preventive and GTCS forecasting measures.",,"['affective-computing', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/09/08/22211_eda_cwt.png,391159,,2023-10-23T18:01:19.983Z,"['boyuz@media.mit.edu', 'neska@media.mit.edu']",True,sleep-and-epilepsy,2022-10-14,"['artificial-intelligence', 'data', 'health', 'neurobiology', 'machine-learning', 'physiology']",Sleep and Epilepsy: Demystifying the thousand-year-long conundrum,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-10-17T16:36:59.148Z,"Facemasks are used to reduce exposure to viruses and other environmental hazards such as air pollution, and integrating wearable electronics into facemasks could provide valuable insight into personal and public health. However, relevant wearable devices are typically designed only to monitor biological information such as breathing patterns and do not provide information about the status of the facemask. Here we report a conformable sensory interface that can be attached to the inside of any user-supplied facemask and used to monitor signals related to infectious diseases, environmental conditions, and the wear status of the facemask. Multimodal signals from the sensory facemask are wirelessly transmitted to a server through a custom-made mobile app. The system can monitor multiple signals simultaneously, including skin temperature, humidity, verbal activity, breathing pattern, and fit status of the facemask. We also develop a machine learning algorithm that can be used to reliably decode the facemask position. ",,"['conformable-decoders', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/20/7_mKjGCKB.gif,391214,,2023-09-06T21:17:54.429Z,"['jinhoonk@media.mit.edu', 'canand@media.mit.edu', 'c_marcus@media.mit.edu']",True,conformable-multimodal-sensory-facemask-cmask-for-decoding-biological-and-environmental-signals,2022-10-17,"['design', 'bioengineering', 'consumer-electronics', 'health', 'sensors', 'machine-learning', 'biology', 'public-health', 'electrical-engineering', 'wellbeing', 'medicine', 'women', 'covid19', 'microfabrication']",conformable Multimodal Sensory Facemask (cMaSK),PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2022-10-17T20:52:56.722Z,"Smart watches contain a wealth of sensors with the potential to transform health research and intervention, but few teams have the the technical skills to use these devices.  Biodata is a smartwatch app which can simplify acquiring health data, based on our lessons learned building the SleepStim system. Biodata transforms a Wear OS smartwatch into a reliable, configurable data acquisition device for research. Because smartwatches are easily worn by participants, it can be used for a wide range of research tasks including* Characterizing sleep and activity patterns* Detecting abnormalities like tremor or irregular heart rate* Tracking emotional and physiological states* Measuring changes in behavior when participants receive an intervention* Real-time biofeedbackBiodata is an early-stage project which we hope will open up new possibilities for health research. You can follow the project at https://github.com/nathanww/BioData.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/17/ecg2.jpg,391250,,2023-09-04T20:04:11.800Z,"['nathanww@media.mit.edu', 'pattie@media.mit.edu']",True,biodata,2022-10-17,[],Biodata,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-10-17T20:55:42.582Z,"Teddy is a cutting edge data collection platform available for both Android and iOS that lets you collect data from study participants in a fun and easy manner. The animated bear Teddy guides participants through the steps necessary to collect voice, video, and survey data, all while easily explaining what is being collected and why.Teddy is fully compatible with platforms like Qualtrix or Redcap, allowing you to deeplink from a Qualtrix form into Teddy, then back to another form seemlessly!All video data collected with Teddy is encrypted using algorithms the NSA uses for top secret data. You can read all about it in our Privacy Policy.Teddy is already being used with our partners at UW-Madison Center for Health Minds and has already collected hundreds of videos of participants. To read more about Teddy's capabilities, please see our documentation.",,"['affective-computing', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/10/17/projectpage.png,391258,,2023-09-04T20:03:33.905Z,"['fergusoc@media.mit.edu', 'picard@media.mit.edu']",True,teddy,2022-10-17,"['design', 'data', 'affective-computing']",Teddy,PUBLIC,https://teddy.media.mit.edu/,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2022-10-18T05:27:35.986Z,"In an effort to design alternatives to opinion-based discourse at scale online that exploded with polarization, toxicity, and misinformation in the last two US election cycles, the Local Voices Network hosts small group conversations in partnership with grassroots and public organizations, where the emphasis is on the respectful sharing of personal experiences rather than opinion. Facilitation by trusted community leaders and organizers, as well as active recruitment of voices that are traditionally underheard or actively excluded in civic processes yields intimate conversations that often dip into powerful personal narrative, shedding light on a constellation of public issues in the process. Quantitative and qualitative insights from these conversations have been used to inform questions in the mayoral candidate debate last year in Boston, as well as the hiring of a police chief in Madison, Wisconsin.To date, the LVN platform has largely used human sensemakers to qualitatively analyze the transcripts and summarize findings. We present methods for automatically analyzing the conversations at scale. Checking if facilitators followed the conversation guide, extracting interesting passages that address questions of interest, and coding speaker turns for topical themes are all processes that can take months for human sensemakers who work on these conversations. Similar challenges face organizations and researchers who make sense of focus group transcripts or semi-structured interviews en masse for research, so this problem has far wider reach than just this data set of LVN conversations. We use sentence embeddings to check for facilitator adherence to the conversation guide, then fine-tune a GPT-3 model to identify structural labels for speaker turns with remarkable accuracy. Bringing humans together with AI methods can save human experts immense time!",,"['center-for-constructive-communication', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",,391285,,2023-09-06T21:44:23.216Z,['hopes@media.mit.edu'],True,large-language-models-for-automated-sensemaking,2022-08-08,"['human-computer-interaction', 'artificial-intelligence', 'natural-language-processing']",Large Language Models for Automated Sensemaking,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-10-21T12:51:11.116Z,"This work is recently submitted as a Full Paper to ACM CHI 2023.Corsetto is a full stack system design and platform for upper-body haptics. The Corset-like garment and its control architecture are developed as a platform for respiratory regulation, in this case for mediating skill learning and transfer between a voice teacher and student. Using OmniFiber technology, we fabricated a robotic upper body garment that can capture and stimulate the movement of muscle groups employed in respiration. Our initial testing was in the context of vocal pedagogy. However, a similar approach can be used to help runners with respiration support and recovery before, during and after practice; for supporting emotional regulation with deep pressure feedback onto the upper body; and as a mechanical counterpressure (MCP) suit similar to that of the MIT BioSuit.",2025-08-01,"['tangible-media', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/10/22/Corsetto_garment design.png,391393,,2023-10-17T23:38:02.916Z,['ozgun@media.mit.edu'],True,corsetto,2022-08-01,[],Corsetto: AI-empowered Modular Robotic Garment for Capture  and Entrainment of Breathing Techniques,PUBLIC,http://www.ozgunkilic.com/,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-10-21T19:57:16.918Z,"Data is the main fuel of the modern world enabling artificial intelligence and driving technological growth. The demand for data has grown substantially, and data products have become valuable assets to purchase and sale since it is extremely valuable for sectors to acquire high quality data to discover knowledge. As a valuable resource, it is important to establish a principled method to quantify the worth of the data and its value for the data seekers. This is addressed via data valuation which is the essential component for realization of a fair data trading platform for owners and seekers.Problem StatementConsider the case when a Pharma Company would like to purchase data from a Hospital. The challenge is that how the Pharma Company can  value the worth of the data available at the Hospital without having access to it.  In other words, the challenge is valuing invisible decentralized data that is not available locally.   Furthermore, we consider data valuation without focusing on a specific task; that is, the Pharma Company would like to know the worth of the data available only at the Hospital without disclosing the task that they may want to purchase the data for. This is called an intrinsic data valuation, or a data-driven data valuation approach.",,"['camera-culture', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2022/10/21/Data_market_up7he72.jpg,391403,,2023-09-04T20:02:23.106Z,"['mamiri@media.mit.edu', 'fberdoz@media.mit.edu', 'raskar@media.mit.edu']",True,data-valuation,2022-10-21,[],Data Valuation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-10-24T21:18:46.089Z,"Urban mobility can often be categorized as a complex system—e.g., a  nonlinear system composed of many components that interact with each  other and have interdependent relationships. Current trends in urban  mobility systems point toward shared, more lightweight, and more  autonomous vehicles, with planning solutions that are less centralized  and can handle increasingly complex transportation networks. In this study, we study planning solutions for shared micro-mobility systems of  autonomous vehicles using the MIT Autonomous Bicycle, which has both  self-driving and manual driving modes. Vehicle rebalancing in shared  micro-mobility systems is a key technical challenge and has a  substantial environmental and economic impact. This research proposes a  fully decentralized approach for autonomous bicycles to self-organize  their own rebalancing based on stigmergy, a bio-inspired mechanism for  indirect communication. While the bicycles autonomously navigate their  urban environment, they locally update RFID tags at intersections,  leaving virtual pheromone trails to collectively guide each other toward  high-demand areas. After designing and implementing a realistic  agent-based model of a high-demand urban area (Cambridge, USA), this  study demonstrates that autonomous bicycles could tackle the vehicle  rebalancing problem in a self-organized manner, using strictly  decentralized local communication. The proposed method is shown to  significantly reduce the average user wait time compared to no  rebalancing and random rebalancing.",,"['city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/10/24/MLWebsiteGIF.gif,391436,,2023-09-07T22:12:56.933Z,"['naroa@media.mit.edu', 'jmugicag@media.mit.edu', 'agrignar@media.mit.edu', 'ecstll@media.mit.edu', 'alonsolp@media.mit.edu', 'kll@media.mit.edu']",True,urban-mobility-swarms,2021-06-01,[],Urban Mobility Swarms: towards a decentralized autonomous bicycle-sharing system,PUBLIC,https://www.linkedin.com/in/naroacoretti/,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0
False,2022-10-24T23:59:06.987Z,"Could we substitute cars by shared autonomous lightweight vehicles for food delivery? We have developed an agent-based simulation model that analyzes the performance of  shared autonomous micro-mobility compared to the traditional car-based deliveries.The population growth and the technology development are leading to new realities. One of them is the increase of online food deliveries, which are usually done by traditional combustion cars in the US. In the City Science research group we are exploring new vehicles to accomplish this task.",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/10/25/Prueba_AdobeExpress.gif,391444,,2023-10-02T13:44:49.721Z,"['naroa@media.mit.edu', 'kll@media.mit.edu', 'alonsolp@media.mit.edu', 'ainhoa3@media.mit.edu']",True,autonomous-food-delivery,2022-04-01,"['data', 'environment', 'transportation', 'urban-planning', 'engineering', 'autonomous-vehicles']",Autonomous micro-mobility for food deliveries,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-10-25T03:08:12.362Z,"Critical thinking is an essential human skill. Despite the importance of critical thinking, research reveals that our reasoning ability suffers from personal biases and cognitive resource limitations, leading to potentially dangerous outcomes. This project presents the novel idea of AI-supported self-explanations that reframe information as questions to actively engage people's thinking and scaffold their reasoning process. We conducted a study with 210 participants comparing the effects of AI-supported self-explanations on users' ability to discern logically validity of statements in comparison with an AI that provides recommendations with typical causal AI explanations as well as a no-feedback condition. Our results show that AI-supported self-explanations significantly increase human discernment accuracy of logically flawed statements over these other conditions as well as users' desire to verify information with additional sources. Our experiment exemplifies a future style of Human-AI co-reasoning system, where the AI becomes a critical thinking stimulator rather than an information teller.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/04/02/askme_whisper_03_PsBmWWN.png,391523,,2023-09-04T20:01:54.032Z,"['vdanry@media.mit.edu', 'patpat@media.mit.edu', 'pattie@media.mit.edu']",True,ask-me-don-t-tell-me,2022-10-24,"['artificial-intelligence', 'technology', 'cognitive-science']","Don't Just Tell Me, Ask Me",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-11-02T14:42:49.796Z,"In January 2023, a team led by MIT PhD Candidate and MIT CCC Research Assistant Belén Saldías hosted several workshops on Human-Centered Machine Learning, Natural Language Processing, Data Visualization, and Causal Inference for the MIT–Chile Research Workshops. This initiative exposed students and general audience participants to Artificial Intelligence, Visualization, and Causality research through lectures, hands-on workshops, and scholar discussions and opportunities in Chile.Read the full workshop recap here: https://sites.google.com/media.mit.edu/mit-chile-hcai-viz/home!Or go to our gallery, here: https://www.media.mit.edu/galleries/mit-chile !Our collaborators' support allowed us to host more than 100 students, out of whom 44% self-identify as women or non-binary, and 30% are not from Santiago (traveling internationally from 6 different countries and from northern and southern regions of Chile–for whom we also provided travel funding). It was four days in Concepción and four days in Santiago.Santiagohttp://chileconf.mit.edu/santiago MIT–Chile Human-Centered AI & Visualization Research Workshop is a collaborative initiative by MIT, CENIA, and iHealth to bring together research-focused lectures and talks for the scientific community in Chile. We present the convergence of two Human-Centered spheres, Machine Learning (ML) and Visualization (Viz).During this week-long workshop, the public audience will have the opportunity to engage in research talks by experts in Human-Centered ML and Data Viz. While these talks are tailored to be specialized in one of the areas, we are excited about our lecture's curriculum, which offers a unique hands-on deep dive into the processes and methods in the intersection of Human-Centered AI and Data Viz.Concepciónhttp://chileconf.mit.edu/concepcionCollaborative initiative between Massachusetts Institute of Technology (MIT) and Universidad de Concepción (UdeC) that brings together research-focused lectures and talks for the scientific community in Chile. During this 4-day workshop, the public audience will have the opportunity to engage in research talks and discussion panels with researchers from MIT and UdeC.",,"['center-for-constructive-communication', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/uuid/fe351206-021a-4de6-810e-f97f8252d1a6,391617,,2023-09-04T19:30:02.385Z,"['belen@media.mit.edu', 'mhughes4@media.mit.edu', 'velouria@media.mit.edu', 'ngillani@media.mit.edu', 'avujic@media.mit.edu']",True,mit-chile-research-workshops-human-centered-artificial-intelligence-and-data-visualization,2022-11-02,"['artificial-intelligence', 'data', 'extended-intelligence', 'data-visualization']",MIT–Chile Research Workshops | Human-Centered Artificial Intelligence and Data Visualization,PUBLIC,https://chileconf.mit.edu,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-11-13T23:33:07.587Z,"An electronic device that could sense body temperature and humidity in real time and display it using a LCD display. It has a sensor and a LCD display with a piezo-buzzer and an array of light emitting diodes as a medium of warning/alarming the user if the temperature/humidity surpasses any given threshold set by the user. In summary, it is a device to monitor part of the human thermoregulatory system by monitoring temperature of any target region of the body along with the associated humidity conditions.The fundamental purpose of the project is to build an electronic device using off-the-shelf electronics. Such an important physiological signal could help users, like soldiers or firefighters, monitor their body temperature during their activity or field operations. If added with a bluetooth or wireless device, the user-body conditions could be monitored from a remote controlling center. In case of any field-emergency, they can deploy additional human resources to save the user of my device. Such a device could come in handy for montoring thermal-work strain monitoring. Also, such a system can be used for immobile patients (e.g., patients suffering from paraplegia or pressure ulcer) as the alarm/alert-system can call for help from people nearby.",,['conformable-decoders'],https://dam-prod2.media.mit.edu/x/2022/11/13/IMG_6603(brighter)_SVVqynC.jpg,391698,,2022-11-15T21:23:11.509Z,['ikra@media.mit.edu'],True,wearable-device-for-paraplegic-immobile-patients,,[],Ikra's Wearable Device for Paraplegic & Immobile Patients,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2022-11-15T06:02:46.287Z,"While some Human-Computer Integration (HInt) systems have successfully demonstrated that humans and technology can be physically and functionally integrated, we find that these integrations are not necessarily part of the users identity (i.e. self-judgment) or felt as part the user (i.e. experienced with a sense of self) and that they can even create feelings of self-dissociation. Literature on how to elicit these self-experiences is often inconsistent and vague, which complicates the metric for success and hinders the advancement of research. To help designers elicit and systematically evaluate in particular a sense of self, we draw metrics and theory from phenomenology and cognitive science. ",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/11/15/senseofself_seW54lA.jpg,391715,,2023-09-04T19:28:52.109Z,"['patpat@media.mit.edu', 'sangwon@media.mit.edu', 'vdanry@media.mit.edu']",True,eliciting-a-sense-of-self-when-integrating-with-computers,2022-11-15,[],Eliciting a Sense of Self when Integrating with Computers,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1
False,2022-11-15T06:35:26.599Z,"With the expansion of intelligent and ubiquitous computers everywhere, we have in many ways become cognitive cyborgs: most of us are inseparable from our personal devices and increasingly rely on them in our thinking, memorization, learning, communication, etc. However, while these devices significantly improve the functional capabilities and performance of our brain and body, they are still not experienced similarly to the phenomenology of processes in our brain and body, that is, by being felt a certain way. Decision making, for instance, is not just about arriving at a conclusion through systematic thinking; it involves among other things “intuitions”, “gut-feelings”, and different degrees of conscious awareness and control. Thus, it is merely not enough to augment the functionality of human cognitive and perceptual systems: we also need to consider the natural phenomenology of the human body, thoughts and world. As we integrate with technology, we would want to feel that we are the ones thinking, feeling, expressing, and experiencing reality, NOT the technology. With the newly-found focus on consciousness and phenomenology within experimental science, there are now ample opportunities in applying such research with technology to elicit not just physical integration but ""Experiential Integration."" Our aim is to shed light on the importance of experiential integration and provide researchers with a scientifically driven foundation for designing technologies that integrate with peoples natural experience of themselves and the world",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2022/11/15/experiential-integration_HaFWkHt.jpg,391723,,2023-09-04T19:28:23.791Z,['vdanry@media.mit.edu'],True,experiential-integration-of-humans-and-computers,2022-11-15,[],Theme | Experiential Integration of Humans and Computers,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2022-12-01T23:26:04.708Z,"We present a novel video compression pipeline, called Txt2Vid, which dramatically reduces data transmission rates by compressing webcam videos (“talking-head videos”) to a text transcript.Video represents the majority of internet traffic today, driving a continual race between the generation of higher quality content, transmission of larger file sizes, and the development of network infrastructure. In addition, the recent COVID-19 pandemic fueled a surge in the use of video conferencing tools. Since videos take up considerable bandwidth (~100 Kbps to a few Mbps), improved video compression can have a substantial impact on network performance for live and pre-recorded content, providing broader access to multimedia content worldwide. We present a novel video compression pipeline, called Txt2Vid, which dramatically reduces data transmission rates by compressing webcam videos (“talking-head videos”) to a text transcript. The text is transmitted and decoded into a realistic reconstruction of the original video using recent advances in deep learning based voice cloning and lip syncing models.Our generative pipeline achieves two to three orders of magnitude reduction in the bitrate as compared to the standard audio-video codecs (encoders-decoders), while maintaining equivalent Quality-of-Experience based on a subjective evaluation by users ( n = 242) in an online study. The Txt2Vid framework opens up the potential for creating novel applications such as enabling audio-video communication during poor internet connectivity, or in remote terrains with limited bandwidth.The code for this work is available at https://github.com/tpulkit/txt2vid.git.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2022/12/01/Screen Shot 2022-12-01 at 6.25.17 PM.png,391915,,2023-09-06T21:17:16.639Z,"['sra@media.mit.edu', 'patpat@media.mit.edu', 'pattie@media.mit.edu']",True,txt2vid,2022-12-01,[],Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2022-12-05T18:09:40.607Z,"A Distance Learning Model for Global Laboratory-Based Synthetic Biology Education Synthetic Biology ( SynBio) tools hold the promise to solve global challenges such as sustainability, climate change, and infectious diseases, yet SynBio education is largely still confined to students pursuing advanced STEM degrees in highly resourced institutions. With HTG(A)A we lay out a vision for the democratization of SynBio education through a scalable distributed network model of distance learning that will be accessible globally and across disciplines and backgrounds.",,"['molecular-machines', 'community-bio', 'city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/a2550fd0-923c-4e08-8899-f6c6503062fc,391944,,2023-09-07T22:33:54.540Z,"['patpat@media.mit.edu', 'eyalp@media.mit.edu', 'aricom@media.mit.edu', 'dkong@media.mit.edu', 'jacobson@media.mit.edu', 'suryaj@media.mit.edu']",True,how-to-grow-almost-anything-1,2021-02-01,"['robotics', 'bioengineering', 'learning-teaching', 'synthetic-biology', 'creativity', 'community', 'biology', 'biomechanics', 'alumni', 'biotechnology', 'fabrication', 'medicine', 'covid19']",How to Grow (Almost) Anything - A New Model for Global Synthetic Biology Education,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-12-09T12:53:27.147Z,"CoCo is a new live co-creative learning platform designed to empower educators to support young people in both physical and remote settings in Being. Creative. Together.The platform supports real-time co-creation and collaboration with peers for block-based coding, text-based coding, digital art, and creative writing. The live computational environments in CoCo currently extend and build on top of the highly expressive Scratch 3.0 and p5.js programming environments. Additionally, CoCo also enables educators to create private digital corridors for their communities or classrooms so that learners can freely engage and collaborate with their peers online in a safe context.Join the invite list for early beta at coco.build. Watch the video below to see CoCo in action. Learn more about the underlying ideas and values of this work in the introductory blog post linked below. The project is co-created by PhD students Manuj Dhariwal and Shruti Dhariwal.",,"['lifelong-kindergarten', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/12/11/cocobuild cover.gif,391972,,2023-08-16T21:13:34.455Z,"['manuj@media.mit.edu', 'shrutid@media.mit.edu']",True,cocobuild,2019-08-15,"['kids', 'learning-teaching', 'social-science', 'behavioral-science', 'computer-science', 'creativity', 'technology', 'cognitive-science', 'code']",👋 CoCo — A New Real-Time Co-Creative Platform for Young People,PUBLIC,https://coco.build/,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2022-12-13T16:35:17.945Z,"Humans, Robots, Space, and the Future of ConnectednessWhat is the future of the human connection to each other, to machinery, to the universe? Psychologists use the level of one's “connectedness” as a shorthand measure of mental health. The pandemic has shown us that personal connectedness can decrease in some areas and expand in others. Human and robotic space exploration presents us with a potential future where people, places, and things connect in newly imagined ways.How can we enable relationships across billions of miles?How can we enable companionship during isolation?How can we enable scientists to more deeply connect with their research?How can we allow the public to connect with NASA discoveries?How can we make science richer?This project brings artists and scientists together to explore the sensory interpretation of NASA science data. This project aims to provide greater access to planets, moons, and stars by providing greater (sensory and experiential) access to the data. Imagine if we could make the data felt, or heard. What if we could feel a Marsquake? Or feel the ocean temperatures on TRAPPIST-1d?Speakers:Refik Anodol - Art, Data, Machine LearningSaskia Wilson Brown - Olfaction Danielle Krettek Cobb - EmpathyAmy Crawford - Music and EmotionJosh Goldblum - Experience Design, StorytellingBjarke Ingels - Architect, BIG Glenn Kaino - Artist and Magician Prathima Muniyappa - Indiginous PerspectivesShane Myrbeck - Sound and MeaningSeth Shostak - Senior Astronomer SETI Nathan Smith - Trust & SecurityMarco Tempest - Technology & MagicNIck Tidball - Robotic Companions for ExplorersAnjali Tripathi - Science and PolicyLiam Young - Speculative Architecture, StorytellingVictoria Vesna - Data and ArtReggie Watts - Comedian, Musician",,"['space-enabled', 'media-lab-research-theme-future-worlds']",,392020,,2023-09-04T19:26:56.700Z,['prathima@media.mit.edu'],True,cosmopoetics-of-the-root-weavers-sympoietic-ethnography-of-the-khasi-cosmovison,2022-12-13,[],"Humans, Robots, Space, and the Future of Connectedness",PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2022-12-13T18:06:46.290Z,"The Mars Amity Research Station (MARS) whose construction is currently underway aims to be India's first party to establish a planetary simulation station in a remote planetary analogue site, the cold desert of Ladakh and have highly visible and tangible presence as a leader and research Astrobiology, Space Biology, Climate Change, Sustainable Energy, Education and Space Experience Tourism.The MARS Project is being co-designed with due consideration to the Tso-Kar  communities of the Changthang plains, the MARS station will undertake collaborative research with the indigenous people to enable documentation and preservation of their storied traditional knowledge relating to the ecology and their commensurate cosmology. This research is instigated to provoke a cross disciplinary pollination between the science based knowledge production of the this centre promises to be a fount of and the existing knowledge stewarded by these communities. This search is undertaken with Dr. Siddharth Pandey, Prathima Muniyappa, Dr. Daniella Scallice, NASA’s astrobiology program, and Annalea Beattie, who studies the role of art for communities in extreme environments. ",2021-07-31,"['space-enabled', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/12/13/Screenshot 2022-12-13 at 2.09.55 PM.png,392031,,2023-09-07T22:07:21.210Z,['prathima@media.mit.edu'],True,art-science-residency-at-the-mars-analog-research-site-in-ladakh-india,2021-07-01,[],"THE COSMOLOGICS OF THE CHANGTHANG PLAINS AT THE MARS ANALOG RESEARCH STATION IN LADAKH, INDIA",PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1
False,2022-12-13T20:20:08.288Z,"The Khasi people trace their ancestry back to the stars. Hailing from the Northeastern state of Meghalaya in India, the Khasis are famous for having evolved a synergistic technology of weaving living architecture by braiding tree roots into architectural structures, like bridges, platforms and stairs. Their origin mythology is replete with allusions to the cosmos, their origin mythology abound with stories of space travel to meet their sister tribes in the sky. Rich in oral tradition, the Khasi cosmology offers compelling narrative accounts of how the tribes settled the earth, entered into kinship relations with the sky, the land, the creatures and other non-human entities. Tales of a tree whose roots sank into the very depths of the earth and whose trunk was so tall that its canopy reached the heavens, a tree that was a celestial stairway for space exploration, arising from the very navel of the earth, called Sophtehpenanang forms the fulcrum of their oral histories. This research illustrates the methodology and outcome of working closely with the Khasi community to develop an artistic piece that engages complex nuanced questions about space exploration’s relationship to the environment and climate change as well as explores what indigenous participation in space exploration might look like.This research is undertaken in collaboration with the Living Root Bridge Foundation, Pynursula. ",,"['space-enabled', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/12/13/Screenshot 2022-05-25 at 12.15.57 PM.png,392045,,2023-09-04T19:26:00.337Z,['prathima@media.mit.edu'],True,braided-cosmopoetics-the-khasi-cosmo-vison-for-an-ethical-space-exploration,2022-12-13,[],Braided Cosmopoetics: The Khasi cosmo-vison for an ethical space exploration,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1
False,2022-12-13T20:38:39.093Z,"Ethnographic field research that explores the Changpa nomads' cosmological orientation, practices of celestial navigation and traces lines of dwelling as a relational con-subjectivity that attends to presence, and seeks to understand the arts of becoming aware of presences, and bringing their presence into consciousness. Their language and life ways hold the veil open to afford world that is saturated with possibilities disruptive of mere presence, attunements to the more than human world and moments of holographic condensation composed at once of multiple temporalities. This research aims to situate an  artistic inquiry at the very limits of ‘thinkable thought’ to ask what/who is obscured when the unknown becomes known. By asking what does in situ practices of space exploration tell us about the nature of scientific practice and its relationship to ethics? Indeed are voices/cultures are rendered invisible in the objective pursuit of the unknowable and how do we respectfully and creatively engage with communities and other ontologies  so that the world itself  does not get lost in doctrines of representation and scientific objectivity as Donna Haraway so astutely reminds us. ",2021-07-31,"['space-enabled', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2022/12/13/Screenshot 2022-12-13 at 4.30.08 PM.png,392053,,2023-09-07T22:08:19.122Z,['prathima@media.mit.edu'],True,earth-and-space-exploration-in-mars-analog-research,2021-07-01,[],"ART-SCIENCE RESIDENCY AT THE  EARTH AND SPACE EXPLORATION  EXPEDITION IN THE MARS ANALOG SITE OF LADAKH, INDIA",PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1
False,2022-12-23T09:57:32.150Z,"This project seeks to improve the use of satellite-based Earth Observation as an input to a Drought Decision Support System to inform the response to drought and floods in southern Angola. Specifically, the government of Angola needs to make decisions to evaluate the effectiveness of three categories of interventions to determine if these interventions are delivered to the regions in which residents face high vulnerability based on their sensitivity, exposure and adaptive capacity to drought hazards. The three interventions include: 1) Sending water trucks to deliver water to residents impacted by drought; 2) Providing funding, equipment and personnel to improve boreholes; and 3) Investing in long term infrastructure improvements in the Cunene River to allow catching and pumping of water during rainy periods. In addition, the Angola Drought Decision Support System contributes to a capability for the Government of Angola to organize information about the various entities that provide drought relief (included national government, regional government and non-profit entities such as UNICEF) and determine whether the combination of drought response efforts is delivered collectively to the regions with high vulnerability. To summarize, the government of Angola needs to use the Drought Decision Support System to make four high level decisions: 1) What routes & schedule should trucks use to deliver emergency water supplies to High Vulnerability regions? 2) What locations should be prioritized for borehole improvement projects to serve High Vulnerability regions? 3) What region of the Cunene River should be prioritized for long term infrastructure improvements to serve High Vulnerability regions? and 4) How should work on the three interventions be divided among government and nonprofit agencies to ensure that high vulnerability regions are served?A Drought Decision Support System can assist leaders in the Government of Angola to answer the questions above by creating a tool that presents a geographic visualization showing where hazards occur due to drought and flood. The geographic visualization informs the development of a Vulnerability Index that spatially shows regions where people are likely to have high impact due to the environmental hazard based on their sensitivity, exposure and ability to adapt. Using a Drought Decision Support System, the government of Angola and their collaborators from nonprofit organizations can both assess the extent to which their drought response projects address the High Vulnerability areas. They can also coordinate to confirm whether the combination of public and private initiatives are serving the High Vulnerability areas to the best level possible given resource constraints. As noted later, the project includes a process to collaborate with the local collaborators in Angola to define how they determine which areas are High Vulnerability. We use a foundational framework guiding the project called “Environment-Vulnerability-Decision-Technology” (EVDT), which allows a structured process to created integrated, spatially presented views of complex scenarios in which environmental state, human experience, policy decisions and the use of technology as a source of information play a key role. For this project, the US team led by PI Danielle Wood will collaborate with the Unfunded Collaborator led by Dr. Zolana Joao, Director of the Angolan Management Office of the National Space Program (the acronym is GGPEN in Portuguese). At the request of Dr. Joao, the US-team proposes to collaborate with GGPEN to build an Angola Decision Support System offering an interactive website, based on the Environment-Vulnerability-Decision-Technology Integrated Modeling Framework. EVDT has a generic formulation that can be adapted to a specific focus area and geographic region.All EVDT analyses ask four common questions: 1) What is happening in the Environment? 2) How are humans impacted by the Environment? 3) What decisions are humans making that relate to the Environment? and 4) What technology is providing Environmental information for humans and is the technology adequate to answer human questions? To apply EVDT to a specific topic and geographic region, analysts customize the four questions and the streams of data. The framework for the methods in this project highlights the Decision Component focusing on evaluating the deployment of water deliveries, borehole projects and water infrastructure improvements. To make this decision, the Decision Makers seek to implement drought interventions in geographic locations that serve High Vulnerability human communities. Thus, the Socioeconomic Vulnerability component of EVDT outputs a map of human vulnerability to drought that is informed by demographic maps. The Vulnerability component of EVDT produces this map by combining a satellite and in-situ data set showing the assessment of Meteorologic, Agricultural and Hydrologic Drought severity in a spatial view. The Technology Design Model is used in a minimal way in this proposal; it simply contains the list in Table 1 of data sources. The southern provinces of Angola, especially Cunene, Huila and Namibe, depend heavily for their economic well-being and culture on the raising of livestock (especially cattle) and crops (especially millet and sorghum). The region is also highly impacted by drought. According to a Post Disaster Needs Assessment preformed at the request of the Government of Angola after the droughts from 2012-2016, about 4 million people were living in Cunene, Huila and Namibe provinces at the time of publication of the report and almost 70% of the population depended on farming as their principal economic activity. Many families raised cattle as their main livestock, in addition to goats, sheep, pigs and poultry. The production and use of milk in the diet and commercial activities was key to the community culture. In order to raise cattle in the region with uncertain rainfall, family members, primarily male, participate in transhumance pastoralism, traveling to distant locations to find safe pasture. The rural populations in Cunene, Huila and Namibe provinces were reported to have high illiteracy rates (about between 25 and 35%) as well as limited access to consistent water sources and electricity. Electricity access ranged from 16% to 32%; those without electricity depend heavily on charcoal and firewood for energy (Rocha et al, 2017).The geographic location of these provinces exposes them to cyclic natural disasters, including droughts and floods (Gjerstad 2014). Major challenges due to drought have been documented in recent decades from 1995 through the present, noting a pattern of low rainfall every five years and alternative times of flooding. In 2018 and 2019, a year-long period of drought is estimated to have “severely affected 880,172 people and over one million heads of cattle and goats, killing 30,000 heads of cattle” (Agencia Angola Press November 2019). Cunene province experienced heavy rainfall in October and December 2019, and it causes flooding in areas that have land degradation due to drought (Agencia Angola Press 4 December 2019). In November 2019, the Ministry of Energy and Waters initiated the construction of a new systems for catching, pumping and managing water from the Cunene River as one of several measures to respond to the drought and flood cycle. The government also responds by providing emergency water deliveries via trucks and develops projects to improve boreholes (Agencia Angola Press October 2019; Agencia Angola Press 31 December 2019). The Angolan Management Office for the National Space Program (GGPEN) has invited a team led by Professor Danielle Wood and Professor Dara Entekhabi to collaborate on the development of an information system to help the government of Angola respond to the ongoing challenges of drought in the Cunene, Huila and Namibe Provinces of southern Angola. The GGPEN team made a field visit to the Cunene province to learn what impacts are happening for local communities and to understand what methods are being used to provide relief from the drought.In November 2019, Professor Danielle Wood and MIT Research Scientist Dr. Katlyn Turner joined an international delegation to visit Cunene to present to local government representatives the opportunities to use satellite earth observation to support the efforts to mitigate the harmful impacts of the drought/flood cycle. Professor Wood worked with Eric Ashcroft of the Blue Raster data analytics firm to develop a prototype of an Angola Drought Decision Support System that presents satellite-based analysis of rainfall, soil moisture, vegetation health and surface water area in a graphic map available on the internet. Professor Wood presented the initial version of this Angola Drought Decision Support System. For the scope of the proposed project, the Management Office of the National Space Program (GGPEN) of Angola serves as the primary point of contact between the Government of Angola and the US-based project team (with collaborators from MIT, Blue Raster and Resources for the Future). One mandate of GGPEN is to increase the use of satellite-based Earth Observation data among government agencies in Angola. GGPEN will coordinate within the government of Angola on behalf of the project to identify the multiple agencies that play a role to shape the drought response at the national and local level. GGPEN is not the decision maker who specifically implements the interventions to respond to the drought (such as water truck deliveries or borehole projects). However, GGPEN will represent the perspective of the decision maker for the US-based team from MIT, Blue Raster and Resources for the Future. The Decision Maker will be conceived as the “Government of Angola,” while GGPEN will communicate with the US-based project team to ensure the work by the US team responds to Decision Maker needs. For the Transition Plan, GGPEN ensures long term application of the Angola Drought Decision Support System providing qualified personnel to operate the software of the Decision Support System and making the tool available for use by agencies responding to drought.The process for GGPEN to serve as a coordinator for linking the work by the US to relevant organizations in Angola started during a pilot study on this project during 2019. Drought response organizations include World Vision and the United Nations Children’s Fund (UNICEF); from Angola this includes Gabinete Provincial de Infra-Estrutura e S. Técnicos (Provincial Infrastructure Cabinet); GAPP - Gabinete de Agricultura, Pecuária e Pesca (Cabinet for Agriculture, Livestock and Fishing). The baseline status of the decision-making activity was reviewed by GGPEN in a field visit to the Cunene Province. The field visit revealed that there is limited use of satellite-based earth observation data to inform drought response in Cunene by government and private non-profits but there is not a centralized integrated view that mixes scientific environmental observation with social science methods to estimate human vulnerability. Also, the field visit showed that the drought response is not currently being evaluated based on the metric of delivering services to High Vulnerability communities. This project fills the gap.",,"['space-enabled', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/12/23/angola project.jpeg,392195,,2023-10-17T23:57:48.308Z,"['katlyn@media.mit.edu', 'drwood@media.mit.edu']",True,decision-support-system-for-drought-response-in-angola,2022-12-05,[],Decision Support System for Drought Response in Angola,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2022-12-23T10:18:42.736Z,"Gold mining has played a significant role in Ghana's economy for centuries. Regulation of this industry has varied over time and while industrial mining is prevalent in the country, the expansion of artisanal mining, or Galamsey has escalated in recent years. Many of these artisanal mines are not only harmful to human health due to the use of Mercury (Hg) in the amalgamation process, but also leave a significant footprint on terrestrial ecosystems, degrading and destroying forested ecosystems in the region. In this study, the Landsat image archive available through Google Earth Engine was used to quantify the total footprint of vegetation loss due to artisanal gold mines in Ghana from 2005 to 2019 and understand how conversion of forested regions to mining has changed over a decadal period from 2007 to 2017. A combination of machine learning and change detection algorithms were used to calculate different land cover conversions and the timing of conversion annually. Within the study area of southwestern Ghana, our results indicate that approximately 47,000 ha (⨦2218 ha) of vegetation were converted to mining at an average rate of ~2600 ha yr−1. The results indicate that a high percentage (~50%) of this mining occurred between 2014 and 2017. Around 700 ha of this mining occurred within protected areas as mapped by the World Database of Protected Areas. In addition to deforestation, increased artisanal mining activity in recent years has the potential to affect human health, access to drinking water resources and food security. This work expands upon limited research into the spatial footprint of Galamsey in Ghana, complements mapping efforts by local geographers, and will support efforts by the government of Ghana to monitor deforestation caused by artisanal mining.",,"['space-enabled', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2022/12/23/ghana_OLK01Iq.webp,392204,,2023-09-06T21:15:27.658Z,['drwood@media.mit.edu'],True,using-satellites-to-monitor-deforestation-due-to-mining-in-ghana,2022-12-05,[],Using satellites to monitor deforestation due to mining in Ghana,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-01-09T16:47:58.638Z,"The MIT City Science team will work with the Diputación Foral de Gipuzkoa and Mubil Fundazioa on a new collaboration to create the City Science Lab@ Gipuzkoa. Teams will research new systems and strategies for communities with the aim to develop and test experimental projects related to mobility and open innovation.The Diputación Foral de Gipuzkoa, or DFG, is a local council  responsible for governing and administering the Historical Territory of Gipuzkoa (belonging to the autonomous community of the Basque Country in Spain.) This collaboration  is encompassed and conducted within the framework of the Etorkizuna Eraikiz (which means “building our future” in Basque) an initiative led by DFG. Etorkizuna Eraikiz is aimed at fostering the community capacity to collaboratively understand and address the challenges of our time. Through listening and experimentation, the program comprises different projects in which public authorities and citizens (represented in entities such as businesses and societal, educational, research and civic organizations) co-participate to define and implement Gipuzkoa’s agenda and contribute to making sustainable policies.Mubil is a non-profit organization that was set up ""with the aim of incorporating public and private-sector companies and organizations to implement the smart and sustainable mobility strategy.""The City Science Lab @Gipuzkoa is also collaborating with the School of Engineering at  Tecnun Universidad de Navarra, an undergrad and graduate institution based in San Sebastian with additional locations in Madrid and Pamplona.",,"['city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/02/13/Tolosa_SARA @OLATZPEON.JPG,392594,,2023-09-04T19:06:54.585Z,"['alonsolp@media.mit.edu', 'maitanei@media.mit.edu', 'naroa@media.mit.edu', 'kll@media.mit.edu', 'mdchurch@media.mit.edu', 'tolano@media.mit.edu', 'ainhoa3@media.mit.edu']",True,city-science-lab-san-sebastian,2023-01-10,[],City Science Lab @ Gipuzkoa,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-01-12T20:39:36.208Z,"The City Science Summit is an annual gathering of the City Science Network: an international community of institutions and researchers that share the common goal of enabling more livable, equitable, and resilient communities. We propose that the grand transnational challenges of our era are best addressed in cities, one community at a time.Past summits include: 2017 Andorra la Vella, AndorraCo-hosted with the Andorra Innovation Hub2018 Shanghai, ChinaCo-hosted with Tongji University College of Design and Innovation 2019 Hamburg, Germany Cities WithoutCo-hosted with HafenCity University2020 Guadalajara, MéxicoThe Power of WithoutCo-hosted with the University of Guadalajara2021 Taipei, TaiwanCities With(in)Co-hosted with Taipei Tech 2022 MIT Media Lab - Cambridge, USAHyper-LOCAL Solutions to GLOBAL ChallengesIn collaboration with the Norman Foster Foundation Learn more about each summit as posted below or on the City Science Network website.",,"['city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/01/12/Screenshot 2023-01-12 at 3.47.02 PM.png,392793,,2023-09-04T19:06:27.465Z,[],True,the-city-science-summit,2017-01-01,[],The City Science Summit - Overview,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-02-04T17:26:51.341Z,"Thank you for your interest in our  Data Activism program. All available spots for this program have been filled. If you would like to be put on the waitlist, please email Raechel Walker (raechelw@media.mit.edu). We invite you to keep an eye on our website and stay tuned for future Data Activism programs.If you have already applied for the current program, you should expect to receive an email regarding the required consent and assent in the near future. Thank you again for your interest and we hope to have the opportunity to work with you in the future.My name is Raechel Walker. I'm a 2nd year master's student in the Personal Robots group. I am ecstatic to announce that I will be teaching the second Data Activism Program from Feb 18th - April 22nd. Here is a short description of the program:The Data Activism Research Study aims to teach African American high school students in Boston and Cambridge to use computer programming, data science, and art to challenge power inequalities, such as racism. Additionally, the high school students will collaborate with community organizations on their projects that require data analysis. Community partners and students will be part of every aspect of the research project, such as conducting background research about a specific example of systemic racism, finding a data set, and implementing a data analysis. Students will be taught from February 18th through April 22nd on Saturdays in the MIT Media Lab from 9:00 a.m. - 2:00 p.m. on Saturdays.  Also, there will be 1 hybrid class during the week based on the student’s availability for 1 ½ hours.  Based on the student’s schedule, they can join the hybrid class in-person or virtually for the class that happens during the week. Here is the link to the more explicit schedule with all the days they will need to be in person or virtual. Overall, the project will introduce students to critical thinking and teach them the computer programming skills required to challenge social injustice.  All students will be compensated for their participation at a rate of $10 per hour.  If you're interested in learning about the summer data activism class that was taught in 2022, please read this MIT News article. Additionally, if you would like to see the summer data activism student projects in more detail, here is a link to a brochure of their work for your review. Also, please watch the video below about the data activism program that will start Feb 18th, 2023:",2027-08-27,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/uuid/83b68c1c-4176-498c-b957-433a11dc19b1,393722,,2023-10-03T21:20:43.629Z,"['raechelw@media.mit.edu', 'cynthiab@media.mit.edu', 'xiaoxued@media.mit.edu', 'mataylor@media.mit.edu']",True,data-activism-program,2023-02-18,"['data', 'kids', 'learning-teaching', 'civic-action']",Data Activism Program,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-02-07T17:04:41.727Z,"Goal / InterestUsing ambient temperature to actuate, morph, and transform structures for pre-programmed tasks ",,"['tangible-media', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/03/06/BB6A5768.JPG,393837,,2023-09-04T19:05:54.602Z,"['yang0129@media.mit.edu', 'jackform@media.mit.edu', 'ishii@media.mit.edu']",False,lineup,2023-02-07,"['robotics', 'design', 'human-computer-interaction']",LineUP!: Research of Ambient Thermal Responsive Self-assembling Structure,PUBLIC,http://www.LyouYeung.com,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-02-07T21:43:03.101Z,"DayDreamNight was a gathering organized for the MIT community to explore projects produced in collaboration between CAST Visiting Artist Carsten Höller and MIT’s Fluid Interfaces group. Dream altering toothpaste, inflatable pajamas, a live sleeper and more were there for people to experience. We showed dream incubation device prototypes and displayed recordings of dreams as evidence to be measured and directly felt. We offered dream altering foods, teas to alter circadian cycles, and a public discussion between sleep scientist Dr. Robert Stickgold, art historian Professor Caroline Jones, and artist Carsten Höller on the possibilities for collaborative art and science to open up avenues for understanding and shifting our sleep, dreams, and daydreams. See a video of our exhibition here. See the exhibition catalogue here.This exhibition was put on in collaboration with the MIT Museum Studio, and created with support from MIT CAST and MIT CAMIT.  Prototypes created through this collaboration have since been shown at events at the V&A Museum and LUMA Arles. ",2023-02-07,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/02/07/Screen Shot 2023-02-07 at 12.45.30.png,393845,,2023-09-04T19:04:13.401Z,"['adamjhh@media.mit.edu', 'pattie@media.mit.edu', 'patpat@media.mit.edu', 'tomasero@media.mit.edu', 'abyjain@media.mit.edu', 'eyalp@media.mit.edu']",True,daydreamnight,2022-02-01,[],DayDreamNight,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-02-28T14:42:50.351Z,"​Can you ever be what you say and say what you are?Can your speech genuinely represent yourself and convey your thoughts and meaning?Does your speech hold your thoughts and intended meaning?Every word spoken out loud can lose its original intent and meaning once it departs from your mouth and is transferred to any media to be heard, leaving you with a sense of detachment when you hear your recorded voice.Sometimes, I feel disconnected from myself when I hear what I have just spoken out loud. Is that really what I wanted to say? Is that truly what I have been thinking? Have I ever even thought about this before? Once your voice is out, it no longer belongs to you.This interactive art installation creates a strange feeling as if you are speaking to yourself in a soliloquy while having the presence of an invisible entity that might be you or not. The automated typewriter recognizes your speech and translates it into Morse code, excluding the words 'I' and 'you' and those letters. It creates a new perception of distance and captures the moment of losing authority over your own thoughts.",,"['tangible-media', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/03/14/strangerGIF.gif,394918,,2023-09-04T19:03:01.476Z,"['yun_choi@media.mit.edu', 'ishii@media.mit.edu']",True,the-stranger,2022-11-01,"['human-computer-interaction', 'art', 'interactive']",The Stranger - .... .   ... - .-. .- -. --. . .-.,PUBLIC,http://www.mallcong.com,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2023-02-28T21:28:15.703Z,"The most meaningful connections between people are often formed through expression of shared vulnerability and emotional experiences. Despite the number of ways we are able to connect through technology-mediated platforms today, loneliness, apathy, and mental distress are still pervasive around the world. We aim to use artificial intelligence (AI) as a tool to humanize personal experiences through identifying similarity in personal narratives based on empathic resonance as compared to raw semantic similarity. While people are naturally able to empathize and relate experiences, today's state-of-the-art AI systems are limited in such emotional reasoning capabilities.This work focuses on endowing machines with the ability to reason and quantify similarity in lived emotional experiences, which we coin as ""empathic similarity.'' We operationalize empathic similarity in personal stories using large language models and insights from social psychology and narratology. We introduce EmpathicStories, a crowdsourced dataset of emotional personal experiences, and present a novel task to retrieve stories in line with human judgments of empathic similarity. Such an approach allows for retrieval of stories that are truly relevant to a person’s lived emotional experiences. Using prompting approaches, we probe GPT-3's empathic similarity understanding capabilities. However, computing similarities of the query story with all stories in the database is inefficient and expensive. Therefore, we propose a few methods combining fine-tuning and prompting approaches to learn story embeddings that can be compared quickly using distance metrics. We evaluate our method against state-of-the-art language models and  using human evaluation to assess the  impact of our system on improving users’ empathy and connectedness. Furthermore, we explore the interpretability of the model's underlying emotional reasoning capabilities in order to better understand transparency of the system if deployed in the real world. Such work could have strong implications in studying the social-emotional reasoning capabilities of large language models and the potential for responsibly designed AI systems to foster prosocial behaviors and strengthen human-human connections.",,"['personal-robots', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/10/03/Screenshot 2023-10-03 at 4.57.07 PM.png,394926,,2023-10-03T20:59:02.432Z,['joceshen@media.mit.edu'],True,modeling-empathic-similarity-in-personal-narratives,2023-02-28,"['human-computer-interaction', 'artificial-intelligence']",Modeling Empathic Similarity in Personal Narratives,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2023-02-28T21:39:32.374Z,"When people listen to stories, the words are colored with emotions conveyed through prosodic features beyond the text alone. Visual font design provides an opportunity to enhance the empathic quality of a story compared to plain text. In this paper, we present the design, implementation, and evaluation of Affective Typography (AffType), an AI-driven system that extracts prosodic information and sentiment from speech and maps these properties to typographic styles. We conduct a crowdsourced study (N=140) to assess how different font design elements impact readers' empathy with personal stories. While our empathy survey results were not statistically significant, we found that participants had a preference for color to express emotion and saw an increase in average empathy for stories with color-based text alterations. In addition, we offer design insights as to what display features best convey emotional qualities of personal stories for future applications that use affective fonts to create more expressive digital text.",,"['personal-robots', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/02/28/example_AE919yU.png,394935,,2023-09-04T19:01:25.106Z,['joceshen@media.mit.edu'],True,affective-typography-for-empathetic-story-reading,2023-02-28,[],Affective Typography for Empathetic Story Reading,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-03-02T22:09:24.042Z,Cloud Light is a lighting object that draws inspiration from signed distance function geometries and  nature. It functions as a lamp and draws a comparison between smooth-unioned SDF spheres  and clouds illuminated by rays of sunlight. ,,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/997d2791-324f-4587-ad21-9677f035be39,395053,,2023-09-25T18:00:29.136Z,['dexterc3@media.mit.edu'],True,sdf-light,2023-03-14,"['design', 'creativity', 'code']",Cloud Light – Designing with signed distance functions (SDFs),PUBLIC,https://dextercallender.com,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-03-02T22:12:07.726Z,This project explores a computational method to use light as a medium for illustration. I experiment with a process to transform pixels into caustic images—patterns of refracted light.,,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/08/09/RESULT IMAGE small 2.jpeg,395061,,2023-10-12T18:49:37.507Z,['dexterc3@media.mit.edu'],True,refraction,2023-04-02,"['design', 'manufacturing', 'creativity']",Refraction - Using computation to illustrate with light,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-03-05T16:22:10.436Z,"Prismo lamp is designed and constructed based on a classical example of the prismatic structure. It takes advantage of the periodic and space-filling properties of the prismatic geometry to tessellate a series of convex polyhedra that moves synchronously with each other to achieve the final effect of the lamp’s kinetic aesthetic.The transformative process of prismatic geometry indicates great potential for unexpected lighting effects once being utilized as a light-reflective vessel. Its ability to metamorphose into a myriad of states provides opportunities for different lighting conditions. Therefore, by integrating reflective materials and light sources into the interior of the geometry, the prismo lamp is able to express its kinetic properties through light refractions and further amplify them through the materials used.The lamp’s movement is driven by the actuation of its base parallelogram. By fixing one side of the parallelogram to the base, we were able to achieve one degree of freedom which is actuated through a servo motor. The panels of the lamp are fabricated through SLS printing which resulted in a clean black matte finish. The movement of the panels is enabled through mechanical hinges which are integrated into the panel designs. The mesmerizing light reflections of this lamp suggest endless design possibilities for further geometry explorations as well as actuation engineering.",2023-05-31,"['tangible-media', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/0bdb852e-b9e8-45af-84c6-14dfab91fab0,395179,,2023-09-04T18:59:47.168Z,['yang0129@media.mit.edu'],True,prismo,2022-09-01,"['robotics', 'design', 'human-computer-interaction', 'art']",Prismo - Transformable floor lamp based on prismatic structure,PUBLIC,,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-03-10T19:21:49.316Z,"Resonance at low frequencies strongly influences a room's perception of sound, which can enhance or deteriorate the acoustic experience. Previous research showed that low-frequency modes are also linked to the emotional impact of the acoustic environment; this link calls for further understanding of the effect of resonance at low frequencies on sound perception. However, most acoustic software programs adopt geometric-based simulations, which are not accurate at low frequencies. In this project, we adopt wave-based simulations to characterize room modes at low frequencies depending on the room geometry through a parametric analysis. The study outlines the contribution of design elements, such as curves and sub-volumes, in shaping the acoustic environment by manipulating low frequencies.This project is in collaboration with Ahmed Allam—­­a Postdoctoral Associate at the  Signal Kinetics group.",,"['signal-kinetics', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",,395391,,2023-09-04T18:58:44.112Z,['allam@media.mit.edu'],True,enhancing-well-being-through-low-frequency-resonance,2023-03-10,"['design', 'architecture', 'environment']",Enhancing well-being through low-frequency resonance: a parametric analysis of space geometry,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2023-03-10T19:26:18.147Z,"Machine learning in acoustics is an emerging field with great potential yet to be explored. Previous research includes predicting acoustic properties, such as the impulse response (sound signature) from visual features that include an image of a room. However, one application that can significantly transform the acoustic environment design process is reconstructing the room geometry based on its acoustic properties. This project explores applying machine learning in predicting space geometry from an impulse response and aims to provide a tool for designers to design a space based on the desired acoustic experience.This project is in collaboration with Nikhil Singh—a PhD student at the Opera of the Future group.  ",,"['opera-of-the-future', 'signal-kinetics', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",,395399,,2023-09-04T18:57:33.996Z,"['nsingh1@media.mit.edu', 'allam@media.mit.edu']",True,machine-learning-approach-to-design-acoustic-experiences,2023-03-10,"['design', 'architecture', 'artificial-intelligence', 'environment']",Machine learning approach to design acoustic experiences,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2023-03-15T00:08:54.002Z,"An essential element of K-12 AI literacy is educating learners about the ethical and societal implications of AI systems. Previous work in AI ethics literacy have developed curriculum and classroom activities that engage learners in reflecting on the ethical implications of AI systems and developing responsible AI. There is little work in using game-based learning methods in AI literacy. Games are known to be compelling media to teach children about complex STEM concepts. In this work, we developed a competitive card game for middle and high school students called “AI Audit” where they play as AI start-up founders building novel AI-powered technology. Players can challenge other players with potential harms of their technology or defend their own businesses by features that mitigate these harms. The game mechanics reward systems that are ethically developed or that take steps to mitigate potential harms. In this paper, we present the game design, teacher resources for classroom deployment and early playtesting results. We discuss our reflections about using games as teaching tools for AI literacy in K-12 classrooms.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/03/14/Screen Shot 2023-03-14 at 8.12.37 PM_HGdhUQD.png,395579,,2023-09-04T18:56:50.306Z,['safinah@media.mit.edu'],True,ai-ethics,2022-11-01,"['artificial-intelligence', 'learning-teaching']",AI Audit: AI Ethics Literacy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0
False,2023-03-16T04:32:00.822Z,"Recognizing and interpreting group behaviors is much more challenging than that of individual behaviors. Firstly, the system must perform well in recognizing individual behavioral cues. Secondly, it must do so simultaneously, while keeping track of every individual in the group. Finally, it must also recognize the subtle interactions that take place between group members as it can provide more insights into what is being communicated. Natural human conversations are interactively contingent, where people act and react in a coordinated fashion in turns. Consequently, understanding group behavior in multiparty conversations requires recognizing contingent behaviors between group members.To address these challenges, we propose a new model , the Multiparty-Transformer (Multipar-T), which is able to handle multiple streams of input data for all of the members of the group. Work has been accepted at IJCAI 2023, and can be found here: https://arxiv.org/pdf/2304.12204.pdf",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/04/25/multipart.png,395664,,2023-09-04T18:56:16.901Z,"['ybkim95@media.mit.edu', 'haewon@media.mit.edu', 'cynthiab@media.mit.edu', 'dongwonl@media.mit.edu', 'picard@media.mit.edu']",True,contingent-multiparty,2022-09-14,"['computer-vision', 'human-computer-interaction', 'machine-learning']",Modeling Contingent Behaviors in  Multiparty Conversations,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-03-16T04:46:36.780Z,"The social robot intelligence benchmark is a large-scale multimodal dataset comprising of many diverse contexts, tasks and environments to evaluate a social robot's  social intelligence. In this work, we first define a taxonomy of social intelligence in the context of of social robots  grounded in findings from psychology, cognitive science, and human-robot-interaction.  Secondly, we enable and establish a suite of machine learning tasks and and evaluation metrics which carefully measure different aspects of an AI agent's social intelligence. Finally, we establish and compare current state-of-the-art  machine learning models on these tasks. Our unified, large-scale, rich, and accessible dataset enables further research to improve social intelligence in social embodied agents.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/03/16/prg-dataset.jpg,395672,,2023-09-04T18:55:49.883Z,"['joceshen@media.mit.edu', 'ybkim95@media.mit.edu', 'safinah@media.mit.edu', 'pmalacho@media.mit.edu', 'pe25171@media.mit.edu', 'sharifah@media.mit.edu', 'haewon@media.mit.edu', 'cynthiab@media.mit.edu', 'dongwonl@media.mit.edu']",True,social-intelligence-benchmark,2023-02-01,[],The Social Robot Intelligence Benchmark,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-03-16T16:32:15.443Z,"Today’s artificial intelligence (AI) technologies are changing how many people play, work, learn, and govern themselves. Due to this rapid technological change, tech and education leaders must consider how to prepare a diverse citizenry to thrive in the emerging age of AI. We propose Impact.AI - AI curricula and educational platforms that support K-12 students in fostering identities as technosocial change agents while they learn about AI.",2024-01-01,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/03/16/impact_ai_header.PNG,395680,,2023-10-04T16:59:36.803Z,"['randiw12@media.mit.edu', 'cynthiab@media.mit.edu', 'hal@media.mit.edu']",True,impact-ai-k-12,2021-07-01,"['artificial-intelligence', 'kids', 'learning-teaching', 'technology']",Impact.AI: K-12 AI Literacy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2023-03-16T17:24:09.425Z,"S.P.A.R.K.I. (Students'  Personal Assistant for Reinforcing Knowledge and Innovation) is a GPT-powered chatbot that scaffolds children's work on AI projects. Sparki provides programming assistance, gives students feedback on their ideas, and socially mediates creative thinking.",2024-01-01,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/uuid/51354376-d52c-4fd4-ba61-f515b2c58533,395688,,2023-09-04T18:54:37.929Z,"['cynthiab@media.mit.edu', 'hal@media.mit.edu', 'randiw12@media.mit.edu', 'safinah@media.mit.edu']",True,sparki-ai,2022-12-01,[],Sparki: An Interactive Learning Companion for AI Education,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-03-18T07:52:34.230Z,"Emotional wellbeing is an important indicator of overall health and poor emotional wellbeing can equally adversely affect one's overall health and wellbeing (and vice versa).  Systems that help monitor and self-manage their emotional well-being by providing feedback and interventions can promote positive emotional well-being and perhaps, more significantly, might increase overall well-being. ",,"['personal-robots', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/03/18/thumbnail11_Hbq8FUQ.jpg,395818,,2023-09-04T18:54:19.960Z,"['ybkim95@media.mit.edu', 'cynthiab@media.mit.edu', 'haewon@media.mit.edu', 'sharifah@media.mit.edu']",True,wearable,2023-02-01,"['robotics', 'human-computer-interaction']",Wearable Device-assisted Emotional Wellbeing Intervention with a Social Robot,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-03-20T20:05:33.942Z,"Memory is a fundamental cognitive ability that plays an integral role in a wide array of downstream cognitive functions, including reasoning, problem-solving, learning, and communication. It is a computational resource that is at the foundation of many of our activities. Memory involves a continual cycle of interactions of encoding, storing, and retrieving information. The advent of AI has led to a transformation in computer capabilities, and current research is exploring how an AI-enhanced human may be able to demonstrate an improved memory capacity and consequently make more informed decisions and better communicate in a variety of contexts. By developing wearable systems and conversational interfaces for real-time memory support, we are exploring how to augment social interactions and memory performance for people of all ages.This project is a work-in-progress with two prototypes under experimentation. ",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/03/20/mem_aug_1 copy.png,395920,,2023-09-04T18:51:31.983Z,"['wazeer@media.mit.edu', 'swtchan@media.mit.edu', 'rliu34@media.mit.edu']",True,memory-augmentation,2023-03-20,[],Memory Augmentation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-03-20T23:20:15.540Z,"Rhythmic sensory stimuli can influence the natural electrical signals (brainwaves) of the brain with benefits like improving memory. Our lab is working to develop wearable, unintrusive devices that use stimuli like vibration to shape brain activity and improve cognition.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/03/20/programmable body rhythms.jpg,395937,,2023-09-04T18:50:56.872Z,"['nathanww@media.mit.edu', 'swtchan@media.mit.edu', 'wazeer@media.mit.edu', 'pattie@media.mit.edu']",True,programming-body-rhythms,2023-03-20,[],Programming Brain Rhythms,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2023-03-21T00:16:36.268Z,"InExChange is comprised of a series of research studies and mixed-reality experienced designed to explore the impacts of shared physical sensation, such as the sensation of breath, through wearable devices.Mixed reality experience (accepted to CHI '23)InExChange is an interactive mixed reality experience centering around an inflatable vest which conveys a physical sense of shared breathing on the diaphragm between two or more participants. The experience is composed of three acts in which the participants' breaths are transformed into metaphorical projected representations: expansive waves, flowing light trails, and growing tree branches. The inflatable wearable devices physically enact in near real-time the inhale/exhale pattern of the other person's breath, varying in intensity level to create an attention interplay between the embodied sensation and the projection. Through this embodied sense of playful shared breathing, we aim to cultivate a genuine feeling of connection and contribute to the integration of somaesthetic design principles in mixed reality HCI.Research study (ongoing, abstract accepted to journal)Technologies on the body that require explicit awareness to be operated or monitored often risk disrupting human awareness and induce stress and excessive cognitive load. With the increasing interest in body-centric technologies, it is thus essential to understand how to build technologies that interface with human awareness without disrupting or requiring too many cognitive resources. In this paper, we build and evaluate a wearable system that uses different feedback types to alter human awareness (of the device). We further demonstrate how this awareness impacts cognitive load and sense of body-ownership and sense of agency, which are often essential antecedents to successful and continued use. In particular, we demonstrate how a wearable system that alters users’ breathing patterns can induce or disrupt awareness and consequently deliver or diminish a sense of body-ownership and agency over the system.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",,395978,,2023-09-04T18:50:34.738Z,['camorris@media.mit.edu'],True,inexchange,2023-03-20,[],InExChange,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-03-21T00:53:05.833Z,"We demonstrate a multimodal approach for perceptual learning that supplements a perceptually-ambiguous audio stimulus with haptics. In our study, participants hear an interval (a sequence of two tones) while simultaneously receiving a vibrotactile stimulus from a custom-designed wearable device. The results suggest that novice learners who experienced the multimodal auditory and haptic feedback could identify intervals more accurately and consistently than those who only received audio. Future work can leverage this insight to investigate the longer-term effects of multimodal learning, such as the retention of the ability to identify musical intervals after haptic feedback is removed. ",,"['fluid-interfaces', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/03/28/front.png,395987,,2023-09-27T19:30:03.687Z,"['chins@media.mit.edu', 'catfang@media.mit.edu']",True,purrfectpitch,2023-03-20,[],PurrfectPitch: Exploring Musical Interval Learning through Multimodal Interfaces,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-03-22T17:21:22.749Z,"The goal of non-line-of-sight (NLOS) imaging is to image objects that lie outside the camera’s field of view. We use two-bounce light to perform NLOS imaging, a scenario in which the information about the hidden object is encoded within the cast shadows of the hidden object. When using the space-time propagation of shadows, we can estimate the 3D shape of the hidden object with just a few image captures, making the method feasible for practical applications. For example, imagine an autonomous vehicle in a tunnel being able to see ahead of the car in front of it by using two-bounce signals from the sides of the tunnel.",2023-11-15,"['camera-culture', 'media-lab-research-theme-life-with-ai']",,396085,,2023-09-04T18:43:28.975Z,['sidsoma@media.mit.edu'],True,using-shadows-in-space-time-to-see-behind-occluders,2022-06-15,[],Using Shadows in Space-Time to See Hidden Objects,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-03-25T19:09:58.366Z,"ORILUM features a kinetic origami interface that aims to foster connections among individuals who are geographically distant from each other. ORILUM utilizes touch, movement, and light, to create a synchronized experience that promotes relaxation and a sense of closeness.Its capacitive thread-stitched surface, embedded LED light wires, and motor-activated linear motion mechanism allow ORILUM's membrane to contract and expand, simulating the rhythmic movements associated with breathing.Check out the full description of the project, completed as part of MIT Media Lab's How to Make (Almost) Anything in the Fall of 2022.",2023-12-15,"['city-science', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/05/18/ORI.gif,396247,,2023-10-10T15:56:08.405Z,['leticiai@media.mit.edu'],True,orilum-touch-sensitive-light-talker,2023-09-15,[],ORILUM - Touch Sensitive Light Talker,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-03-28T17:51:59.086Z,A distributed sensing system for managing water resources in informal communities.,,"['city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/03/28/bucket.gif,396389,,2023-09-28T15:32:28.938Z,"['aricom@media.mit.edu', 'kll@media.mit.edu']",True,axol,2022-04-02,"['sensors', 'community', 'water']",Axol,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-03-28T21:22:30.160Z,"Cloud based machine learning inference is an emerging paradigm where users query with their data to a service provider who runs a ML model on the data and returns back the answer. Due to increased concerns over data privacy, recent works have proposed using Adversarial Representation Learning (ARL) to learn a privacy-preserving encoding of sensitive user data before it is shared with an untrusted service provider. Traditionally, the privacy of these encodings is evaluated empirically as they lack formal guarantees. In this work, we develop a new framework that provides formal privacy guarantees for an arbitrarily trained neural network by linking its local Lipschitz constant with its local sensitivity. To utilize local sensitivity for guaranteeing privacy, we extend the Propose-Test-Release~(PTR) framework to make it tractable for neural network based queries. We verify the efficacy of our framework experimentally on real-world datasets and elucidate the role of ARL in improving the privacy-utility trade-off.",,"['camera-culture', 'media-lab-research-theme-decentralized-society']",,396399,,2023-09-04T18:39:35.473Z,"['abhi24@media.mit.edu', 'raskar@media.mit.edu', 'vepakom@media.mit.edu']",True,posthoc-privacy-guarantees-for-neural-network-queries,2023-03-28,[],Posthoc Privacy guarantees for neural network queries,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2023-03-30T19:38:02.228Z,"A principal vulnerability of a proof-of-work (""PoW"") blockchain is that an attacker can re-write the history of transactions by forking a previously published block and build a new chain segment containing a different sequence of transactions. If the attacker’s chain has the most cumulative mining puzzle difficulty, nodes will recognize it as canonical. We propose a modification to PoW protocols, called ADESS, that contains two novel features. The first modification enables a node to identify the attacker chain by comparing the temporal sequence of blocks on competing chains. The second modification penalizes the attacker by requiring it to apply exponentially increasing hashrate in order to make its chain canonical. We demonstrate two things; (i) the expected cost of carrying out a double-spend attack is weakly higher under ADESS compared to the current PoW protocols and (ii)  for any value of transaction, there is a penalty setting in ADESS that renders the expected profit of a double-spend attack negative.",,"['digital-currency-initiative-dci', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/50ea6777-5736-4570-8964-678ac17bb74c,396477,,2023-09-04T18:38:55.357Z,['daronoff@media.mit.edu'],True,adess,2023-02-01,[],ADESS: A Proof of Work Blockchain Protocol Modification to Deter Double-Spend Attacks,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-03-30T19:55:56.057Z,"ImmerScope City offers new ways of visualizing complex urban designs. It generates rendered images of streetscapes to allow for collaboration and rapid scenario testing in an immersive setting. This goes beyond traditional static representations and fosters a deeper understanding of the spatial qualities and experiential aspects of urban design.Talk about the layout of the city, and wait for a response.Using 3D models sourced from the City Scope platform, ImmerScope applies a diffusion algorithm that is directed by a depth map and an audio description of the desired design outcomes. Through this process, images are generated that meet the user's specifications and incorporate depth and texture for a realistic result.",2023-12-31,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/04/11/Screenshot 2023-04-11 at 7.08.23 PM.png,396485,,2023-10-10T15:57:24.081Z,"['leticiai@media.mit.edu', 'vincy@media.mit.edu', 'gba@media.mit.edu', 'noyman@media.mit.edu', 'nawyn@media.mit.edu']",True,an-immersive-user-interface-experience-for-urban-exploration,2023-01-01,[],ImmerScope City: Blending Perspectives and Design Experiences,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2023-03-30T20:22:20.468Z,"The aim of this project  is to provide a detailed comparison of the security of Proof of Work (e.g., Bitcoin) versus Proof of Stake (e.g., Ethereum 2) protocols along several dimensions, including double-spend attacks, use of market power in mining and staking; disruption of consensus and transaction finality.",,"['digital-currency-initiative-dci', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/8f309f81-5bb4-4752-a76f-03b34dcb382a,396501,,2023-09-04T18:26:55.560Z,"['narula@media.mit.edu', 'daronoff@media.mit.edu', 'ryoungbl@media.mit.edu']",True,a-comparison-of-the-security-of-proof-of-work-versus-proof-of-stake-blockchain-protocols,2023-01-01,[],A Comparison of the Security of Proof of Work versus Proof of Stake Blockchain Protocols,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-03-30T20:24:28.145Z,"In a Proof-of-Work blockchain the equilibrium hashrate is a monotone function of the fiat exchange value of the block reward. There is no lower or upper bound on hashrate. This creates two types of inefficiency. First, the vulnerability to an attack increases as hashrate falls since (we argue) the profitability of a double-spend or disruptive attack is a function of the hashrate required to gain control of the network. Second, external costs grow as hashrate rises. Hashing consumes electricity which contributes to global warming and increases the cost of energy for alternative uses. There is an interval of hashrate within which the combined costs are minimized. We propose a modification of the Proof-of-Work protocol that, for a target interval, incentivizes miners to push hashrate toward the interval. When hashrate drops below the lower bound, the miner receives a supplemental reward which induces increased hashrate. When hashrate exceeds the upper bound, the miner receives a fraction of the block reward, which induces reduced hashrate. Puzzle difficulty, a sufficient statistic for hashrate, governs the adjustment to the miner’s reward. Monetary neutrality can be maintained by balancing the reduced reward when hashrate is above target with the supplement to the reward when hashrate is below target.",,"['digital-currency-initiative-dci', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/57ec249a-f3a2-4b71-b7f0-9d07424e86e2,396517,,2023-09-04T18:25:08.942Z,['daronoff@media.mit.edu'],True,nakamoto-brc-a-proof-of-work-protocol-to-reduce-blockchain-energy-consumption,2023-07-01,['cryptocurrency'],Targeted Nakamoto: A Proof-of-Work Protocol Modification to Improve Security and Limit Energy Consumption,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-03-31T17:47:51.126Z,"Latent Lab - Navigating Innovation: Interactive Data Exploration & InspirationLatent Lab is an AI-driven, graphical knowledge exploration system. Full text and (soon) media analysis reveal conceptual overlaps between elements at any scale. The interface is designed for exploration rather than search and is intended to inspire a graphical conversation from which new ideas will emerge. A sidebar offers a dedicated space for delving into the specifics of selected data and crafting unique ""recipes"" by combining multiple documents. Recipes are synthesized as new ideas in real-time for further ideation and refinement by users. Latent Lab includes data from the Media Lab, US Patent Office, tweets on COVID-19, and various user-uploaded datasets. The system creates a 1,536 dimension vector for each dataset that is reduced to 2D and rendered on-screen for visualization and interactive exploration.",,"['viral-communications', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/03/31/LLL_timeline.gif,396560,,2023-10-18T15:42:07.135Z,"['dunnell@media.mit.edu', 'lip@media.mit.edu', 'apstodd@media.mit.edu']",True,latent-lab,2023-01-09,"['design', 'human-computer-interaction', 'artificial-intelligence', 'data']",Latent Lab,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2023-03-31T21:41:15.979Z,"The Generative Land Use project aims to revolutionize urban planning by developing a cutting-edge algorithm that generates the ideal land use distribution based on optimizing a given area's economic, cultural, social, and environmental aspects. Our approach is human-centric, meaning that we prioritize the needs and preferences of local communities in our optimization process. By incorporating these factors, we aim to create urban environments that are not only functional but also sustainable and enjoyable places to live, work, and play.Once we gather metrics about the area's social, economic, environmental, and cultural aspects, we use that information to model an objective function that will be optimized using genetic algorithms and particle swarm optimization techniques. Our data-driven and evidence-based approach allows us to make informed decisions about the optimal distribution of land use. Additionally, we consider any existing land restrictions, such as roads or building placement, to ensure that our plans are feasible and realistic.We utilize a genetic algorithm approach to generate diversity in our land use distribution. In each generation, the algorithm selects the best individuals from the population and uses them to create the next generation of possible solutions through crossovers and mutations. The mutations involve moving land uses around the area, creating new ones, and adjusting their size. Throughout this process, we ensure no collisions between the different land uses. This allows us to explore various possibilities and find the optimal solution for our human-centric urban planning approach.In addition to the genetic algorithm approach, we incorporate particle swarm optimization into our land use distribution modeling. In this technique, each land use is represented as a particle, and these particles move around the area in search of an optimal configuration. The particles are attracted to the fixed points of interest, such as parks or elevator shafts. By grouping the land uses around these points of interest, we can create a more cohesive and efficient urban environment that maximizes the benefits for local communities. These techniques also allow us to consider the dynamic nature of urban environments and adapt to changing needs and circumstances over time.After we have found the optimal distribution of land uses using our genetic algorithm and particle swarm optimization techniques, we need to translate this solution into a 3D model. In this model, we determine the best position for each land use and decide which floor it should be located on, considering any restrictions on the maximum number of floors. Once the 3D model is complete, we can geolocate the results onto a map, providing a more intuitive and visual representation of the land use distribution.To test and validate the model, we will use agent-based simulations that model the behavior of individuals in the urban environment. These simulations provide valuable insights into how land use distribution impacts the community's social, economic, and environmental aspects, allowing us to refine and improve the model as needed. Ultimately, our approach to human-centric urban planning using genetic algorithms and particle swarm optimization provides a comprehensive and innovative framework for creating sustainable, livable, thriving urban environments.",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/04/02/Generative Land Use in Volpe.gif,396569,,2023-09-04T18:24:22.693Z,"['laura122@media.mit.edu', 'parfait@media.mit.edu', 'doorleyr@media.mit.edu', 'tolano@media.mit.edu', 'hatto@media.mit.edu', 'majue@media.mit.edu']",True,generative-land-use-human-centrics-communities,2022-11-14,[],Generative Land Use for Human-centric Communities,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2023-04-01T23:59:27.033Z,"Rectifying the damage caused by buildings requires an ideological shift, one that involves working with invisible microscopic systems. The very same living organisms that have helped shape the Earth’s ecosystems over billions of years. Bacterial biocement is a living material that offers a path to address the repercussions of the built environment while also transforming how we interact with buildings over their lifespan. All made possible through a collaboration with these microscopic living organisms. Living brick modules were grown with biocementing bacteria. This engineered living material has the potential to mitigate the excessive use of concrete. The bacteria remain alive in the bricks and can communicate with colors in response to environmental factors such as pollutants, helping to map our eco-health. This process of biocementation results in living materials that can address both the repercussions and our perception of the built environment. Transforming buildings from static entities situated in a landscape, to living systems integrated within their ecosystems. Read the thesis of Laura Gonzalez, entitled 'Beyond the Brick: Collaborations with a Sensing Microbial System in the Built Environment,' here. ",,"['community-bio', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/04/01/Screen Shot 2023-04-01 at 8.04.19 PM.png,396630,,2023-09-04T18:23:53.549Z,['dkong@media.mit.edu'],True,bacterial-bio-cement,2023-04-01,[],Bacterial Bio Cement,PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2023-04-02T22:01:27.799Z,"Computer-mediated forms of communication (i.e., social media, texting) have quickly become the predominant way that we keep in touch with one another. Such a profound change calls for a reexamination of the role that digital systems play in human connection. It seems necessary to intentionally explore the ability for technology to enhance rather than detract from emotional intimacy.Closer Worlds is an ML-assisted 2-person experience which asks personal questions and creates images in a playful world-building scenario. This project seeks to empower users to engage in meaningful conversations by using low-stakes, creative play to offer a comfortable space for vulnerability. To understand the effectiveness of design principles inspired by facilitation research, we have conducted a first-phase pilot study on an MVP. Our goal is to advance the project through future iterations and research assessments.",,"['affective-computing', 'opera-of-the-future', 'center-for-constructive-communication', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/04/02/Closer_worlds_hero_game_no_text.png,396687,,2023-09-04T18:22:19.278Z,"['picard@media.mit.edu', 'neska@media.mit.edu', 'jmindel@media.mit.edu', 'cass_lee@media.mit.edu']",True,closer-worlds,2022-10-01,"['design', 'gaming']",Closer Worlds,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-04-02T23:57:05.879Z,"We seek to empower everyone to become conscientious consumers, ethical engineers, creative problem solvers, and informed advocates for responsible AI.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/04/02/AIHopes.jpeg,396695,,2023-09-04T18:21:50.484Z,[],True,theme,2023-04-02,[],Theme | AI Literacy,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2023-04-03T03:46:37.680Z,"How can we make embodied agents have multimodal conversation with individual and group users that is not only natural, but also relational, empathetic, commonsensical, contextual, and meaningfully continuous?",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",,396736,,2023-09-04T18:21:22.022Z,[],True,theme-1,2023-04-02,[],Theme | Multi-modal Interaction + Language,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-04-03T03:53:34.821Z,An embodied virtual agent is a promising oracle for delivering appropriate just-in time interventions. We aim to build a system and design that supports long-term human-agent interaction based on personalization to improve user’s emotional well-being and incrementally co-construct knowledge and mental states.   ,,"['personal-robots', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/04/03/output_efCmLwr.jpeg,396744,,2023-09-04T18:20:59.425Z,[],True,theme-health-wellness,2023-04-02,[],Theme | Health + Wellness,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-04-03T03:54:14.123Z,We create and employ ethical design frameworks to work towards equitable and inclusive HRI research and technologies.,,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/06/13/IMG_1319.png,396752,,2023-09-04T18:20:32.638Z,[],True,theme-justice-ethics,2023-04-02,[],Theme | Justice + Ethics,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0
False,2023-04-03T03:55:28.707Z,We are building autonomous agents that are socially and emotionally saturated in single-person (child-robot) or multi-party (child-parent-robot) contexts. Our projects investigate effective online personalization methods that amplify a child’s learning experience and interpersonal social dynamics. ,,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/04/02/pasted image 0 (1).png,396760,,2023-09-04T18:17:12.207Z,[],True,theme-personalization-early-childhood-education,2023-04-02,[],Theme | Personalization + Early Childhood Education,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-04-03T05:46:01.386Z,"​The Ddog project features a Spot robot from Boston Dynamics and a Brain-Computer Interface (BCI) system powered by AttentivU, a pair of wireless glasses that can measure person’s Electroencephalography (EEG – brain activity) and Electrooculography (EOG – eye movements) signals. Ddog project is the next step in extending the Brain Switch application, a real-time, closed-loop BCI system allowing for real-time correspondence of simple user needs to a caretaker non-verbally. The Brain Switch aims to help support basic communication needs for those with physical challenges (ALS, CP, SCI). Ddog project is built using the same tech stack and infrastructure as Brain Switch.  The biggest advantage of the Ddog is its mobility: it is the first fully autonomous, brain-powered, wireless system that features a Spot robot and runs on two iPhones, with no need for using sticky electrodes and backpacks for computing. Ddog is designed with manipulation assistance in mind: the arm of Spot is used to deliver groceries, bring a chair, a book or a toy, etc.Check the video of the project as well as the interview with Nataliya Kosmyna, Ph.D, the project lead, on why we created Ddog, below!Past contributors include:Yasmeen Hmaidan, Edward Hartley, and of course, Spot!For inquiries – please contact nkosmyna AT media DOT mit DOT edu .",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/04/03/HEROimageDdog.jpeg,396768,,2023-10-17T23:46:08.632Z,['nkosmyna@media.mit.edu'],True,ddog,2022-08-01,[],Ddog,PUBLIC,http://braini.io,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-04-03T15:57:29.061Z,"US financial institutions are subject to accounting rules and leverage regulations that require an agent to record the gross assets of a repo trade on its balance-sheet while placing an upper bound on total assets, which limits the volume of repo trades. Our smart contract increases the attainable volume of repo trades without compromising financial stability. It novates and replaces repo contracts with an arrangement wherein intermediaries become guarantors of contracts between ultimate repo borrowers and lenders, without materially altering payoffs. Transforming an agent from principal to guarantor reduces the balance-sheet impact of a repo trade, thereby increasing the volume of repo the agent can intermediate. In contrast to current proposals to increase repo volume, our solution does not require a loosening of financial regulation or a restructuring of the repo market. It preserves transaction privacy while enabling regulators to timely audit information. More generally, we demonstrate that when financial objects are appended to programmable electronic ledgers it is possible to combine decentralized markets and privacy with informed regulatory oversight.",,"['digital-currency-initiative-dci', 'media-lab-research-theme-decentralized-society']",,396776,,2023-09-04T18:16:16.645Z,['daronoff@media.mit.edu'],True,a-smart-contract-to-increase-intermediation-capacity-in-the-repo-market,2023-04-03,['banking-and-finance'],A Smart Contract to Increase Intermediation Capacity in the Repo Market,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-04-06T21:23:17.406Z,"Textiles are unique materials that could imbue the natural and built environment landscape with fluidity, softness, and a dynamic quality. By drawing inspiration from the artistry and construction techniques used in traditional textiles, as well as the notion of a pavilion or pagoda serving as a communal space, we aim to infuse a new perspective of technical textiles by fusing novel materials, sensing technologies, and digital fabrication processes to build a large-scale interactive textile, while also integrating them with intricate details and avenues for self-expression. The Living Knitwork Pavilion is a customized and modular textile shade structure (dodecagonal pyramid: 18ft high and 26ft wide) consisting of 3D-knitted optically and electrically-active yarns that sense activities and dynamically change color and light up through the day and night.  During sunny days, hidden patterns of humans, nature, flora-fauna, and synthetic beings inspired by the future worlds are revealed through photochromism. The pavilion, working as a shade structure by day and responsive lantern by night, serves as a communal place. Our presence, movements, and environmental changes constantly contribute toward the entire glow, visual effects, and ambiance of the space, as they are reprojected onto the pavilion as living memories, fostering and amplifying moments of discovery, reflection, and connectedness.",,"['responsive-environments', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/10/18/LivingKnitwork_Day0.png,396939,,2023-10-18T16:15:35.046Z,"['irmandy@media.mit.edu', 'gba@media.mit.edu', 'aprubio@media.mit.edu', 'chins@media.mit.edu', 'nbakker@media.mit.edu', 'strand@media.mit.edu', 'manaswim@media.mit.edu']",True,living-knitwork,2023-04-01,"['design', 'architecture', 'art', 'human-machine-interaction', 'wearable-computing', 'sensors', 'community', 'fabrication', 'textiles']",Living Knitwork Pavilion,PUBLIC,https://www.livingknit.work,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0
False,2023-04-18T05:03:05.503Z,"What is the real impact of your smartphone on your ability to deeply engage in your daily activities?David Ramsay sets out to answer that question for himself by measuring his focus throughout his daily life with a suite of wearables, behavioral tools, surveys, and videos.  After a month of data collection, he'll lock away his cell phone and replace it with a vintage rotary phone for all of his communication.Watch him on Twitch (https://twitch.tv/dramsay9) as he streams his activities and the relevant data!Go to https://eastcampus.davidbramsay.com for more information.",2023-09-01,"['responsive-environments', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/04/18/twitch_sample_K3Bf6qv.png,397448,,2023-09-04T18:14:58.247Z,['dramsay@media.mit.edu'],True,the-smartphone-free-living-experiment,2023-04-07,[],The Smartphone Free Living Experiment,PUBLIC,https://eastcampus.davidbramsay.com,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2023-04-19T17:17:54.216Z,"Ultrasound-induced transdermal drug delivery (sonophoresis) has long lingered in the by-lanes of academic and industrial research, and has failed to attain tangible commercial success. While localized, needleless delivery of drugs is an exciting prospect, the bulky, power-consumptive equipment and long exposure times do not justify the highly-variable, operator-dependent outcomes in the permeability enhancement effects observed in vivo. We report a conformable patch (cUSP) with embedded bulk piezoelectric elements to provide short-exposure (10 minutes) ultrasound and effect a 26.2-fold enhancement in niacinamide transport to the dermis. The final system is packaged in a compact form-factor on a flexible polymer substrate that can be applied on facial skin to aid hands-free penetration of popular cosmeceuticals. The ease-of-use and high-repeatability offered by the proposed system provides a game-changing alternative to patients and consumers suffering from skin conditions and premature skin aging.",,"['conformable-decoders', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/04/19/P06 - cUSP on face v2_WHITE (2).png,397497,,2023-09-07T18:54:11.311Z,"['aasthas@media.mit.edu', 'ccyu@media.mit.edu', 'c_marcus@media.mit.edu', 'nayeem@media.mit.edu', 'canand@media.mit.edu']",True,conformable-ultrasound-patch-cufpatch-for-cavitation-enhanced-transdermal-drug-delivery,2023-04-19,"['design', 'bioengineering', 'consumer-electronics', 'fashion', 'health', 'wearable-computing', 'biology', 'technology', 'electrical-engineering', 'wellbeing', 'medicine']",Conformable Ultrasound Patch (cUFP) For Cavitation-Enhanced Transdermal Drug Delivery,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2023-04-21T13:59:36.392Z,"This paper introduces VR Haptics at Home, a method of repurposing everyday objects in the home to provide casual and on-demand haptic experiences. Current VR haptic devices are often expensive, complex, and unreliable, which limits the opportunities for rich haptic experiences outside research labs. In contrast, we envision that, by repurposing everyday objects as passive haptics props, we can create engaging VR experiences for casual uses with minimal cost and setup. To explore and evaluate this idea, we conducted an in-the-wild study with eight participants, in which they used our proof-of-concept system to turn their surrounding objects such as chairs, tables, and pillows at their own homes into haptic props. The study results show that our method can be adapted to different homes and environments, enabling more engaging VR experiences without the need for complex setup process. Based on our findings, we propose a possible design space to showcase the potential for future investigation.",,"['tangible-media', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/04/21/cannon_eg.png,397612,,2023-09-04T18:13:04.626Z,['catfang@media.mit.edu'],True,vr-haptics-at-home,2023-04-01,[],VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences,PUBLIC,https://cathy-fang.com,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-05-02T13:31:12.610Z,"This project explores a few computational approaches to use the shape of a knitted garment as input for its pattern.This experiment combines machine knitting techniques with digital tools, allowing for the creation of intricate and complex patterns that would be difficult or impossible to produce by hand. One of the challenges of such an approach is the need for specialized software to generate the patterns. This post documents the development of a tool that generates computational knitting patterns and can send them to a knitting machine.Computational knittingThis tool is made for knitting machines, but in this case, I specifically used an electronic Brother KH-910 with a hardware modification. The machine was produced in 1980 when computer memory was comparatively limited. While the memory in this machine would allow the user to scan images that could then be knitted, the size of these images did not span the full range of the knitting bed and produced quite small unique patterns. In our machine, this limitation was circumvented by replacing the memory with an Arduino and an AYAB shield. This open-source shield and the accompanying software allow us to connect the knitting machine to any computer, manage the data storage from the computer, and thus print infinite unique patterns.Textile production was one of the first processes that resembled computation: the Jacquard loom and its interchangeable punchcards even inspired the design of early computers. Also nowadays, textile production lends itself perfectly to the use of computation:  an effective approach to quickly generate intricate patterns for which the rules can be defined beforehand. With this experiment, the goal is to create patterns that adapt themselves to the cut or the shape of (parts of) a garment.Some of the key features I wanted to include in the computational knitting pattern generator:Displaying a selection of computational approaches that take the shape of a garment as input.Customization of each computational approach, such as changing the intensity, inverting stitch types, and modifying it for any stitch count and row count.Previewing the pattern in real-time, allowing the user to see the changes in the pattern as they adapt parameters.The ability to create different stitches, such as knit, purl, and increasing or decreasing stitches.The last feature requires a part we have not yet obtained in our lab called a ribber: an additional knitting bed that can be attached to the main bed of the machine.",2024-05-01,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/05/02/computationalknittingpatterns.png,398092,,2023-09-26T14:09:39.544Z,['veraseyp@media.mit.edu'],True,computational-knitting-patterns,2023-01-01,"['design', 'art', 'fashion']",Computational Knitting Patterns,PUBLIC,https://knit.veravandeseyp.com/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-05-04T12:18:55.196Z,"NeuraLiminal: Choose Your Reality NeuraLiminal is an immersive art exhibition that celebrates the union between human and machine, showcasing 35 prototypes of brain-computer interfaces (BCIs), alongside artifacts from science fiction, infographics, and thought-provoking installations. The human mind has long been the subject of fascination and exploration, and with the emergence of BCIs, we are now able to provide access to our brain activity to any computer-based system on this planet, as well as in space, in unprecedented ways. At the heart of NeuraLiminal is the concept of the liminal space, a site of transition between human and computer perception. Through our carefully curated exhibits, we explore how the choices we make today will shape the world of tomorrow. Visitors can experience the rituals of adopting BCIs, from the early days of brain sensing systems to the current state of the art and the uncertain future that awaits us. Each exhibit is designed to provoke thought and inspire wonder, as we explore the ways in which BCIs are changing the very fabric of our lives.   From the awe-inspiring history of BCIs to the limbo of present technology and the overstimulation of an uncertain future, we invite you to consider the implications of this cutting-edge technology and contemplate the role that we as individuals and as a society must play in shaping its future.Curated by: Nataliya Kosmyna, Ph.D Artists: Marshall Wang, Yasmeen Hmaidan and the team behind NeuraFutures 1.0!Location site: Boston CyberArts, April 17 - July 13, 2023More about the project: NeuraFutures",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/05/04/suqare_neura liminal_logo.jpg,398186,,2023-09-04T18:07:53.808Z,['nkosmyna@media.mit.edu'],True,neuraliminal,2023-04-17,[],NeuraLiminal,PUBLIC,https://neurafutures.com,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2023-05-15T19:05:51.612Z,"Press: - MIT News:  Using reflections to see the world from new points of view- TechXplore, EuropaPress, La Nacion (in Spanish!), - Media Lab Twitter, MIT TwitterPublication: Publication Link, Conference on Computer Vision and Pattern Recognition, CVPR 2023Project Website: https://ktiwary2.github.io/objectsascam/ Abstract: Reflections on glossy objects contain valuable and hidden information about the surrounding environment. By converting these objects into cameras, we can unlock exciting applications, including imaging beyond the camera's field-of-view and from seemingly impossible vantage points, e.g. from reflections on the human eye. However, this task is challenging because reflections depend jointly on object geometry, material properties, the 3D environment, and the observer viewing direction. Our approach converts glossy objects with unknown geometry into radiance-field cameras to image the world from the object's perspective. Our key insight is to convert the object surface into a virtual sensor that captures cast reflections as a 2D projection of the 5D environment radiance field visible to the object. We show that recovering the environment radiance fields enables depth and radiance estimation from the object to its surroundings in addition to beyond field-of-view novel-view synthesis, i.e. rendering of novel views that are only directly visible to the glossy object present in the scene, but not the observer. Moreover, using the radiance field we can image around occluders caused by close-by objects in the scene. Our method is trained end-to-end on multi-view images of the object and jointly estimates object geometry, diffuse radiance, and the 5D environment radiance field.",,"['camera-culture', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/05/15/combined_gifs.gif,398572,,2023-09-04T18:07:33.618Z,"['tzofi@media.mit.edu', 'nbehari@media.mit.edu', 'raskar@media.mit.edu', 'ktiwary@media.mit.edu']",True,objects-as-cameras,2023-05-15,"['robotics', 'computer-vision', 'artificial-intelligence', 'data', 'sensors', 'imaging', 'machine-learning', 'computer-science', 'technology', 'autonomous-vehicles']",Objects as Cameras,PUBLIC,https://www.kushagratiwary.com/,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0
False,2023-06-13T13:42:50.818Z,"Tod Machover’s first opera (he has now composed many more groundbreaking ones) was VALIS, and it made history when it premiered for the 10th Anniversary of the Centre Georges Pompidou in Paris in December 1987. Called “the first opera of the 21st century” (The Boston Globe) and “the most intelligent of new American operas” (The New Yorker), it was the first major international art project after the founding of the Media Lab and launched a new era of computer-interactive performance through the invention of Hyperinstruments. In addition, it told a bold Philip K. Dick story about the potential and dangers of AI that was powerfully prescient in 1987, and even more urgent and relevant now. VALIS toured the U.S., Europe and Asia and has been recorded on a still-best-selling CD, but it has not received a new live production for over 20 years.",,"['opera-of-the-future', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/6968b692-4d92-4cf2-9dfe-dd1c72aba586,399832,,2023-09-04T18:06:18.392Z,['tod@media.mit.edu'],True,valis-2023-production,2023-06-13,"['art', 'artificial-intelligence', 'music', 'ethics', 'creativity', 'technology']",VALIS: 2023 Production,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2023-06-13T19:44:25.305Z,"In a series of design workshops, the HRI Equitable Design framework from previous work (see Ostrowski et al. 2022) was expanded to a workshop format and card-based tool that has been deployed with industry members, academic labs, and policy thinkers focused on robotics. The design workshops are a 1 time 4 hour workshop or 2, 2 hour workshops, depending on what would work best for the workshop participants. Both design workshops are structured with two parts: a case study portion where participants applied a design method to the case study to gain familiarity with the method; and an applied portion where participants applied the same design method to their own design problem. The design method for the first workshop are design fictions. The second workshop uses a structured ideation design justice and policy toolkit as the design method. Results demonstrate that participants gain a better understanding of design justice and how to apply it to the policy context and express how the HRI Equitable Design Framework can be used as a tool in their future work in the policy space. Participants comment on the ability of the framework and design workshops to support more reflective design decisions and proactive policy making around robots and new innovative and disruptive technologies.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/06/13/IMG_1286.png,399853,,2023-09-04T18:03:58.623Z,"['cynthiab@media.mit.edu', 'akostrow@media.mit.edu']",True,design-justice-workshops,2022-08-01,"['robotics', 'design', 'human-computer-interaction', 'social-justice']",Design Justice Workshops,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0
False,2023-06-23T15:42:39.621Z,"This project hosts a few experiments of 2D visualization of 3D moving processes.  All of the experiments relate to repeating, machine processes, that are visualized by a singular shape. There are a few rules on how to use the shape:The shape can be used infinitely.The shape can be any size, as long as the ratio is respected and only one size is used per experiment.Segments of the shape can be shown as long as the ratio is respected.This project can be used as a pedagogical tool to explain 2D transformations function. Experiments address different types of 2D transformation, such as translation, rotation, skewing, and scaling in javascript, supported by the library p5.js that addresses the function. Additionally, the experiments serve as an open visual series. They are collected in a site where you can generate your own version and download them. We worked with the riso printing facility, PPPPRESS, to print our own series on their machine.",2030-03-03,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/06/23/machinemotion-header-veravandeseyp.jpg,400228,,2023-09-26T14:05:10.732Z,['veraseyp@media.mit.edu'],True,computational-motion,2023-01-16,"['design', 'art', 'computer-science', 'creativity', 'culture']",Simulating Motion with Computation,PUBLIC,https://machinemotion.veravandeseyp.com/,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-06-23T17:23:06.122Z,"​In this new adjacent implementation of Photorythms as an interactive art installation, members of the public are invited to sit in front of the installation space and have their faces detected and augmented into different generative forms and patterns live and in real-time. This led to the creation of a fun, explorative art installation in which people played with their facial expressions in a sort of two-way performance, communication, and “dance” with the software system in response to witnessing their detected faces and portraits being chopped and spliced into randomized visual forms.​",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/09/08/Screenshot 2023-09-08 at 4.58.07 PM.png,400236,,2023-10-02T18:44:18.453Z,['chelsi@media.mit.edu'],True,photorythms-interactive,2023-06-23,"['art', 'computer-science', 'interactive']",Photorythms: Interactive,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-07-07T20:48:51.155Z,"OverviewWe found that vegan and vegetarian labels commonly found on menu items have a significantly negative impact on consumer's likelihood to choose these menu items.These findings are from randomized controlled experiments that we conducted within our community and then expanded to a larger sample of US participants from surrounding states.We also did not find removing these labels on menus would negatively impact vegans/vegetarians by leading them to accidentally choose options with meat.Our experiment suggests these labels should be removed from menus to normalize and encourage vegetarian and vegan eating, which is more environmentally sustainable  - overall these labels do more harm than good.MotivationFood systems have an important impact on environmental resources and are globally responsible for a third of greenhouse gas (GHG) emissions. Reducing consumption of animal products is a critically important challenge in efforts to mitigate the climate crisis. Despite this, meals containing animal products are often presented as the default versus more environmentally sustainable vegetarian or vegan options. What we didWe tested whether vegetarian and vegan labels on menu items negatively impact the likelihood of US consumers choosing these items by using a between-subjects experimental design, where participants chose a preference between two items.  Menu items were presented with titles and descriptions typical at restaurants, and a random group saw ""vegan"" or ""vegetarian"" labels in the titles of one of the two items. We conducted field studies at the MIT Media Lab, where people selected what to eat via event registration forms. The methodology was extended to an online study, where US consumers selected what to hypothetically eat in a series of choice questions. ResultsOverall, results showed the menu items were significantly less likely to be chosen when they were labeled, with much larger effects in the field studies, where choice was not hypothetical. In addition, the online study showed male participants had a significantly higher preference for options containing meat versus other participants. Results did not indicate the impact of labels differed by gender. Furthermore, this study did not find that vegetarians and vegans were more likely to choose items with meat when the labels were removed, indicating that removing labels did not negatively impact them. The results suggest removing vegetarian and vegan labels from menus could help guide US consumers towards reduced consumption of animal products.This experiment was approved by the Massachusetts Institute of Technology institutional review board (IRB): protocol 2203000615.",,"['city-science', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/07/07/Screenshot 2023-07-07 at 4.49.51 PM.png,400903,,2023-10-11T15:51:18.430Z,"['aberke@media.mit.edu', 'kll@media.mit.edu']",True,v-labels,2022-03-01,"['communications', 'environment', 'food', 'sustainability']",The negative impact of vegetarian and vegan labels: Results from randomized controlled experiments,PUBLIC,http://aberke.com,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2023-07-11T21:07:42.540Z,"AbstractAs conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person’s mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI’s inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user’s mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced. ",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/10/02/nature-nmi-2.png,401047,,2023-10-16T00:15:53.438Z,"['rliu34@media.mit.edu', 'patpat@media.mit.edu', 'pattie@media.mit.edu']",True,beliefs-about-ai,2023-07-11,"['human-computer-interaction', 'artificial-intelligence', 'cognition', 'human-machine-interaction', 'social-science', 'machine-learning', 'behavioral-science', 'computer-science', 'technology', 'mental-health']","Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy, and effectiveness",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-07-18T15:20:46.559Z,"Ultrasound   is   widely   used   for   tissue   imaging   such   as   breast   cancer   diagnosis;   however, fundamental challenges limit its integration with wearable technologies, namely imaging over large-area curvilinear organs. We introduced a wearable, conformable ultrasound breast patch (cUSBr-Patch) that enables standardized and reproducible image acquisition over the entire breast with less reliance on operator training and applied transducer compression.  A nature-inspired honeycomb-shaped patch combined with a phased array is guided by an easy-to-operate tracker that provides for large-area, deep scanning and multi-angle breast imaging capability. The clinical trials reveal that the array using a piezoelectric crystal [Yb/Bi-Pb(In1/2Nb1/2)O3-Pb(Mg1/3Nb2/3)O3-PbTiO3] (Yb/Bi-PIN-PMN-PT) exhibits a sufficient contrast resolution (~3 dB) and axial/lateral resolutions of 0.25/1.0 mm at 30 mm depth, allowing the observation of small cysts (~ 0.3 cm) in the breast. This research develops a first-of-its-kind ultrasound technology for breast tissue scanning and imaging which offers a non-invasive method for tracking real-time dynamic changes of soft tissue.",,"['conformable-decoders', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/07/18/3.jpg,401344,,2023-09-04T17:53:53.975Z,"['wenyadu@media.mit.edu', 'linzh@media.mit.edu', 'c_marcus@media.mit.edu', 'dsadat@media.mit.edu', 'canand@media.mit.edu']",True,conformable-ultrasound-breast-patch-for-deep-tissue-scanning-and-imaging,2023-07-18,"['bioengineering', 'civic-technology', 'health', 'sensors', 'imaging', 'technology', 'healthcare', 'medicine', 'biomedical-imaging', 'microfabrication']",Conformable Ultrasound Breast Patch (cUSBr-Patch) for Deep Tissue Scanning and Imaging,PUBLIC,https://conformabledecoders.media.mit.edu/people/canand.html,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-07-20T17:20:13.500Z,"A modular, membrane protein design based biomimetic sensing system named RESENSA (Receptor S-layer Electrical Nano Sensing Array) can transduce biological response to electrical outputs, and enable multiplex sensors for tailored diagnostics.The RESENSA sensing device has an array of 200 individual sensors with QTY code designed water-soluble CXCR4 receptors that are anchored on an S-layer protein 2D crystal lattice with 100% binding sites upward exposing to external molecules. The S-layer protein 2D crystal lattice not only serves as the anchor, but also serves as a soft carpet to keep CXCR4 receptor proteins stable. The sensor has a density of ~4 trillion per square centimeters.",,"['molecular-machines', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/07/20/unnamed.jpeg,401426,,2023-09-04T17:51:32.880Z,"['ruiqing@media.mit.edu', 'shuguang@media.mit.edu']",True,receptor-s-layer-electrical-nano-sensing-array-resensa,2023-07-20,"['health', 'sensors', 'biology', 'technology', 'cells', 'nanoscience']",Receptor S-layer Electrical Nano Sensing Array (RESENSA),PUBLIC,,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-07-20T18:16:27.029Z,"What would it be like if we could see our movement? Illuminate (2023, Chelsi Alise Cocking) is an interactive art installation in which the movements of a person through open space are visually augmented and brought to life in front of them in real time through custom-coded interactive visualization software. Seamlessly merging physical and digital space, Illuminate submerges us into an artificial reality in which our usually unseen paths of movement become visible– seemingly levitating in space. This aims to give us a visceral yet magical moment in which we can see, interact with, and play with our once invisible wakes of motion—pushing the boundaries of our senses and making the invisible visible. The project also explores the themes of spatial computing, bodily expression, abstraction, and choreographic interfaces.",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/07/22/_MG_6367.jpg,401436,,2023-09-08T21:16:23.994Z,['chelsi@media.mit.edu'],True,illuminate,2023-06-18,"['design', 'human-computer-interaction', 'art', 'computer-science', 'interactive']",Illuminate,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-07-25T19:39:26.800Z,"The NASA Science Mission Directorate (SMD) Science Activation (SciAct) program connects NASA Science with diverse learners of all ages in ways that activate their minds and promote a deeper understanding of our world and beyond, with the ultimate Vision: To increase learners’ active participation in the advancement of human knowledge.  The MIT Media Lab provides systems integration and analysis for the overall program.  The impacts of the SciAct program in 2022 include 48 teams collaborating to facilitate more than 50 million learner interactions in the United States and 1/4 million across the globe.  This $50 million program creates impact and value that far exceed its annual expenditures, improving coordination across NASA Science Mission activities and allowing for the increasingly efficient, effective, and sustainable use of SMD Science discoveries and experts for engaging learners.",2025-10-31,['media-lab-research-theme-future-worlds'],https://dam-prod2.media.mit.edu/x/2023/07/26/2023 NASA Earth Day Image_trVLeLF.jpg,401616,,2023-09-04T17:48:37.866Z,['rachelbc@media.mit.edu'],True,nasa-science-activation-program,2020-12-01,[],NASA Science Activation Program,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2023-07-25T20:36:55.271Z,"HexSense is a kind of sensor node that can be ballistically deployed to the desired area and collect data of interest. Each sensor node is a modular design and can carry different sensor payloads.After being deployed on the ground, the sensor node can automatically stand up and make sure the body of the sensor node stands off the ground which guarantees a good line of sight for wireless connections.",,"['responsive-environments', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/uuid/9c0ead58-1a84-47d0-8143-65a41399a216,401626,,2023-10-18T22:29:20.776Z,['fzliu@media.mit.edu'],True,hexsense,2023-07-01,[],HexSense,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0
False,2023-08-01T14:22:24.353Z,"Parallelized Architecture for Scalably Executing smart Contracts (PArSEC)AuthorsJames Lovejoy, Federal Reserve Bank of BostonAnders Brownworth, Federal Reserve Bank of BostonDr. Madars Virza, MIT Media LabDr. Neha Narula, MIT Media LabPArSEC is the result of the DCI’s latest research on flexible technical architectures for centralized digital currency at scale. PArSEC supports a wide class of smart contract runtimes, including the Ethereum Virtual Machine. It supports flexible research and testing: the system can support clients easily updating their contracts’ logic, or even switching to different smart contract runtimes, without having to understand and alter the core architecture of the underlying transaction processor. Our research focused on smart contracts because they provide the highest degree of expressivity and functionality to users. In addition to a research paper and executive summary, PArSEC is free and open source software, publicly available on GitHub. We invite researchers and policymakers to work with us to develop the system further.",2024-08-01,"['digital-currency-initiative-dci', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2023/08/01/PArSEC Logo 3 Final  7312023.png,401907,,2023-09-04T17:44:13.580Z,"['madars@media.mit.edu', 'narula@media.mit.edu']",True,parsec,2023-08-01,[],PArSEC: Executing Smart Contracts in Parallel,PUBLIC,https://dci.mit.edu/parsec,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2023-08-03T21:10:45.093Z,"A robust and resilient communication infrastructure is essential to the development of democratic processes and knowledge formation. Decentralized systems have the potential to enable and defend a broad spectrum of constructive speech, if they are able to provide the necessary mechanisms to counteract attacks on the system such as from AI, private interests, and state controlled influence campaigns. How might we design decentralized systems to deal with these issues? Can they avoid context collapse and promote normative coherence, but still allow space for evolving ideas and constructive debate?Decentralized sociotechnical systems can enable scaled cooperation across communities, from the beautifully small to the formidably global. In order to do so effectively, we must be able to design systems for identity, reputation, moderation, and governance. Project Odessa builds on theoretical designs and provides a mobile app sandbox for such experimental implementations. This work-in-progress allows researchers to empirically evaluate strategies for constructive communication and social cooperation.",,"['center-for-constructive-communication', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/uuid/cb8a974b-d7f0-4466-a4fd-80b4cd22474e,402043,,2023-10-20T02:21:52.309Z,['wesc@media.mit.edu'],True,decentralized-social-systems,2023-04-28,[],Decentralized Social Systems,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2023-08-04T16:07:39.208Z,"At the MIT Media Lab, our aim is to influence and improve human lives. The proliferation of mobile devices and wide connectivity, as well as the availability and growth of machine learning, crowdsourcing, Big Data, and the Internet of Things offer new possibilities for research and development. In both developed and Emerging Worlds, there is a great opportunity for game-changing innovations that will impact billions.BOTTOM UP, CO-INNOVATIONWe have built a platform that brings together technical experts, innovators, academic institutions, implementation collaborators and progressive corporate collaborators that will allow us to solve the most pressing challenges within India, and other developing countries. The focus is on finding solutions that help serve the ""The Next 5 Billion"" citizens by:Identifying community specific challenges that may have solutions that could also be applied to multiple locations.Providing mentoring, training and technical support from corporate collaborators, the MIT Media Lab, and other leading institutions to assist innovators creating novel technical solutions for rapidly emerging and pop-up cities.Pairing innovators with implementation collaborators on the ground to ensure that each solution responds to real needs and enters the market with speed and scale.",,"['camera-culture', 'media-lab-research-theme-decentralized-society']",,402084,,2023-09-04T17:43:42.180Z,['raskar@media.mit.edu'],False,emergingworld,2023-08-04,[],Emerging World,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2023-08-08T08:27:26.843Z,"CircuitScout is a DIY probe system designed to simplify the process of testing PCBs, by automatically probing desired signals. Through an intuitive web-based GUI, users can upload their KiCad files and have the system automatically navigate and probe their PCBs. With the help of CircuitScout, all you need to do are selecting signals from the schematic and checking the output.",,"['responsive-environments', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/08/08/PCBPT1.jpg,402239,,2023-09-04T17:42:54.989Z,['fzliu@media.mit.edu'],True,circuitscout,2023-07-01,[],CircuitScout,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2023-08-08T20:41:26.311Z,"This series of artwork repurposes the movement of light as the basis for sculpture, and investigate how architecture meets the natural world.  I explore the juncture between the impermanent and the permanent as a means of reflecting on architectural history.",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/10/12/Combined Image.jpg,402251,,2023-10-12T18:48:18.924Z,['dexterc3@media.mit.edu'],True,light-sculpture,2023-08-08,"['design', 'computer-vision', 'architecture', 'creativity', 'code']",Light Sculptures,PUBLIC,https://dextercallender.com,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1
False,2023-08-16T22:07:59.663Z,"Have you ever needed a single-line font rather than an outline of a typeface? For instance in the use of digital fabrication processes such as laser cutting, embroidery, or CNC milling?SkeleText is an open-source creative tool that translates the character of any given font into a skeletal form through a series of geometrical transformations. ",2030-03-03,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/8353d2e9-2188-47b8-b78f-7d901ba651a0,402567,,2023-09-26T14:07:21.242Z,['veraseyp@media.mit.edu'],True,skeletype,2023-02-01,"['design', 'art', 'computer-science', 'creativity', 'culture']",SkeleText: Skeletonization of Typefaces,PUBLIC,https://veravandeseyp.com/skeletext/,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-08-31T15:11:49.114Z,"Individual mobility, both within cities and across cities, significantly impacts various societal aspects, such as well-being, the spread of epidemics, and environmental quality. Commuting flow, a crucial and predictable aspect of human movement, is typically monitored by national statistical agencies. However, relying solely on existing statistical data is insufficient, as it may suffer from issues of timeliness and data completeness.In cases where mobility flow information is unavailable for a specific region of interest, traditional methods resort to mathematical-physical models, like the gravity model and the radiation model, to generate the required data. The gravity model is widely used to predict flow patterns, but it depends on adjustable parameters that vary from region to region and may have known analytical inconsistencies. The radiation model introduces a stochastic process to capture local mobility decisions, enabling the analytical derivation of commuting and mobility fluxes based solely on information about population distribution. It has the advantage of being interpretable by design and addresses many of the problems of the gravity model. However, it also has some limitations: (1) It relies on a restricted set of variables, usually just the population and distance between locations, neglecting crucial information about the geographical landscape such as land use, points of interest (POIs), and the transportation network. (2) It assumes the determinants of flow are based on a homogeneous ideal space, while in reality, human mobility is influenced by multiple factors, including transportation conditions, topography, land use, and their distribution patterns. As a result, the distances traveled in different directions from the origin may vary when the time cost of travel is constant, leading to an inability to accurately capture the structure and greater variability of real flows.To address these challenges, it becomes necessary to use more detailed input data and more flexible models to generate realistic mobility flows. This can be achieved by extracting a rich set of geographical features from freely available online data and using powerful nonlinear models like deep artificial neural networks to model complex spatial relationships, including the spatial heterogeneity mentioned earlier. Existing research, like deep gravity, has attempted to improve the gravity model using deep learning techniques, but it fails to overcome the fundamental limitations that the radiation model addresses.To overcome these issues, this study introduces a transformer-based deep learning framework that neuralizes the radiation model. Specifically, we demonstrate that transformers are more suitable than graph neural networks (GNNs), a mainstream deep learning backbone, for existing deep learning-based flow generation solutions. Furthermore, we design a positional encoding module tailored to geographical relationships, utilizing radial basis functions to encode relative and absolute distances between locations and using spatial heterogeneity as a supervisory signal to guide model training. Our experiments, conducted on mobility flows across several states in the US, show that our framework significantly improves performance, especially in densely populated regions, compared to traditional mathematical-physical models and state-of-the-art deep learning models. Additionally, our model exhibits remarkable generalization abilities, generating realistic flows even in geographical areas lacking training data and achieving inter-state generalization. Finally, we demonstrate how the flows generated by our model can be explained in terms of geographic features. We identify significant differences among the three states considered, interpreting the model's predictions using explainable AI techniques.",,"['city-science', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/uuid/6a5efdb2-a896-4ace-af50-c8fdb95bb7b3,403218,,2023-09-04T17:37:22.126Z,"['doorleyr@media.mit.edu', 'kll@media.mit.edu', 'yluo97@media.mit.edu']",True,commuting,2023-02-01,"['artificial-intelligence', 'social-science', 'data-science']",Commuting Flow Generation,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-09-05T17:40:14.562Z,"Van-Atta Acoustic Backscatter (VAB) is the first technology for ultra-low-power underwater networking and communication, which can transmit signals across kilometer-scale distances. The technology builds on a technique we invented—called underwater backscatter—which communicates at 1 million times less power than existing underwater communication methods. The newest generation of our technology is the first that can communicate at ultra-low power and over distances long enough to support practical applications in coastal monitoring, underwater climate change monitoring, weather prediction, disaster response, aquaculture, and much more.How does it work? Underwater backscatter nodes communicate by reflecting underwater acoustic signals. One challenge with our earlier generations is that they were not very efficient: they reflected signals in all directions. The key innovation in our newest design is that it can retrodirect its signal. Specifically, if sound arrives from a specific angle, it is re-transmitted back towards that same angle that it arrived from—this is retrodirectivity. The special configuration of piezoelectric transducers is known as a Van Atta array, and it sends incoming sound back in the direction that it came from instead of towards undesired directions, regardless of the array's orientation. This property enables the system to focus sound energy where it matters most, allowing the retroreflected sound to travel more efficiently and across longer distances. The operation of Van-Atta Acoustic Backscatter (VAB) is shown in the figure below.",,"['signal-kinetics', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/09/05/VAB_animation_eDl534u.gif,403306,,2023-09-06T15:02:42.366Z,"['fadel@media.mit.edu', 'allam@media.mit.edu', 'prwang@media.mit.edu', 'wakbar@media.mit.edu', 'jradema@media.mit.edu', 'alineeid@media.mit.edu']",True,ocean-connectivity-at-ultra-low-power,2023-09-05,"['communications', 'ecology', 'networks', 'sensors', 'electrical-engineering', 'internet-things', 'ocean', 'climate-change', 'wireless']",Ocean Connectivity at Ultra-Low Power,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-09-08T21:23:58.027Z,"Can computer graphics algorithms be taught through drawing?In this prototypical activity book, we present the steps of four computer graphics algorithms in a fun, lightweight, friendly series of drawing activities. Our goal is to create an introduction to highly technical topics in an inviting way, encouraging the view of computation as a creative medium and spreading understanding of computational craft and thinking in an approachable form.",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/09/09/Screenshot 2023-09-09 at 2.54.06 PM.png,403421,,2023-10-03T14:01:53.893Z,['chelsi@media.mit.edu'],True,thinking-like-a-computer-drawing-activity-book-prototype,2022-06-01,"['learning-teaching', 'computer-science']",Thinking like a Computer: Drawing Activities (Prototype),PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-09-19T21:11:13.956Z,"Analogia is a game for sharing honest and sometimes challenging stories using a combination of talking prompts and absurd images. The cards and images used in the game are specially generated by AI using free-text data collected about the community before the game begins. By using text-to-image and large languages models to create a customized conversational game space, Analogia provides a novel and fun structure for communities to use AI to trigger conversations about the complex emotional issues impacting them.  ",,['center-for-constructive-communication'],https://dam-prod2.media.mit.edu/x/2023/09/19/analogia_tablesetup_artbeat.JPG,403577,,2023-10-12T20:30:53.146Z,['cass_lee@media.mit.edu'],True,analogia,2023-06-01,[],Analogia,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0
False,2023-09-25T12:59:39.396Z,"SEI Zero-G Outreach OverviewThe MIT Space Exploration Initiative Prototyping our Sci-Fi Space Future: Designing & Deploying Projects for Zero Gravity Flights (MAS.838 / 16.88) course on project development, prototyping, and deployment readiness for parabolic flights, culminates in an annually chartered research flight with Zero-G. In 2021, as part of SEI's commitment to democratize the future of space exploration, the SEI Zero-G Outreach program (0G-O) was developed; an outreach program to engage with the Cambridge Public School’s (CPS) grade 7 classes to teach them about research in microgravity, parabolic flight, and experiment design. The goal of the outreach program is to inspire the next generation of space scientists and engineers using virtual reality. The OG-O is now in it's third year of operation.The CPS grade 7 curriculum covers ‘Mysteries of the Universe’ where students consider the role of gravity in the solar system. They also complete a unit on roller coasters in which they learn about key ideas of force, motion, and energy through the context of roller coasters. The 0G-O ties these two units together through video content developed by SEI teaching students the basics of parabolic flight and how we perceive gravity, gravity and its function in the universe, why and what we can study in microgravity, and how to design an experiment for microgravity. The students then design their own experiments for microgravity creating a formal payload proposal to be reviewed by students at MIT. These are down-selected to the top 5 experiments by the MIT students.  These top 5 are provided with supplies to build their experiments which they then demonstrate to an astronaut.  The astronaut then selects the top experiment which flies on the MIT chartered Zero-G parabolic flight. The flight and the experiment are filmed in VR video. Each participating school is provided with an Oculus Quest 2 VR headset and a flight day is held at each school where the students experience the Zero-G flight and see the results of their experiment in an immersive VR environment.  The 0G-O program is run annually in parallel with SEI's parabolic flight and is currently in it's third year of operation.",,['space-exploration'],https://dam-prod2.media.mit.edu/x/2023/09/25/Zero-G_mix.jpg,403625,,2023-10-10T13:35:14.492Z,['cpaige@media.mit.edu'],False,mit-zero-g-outreach-initiative,2023-09-25,[],SEI Zero-G Outreach,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-09-25T21:05:04.094Z,"Digital ethnography and large-scale urban sensing to analyze distress in informal settlementsOne of the problems we encountered is being able to track points of unrest in the community in real time. Furthermore, we do not want to take people away from their path and sensations when walking, but rather to allow them to inform us of their perception in a natural way. That's why the sensor has a button that you carry in your hand, and it simply activates when you feel unsafe. It also has GPS and lighting sensor. These data are intended to be correlated to see what time of day and where the person feels unsafe.The sensor was used for 10 days and a group of women tested it. The results show clear areas of distress.Subsequently, several workshops were held with the community to ensure that the data was correct about the areas considered most unsafe, and to test our assumptions.Later we developed an application to scale the sensor and be able to collect broader data and in different locations.",2023-12-15,['city-science'],https://dam-prod2.media.mit.edu/x/2023/09/25/SENSOR.001.png,403638,,2023-10-03T17:26:28.991Z,"['leticiai@media.mit.edu', 'aricom@media.mit.edu']",False,what-public-spaces-do-we-dislike-designing-wearable-sensing-technology,2023-01-01,[],What spaces do we dislike? Wearable sensing for location-based collective stress in urban environments,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-09-27T03:53:36.182Z,"Dealing with rapidly spreading wildfires is a complex task that involves the coordination of multiple government agencies and on-site crews. We  explore how AI can help improve our decision making in these complex and uncertain dynamic environments. To this end we are developing ""Wildfire Whisperer,"" an AI agent that uses a Dual-System Machine Learning approach to learn from its users and in turn help them to make more robust decisions based on deep knowledge combined with quick responses.  We are exploring combining the visualization and AI the agent within Unity to make a stand-alone system.",,"['viral-communications', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/09/26/FireSimulation Picture.png,403673,,2023-10-18T20:46:16.236Z,"['mhjiang@media.mit.edu', 'lip@media.mit.edu']",True,wildfire-whisperer,2023-09-01,[],Wildfire Whisperer,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2023-10-01T15:48:06.231Z,"Organology: Real-Time Biometrics x Sound Synthesis October 19th @ 8:00 PMMIT Media Lab 3rd Floor AtriumWhat role does the body play in our encounters with music? Bringing together artists, designers, engineers, and scientists at MIT and beyond, Organology taps into the physiological and personal dimensions of fundamental questions about music and the body. In a semi-improvised musical performance, ensemble members debut the BioSynth, a novel synthesizer whose parameters are modulated by the real-time biometrics of the performers. Over the course of the performance, ensemble members come to represent a dynamic artistic body, one that emerges from mindful musical collaboration and latent physiological resonance. The performance will be followed by a light reception and talkback with the artists. CollaboratorsManuel Cherep (G), co-directorJessica Shand (G), co-directorBahar Badieitabar (Berklee ‘23), oudDani Beck (Berklee ‘23), cello/voiceDexter Callender III (G), tenor saxophoneRafael Moises Heredia Horimoto (Berklee ‘27), percussionMike Jiang (G), keyboard/keytarVeronica Leahy (Harvard/Berklee ‘23), soprano saxophone/clarinetAndrew Li (MIT ‘25), alto saxophoneSam Chin (G), magic effectsChris Lock (Harvard GSAS ‘24), electronics (Max/MSP)Ana Schon (G), live sound Nikhil Singh (G), electronics (Max/MSP)Cassandra Lee (G), moderator AcknowledgementsContributions by Alfonso Parra Rubio (G), Jocelyn Riseberg Scheirer (G), Patrick Chwalek (G), Philip Tan (MIT Game Lab), and Levy Lorenzo helped make possible the technical components of this project. Jimmy Day (ML Comms) and Kimaya Lecamwasam (G) provided generous assistance with visual documentation of the process and performance. Thanks also to Andrea Volpe and Lydia Brosnahan (MIT Office of the Arts), Prof. Miguel Zenon, Fred Harris, Keala Kaumeheiwa, and Riley Vogel (MTA), Prof. Rosalind Picard/Affective Computing, Prof. Tod Machover/Opera of the Future, Prof. Zach Lieberman/Future Sketches, and Visiting Prof. Alaa Algargoosh (ML), Bill Lombardi (ML Facilities), and Clémence Taillandier (ML). This project has been generously funded by a project grant from the Council for the Arts at MIT (CAMIT).",2023-12-31,"['opera-of-the-future', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/10/11/Insta-1.png,403739,,2023-10-21T18:25:16.567Z,"['mcherep@media.mit.edu', 'shand@media.mit.edu']",True,organology,2023-01-01,"['cognition', 'music', 'affective-computing']",ORGANOLOGY: Real-Time Biometrics x Sound Synthesis,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-10-03T20:52:48.547Z,"Despite living in an increasingly connected world, loneliness and social isolation are prevalent issues today. While social robots have been explored as tools to enhance social connection through companionship, their potential as asynchronous social platforms for mediating empathetic exchanges between humans has received less attention. In this work, we introduce a one-on-one social support companion that facilitates the exchange of emotionally relevant stories and employs narrative therapy techniques to enhance feelings of connection and belonging. We investigate how the robot’s delivery impacts connection and empathy – specifically how the robot serving as a social proxy facilitating human stories, passing stories from other human narrators to the user, versus adopting stories as its own impacts interaction outcomes. To this end, we conduct a real-world deployment of 40 robot stations in users’ homes and find that participants have significantly higher compassionate love towards humanity and empathize more with stories told by a robot that serves as a social proxy as opposed to adopting stories as its own. Our work contributes to the development of social robot systems that serve as social platforms to enhance human-human empathy and connection.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/10/03/study(1).png,403810,,2023-10-04T17:00:09.096Z,"['haewon@media.mit.edu', 'sharifah@media.mit.edu']",True,social-robots-for-social-connection,2023-10-03,[],Social Robots for Social Connection,PUBLIC,,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0
False,2023-10-03T20:58:28.711Z,"Empathy is a driving force in our connection to others, our mental well-being, and our resilience to challenges. The human capacity for empathy extends beyond people; Research shows that we can empathize with characters in literature and even with inanimate or artificial systems. With the rise of generative AI systems that interact with us in daily life, such as ChatGPT, it is important to understand how empathy unfolds toward stories from human vs. AI narrators and how empathy might change when the author of a story is made transparent to users. In this work, we conduct 4 crowd-sourced studies with N=985 participants to understand how and why empathy shifts across human-written vs AI-written stories. To this end, we trained a model on stories annotated with empathic similarity relationships (ie. two narrators empathize with one another), and used this model to retrieve stories users might empathetically resonate with in response to their own personal stories. We compared across conditions where the author of the story is made transparent to users and discuss ethical implications of fostering empathy towards AI as well as the role of deception in this phenomenon. We find that participants consistently empathized less with retrieved AI-written stories whether they know the author is an AI or not, but participants showed significantly greater willingness to empathize with AI when the author of a story is disclosed.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",,403818,,2023-10-04T17:01:11.191Z,"['safinah@media.mit.edu', 'dipaola@media.mit.edu']",True,disclosing-ai-authors-shifts-attitudes-and-empathy-towards-artificial-experiences,2023-10-03,[],"Disclosing AI Authors Shifts Attitudes and Empathy Towards Artificial ""Experiences""",PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1
False,2023-10-05T03:15:13.791Z,"The Clubhouse Network has a long history of activism and advocacy. This new initiative aims to enhance and amplify the work already being done in Clubhouses throughout the Network. It’s an opportunity to be explicit in the Network’s commitment to “valuing and centering the voices of young people who are the most impacted by the intersections of oppression, including, but not limited to, structural racism.” The program was proposed in 2020, shortly after George Floyd was killed by the Minneapolis Police Department. There are five Clubhouses located in Minneapolis. It was evident that young people who attend Clubhouses are impacted by social justice issues in their community and that they have a desire to create change. The young people in Clubhouses throughout the world are impacted by many intersecting issues by proximity. One of the goals of the program is to create an infrastructure that supports youth in activism projects/initiatives on an ongoing basis. Young people at individual Clubhouses throughout the Network have produced high-quality and well-researched projects with the goal of making community members aware of social issues. The expertise and creativity that go into their awareness campaigns have the potential to create systemic change.We are currently supporting the following 10 sites:YWCA Canberra Clubhouse, Australian Capital Territory, AustraliaEducation about identity, sex, gender, and sexual orientation in an effort to combat transphobia and homophobia in schools. Redwood City Boys & Girls Clubs of the Peninsula, Redwood City, CA Undocumented immigrant rights. Summit Academy OIC,  Minneapolis, MNTransportation safety La Chorrera Clubhouse, La Chorrera, PanamáEnvironmental justiceEden Youth & Family Center, Haywood, CAGentrificationThe Best Buy Teen Tech Center @ PHASE 4 Learning Center, Pittsburgh, PAFood insecurityVSDEC South LA Best Buy Teen Tech Center, Los Angeles, CA.Food insecurityBrian Coyle Best Buy Teen Tech Center, Minneapolis, MNAdvocacy for youth and community at large.Al Bireh Youth Development Resource Center, Al-Bireh, PalestineYouth apathy in PalestineSci-Bono Clubhouse, Johannesburg, South AfricaEconomic disparities",,"['lifelong-kindergarten', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/10/23/IMG_0784.PNG,403879,,2023-10-23T17:59:45.269Z,['jaleesat@media.mit.edu'],True,youth-activism-and-advocacy,2022-08-01,[],Youth Activism and Advocacy,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-10-06T15:40:38.784Z,"Today's children will live in a world saturated with artificial intelligence. Understanding the key idea behind the functioning of intelligent systems will help them to have more agency and choice in that world. This is why we, together with a network of STEM preschools Astar Explorer, are interested in developing materials that introduce AI concepts to kids starting at the age of five.We think that robots (real or virtual ones) are excellent for introducing AI concepts, because children relate to them as characters. This allows to introduce all sorts of analogies  between AI and the child's own natural intelligence. It also brings a socio-emotional component to our designs.We are inspired by constructionist approach to learning, embodied in such designs as Scratch and Scratch Jr, where children learn in process of working on their own ideas, following their own passions. But we are also aware of the necessity of scaffolding (guidance), particularly with young learners. In this project, we experiment with delivering scaffolding via a story. Similar to the Microworlds project, we use the idea of ""close-started, open-ended"": the initial interaction is driven by a story, but in the end we hope to guide children towards open-ended interaction with our medium, so that they can use it to create stories of their own.Below is a prototype story that we developed introducing children to block-based programming. We intend to use the same format to explain such concepts as internal representations, reasoning, learning and artificial creativity.",,"['personal-robots', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/10/06/IMG_7299 (1).jpg,403901,,2023-10-06T16:09:26.546Z,['eadler@media.mit.edu'],True,teaching-ai-and-robotics-through-digital-storybooks,2021-09-01,[],Teaching AI and Robotics through digital storybooks,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-10-12T17:24:42.798Z,"HairIO uses hair-based sensors to detect and transmit data. HairIO explores the possible interaction space of fur, and it has the potential to be used as a sensor, actuator, and wearable tool for sharing different types of data. The electronic components are seamlessly integrated into the skin patches. The sensing and actuation modules could also be combined, so that each patch would have the ability to do both, and the paired patches could be separated from each other by greater distances.The Hairio interface also has the potential to be used as a kind of sensor that could collect data from a variety of situations. For example, it could be adapted to collect wind or other weather data and transmit it to someone inside, allowing them to feel the rain or snow without getting wet. It could also be used to subtly convey emotional data to those around you, which would otherwise be imperceptible to anyone but yourself.We envision this technology being used in a wearable telepresence device that mediates intimacy between two emotionally close but physically separated humans.",2021-04-01,['city-science'],https://dam-prod2.media.mit.edu/x/2023/10/12/HairIO.001_n47aNCk.png,404004,,2023-10-17T06:47:03.877Z,"['hrl@media.mit.edu', 'karstens@media.mit.edu', 'alicesh@media.mit.edu', 'liang_zp@media.mit.edu']",True,hairio-an-unobtrusive-non-verbal-communication-device,2021-04-01,[],HairIO: A Discreet Wearable Device for Realistic Tactile Sensation,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0
False,2023-10-12T20:02:59.159Z," Recreating John Maeda's ""Design By Numbers"" programming environment in JavaScript Try Online    |    GitHub",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/10/12/ezgif.com-optimize (4).gif,404020,,2023-10-19T13:55:22.968Z,['lingdong@media.mit.edu'],True,design-by-numbers-again,2023-10-12,[],Reviving Design by Numbers (1999): A Technical Writeup,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-10-16T16:25:11.055Z,"Wild bees are essential for food security and biodiversity, but they face rapid decline. One of the main threats is the commercialization of bumble bees, which introduces exotic species that compete and infect native ones. Bumble bees are key pollinators of crops and wildflowers worldwide, but we lack information on their distribution, health, and habitat.We aim to develop a noninvasive
system to monitor wild bee populations and understand their needs. Currently,
lethal collection is the standard to study and identify bee species. We propose
to use acoustic sensors and cameras to detect bumble bees and capture their
color patterns, which are unique for each species. This multi-modal system
could enhance conservation monitoring for bumble bees in their natural ",,"['responsive-environments', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/10/16/Screenshot 2023-10-16 at 12.28.09 PM.png,404086,,2023-10-17T17:51:20.500Z,['chwalek@media.mit.edu'],True,buzzcam,2023-06-01,[],BuzzCam,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0
False,2023-10-16T16:31:45.327Z,"Global biodiversity faces threats from climate change and human disturbance, which affect many species, especially large predators. These threats influence their movements, behaviors, and conflicts with humans. However, we lack data on how animals and their environments interact. This data is essential for guiding conservation efforts.We are developing CollarID, a device that tracks animals and their environment synchronously. It is small, lightweight, robust, and long-lasting, ideal for long and undisturbed deployments. The device collects various environmental data (e.g., sound, temperature, humidity, particles, gas, etc.) along with animal movement data. These data show the conditions the animals experience and the human activity around them. With CollarID, we can observe the animals’ movements, behaviors, and conflicts with humans in detail.",,"['responsive-environments', 'media-lab-research-theme-future-worlds']",https://dam-prod2.media.mit.edu/x/2023/10/16/Screenshot 2023-10-16 at 12.33.57 PM.png,404094,,2023-10-17T17:53:27.362Z,['chwalek@media.mit.edu'],True,collarid,2023-06-01,[],CollarID,PUBLIC,,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0
False,2023-10-16T18:35:45.871Z,"Door-to-door drone package deliveries are taking off across the US.  In this work we expose and study a newly emerging privacy issue. FAA safety regulations require drones to broadcast their locations with an identifier (Remote ID). This allows any third party to track drones from vendors where they pick up products to customers' location, easily linking customers with products ranging from take-out food to prescription medications.In our research ""Routing with Privacy for Drone Package Delivery Systems"" we first described this risk. We then developed routing strategies to mitigate the risk and studied their impact on efficiency.  Our research paper was presented at the 2022 International Conference on Research in Air Transportation (ICRAT) and won a best paper award.https://www.media.mit.edu/publications/routing-with-privacy-for-drone-package-delivery-systems/",,"['city-science', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/uuid/3651a126-3529-4121-99d0-65fedf215d9a,404105,,2023-10-17T13:59:28.470Z,"['aberke@media.mit.edu', 'kll@media.mit.edu']",True,drone-package-delivery-and-consumer-privacy,2023-10-16,"['robotics', 'data', 'privacy', 'transportation']",Drone package delivery and consumer privacy,PUBLIC,http://aberke.com,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2023-10-19T00:22:50.604Z,"Through an embodied, spatial audio experience, Collective Echos makes tangible a living library of human experiences.In the face of intensifying social fragmentation and collapse of shared reality, we look to ancient wisdoms such as small-group dialogue and sensemaking to inform the design of a novel, AI-supported pro-social communication space: the social dialogue network. Here, we gather communities in facilitated, constructive conversation and equip them with visualizations and machine learning-backed insights that empower them to “make sense” of their collective experience. Throughout, participants maintain agency over their voice. Structures which provide the freedom and protection for each individual to dictate how and within which spheres their stories are shared enable online and in-person gatherings where all can be brave not loud. This partnership between cutting-edge and ancient social technologies enables us to nurture agency and trust in civic life, and, ultimately, bolster values core to a functioning democracy.Collective Echos shepherds us towards an understanding of the value of personal narrative in engendering trust and how we surface underheard voices with privacy and authenticity. We invite visitors to listen and explore the breadth of our library through accompanying visualizations. This network of stories makes space for complex, empathetic interactions that we hope inspire visitors to participate in this intimate system firsthand.Collective Echos is a spatial audio and data visualization exhibition debuted at Ars Electronica from September 6-10, 2023.",,"['center-for-constructive-communication', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/10/18/AL0A0277.JPG,404231,,2023-10-19T16:05:42.775Z,"['naanaom@media.mit.edu', 'mhughes4@media.mit.edu', 'cass_lee@media.mit.edu', 'dkroy@media.mit.edu']",True,collective-echos,,"['design', 'social-networks', 'human-computer-interaction', 'art', 'artificial-intelligence', 'social-science', 'language-learning', 'community', 'visualization', 'data-visualization', 'natural-language-processing']",Collective Echos,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2023-10-19T03:34:45.327Z,"Be My Guest! is currently on display at the Vantaa Art Museum, Finland until December 3, 2023 as part of their ""SYSTEMS"" exhibition.At the intersection between art, architecture and research, Be My Guest! explores the spatial, cultural and social relationships of design, new media technologies and AI in one of the most common and oldest gathering rituals in our everyday lives: a dinner with friends where AI becomes the host, co-creator of the spatial experience, and co-author of the spatial menu. Every dish becomes a surreal experience, with AI co-designed objects staging the physical environment and hiding hybrid affordances yet to be discovered. The project situates itself in the speculative future of living and proposes a mixed-reality dinner event, combining spatial storytelling, AI as a social companion, and posthuman theories to open a ground for discussion about the emerging socio-cultural role of artificial intelligent models and their integration through new media technologies in our everyday social scenarios for the future of our livable domains. what happens “when species meet”?Everything in Be My Guest! from the unique tableware and food to the virtual reality spaces are created using generative AI models, with the host of the dinner event also being a generative model.",,"['fluid-interfaces', 'media-lab-research-theme-life-with-ai', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/x/2023/10/18/ME&OTHERME_IMAL_EDIT_byRI_17.jpg,404241,,2023-10-19T16:13:40.296Z,['vdanry@media.mit.edu'],True,be-my-guest-a-dinner-event-hosted-by-artificial-intelligence,2023-10-18,[],Be My Guest! A Dinner Event Hosted by Artificial Intelligence,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1
False,2023-10-19T14:09:37.523Z,"Using natural language processing and machine learning, we discover emergent themes and patterns within our dialogue data set. Exploring how those relate to human-generated themes and patterns, we can identify how various means of sensemaking relate to one another. Further, by analyzing the dialogue as a whole, we are able to identify relationships between themes temporally and visualize how conversations unfold overtime to help interpret and understand conversations and communities. ",,"['center-for-constructive-communication', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2023/10/20/photo_one_best_hn2BL4u.jpg,404250,,2023-10-20T19:58:44.625Z,"['mhughes4@media.mit.edu', 'dsjen@media.mit.edu', 'bcroy@media.mit.edu']",True,conversation-chains,2023-03-01,[],Conversation Chains,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-10-19T17:09:37.440Z,"Gastronomy and Beyond (G&B) aims to unite individuals who contribute to innovation in the realms of culture, products, territories, sustainability, and community. Our mission is to expand horizons, foster creativity, and explore a myriad of concepts. ENCOUNTERS - BEYOND VIRTUAL21 September, 12pm EDTChef Eneko Atxa -Chef Owner Azurmendi - 3 Michelin StarEneko understands the gastronomy as a universal language, full of dialects which he uses to enhance the value of his territory, his culture and its roots through innovation, memory, and multidisciplinary cooperation of the people around him and those he encounters in his trips around the world. From this starting point was born his first project, Azurmendi, and the rest that have followed.",,"['city-science', 'media-lab-research-theme-future-worlds', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/10/19/Screen Shot 2023-10-19 at 1.14.05 PM.png,404276,,2023-10-23T13:41:31.978Z,"['alonsolp@media.mit.edu', 'leticiai@media.mit.edu', 'kll@media.mit.edu']",True,gastronomy-and-beyond,2023-10-19,[],Gastronomy And Beyond,PUBLIC,,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-10-19T19:22:51.288Z,"FrameGrabber is a simple, browser-based tool that lets the user upload an animation and convert it to a grid of stills.",,"['future-sketches', 'media-lab-research-theme-cultivating-creativity']",https://dam-prod2.media.mit.edu/uuid/7ec6edaf-3c37-44de-a21e-a03d241a1c32,404284,,2023-10-20T15:25:31.822Z,['veraseyp@media.mit.edu'],True,framegrabber,2023-02-01,"['design', 'computer-science']",FrameGrabber: Video to stills,PUBLIC,https://veravandeseyp.com/,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0
False,2023-10-19T22:30:23.419Z,"We further explore conversation visualization as a method of extracting and translating meaning for third party audiences who wish to better understand the interests of conversation participants. In this theme sort, the durations of participants’ contributions begin to illuminate dominant topics across conversation collections, a jumping off point for further analysis and evaluation of our conversation structure, facilitation, and participant dynamics.",,"['center-for-constructive-communication', 'media-lab-research-theme-future-worlds']",,404292,,2023-10-20T15:27:09.430Z,['naanaom@media.mit.edu'],True,realtalk-themes,,[],Conversation Theme Sorting,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-10-20T16:46:17.605Z,"We examine whether and how the speed of spread and decay of news events differ between social media and broadcast media based on a sample of 517,000 hours of talk radio and 18 million tweets produced by elite journalists, politicians and everyday users of the platform. An analysis of the spread of 1,694 news events from 2019 to 2021 reveal that news both breaks faster and decays significantly faster on Twitter versus radio. Furthermore, the discourse surrounding news events is more negative and outraged on Twitter.  The results suggest that as news and journalism straddles old and new mediums, audiences that get receive their news via social media will tend to experience faster attention shifts, less persistent attention to any particular news event before moving on to the next, with the news stories themselves framed in more negative and outrageous terms.",,"['center-for-constructive-communication', 'media-lab-research-theme-decentralized-society']",,404304,,2023-10-20T19:57:16.546Z,['wbrannon@media.mit.edu'],True,the-speed-of-news-in-twitter-versus-radio,,[],The speed of news in Twitter versus radio,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1
False,2023-10-20T16:59:43.458Z,"Making visible dynamics within conversations enables us to see conversation quality, dynamics between participants, the impact of facilitators, and perhaps even quality of the conversation itself. In this project, we leverage natural language processing, machine learning, data visualization, and human-computer interaction techniques to unveil and create metrics for dynamics within conversations in an effort to evaluate quality of conversation. ",,"['center-for-constructive-communication', 'media-lab-research-theme-decentralized-society']",https://dam-prod2.media.mit.edu/x/2023/10/20/Screenshot 2023-09-03 at 2.23.51 PM.png,404312,,2023-10-22T17:47:46.622Z,"['bcroy@media.mit.edu', 'mhughes4@media.mit.edu']",True,conversation-dynamics,2023-06-02,[],Conversation Dynamics,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2023-10-21T14:03:29.065Z,"Recent advances in Brain Computer Interfaces (BCI) show different techniques of image classification and reconstruction from the visually evoked brain activity. The majority of these methods are based on costly and stationary equipment, limiting their real-world use while increasing the time and monetary resources required to enter this field of research. Of the few studies that have tried to accomplish the same task with an electroencephalogram (EEG)-based system, several have been flawed (predicting on temporal dynamics inherent to the EEG hardware by presenting visual stimuli in a block-fashion during data acquisition). Additionally, those paradigms commonly utilized high-density EEG systems, which again restricts their affordability and portability.The goal of our study was to use a low-density, portable EEG to attempt classification and reconstruction of visual stimuli from the evoked brain activity. For that, we first designed an experiment to gather 600 EEG-image pairs belonging to 20 different image classes from human participants. To improve the aforementioned limitations, we recorded the evoked responses with an 8-channel portable EEG and randomly shuffled the stimuli for every experiment run to avoid artifactual prediction. After the data acquisition, we preprocessed the data and continued to build subject-wise models to classify the category of an image from the recorded brain data. We utilized 10 of the 12 recordings for training a model and validated our results on a hold-out validation set, before finally evaluating the best validation model on the hold-out test-recording. Our preliminary results indicate that it is possible to predict the image-class from the preprocessed EEG signal with up to 53% accuracy.In a subsequent step, we use an intermediate representation of the best classification model to condition a latent diffusion model for the visual reconstruction of the seen images.",,"['fluid-interfaces', 'media-lab-research-theme-connected-body-mind']",https://dam-prod2.media.mit.edu/x/2023/10/21/Screenshot 2023-09-10 at 9.21.53 AM.png,404347,,2023-10-23T17:44:19.661Z,"['pattie@media.mit.edu', 'sven92@media.mit.edu', 'nkosmyna@media.mit.edu']",True,image-classification-and-reconstruction-from-low-density-eeg,2023-04-30,[],Image Classification and Reconstruction from Low-Density EEG,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0
False,2023-10-23T01:32:10.662Z,"A Virtual Platform for Bridging the Climate Science Communication GapEarth Mission Control (EMC) - is an immersive visualization platform for multi-modal and multi-user design and data exploration. Designed with the primary goal of enhancing the way scientists and educators communicate the intricate tales of climate impacts, EMC allows users to design comprehensive experiences that incorporate various visualization modalities, from NASA’s Hyperwall animations to NOAA’s Science on a Sphere spherical projections. Whether it's map tables, virtual environments, 360-degree videos, or human-scale immersive experiences, EMC harnesses the unique strengths of each modality, offering users a more complete view into the world of climate data. One of the unique features of EMC is its integration with an AI-powered virtual assistant called EarthBot. This addition ensures that interactions within the VR environment are not just immersive but also intuitive and natural. The assistant enhances the user's sense of presence, making the exploration of climate data stories more engaging. The Earth Mission Control platform is a step towards a future where data visualization moves beyond traditional boundaries. By bridging the gap between complex climate data and immersive storytelling, EMC can help create a more informed and proactive global community.",,['media-lab-research-theme-future-worlds'],https://dam-prod2.media.mit.edu/x/2023/10/22/tooltips.png,404370,,2023-10-23T17:53:28.917Z,"['pcherner@media.mit.edu', 'rachelbc@media.mit.edu', 'minoo@media.mit.edu', 'dnewman@media.mit.edu']",True,earth-mission-control,2022-09-07,"['virtual-reality', 'human-computer-interaction', 'augmented-reality', 'communications', 'environment']",Earth Mission Control (EMC),PUBLIC,https://www.cherner.dev,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0
False,2023-10-23T13:41:08.970Z,"This project collates and explores public data from 200+ facilitated dialogues that occurred through the Fora platform in the last several years. By exploring this data, we learn about healthy conversation, commonalities across facilitation styles, and differences across conversation collections.",,"['center-for-constructive-communication', 'media-lab-research-theme-decentralized-society']",,404380,,2023-10-23T17:55:56.551Z,['hopes@media.mit.edu'],True,fora-data-set,2023-10-23,[],Fora Data Set,PUBLIC,,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0
False,2023-10-23T17:55:57.059Z,"Ophiuchus is a Coarse-Graining SO(3)-Equivariant Autoencoder architecture for representing all-atom protein structures in a scalable way. Our model can sample complete protein sequence and structure in one-shot and at great efficiency, through its deep geometric latent space.",,"['molecular-machines', 'media-lab-research-theme-life-with-ai']",https://dam-prod2.media.mit.edu/x/2023/10/23/Pasted image 20230512133009.png,404396,,2023-10-23T20:45:06.642Z,"['manvitha@media.mit.edu', 'allanc@media.mit.edu']",True,ophiuchus-scalable-modeling-of-protein-structures-through-coarse-graining-so-3-equivariant-autoencoders,2023-10-23,[],Ophiuchus: Deep Neural Protein Modeling from First-Principles of Symmetry,PUBLIC,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
